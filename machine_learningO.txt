    "abstract": "Opioid use disorder (OUD) is an addiction crisis in the United States. As recent as 2019, more than 10 million people have misused or abused prescription opioids, making OUD one of the leading causes of accidental death in the United States. Workforces that are physically demanding and laborious in the transportation, construction and extraction, and health care industries are prime targets for OUD due to high-risk occupational activities. Because of this high prevalence of OUD among working populations in the United States, elevated workers' compensation and health insurance costs, absenteeism, and declined productivity in workplaces have been reported.\nWith the emergence of new smartphone technologies, health interventions can be widely used outside clinical settings via mobile health tools. The major objective of our pilot study was to develop a smartphone app that can track work-related risk factors leading to OUD with a specific focus on high-risk occupational groups. We used synthetic data analyzed by applying a machine learning algorithm to accomplish our objective.\nTo make the OUD assessment process more convenient and to motivate potential patients with OUD, we developed a smartphone-based app through a step-by-step process. First, an extensive literature survey was conducted to list a set of critical risk assessment questions that can capture high-risk behaviors leading to OUD. Next, a review panel short-listed 15 questions after careful evaluation with specific emphasis on physically demanding workforces-9 questions had two, 5 questions had five, and 1 question had three response options. Instead of human participant data, synthetic data were used as user responses. Finally, an artificial intelligence algorithm, naive Bayes, was used to predict the OUD risk, trained with the synthetic data collected.\nThe smartphone app we have developed is functional as tested with synthetic data. Using the naive Bayes algorithm on collected synthetic data, we successfully predicted the risk of OUD. This would eventually create a platform to test the functionality of the app further using human participant data.\nThe use of mobile health techniques, such as our mobile app, is highly promising in predicting and offering mitigation plans for disease detection and prevention. Using a naive Bayes algorithm model along with a representational state transfer (REST) application programming interface and cloud-based data encryption storage, respondents can guarantee their privacy and accuracy in estimating their risk. Our app offers a tailored mitigation strategy for specific workforces (eg, transportation and health care workers) that are most impacted by OUD. Despite the limitations of the study, we have developed a robust methodology and believe that our app has the potential to help reduce the opioid crisis.", 
    "abstract": "The use of machine learning (ML) with metabolomics provides opportunities for the early diagnosis of disease. However, the accuracy of ML and extent of information obtained from metabolomics can be limited owing to challenges associated with interpreting disease prediction models and analyzing many chemical features with abundances that are correlated and \"noisy\". Here, we report an interpretable neural network (NN) framework to accurately predict disease and identify significant biomarkers using whole metabolomics data sets without ", 
    "abstract": "Functionalization of C-H bonds is a key challenge in medicinal chemistry, particularly for fragment-based drug discovery (FBDD) where such transformations require execution in the presence of polar functionality necessary for protein binding. Recent work has shown the effectiveness of Bayesian optimization (BO) for the self-optimization of chemical reactions; however, in all previous cases these algorithmic procedures have started with no prior information about the reaction of interest. In this work, we explore the use of multitask Bayesian optimization (MTBO) in several ", 
    "abstract": "RNA sequencing analysis is an important field in the study of extracellular vesicles (EVs), as these particles contain a variety of RNA species that may have diagnostic, prognostic and predictive value. Many of the bioinformatics tools currently used to analyze EV cargo rely on third-party annotations. Recently, analysis of unannotated expressed RNAs has become of interest, since these may provide complementary information to traditional annotated biomarkers or may help refine biological signatures used in machine learning by including unknown regions. Here we perform a comparative analysis of annotation-free and classical read-summarization tools for the analysis of RNA sequencing data generated for EVs isolated from persons with amyotrophic lateral sclerosis (ALS) and healthy donors. Differential expression analysis and digital-droplet PCR validation of unannotated RNAs also confirmed their existence and demonstrates the usefulness of including such potential biomarkers in transcriptome analysis. We show that find-then-annotate methods perform similarly to standard tools for the analysis of known features, and can also identify unannotated expressed RNAs, two of which were validated as overexpressed in ALS samples. We demonstrate that these tools can therefore be used for a stand-alone analysis or easily integrated into current workflows and may be useful for re-analysis as annotations can be integrated ", 
    "abstract": "Usage of online learning platforms increases day by day and henceforth, there emerges the need for automated grading systems to assess the learner's performance. Evaluating these answers demands for a well-grounded reference answer which aids a strong foundation for better grading. Since reference answers impacts the exactness of grading answers of learners, its correctness remains a great concern. A framework that addresses the reference answer exactness in Automated Short Answer Grading (ASAG) systems was developed. This framework includes material content acquisition, clustering collective content, expert answer as key components which was later fed to a zero-shot classifier for a strong reference answer generation. Then, the computed reference answers along with student answers and questions from Mohler dataset were fed to an ensemble of transformers to produce relevant grades. The aforementioned models' RMSE and correlation values were compared against the past values of the dataset. Based on the observations made, this model outperforms the previous approaches.", 
    "abstract": "Around one-third of epilepsy patients develop drug-resistant seizures; early detection of seizures could help improve safety, reduce patient anxiety, increase independence, and enable acute treatment. In recent years, the use of artificial intelligence techniques and machine learning algorithms in different diseases, including epilepsy, has increased significantly. The main objective of this study is to determine whether the mjn-SERAS artificial intelligence algorithm developed by MJN Neuroserveis, can detect seizures early using patient-specific data to create a personalized mathematical model based on EEG training, defined as the programmed recognition of oncoming seizures before they are primarily initiated, usually within a period of a few minutes, in patients diagnosed of epilepsy. Retrospective, cross-sectional, observational, multicenter study to determine the sensitivity and specificity of the artificial intelligence algorithm. We searched the database of the Epilepsy Units of three Spanish medical centers and selected 50 patients evaluated between January 2017 and February 2021, diagnosed with refractory focal epilepsy who underwent video-EEG monitoring recordings between 3 and 5\u00a0days, a minimum of 3 seizures per patient, lasting more than 5\u00a0s and the interval between each seizure was greater than 1\u00a0h. Exclusion criteria included age <18\u00a0years, intracranial EEG monitoring, and severe psychiatric, neurological, or systemic disorders. The algorithm identified pre-ictal and interictal patterns from EEG data using our learning algorithm and was compared to a senior epileptologist's evaluation as a gold standard. Individual mathematical models of each patient were trained using this feature dataset. A total of 1963\u00a0h of 49 video-EEG recordings were reviewed, with an average of 39.26\u00a0h per patient. The video-EEG monitoring recorded 309 seizures as subsequently analyzed by the epileptologists. The mjn-SERAS algorithm was trained on 119 seizures and split testing was performed on 188 seizures. The statistical analysis includes the data from each model and reports 10 false negatives (no detection of episodes recorded by video-EEG) and 22 false positives (alert detected without clinical correlation or abnormal EEG signal within 30\u00a0min). Specifically, the automated mjn-SERAS AI algorithm achieved a sensitivity of 94.7% (95\u00a0%; CI 94.67-94.73), and an F-Score representing specificity of 92.2% (95\u00a0%; CI 92.17-92.23) compared to the reference performance represented by a mean (harmonic mean or average) and a positive predictive value of 91%, with a false positive rate of 0.55 per 24\u00a0h in the patient-independent model. This patient-specific AI algorithm for early seizure detection shows promising results in terms of sensitivity and false positive rate. Although the algorithm requires high computational requirements on specialized servers cloud for training and computing, its computational load in real-time is low, allowing its implementation on embedded devices for online seizure detection.", 
    "abstract": null, 
    "abstract": "The M50 electrophysiological auditory evoked response time can be measured at the superior temporal gyrus with magnetoencephalography (MEG) and its latency is related to the conduction velocity of auditory input passing from ear to auditory cortex. In children with autism spectrum disorder (ASD) and certain genetic disorders such as XYY syndrome, the auditory M50 latency has been observed to be elongated (slowed).\nThe goal of this study is to use neuroimaging (diffusion MR and GABA MRS) measures to predict auditory conduction velocity in typically developing (TD) children and children with autism ASD and XYY syndrome.\nNon-linear TD support vector regression modeling methods accounted for considerably more M50 latency variance than linear models, likely due to the non-linear dependence on neuroimaging factors such as GABA MRS. While SVR models accounted for ~80% of the M50 latency variance in TD and the genetically homogenous XYY syndrome, a similar approach only accounted for ~20% of the M50 latency variance in ASD, implicating the insufficiency of diffusion MR, GABA MRS, and age factors alone. Biologically based stratification of ASD was performed by assessing the conformance of the ASD population to the TD SVR model and identifying a sub-population of children with unexpectedly long M50 latency.\nMultimodal integration of neuroimaging data can help build a mechanistic understanding of brain connectivity. The unexplained M50 latency variance in ASD motivates future hypothesis generation and testing of other contributing biological factors.", 
    "abstract": "Urban environments continuously generate larger and larger volumes of data, whose analysis can provide descriptive and predictive models as valuable support to inspire and develop data-driven Smart City applications. To this aim, Big data analysis and machine learning algorithms can play a fundamental role to bring improvements in city policies and urban issues. This paper introduces how Big Data analysis can be exploited to design and develop data-driven smart city services, and provides an overview on the most important Smart City applications, grouped in several categories. Then, it presents three real-case studies showing how data analysis methodologies can provide innovative solutions to deal with smart city issues. The first one is an approach for spatio-temporal crime forecasting (tested on Chicago crime data), the second one is methodology to discover mobility hotsposts and trajectory patterns from GPS data (tested on Beijing taxi traces), the third one is an approach to discover predictive epidemic patterns from mobility and infection data (tested on real COVID-19 data). The presented real-world cases prove that data analytics models can effectively support city managers in tackling smart city challenges and improving urban applications.", 
    "abstract": "Most predictive biomarkers approved for clinical use measure single analytes such as genetic alteration or protein overexpression. We developed and validated a novel biomarker with the aim of achieving broad clinical utility. The Xerna\u2122 TME Panel is a pan-tumor, RNA expression-based classifier, designed to predict response to multiple tumor microenvironment (TME)-targeted therapies, including immunotherapies and anti-angiogenic agents.\nThe Panel algorithm is an artificial neural network (ANN) trained with an input signature of 124 genes that was optimized across various solid tumors. From the 298-patient training data, the model learned to discriminate four TME subtypes: Angiogenic (A), Immune Active (IA), Immune Desert (ID), and Immune Suppressed (IS). The final classifier was evaluated in four independent clinical cohorts to test whether TME subtype could predict response to anti-angiogenic agents and immunotherapies across gastric, ovarian, and melanoma datasets.\nThe TME subtypes represent stromal phenotypes defined by angiogenesis and immune biological axes. The model yields clear boundaries between biomarker-positive and -negative and showed 1.6-to-7-fold enrichment of clinical benefit for multiple therapeutic hypotheses. The Panel performed better across all criteria compared to a null model for gastric and ovarian anti-angiogenic datasets. It also outperformed PD-L1 combined positive score (>1) in accuracy, specificity, and positive predictive value (PPV), and microsatellite-instability high (MSI-H) in sensitivity and negative predictive value (NPV) for the gastric immunotherapy cohort.\nThe TME Panel's strong performance on diverse datasets suggests it may be amenable for use as a clinical diagnostic for varied cancer types and therapeutic modalities.", 
    "abstract": "The activation of YAP/TAZ transcriptional co-activators, downstream effectors of the Hippo/YAP pathway, is commonly observed in human cancers, promoting tumor growth and invasion. The aim of this study was to use machine learning models and molecular map based on the Hippo/YAP pathway to explore the prognosis, immune microenvironment and therapeutic regimen of patients with lower grade glioma (LGG).\nSW1783 and SW1088 cell lines were used as \nThe findings showed that XMU-MP-1 significantly enhanced the proliferation of LGG cells. Different Hippo/YAP Pathway activation profiles were associated with different prognostic and clinical features. The immune scores of subtype B were dominated by MDSC and Treg cells, which are known to have immunosuppressive effects. Gene Set Variation Analysis (GSVA) indicated that subtypes B with a poor prognosis exhibited decreased propanoate metabolic activity and suppressed Hippo pathway signaling. Subtype B had the lowest IC50 value, indicating sensitivity to drugs that target the Hippo/YAP pathway. Finally, the random forest tree model predicted the Hippo/YAP pathway status in patients with different survival risk profiles.\nThis study demonstrates the significance of the Hippo/YAP pathway in predicting the prognosis of patients with LGG. The different Hippo/YAP Pathway activation profiles associated with different prognostic and clinical features suggest the potential for personalized treatments.", 
    "abstract": "Unnecessary surgery can be avoided, and more appropriate treatment plans can be developed for patients if the efficacy of neoadjuvant immunochemotherapy for esophageal cancer (EC) can be predicted before surgery. The purpose of this study was to evaluate the ability of machine learning models based on delta features of immunochemotherapy CT images to predict the efficacy of neoadjuvant immunochemotherapy in patients with esophageal squamous cell carcinoma (ESCC) compared with machine learning models based solely on postimmunochemotherapy CT images.\nA total of 95 patients were enrolled in our study and randomly divided into a training group (n = 66) and test group (n = 29). We extracted preimmunochemotherapy radiomics features from preimmunochemotherapy enhanced CT images in the preimmunochemotherapy group (pregroup) and postimmunochemotherapy radiomics features from postimmunochemotherapy enhanced CT images in the postimmunochemotherapy group (postgroup). We then subtracted the preimmunochemotherapy features from the postimmunochemotherapy features and obtained a series of new radiomics features that were included in the delta group. The reduction and screening of radiomics features were carried out by using the Mann-Whitney U test and LASSO regression. Five pairwise machine learning models were established, the performance of which was evaluated by receiver operating characteristic (ROC) curve and decision curve analyses.\nThe radiomics signature of the postgroup was composed of 6 radiomics features; that of the delta-group was composed of 8 radiomics features. The area under the ROC curve (AUC) of the machine learning model with the best efficacy was 0.824 (0.706-0.917) in the postgroup and 0.848 (0.765-0.917) in the delta group. The decision curve showed that our machine learning models had good predictive performance. The delta group performed better than the postgroup for each corresponding machine learning model.\nWe established machine learning models that have good predictive efficacy and can provide certain reference values for clinical treatment decision-making. Our machine learning models based on delta imaging features performed better than those based on single time-stage postimmunochemotherapy imaging features.", 
    "abstract": "Artificial intelligence (AI), particularly deep learning (DL) algorithms, has demonstrated remarkable progress in image-recognition tasks, enabling the automatic quantitative assessment of complex medical images with increased accuracy and efficiency. AI is widely used and is becoming increasingly popular in the field of ultrasound. The rising incidence of thyroid cancer and the workload of physicians have driven the need to utilize AI to efficiently process thyroid ultrasound images. Therefore, leveraging AI in thyroid cancer ultrasound screening and diagnosis cannot only help radiologists achieve more accurate and efficient imaging diagnosis but also reduce their workload. In this paper, we aim to present a comprehensive overview of the technical knowledge of AI with a focus on traditional machine learning (ML) algorithms and DL algorithms. We will also discuss their clinical applications in the ultrasound imaging of thyroid diseases, particularly in differentiating between benign and malignant nodules and predicting cervical lymph node metastasis in thyroid cancer. Finally, we will conclude that AI technology holds great promise for improving the accuracy of thyroid disease ultrasound diagnosis and discuss the potential prospects of AI in this field.", 
    "abstract": "This study aimed to develop a clinical-radiomic model based on radiomic features extracted from digital breast tomosynthesis (DBT) images and clinical factors that may help to discriminate between benign and malignant breast lesions.\nA total of 150 patients were included in this study. DBT images acquired in the setting of a screening protocol were used. Lesions were delineated by two expert radiologists. Malignity was always confirmed by histopathological data. The data were randomly divided into training and validation set with an 80:20 ratio. A total of 58 radiomic features were extracted from each lesion using the LIFEx Software. Three different key methods of feature selection were implemented in Python: (1) K best (KB), (2) sequential (S), and (3) Random Forrest (RF). A model was therefore produced for each subset of seven variables using a machine-learning algorithm, which exploits the RF classification based on the Gini index.\nAll three clinical-radiomic models show significant differences (p < 0.05) between malignant and benign tumors. The area under the curve (AUC) values of the models obtained with three different feature selection methods were 0.72 [0.64,0.80], 0.72 [0.64,0.80] and 0.74 [0.66,0.82] for KB, SFS, and RF, respectively.\nThe clinical-radiomic models developed by using radiomic features from DBT images showed a good discriminating power and hence may help radiologists in breast cancer tumor diagnoses already at the first screening.", 
    "abstract": "Current use of liquid biopsy is based on cell-free DNA (cfDNA) and the evaluation of mutations or methylation pattern. However, expressed RNA can capture mutations, changes in expression levels due to methylation, and provide information on cell of origin, growth, and proliferation status. We developed an approach to isolate cell-free total nucleic acid (cfDNA) and used targeted next generation sequencing to sequence cell-free RNA (cfRNA) and cfDNA as new approach in liquid biopsy. We demonstrate that cfRNA is overall more sensitive than cfDNA in detecting mutations. We show that cfRNA is reliable in detecting fusion genes and cfDNA is reliable in detecting chromosomal gains and losses. cfRNA levels of various solid tumor biomarkers were significantly higher (P\u00a0<\u00a00.0001) in samples from solid tumors as compared with normal control. Similarly, cfRNA lymphoid markers and cfRNA myeloid markers were all higher in lymphoid and myeloid neoplasms, respectively as compared with control (P\u00a0<\u00a00.0001). Using machine learning we demonstrate cfRNA was highly predictive of diagnosis (AUC >0.98) of solid tumors, B-cell lymphoid neoplasms, T-cell lymphoid neoplasms, and myeloid neoplasms. In evaluating the host immune system, cfRNA CD4:CD8B and CD3D:CD19 ratios in normal controls were as expected (median: 5.92 and 6.87, respectively) and were significantly lower in solid tumors (P\u00a0<\u00a00.0002). This data suggests that liquid biopsy combining analysis of cfRNA with cfDNA is practical and may provide helpful information in predicting genomic abnormalities, diagnosis of neoplasms and evaluating both the tumor biology and the host response.", 
    "abstract": "Knowledge of the stage-discharge rating curve is useful in designing and planning flood warnings; thus, developing a reliable stage-discharge rating curve is a fundamental and crucial component of water resource system engineering. Since the continuous measurement is often impossible, the stage-discharge relationship is generally used in natural streams to estimate discharge. This paper aims to optimize the rating curve using a generalized reduced gradient (GRG) solver and the test the accuracy and applicability of the hybridized linear regression (LR) with other machine learning techniques, namely, linear regression-random subspace (LR-RSS), linear regression-reduced error pruning tree (LR-REPTree), linear regression-support vector machine (LR-SVM) and linear regression-M5 pruned (LR-M5P) models. An application of these hybrid models was performed and test to modeling the Gaula Barrage stage-discharge problem. For this, 12-year historical stage-discharge data were collected and analyzed. The 12-year historical daily flow data (m", 
    "abstract": "Forecasting is an attractive topic in every field of study because no one knows the exact nature of the underlying phenomena, but it can be guessed using mathematical functions. As the world progresses towards technology and betterment, algorithms are updated to understand the nature of ongoing phenomena. Machine learning (ML) algorithms are an updated phenomenon used in every task aspect. Real exchange rate data is assumed to be one of the significant components of the business market, which plays a pivotal role in learning market trends. In this work, machine learning models, i.e., the Multi-layer perceptron model (MLP), Extreme learning machine (ELM) model and classical time series models are used, Autoregressive integrated moving average (ARIMA) and Exponential Smoothing (ES) model to model and predict the real exchange rate data set (REER). The data under consideration is from January 2019 to June 2022 and comprises 864 observations. This study split the data set into training and testing and applied all stated models. This study selects a model that meets the Key Performance Indicators (KPI) criteria. This model was selected as the best candidate model to predict the behaviour of the real exchange rate data set.", 
    "abstract": "Radiation-induced toxicities are common adverse events in lung cancer (LC) patients undergoing radiotherapy (RT). An accurate prediction of these adverse events might facilitate an informed and shared decision-making process between patient and radiation oncologist with a clearer view of life-balance implications in treatment choices. This work provides a benchmark of machine learning (ML) approaches to predict radiation-induced toxicities in LC patients built upon a real-world health dataset based on a generalizable methodology for their implementation and external validation.\nTen feature selection (FS) methods were combined with five ML-based classifiers to predict six RT-induced toxicities (acute esophagitis, acute cough, acute dyspnea, acute pneumonitis, chronic dyspnea, and chronic pneumonitis). A real-world health dataset (RWHD) built from 875 consecutive LC patients was used to train and validate the resulting 300 predictive models. Internal and external accuracy was calculated in terms of AUC per clinical endpoint, FS method, and ML-based classifier under analysis.\nBest performing predictive models obtained per clinical endpoint achieved comparable performances to methods from state-of-the-art at internal validation (AUC\u00a0\u2265\u00a00.81 in all cases) and at external validation (AUC\u00a0\u2265\u00a00.73 in 5 out of 6 cases).\nA benchmark of 300 different ML-based approaches has been tested against a RWHD achieving satisfactory results following a generalizable methodology. The outcomes suggest potential relationships between underrecognized clinical factors and the onset of acute esophagitis or chronic dyspnea, thus demonstrating the potential that ML-based approaches have to generate novel data-driven hypotheses in the field.", 
    "abstract": "A key unmet need in the management of hemophilia A (HA) is the lack of clinically validated markers that are associated with the development of neutralizing antibodies to Factor VIII (FVIII) (commonly referred to as inhibitors). This study aimed to identify relevant biomarkers for FVIII inhibition using Machine Learning (ML) and Explainable AI (XAI) using the My Life Our Future (MLOF) research repository. The dataset includes biologically relevant variables such as age, race, sex, ethnicity, and the variants in the ", 
    "abstract": "The objective of the research is to investigate flood susceptibility in the Sylhet division of Bangladesh. Eight influential factors (i.e., elevation, slope, aspect, curvature, TWI, SPI, roughness, and LULC) were applied as inputs to the model. In this work, 1280 samples were taken at different locations based on flood and non-flood characteristics; of these, 75% of the inventory dataset was used for training and 25% for testing. An artificial neural network was applied to develop a flood susceptibility model, and the results were plotted on a map using ArcGIS. According to the finding, 40.98% (i.e., 499433.50 hectors) of the study area is found within the very high-susceptibility zone, and 37.43% (i.e., 456168.76 hectors) are in the highly susceptible zone. Only 6.52% and 15% of the area were found in low and medium flood susceptibility zones, respectively. The results of model validation show that the overall prediction rate is around 89% and the overall model success rate is around 98%. The study's findings assist policymakers and concerned authorities in making flood risk management decisions in order to mitigate the negative impacts.", 
    "abstract": "[This corrects the article DOI: 10.2147/NDT.S404528.].", 
    "abstract": "Circadian rhythm disruption (CRD) represents a critical contributor to the pathogenesis of Alzheimer's disease (AD). Nonetheless, how CRD functions within the AD immune microenvironment remains to be illustrated.\nCircadian rhythm score (CRscore) was utilized to quantify the microenvironment status of circadian disruption in a single-cell RNA sequencing dataset derived from AD. Bulk transcriptome datasets from public repository were employed to validate the effectiveness and robustness of CRscore. A machine learning-based integrative model was applied for constructing a characteristic CRD signature, and RT-PCR analysis was employed to validate their expression levels.\nWe depicted the heterogeneity in B cells, CD4\nOur study revealed CRD-based cell subtypes in the AD microenvironment at single-cell level and proposed a robust and promising CRD signature for AD diagnosis. A deeper knowledge of these mechanisms may provide novel possibilities for incorporating \"circadian rhythm-based anti-dementia therapies\" into the treatment protocols of individualized medicine.", 
    "abstract": "Global Navigation Satellite System (GNSS) multipath has always been extensively researched as it is one of the hardest error sources to predict and model. External sensors are often used to remove or detect it, which transforms the process into a cumbersome data set-up. Thus, we decided to only use GNSS correlator outputs to detect a large-amplitude multipath, on Galileo E1-B and GPS L1 C/A, using a convolutional neural network (CNN). This network was trained using 101 correlator outputs being used as a theoretical classifier. To take advantage of the strengths of convolutional neural networks for image detection, images representing the correlator output values as a function of delay and time were generated. The presented model has an F score of 94.7% on Galileo E1-B and 91.6% on GPS L1 C/A. To reduce the computational load, the number of correlator outputs and correlator sampling frequency was then decreased by a factor of 4, and the convolutional neural network still has an F score of 91.8% on Galileo E1-B and 90.5% on GPS L1 C/A.", 
    "abstract": "Mass Spectrometry Imaging (MSI), using traditional rectilinear scanning, takes hours to days for high spatial resolution acquisitions. Given that most pixels within a sample's field of view are often neither relevant to underlying biological structures nor chemically informative, MSI presents as a prime candidate for integration with sparse and dynamic sampling algorithms. During a scan, stochastic models determine which locations probabilistically contain information critical to the generation of low-error reconstructions. Decreasing the number of required physical measurements thereby minimizes overall acquisition times. A Deep Learning Approach for Dynamic Sampling (DLADS), utilizing a Convolutional Neural Network (CNN) and encapsulating molecular mass intensity distributions within a third dimension, demonstrates a simulated 70% throughput improvement for Nanospray Desorption Electrospray Ionization (nano-DESI) MSI tissues. Evaluations are conducted between DLADS, a Supervised Learning Approach for Dynamic Sampling, with Least-Squares regression (SLADS-LS), and a Multi-Layer Perceptron (MLP) network (SLADS-Net). When compared with SLADS-LS, limited to a single ", 
    "abstract": "Drug design based on kinetic properties is growing in application. Here, we applied retrosynthesis-based pre-trained molecular representation (RPM) in machine learning (ML) to train 501 inhibitors of 55 proteins and successfully predicted the dissociation rate constant (", 
    "abstract": "", 
    "abstract": "The COVID-19 pandemic promoted strict restrictions to human activities in China, which led to an unexpected increase in ozone (O", 
    "abstract": "", 
    "abstract": "The main focus of this study was to compare the predictive value of coagulation, fibrinolysis, thromboelastography, stress response, and immune function in predicting the incidence of deep venous thrombosis (DVT) in lung cancer (LC) patients undergoing thoracoscopic LC resection vs thoracotomy LC resection. To do that, a prospective, single-center, case-control study involving 460 LC patients was conducted. The risk indicators affecting patients with DVT after LC resection in the testing cohort were determined using logistic regression and receiver operator characteristic (ROC) analyses. One validation cohort was used to assess the risk prediction models. DVT incidence was higher in the thoracoscopic group (18.7%) than in the thoracotomy group (11.2%) in the testing cohort (", 
    "abstract": "This study aims to evaluate deep learning (DL) performance in differentiating low- and high-grade glioma. Search online database for studies continuously published from 1st January 2015 until 16th August 2022. The random-effects model was used for synthesis, based on pooled sensitivity (SE), specificity (SP), and area under the curve (AUC). Heterogeneity was estimated using the Higgins inconsistency index (I", 
    "abstract": "Dynamic thermal emitters have attracted considerable attention due to their potential in widespread applications such as radiative cooling, thermal switching, and adaptive camouflage. However, the state-of-art performances of dynamic emitters are still far below expectations. Here, customized to the special and stringent requirement of dynamic emitters, a neural network model is developed to effectively bridge the structural and spectral spaces and further realizes the inverse design with coupling to genetic algorithms, which considers the broadband spectral responses in different phase-states and utilizes comprehensive measures to ensure the modeling accuracy and computational speed. Besides achieving an outstanding emittance tunability of 0.8, the physics and empirical rules have also been mined qualitatively through decision trees and gradient analyses. The study demonstrates the feasibility of using machine learning to obtain the near-perfect performance of dynamic emitters, as well as guiding the design of other thermal and photonic nanostructures with multifunctions.", 
    "abstract": "Gaucher disease (GD) is a genetic lysosomal disorder characterized by high bone marrow (BM) involvement and skeletal complications. The pathophysiology of these complications is not fully elucidated. Magnetic resonance imaging (MRI) is the gold standard to evaluate BM. This study aimed to apply machine-learning techniques in a cohort of Spanish GD patients by a structured bone marrow MRI reporting model at diagnosis and follow-up to predict the evolution of the bone disease. In total, 441 digitalized MRI studies from 131 patients (M: 69, F:62) were reevaluated by a blinded expert radiologist who applied a structured report template. The studies were classified into categories carried out at different stages as follows: A: baseline; B: between 1 and 4 y of follow-up; C: between 5 and 9 y; and D: after 10 years of follow-up. Demographics, genetics, biomarkers, clinical data, and cumulative years of therapy were included in the model. At the baseline study, the mean age was 37.3 years (1-80), and the median Spanish MRI score (S-MRI) was 8.40 (male patients: 9.10 vs. female patients: 7.71) (", 
    "abstract": "Early diagnosis of Parkinson's disease (PD) is important to identify treatments to slow neurodegeneration. People who develop PD often have symptoms before the disease manifests and may be coded as diagnoses in the electronic health record (EHR).\nTo predict PD diagnosis, we embedded EHR data of patients onto a biomedical knowledge graph called Scalable Precision medicine Open Knowledge Engine (SPOKE) and created patient embedding vectors. We trained and validated a classifier using these vectors from 3,004 PD patients, restricting records to 1, 3, and 5 years before diagnosis, and 457,197 non-PD group.\nThe classifier predicted PD diagnosis with moderate accuracy (AUC\u2009=\u20090.77\u2009\u00b1\u20090.06, 0.74\u2009\u00b1\u20090.05, 0.72\u2009\u00b1\u20090.05 at 1, 3, and 5 years) and performed better than other benchmark methods. Nodes in the SPOKE graph, among cases, revealed novel associations, while SPOKE patient vectors revealed the basis for individual risk classification.\nThe proposed method was able to explain the clinical predictions using the knowledge graph, thereby making the predictions clinically interpretable. Through enriching EHR data with biomedical associations, SPOKE may be a cost-efficient and personalized way to predict PD diagnosis years before its occurrence.", 
    "abstract": "Electron ionization (EI) mass spectrum library searching is usually performed to identify a compound in gas chromatography/mass spectrometry. However, compounds whose EI mass spectra are registered in the library are still limited compared to the popular compound databases. This means that there are compounds that cannot be identified by conventional library searching but also may result in false positives. In this report, we report on the development of a machine learning model, which was trained using chemical formulae and EI mass spectra, that can predict the EI mass spectrum from the chemical structure. It allowed us to create a predicted EI mass spectrum database with predicted EI mass spectra for 100 million compounds in PubChem. We also propose a method for improving library searching time and accuracy that includes an extensive mass spectrum library.", 
    "abstract": "The COVID-19 pandemic has resulted in a surge of fake news, creating public health risks. However, developing an effective way to detect such news is challenging, especially when published news involves mixing true and false information. Detecting COVID-19 fake news has become a critical task in the field of natural language processing (NLP). This paper explores the effectiveness of several machine learning algorithms and fine-tuning pre-trained transformer-based models, including Bidirectional Encoder Representations from Transformers (BERT) and COVID-Twitter-BERT (CT-BERT), for COVID-19 fake news detection. We evaluate the performance of different downstream neural network structures, such as CNN and BiGRU layers, added on top of BERT and CT-BERT with frozen or unfrozen parameters. Our experiments on a real-world COVID-19 fake news dataset demonstrate that incorporating BiGRU on top of the CT-BERT model achieves outstanding performance, with a state-of-the-art F1 score of 98%. These results have significant implications for mitigating the spread of COVID-19 misinformation and highlight the potential of advanced machine learning models for fake news detection.", 
    "abstract": "We introduce a user-friendly tool for risk gene, cell type, and drug prioritization for complex traits: GCDPipe. It uses gene-level GWAS-derived data and gene expression data to train a model for the identification of disease risk genes and relevant cell types. Gene prioritization information is then coupled with known drug target data to search for applicable drug agents based on their estimated functional effects on the identified risk genes. We illustrate the utility of our approach in different settings: identification of the cell types, implicated in disease pathogenesis, was tested in inflammatory bowel disease (IBD) and Alzheimer disease (AD); gene target and drug prioritization was tested in IBD and schizophrenia. The analysis of phenotypes with known disease-affected cell types and/or existing drug candidates shows that GCDPipe is an effective tool to unify genetic risk factors with cellular context and known drug targets. Next, analysis of the AD data with GCDPipe suggested that gene targets of diuretics, as an Anatomical Therapeutic Chemical drug subgroup, are significantly enriched among the genes prioritized by GCDPipe, indicating their possible effect on the course of the disease.", 
    "abstract": "Aging disrupts circadian clocks, as evidenced by a reduction in the amplitude of circadian rhythms. Because the circadian clock strongly influences sleep-wake behavior in mammals, age-related alterations in sleep-wake patterns may be attributable, at least partly, to functional changes in the circadian clock. However, the effect of aging on the circadian characteristics of sleep architecture has not been well assessed, as circadian behaviors are usually evaluated through long-term behavioral recording with wheel-running or infrared sensors. In this study, we examined age-related changes in circadian sleep-wake behavior using circadian components extracted from electroencephalography (EEG) and electromyography (EMG) data. EEG and EMG were recorded from 12 to 17-week-old and 78 to 83-week-old mice for 3\u2009days under light/dark and constant dark conditions. We analyzed time-dependent changes in the duration of sleep. Rapid eye movement (REM) and non-REM (NREM) sleep significantly increased during the night phase in old mice, whereas no significant change was observed during the light phase. The circadian components were then extracted from the EEG data for each sleep-wake stage, revealing that the circadian rhythm in the power of delta waves during NREM sleep was attenuated and delayed in old mice. Furthermore, we used machine learning to evaluate the phase of the circadian rhythm, with EEG data serving as the input and the phase of the sleep-wake rhythm (environmental time) as the output. The results indicated that the output time for the old mice data tended to be delayed, specifically at night. These results indicate that the aging process significantly impacts the circadian rhythm in the EEG power spectrum despite the circadian rhythm in the amounts of sleep and wake attenuated but still remaining in old mice. Moreover, EEG/EMG analysis is useful not only for evaluating sleep-wake stages but also for circadian rhythms in the brain.", 
    "abstract": "We propose that in order to harness our understanding of neuroscience toward machine learning, we must first have powerful tools for training brain-like models of learning. Although substantial progress has been made toward understanding the dynamics of learning in the brain, ", 
    "abstract": "", 
    "abstract": "The application of machine learning (ML) techniques in the medical field has demonstrated both successes and challenges in the precision medicine era. The ability to accurately classify a subject as a potential responder versus a nonresponder to a given therapy is still an active area of research pushing the field to create new approaches for applying machine-learning techniques. In this study, we leveraged publicly available data through the BeatAML initiative. Specifically, we used gene count data, generated via RNA-seq, from 451 individuals matched with \nOur work demonstrated that feature selection technique, rather than the classifier, had the greatest impact on model performance. The SHAP technique outperformed the other feature selection techniques and was able to with high accuracy predict outcome response, with the highest performing model: Foretinib with 89% AUC using the SHAP technique and RF classifier. Our ML pipelines demonstrate that at the time of diagnosis, a transcriptomics signature exists that can potentially predict response to treatment, demonstrating the potential of using ML applications in precision medicine efforts.\nhttps://github.com/UD-CRPL/RCDML.\nSupplementary data are available at ", 
    "abstract": "Promoters are the basic functional cis-elements to which RNA polymerase binds to initiate the process of gene transcription. Comprehensive understanding gene expression and regulation depends on the precise identification of promoters, as they are the most important component of gene expression. This study aimed to develop a machine learning-based model to predict promoters in ", 
    "abstract": "Antimicrobial resistance (AMR) is arguably one of the major health and economic challenges in our society. A key aspect of tackling AMR is rapid and accurate detection of the emergence and spread of AMR in food animal production, which requires routine AMR surveillance. However, AMR detection can be expensive and time-consuming considering the growth rate of the bacteria and the most commonly used analytical procedures, such as Minimum Inhibitory Concentration (MIC) testing. To mitigate this issue, we utilized machine learning to predict the future AMR burden of bacterial pathogens. We collected pathogen and antimicrobial data from >600 farms in the United States from 2010 to 2021 to generate AMR time series data. Our prediction focused on five bacterial pathogens (", 
    "abstract": "The objective of this study was to develop, using a genome wide machine learning approach, an unambiguous model to predict the presence of highly pathogenic STEC in \nThe Machine Learning (ML) approach was used in this study on a large curated dataset composed of 1,493 \nIt is remarkable that only using these six genes, EHEC can be clearly identified from ", 
    "abstract": "To identify differences between the composition, abundance, and biological function of the intestinal microbiome of patients with and without lymph-vascular invasion (LVI) colorectal cancer (CRC) and to construct predictive labels to support accurate assessment of LVI in CRC.\n134 CRC patients were included, which were divided into two groups according to the presence or absence of LVI, and their intestinal microbiomes were sequenced by 16SrRNA and analyzed for differences. The transcriptome sequencing data of 9 CRC patients were transformed into immune cells abundance matrix by CIBERSORT algorithm, and the correlation among LVI-associated differential intestinal microbiomes, immune cells, immune-related genes and LVI-associated differential GO items and KEGG pathways were analyzed. A random forest (RF) and eXtreme Gradient Boosting (XGB) model were constructed to predict the LVI of CRC patients based on the differential microbiome.\nThere was no significant difference in \u03b1-diversity and \u03b2-diversity of intestinal microbiome between CRC patients with and without LVI (P > 0.05). Linear discriminant analysis Effect Size (LEfSe) analysis showed 34 intestinal microbiomes enriched in CRC patients of the LVI group and 5 intestinal microbiomes were significantly enriched in CRC patients of the non-lymph-vascular invasion (NLVI) group. The RF and XGB prediction models constructed with the top 15% of the LVI-associated differential intestinal microbiomes ranked by feature significance had good efficacy.\nThere are 39 intestinal flora with significantly different species abundance between the LVI and NLVI groups. ", 
    "abstract": "Artificial intelligence and machine learning applications are becoming increasingly popular in health care and medical devices. The development of accurate machine learning algorithms requires large quantities of good and diverse data. This poses a challenge in health care because of the sensitive nature of sharing patient data. Decentralized algorithms through federated learning avoid data aggregation. In this paper we give an overview of federated learning, current examples in healthcare and ophthalmology, challenges, and next steps.", 
    "abstract": "We reviewed foundational concepts in artificial intelligence\u00a0(AI) and machine learning (ML) and discussed ways in which these methodologies may be employed to enhance progress in clinical trials and research, with particular attention to applications in the\u00a0design, conduct, and interpretation of clinical trials for neurologic diseases. We discussed ways in which ML may help to accelerate the pace of subject recruitment,\u00a0provide realistic simulation of medical interventions, and enhance remote trial administration\u00a0via novel digital biomarkers and therapeutics. Lastly, we provide a brief\u00a0overview of the technical, administrative, and regulatory challenges that must be addressed as ML achieves greater integration into clinical trial workflows.", 
    "abstract": "In the context of Industry 4.0, hydrogen gas is becoming more significant to energy feedstocks in the world. The current work researches a novel artificial smart model for characterising hydrogen gas production (HGP) from biomass composition and the pyrolysis process based on an intriguing approach that uses support vector machines (SVMs) in conjunction with the artificial bee colony (ABC) optimiser. The main results are the significance of each physico-chemical parameter on the hydrogen gas production through innovative modelling and the foretelling of the HGP. Additionally, when this novel technique was employed on the observed dataset, a coefficient of determination and correlation coefficient equal to 0.9464 and 0.9751 were reached for the HGP estimate, respectively. The correspondence between observed data and the ABC/SVM-relied approximation showed the suitable effectiveness of this procedure.", 
    "abstract": "The current study involving 318 essential thrombocythemia (ET) patients with prior thrombosis was designed to identify risk factors that were predictive of recurrent thrombosis. The whole cohort was randomly split into derivation and validation cohorts. The random forest method, support vector machine with built-in recursive feature elimination model, and logistic multivariable analysis were performed in the derivation cohort, and cardiovascular risk factor (CVF) and RBC distribution width with standard deviation (RDW-SD) were finally selected as independent predictors. Subsequently we devise a 3-tiered model (low risk: 0 points; intermediate risk: 1-1.5 points; and high risk: 2.5 points) and it showed good discrimination in all cohorts. Moreover, the model was significantly correlated with rethrombosis-free survival (rTFS) (p\u2009=\u20090.0007 in the derivation cohort; p\u2009=\u20090.0019 in the validation cohort). In the whole cohort, cytoreductive therapy was more effective than antiplatelet agents alone for 10-year rTFS (p\u2009=\u20090.0336). No significant difference in 10-year rTFS was observed among interferon (IFN), hydroxyurea (HU), and IFN\u2009+\u2009HU therapy (p\u2009=\u20090.444). The present study helps identify individuals who need close monitoring and provides valuable risk signals for recurrence in ET patients with prior thrombosis.", 
    "abstract": "Reliable disease management can guarantee healthy plant production and relies on the knowledge of pathogen prevalence. Modeling the dynamic changes in spore concentration is available for realizing this purpose. We present a novel model based on a time-series modeling machine learning method, i.e., a long short-term memory (LSTM) network, to analyze oomycete Plasmopara viticola sporangia concentration dynamics using data from a 4-year field experiment trial in North China. Principal component analysis (PCA)-based high-quality input screening and simulation result calibration were performed to ensure model performance, obtaining a high determination coefficient (0.99), a low root mean square error (0.87), and a low mean bias error (0.55), high sensitivity (91.5%), and high specificity (96.5%). The impact of the variability of relative factors on daily P. viticola sporangia concentrations was analyzed, confirming that a low daily mean air temperature restricts pathogen development even during a long period of high humidity in the field.", 
    "abstract": "The lockdowns and curfews during the COVID-19 pandemic have halted economic and transportation activities across the world. This study aims to investigate air pollution levels in the Marmara region, particularly in Istanbul, before and during the COVID-19 pandemic. The study used real data provided by the General Directorate of Meteorology and applied three machine learning algorithms (ANN, RBFreg, and SMOreg) to analyze air pollution data. In addition, a one-sample t-test was performed to compare air pollution levels before and during the COVID-19 pandemic in the Marmara region and Istanbul. The results of the study showed a significant reduction in the particulate matter (PM) value, which indicates the degree of air pollution, in both the Marmara region and Istanbul during the COVID-19 pandemic. The one-sample t-test results showed that the reduction in air pollution levels was statistically significant in both areas (t\u2009=\u200911.45, p\u2009<\u2009.001 for the Marmara region, and t\u2009=\u20093.188, p\u2009<\u2009.001 for Istanbul). These findings have important practical implications for decision-makers planning for a more sustainable environment. Overall, the study provides valuable insights into the impact of the COVID-19 pandemic on air pollution levels in the Marmara region, particularly in Istanbul. The application of machine learning algorithms and statistical analysis provides a rigorous approach to the investigation of this important issue by comparing before and during the COVID-19 outbreak.", 
    "abstract": "Polarizable force fields are pervasive in the fields of computational chemistry and biochemistry; however, their empirical or semiempirical nature gives them both weaknesses and strengths. Here, we have developed a hybrid water potential, named q-AQUA-pol, by combining our recent ", 
    "abstract": "Pregnancy complicated by type 2 diabetes is rising, while data on type 2 diabetes first diagnosed in pregnancy (overt diabetes) are scarce. We aimed to describe the frequency and characteristics of pregnant women with overt diabetes, compare them to those with known pregestational diabetes, and evaluate the potential predictors for the diagnosis of overt diabetes.\nA retrospective cohort study including all pregnant women with type 2 diabetes evaluated in two public hospitals in Porto Alegre, Brazil, from May 20, 2005, to June 30, 2021. Classic and obstetric factors associated with type 2 diabetes risk were compared between the two groups, using machine learning techniques and multivariable analysis with Poisson regression.\nOvert diabetes occurred in 33% (95% confidence interval: 29%-37%) of 646 women. Characteristics of women with known or unknown type 2 diabetes were similar; excessive weight was the most common risk factor, affecting ~90% of women. Age >30 years and positive family history of diabetes were inversely related to a diagnosis of overt diabetes, while previous delivery of a macrosomic baby behaved as a risk factor in younger multiparous women; previous gestational diabetes and chronic hypertension were not relevant risk factors.\nCharacteristics of women with overt diabetes are similar to those of women with pregestational diabetes. Classic risk factors for diabetes not included in current questionnaires can help identify women at risk of type 2 diabetes before they become pregnant.", 
    "abstract": "Retrospective cohort.\nDesign a risk-stratified benchmarking tool for adolescent idiopathic scoliosis (AIS) surgeries.\nMachine learning (ML) is an emerging method for prediction modeling in orthopedic surgery. Benchmarking is an established method of process-improvement and is an area of opportunity for ML methods. Current surgical benchmark tools often use ranks and no \"gold standards\"for comparisons exist.\nData from 6076 AIS surgeries were collected from a multicenter registry and divided into three datasets: encompassing surgeries performed (1) during the entire registry, (2) the past 10 years, and (3) during the last 5 years of the registry. We trained three ML regression models (baseline linear regression, gradient boosting [GB], and XGBoost [XGB]) on each data subset to predict each of the five outcome variables, length of stay (LOS), estimated blood loss (EBL), operative time, SRS-Pain and -Self-image. Performance was categorized as \"below expected\" if performing worse than one standard deviation of the mean, \"as expected\" if within one standard deviation, and \"better than expected\" if better than one standard deviation of the mean.\nEnsemble ML methods classified performance better than traditional regression techniques for LOS, EBL, and operative time. The best performing models for predicting LOS and EBL were trained on data collected in the last 5 years, while operative time used the entire 10-year dataset. No models were able to predict SRS-Pain or -Self-image in any useful manner. Point-precise estimates for continuous variables were subject to high average errors.\nClassification of benchmark outcomes is improved with ensemble ML-techniques and may provide much needed case-adjustment for a surgeon performance program. Precise estimates of health-related quality of life scores and continuous variables were not possible, suggesting that performance classification is a better method of performance evaluation.", 
    "abstract": "The timely detection of Alzheimer's disease and other related dementias (ADRD) is suboptimal. Digital data already stored in electronic health records (EHR) offer opportunities for enhancing the timely detection of ADRD by facilitating the development of passive digital markers (PDMs). We conducted a systematic evidence review to identify studies that describe the development, performance, and validity of EHR-based PDMs for ADRD.\nWe searched the literature published from January 2000 to August 2022 and reviewed cross-sectional, retrospective, or prospective observational studies with a patient population of 18\u2009years or older, published in English that collected and interpreted original data, included EHR as a source of digital data, and had the primary purpose of supporting ADRD care. We extracted relevant data from the included studies with guidance from the Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies checklist and used the US Preventive Services Task Force criteria to appraise each study.\nWe included and appraised 19 studies. Four studies were considered to have a fair quality, and none was considered to have a good quality. The functionality of the PDMs varied from detecting mild cognitive impairment, Alzheimer's disease or ADRD, to forecasting stages of ADRD. Only seven studies used a valid reference diagnostic method. Nine PDMs used only structured EHR data, and five studies provided complete information on the race and ethnicity of its population. The number of features included in the PDMs ranges from 10 to 853, and the PMDs used a variety of statistical and machine learning algorithms with various time-at-risk windows. The area under the curve (AUC) for the PDMs varied from 0.67 to 0.97.\nAlthough we noted heterogeneity in the PDMs development and performance, there is evidence that these PDMs have the potential to detect ADRD at earlier stages.", 
    "abstract": "Rodent studies have demonstrated that synaptic dynamics from excitatory to inhibitory neuron types are often dependent on the target cell type. However, these target cell-specific properties have not been well investigated in human cortex, where there are major technical challenges in reliably obtaining healthy tissue, conducting multiple patch-clamp recordings on inhibitory cell types, and identifying those cell types. Here, we take advantage of newly developed methods for human neurosurgical tissue analysis with multiple patch-clamp recordings, ", 
    "abstract": "Here, we demonstrate, for the first time, the possibility of distinguishing between geogenic and anthropogenic calcite in a non-destructive and effective way. Geogenic calcite derives from natural sedimentary and metamorphic rocks whereas anthropogenic calcite is formed artificially due to the carbonation process in mortars and plaster lime binders. Currently, their distinction is a major unaddressed issue although it is crucial across several fields such as ", 
    "abstract": "Objective To identify the potential long non-coding RNA (lncRNA) expressed in rheumatoid arthritis (RA) synovium key to RA onset and investigate its association with immune cell infiltration. Methods RA synovium data were downloaded from the GEO database and normalized. The lncRNAs key to RA onset were identified using multiple machine learning methods. Infiltration of 22 immune cell populations in RA synovium was measured by cell-type identification by estimating relative subsets of RNA transcripts (CIBER-SORT). The relationship between the key lncRNA and infiltrating immune cells was analyzed. Finally, real-time quantitative PCR was applied to validate the expression of the key lncRNA in RA synovial cells. Results lncRNA human leukocyte antigen complex P5(HCP5) was identified as the key lncRNA associated with RA onset. Infiltration analysis revealed increased abundance of CD8", 
    "abstract": "Many people were found with pulmonary nodules during physical examinations. It is of great practical significance to discriminate benign and malignant nodules by using data mining technology.\nThe subjects' demographic data, baseline examination results, and annual follow-up low-dose spiral computerized tomography (LDCT) results were recorded. The findings from annual physical examinations of positive nodules, including highly suspicious nodules and clinically tentative benign nodules, was analyzed. The extreme gradient boosting (XGBoost) model was constructed and the Grid Search CV method was used to select the super parameters. External unit data were used as an external validation set to evaluate the generalization performance of the model.\nA total of 135,503 physical examinees were enrolled. Baseline testing found that 27,636 (20.40%) participants had clinically tentative benign nodules and 611 (0.45%) participants had highly suspicious nodules. The proportion of highly suspicious nodules in participants with negative baseline was about 0.12%-0.46%, which was lower than the baseline level except the follow-up of >5\u2009years. In the 27,636 participants with clinically tentative benign nodules, only in the first year of LDCT re-examination was the proportion of highly suspicious nodules (1.40%) significantly greater than that of baseline screening (0.45%) (p\u2009<\u20090.001), and the proportion of highly suspicious nodules was not different between the baseline screening and other follow-up years (p\u2009>\u20090.05). Furthermore, 322 cases with benign nodules and 196 patients with malignant nodules confirmed by surgery and pathology were compared. A model and the top 15 most important clinical variables were determined by XGBoost algorithm. The area under the curve (AUC) of the model was 0.76 [95% CI: 0.67-0.84], and the accuracy was 0.75. The sensitivity and specificity of the model under this threshold were 0.78 and 0.73, respectively. In the validation of model using external data, the AUC was 0.87 and the accuracy was 0.80. The sensitivity and specificity were 0.83 and 0.77, respectively.\nIt is important that pulmonary nodules could be more accurately identified at the first LDCT examination. A model with 15 variables which are routinely measured in the clinic could be helpful to distinguish benign and malignant nodules. It could help the radiological team issue a more accurate report; and it may guide the clinical team regarding LDCT follow-up.", 
    "abstract": "New drugs are needed to treat antipsychotic-resistant schizophrenia, especially those with clozapine-resistant schizophrenia. Atypical antipsychotics have predominantly 5-HT2A and dopaminergic antagonism, but also require investigation of other receptors.\nIn this study, the binding affinities between clozapine, olanzapine, and quetiapine with neuropharmacological, immunological, and metabolic receptors were measured using GNINA (Deep Learning Based Molecular Docking) and AlphaFold (Predicted Protein Structures).\nThrough this study, it was determined that these antipsychotics showed high binding affinity to a variety of receptors, such as CB2, 5-HT1BR, NPYR4, and CCR5. Cyclosporin A and everolimus which show high affinities with those receptors could be used for the development of new antipsychotic drugs based on these drugs.\nIn the future, the method used in this study will be applied to the development of new antipsychotic drugs, including drug repositioning, and to the discovery of the pathophysiology of schizophrenia.", 
    "abstract": "This study employs machine learning and population-based data to examine major factors of antidepressant medication including nitrogen dioxides (NO2) seasonality.\nRetrospective cohort data came from Korea National Health Insurance Service claims data for 43,251 participants with the age of 15-79 years, residence in the same districts of Seoul and no history of antidepressant medication during 2002-2012. The dependent variable was antidepressant-free months during 2013-2015 and the 103 independent variables for 2012 or 2015 were considered, e.g., particulate matter less than 2.5 micrometer in diameter (PM2.5), PM10, NO2, ozone (O3), sulphur dioxide (SO2) and carbon monoxide (CO) in each of 12 months in 2015.\nIt was found that the Cox hazard ratios of NO2 were statistically significant and registered values larger than 10 for every three months: March, June-July, October, and December. Based on random forest variable importance and Cox hazard ratios in brackets, indeed, the top 20 factors of antidepressant medication included age (0.0041 [1.69-2.25]), migraine and sleep disorder (0.0029 [1.82]), liver disease (0.0017 [1.33-1.34]), exercise (0.0014), thyroid disease (0.0013), cardiovascular disease (0.0013 [1.20]), asthma (0.0008 [1.19-1.20]), September NO2 (0.0008 [0.01]), alcohol consumption (0.0008 [1.31-1.32]), gender - woman (0.0007 [1.80-1.81]), July NO2 (0.0007 [14.93]), July PM10 (0.0007), the proportion of the married (0.0005), January PM2.5 (0.0004), September PM2.5 (0.0004), chronic obstructive pulmonary disease (0.0004), economic satisfaction (0.0004), January PM10 (0.0003), residents in welfare facilities per 1,000 (0.0003 [0.97]), and October NO2 (0.0003).\nAntidepressant medication has strong associations with neighborhood conditions including NO2 seasonality and welfare support.", 
    "abstract": "Post-translational modifications (PTMs) either enhance a protein's activity in various sub-cellular processes, or degrade their activity which leads toward failure of intracellular processes. Tyrosine nitration (NT) modification degrades protein's activity that initiates and propagates various diseases including neurodegenerative, cardiovascular, autoimmune diseases and carcinogenesis. Identification of NT modification supports development of novel therapies and drug discoveries for associated diseases. Identification of NT modification in biochemical labs is expensive, time consuming and error-prone. To supplement this process, several computational approaches have been proposed. However these approaches fail to precisely identify NT modification, due to the extraction of irrelevant, redundant and less discriminative features from protein sequences. This paper presents the NTpred framework that is competent in extracting comprehensive features from raw protein sequences using four different sequence encoders. To reap the benefits of different encoders, it generates four additional feature spaces by fusing different combinations of individual encodings. Furthermore, it eradicates irrelevant and redundant features from eight different feature spaces through a Recursive Feature Elimination process. Selected features of four individual encodings and four feature fusion vectors are used to train eight different Gradient Boosted Tree classifiers. The probability scores from the trained classifiers are utilized to generate a new probabilistic feature space, which is used to train a Logistic Regression classifier. On the BD1 benchmark dataset, the proposed framework outperforms the existing best-performing predictor in 5-fold cross validation and independent test evaluation with combined improvement of 13.7% in MCC and 20.1% in AUC. Similarly, on the BD2 benchmark dataset, the proposed framework outperforms the existing best-performing predictor with combined improvement of 5.3% in MCC and 1.0% in AUC. NTpred is publicly available for further experimentation and predictive use at: https://sds_genetic_analysis.opendfki.de/PredNTS/.", 
    "abstract": "Determining the grade and molecular marker status of intramedullary gliomas is important for assessing treatment outcomes and prognosis. Invasive biopsy for pathology usually carries a high risk of tissue damage, especially to the spinal cord, and there are currently no non-invasive strategies to identify the pathological type of intramedullary gliomas. Therefore, this study aimed to develop a non-invasive machine learning model to assist doctors in identifying the intramedullary glioma grade and mutation status of molecular markers.\nA total of 461 patients from two institutions were included, and their sagittal (SAG) and transverse (TRA) T2-weighted magnetic resonance imaging scans and clinical data were acquired preoperatively. We employed a transformer-based deep learning model to automatically segment lesions in the SAG and TRA phases and extract their radiomics features. Different feature representations were fed into the proposed neural networks and compared with those of other mainstream models.\nThe dice similarity coefficients of the Swin transformer in the SAG and TRA phases were 0.8697 and 0.8738, respectively. The results demonstrated that the best performance was obtained in our proposed neural networks based on multimodal fusion (SAG-TRA-clinical) features. In the external validation cohort, the areas under the receiver operating characteristic curve for graded (WHO I-II or WHO III-IV), alpha thalassemia/mental retardation syndrome X-linked (ATRX) status, and tumor protein p53 (P53) status prediction tasks were 0.8431, 0.7622, and 0.7954, respectively.\nThis study reports a novel machine learning strategy that, for the first time, is based on multimodal features to predict the ATRX and P53 mutation status and grades of intramedullary gliomas. The generalized application of these models could non-invasively provide more tumor-specific pathological information for determining the treatment and prognosis of intramedullary gliomas.", 
    "abstract": "Antibacterial peptides can be a potential game changer in the fight against antibiotic resistance. In order for these peptides to become successful antibiotic alternatives, it is essential that they possess high efficacy in addition to just being antibacterial. In this study, we have developed a two-level SVM-based binary classification approach to predict the antibacterial activity of a given peptide (model 1) and thereafter classify its antibacterial efficacy as high/low (model 2) with respect to minimum inhibitory concentration (MIC) values against Staphylococcus aureus, one of the most common pathogens. Based on charge and hydrophobicity of amino acids, we developed a sequence-based combined charge and hydrophobicity-guided triad (CHT) as a new method for obtaining features of any peptide. Model 1 with a combination of CHT and amino acid composition (AAC) as the feature representation method resulted in the highest accuracy of 96.7%. Model 2 with CHT as the feature representation method yielded the highest accuracy of 70.9%. Thus, CHT is found to be a potential feature representation method for classifying antibacterial peptides based on both activity and efficacy. Furthermore, we have also used an explainable machine learning algorithm to extract various insights from these models. These insights are found to be in excellent agreement with experimental findings reported in the literature, thus enhancing the dependability of the proposed models.", 
    "abstract": "Precise forecasting of survival risk plays a pivotal role in comprehending and predicting the prognosis of patients afflicted with esophageal squamous cell carcinoma (ESCC). The existing methods have the problems of insufficient fitting ability and poor interpretability. To address this issue, this work proposes a novel interpretable survival risk prediction method for ESCC patients based on extreme gradient boosting improved by whale optimization algorithm (WOA-XGBoost) and shapley additive explanations (SHAP). Given the imbalanced nature of the data set, the adaptive synthetic sampling (ADASYN) is first used to generate the samples with high survival risk. Then, an improved clustering by fast search and find of density peaks (IDPC) algorithm based on cosine distance and K nearest neighbors is used to cluster the patients. Next, the prediction model for each cluster is obtained by WOA-XGBoost and the constructed model is visualized with SHAP to uncover the factors hidden in the structured model and improve the interpretability of the black-box model. Finally, the effectiveness of the proposed scheme is demonstrated by analyzing the data collected from the First Affiliated Hospital of Zhengzhou University. The results of the analysis reveal that the proposed methodology exhibits superior performance, as indicated by the area under the receiver operating characteristic curve (AUROC) of 0.918 and accuracy of 0.881.", 
    "abstract": "High-throughput profiling methods (such as genomics or imaging) have accelerated basic research and made deep molecular characterization of patient samples routine. These approaches provide a rich portrait of genes, molecular pathways and cell types involved in disease phenotypes. Machine learning (ML) can be a useful tool for extracting disease-relevant patterns from high-dimensional datasets. However, depending upon the complexity of the biological question, machine learning often requires many samples to identify recurrent and biologically meaningful patterns. Rare diseases are inherently limited in clinical cases, leading to few samples to study. In this Perspective, we outline the challenges and emerging solutions for using ML for small sample sets, specifically in rare diseases. Advances in ML methods for rare diseases are likely to be informative for applications beyond rare diseases for which few samples exist with high-dimensional data. We propose that the method community prioritize the development of ML techniques for rare disease research.", 
    "abstract": "Radiotherapy benefits patients with advanced esophageal squamous cell carcinoma (ESCC) in terms of symptom relief and long-term survival. In contrast, a substantial proportion of ESCC patients have not benefited from radiotherapy. This study aimed to establish and validate an artificial neural network-based radiomics model for the pretreatment prediction of the radiotherapy response of advanced ESCC by using integrated data combined with feasible baseline characteristics of computed tomography. A total of 248 patients with advanced ESCC who underwent baseline CT and received radiotherapy were enrolled in this study and were analyzed by two types of radiomics models, machine learning and deep learning. As a result, the Att. Resnet50 pretrained network model indicated superior performance, with AUCs of 0.876, 0.802 and 0.732 in the training, internal validation, and external validation cohorts, respectively. Similarly, our Att. Resnet50 pretrained network model showed excellent calibration and significant clinical benefit according to the C index and decision curve analysis. Herein, a novel pretreatment radiomics model was established based on deep learning methods and could be used for radiotherapy response prediction in advanced ESCC patients, thus providing reliable evidence for therapeutic decision-making.", 
    "abstract": "The importance of molecular diagnostics is increasingly emphasized in the 2021 WHO guidelines for gliomas. There is considerable variability in molecular features and prognosis among glioma patients with the same pathological WHO grade.\nmRNA data and clinical information of human glioma patients were obtained from TCGA and CGGA databases, while expression profiles and TMZ resistance phenotypes of human glioma stem cells were acquired from the GEO database. Differentially expressed genes were identified across distinct WHO grades. Unsupervised clustering was performed on glioma patients based on DEG expression profiles. The Boruta algorithm was employed to identify feature genes for distinct molecular subtypes, and PCA was used to reduce the dimensionality of the feature gene expression data. Grade scores for each sample were calculated and correlated with patients' clinical molecular pathological features and immune microenvironment. Gene set enrichment analysis identified grade score-related functional pathways. Weighted gene co-expression network analysis identified grade score-associated biomarkers. The impact of the hub gene on malignant glioma behavior was validated through in vitro experiments, including CCK-8, EdU, colony formation, Transwell, wound healing, and immunofluorescence assays.\nA total of 672 and 687 samples were screened from TCGA and CGGA databases, respectively, along with 6 control, 24 low-grade, and 40 glioblastoma samples from our hospital. Two robust gene clusters were identified based on the expression profiles of 4,476 DEGs among grades 2, 3, and 4 tissues, revealing distinct prognoses. The grade scores exhibited significant heterogeneity across different WHO grade samples, representing diverse immune microenvironments. Grade scores served as independent risk factors for predicting patient prognosis, with higher sensitivity than traditional biomarkers. KIF20A, identified as a grade score-related biomarker, was independently associated with glioma prognosis. Exclusively expressed in tumor cells, KIF20A knockdown significantly inhibited tumor growth, invasion, and EMT biological behavior in glioma cells. Furthermore, KIF20A could serve as a biological marker for predicting patient response to TMZ treatment.\nThe grade scoring system enhances our understanding of the glioma tumor microenvironment. KIF20A, a novel biomarker for predicting TMZ treatment efficiency, influences malignant tumor behavior by affecting the EMT biological behavior of glioma cells.", 
    "abstract": "Noninvasive X-ray imaging of nanoscale three-dimensional objects, such as integrated circuits (ICs), generally requires two types of scanning: ptychographic, which is translational and returns estimates of the complex electromagnetic field through the IC; combined with a tomographic scan, which collects these complex field projections from multiple angles. Here, we present Attentional Ptycho-Tomography (APT), an approach to drastically reduce the amount of angular scanning, and thus the total acquisition time. APT is machine learning-based, utilizing axial self-Attention for Ptycho-Tomographic reconstruction. APT is trained to obtain accurate reconstructions of the ICs, despite the incompleteness of the measurements. The training process includes regularizing priors in the form of typical patterns found in IC interiors, and the physics of X-ray propagation through the IC. We show that APT with \u00d712 reduced angles achieves fidelity comparable to the gold standard Simultaneous Algebraic Reconstruction Technique (SART) with the original set of angles. When using the same set of reduced angles, then APT also outperforms Filtered Back Projection (FBP), Simultaneous Iterative Reconstruction Technique (SIRT) and SART. The time needed to compute the reconstruction is also reduced, because the trained neural network is a forward operation, unlike the iterative nature of these alternatives. Our experiments show that, without loss in quality, for a 4.48\u2009\u00d7\u200993.2\u2009\u00d7\u20093.92\u2009\u00b5m", 
    "abstract": "The enantioseparation of chiral molecules is a crucial and challenging task in the field of experimental chemistry, often requiring extensive trial and error with different experimental settings. To overcome this challenge, here we show a research framework that employs machine learning techniques to predict retention times of enantiomers and facilitate chromatographic enantioseparation. A documentary dataset of chiral molecular retention times in high-performance liquid chromatography (CMRT dataset) is established to handle the challenge of data acquisition. A quantile geometry-enhanced graph neural network is proposed to learn the molecular structure-retention time relationship, which shows a satisfactory predictive ability for enantiomers. The domain knowledge of chromatography is incorporated into the machine learning model to achieve multi-column prediction, which paves the way for chromatographic enantioseparation prediction by calculating the separation probability. The proposed research framework works well in retention time prediction and chromatographic enantioseparation facilitation, which sheds light on the application of machine learning techniques to the experimental scene and improves the efficiency of experimenters to speed up scientific discovery.", 
    "abstract": "Dissolved organic matter (DOM) sustains a substantial part of the organic matter transported seaward, where photochemical reactions significantly affect its transformation and fate. The irradiation experiments can provide valuable information on the photochemical reactivity (photolabile, photoresistant, and photoproduct) of molecules. However, the inconsistency of the fate of irradiated molecules among different experiments curtailed our understanding of the roles the photochemical reactions have played, which cannot be properly addressed by traditional approaches. Here, we conducted irradiation experiments for samples from two large estuaries in China. Molecules that occurred in irradiation experiments were characterized by the Fourier transform ion cyclotron resonance mass spectrometry and assigned probabilistic labels to define their photochemical reactivity. These molecules with probabilistic labels were used to construct a learning database for establishing a suitable machine learning (ML) model. We further applied our well-trained ML model to \"un-matched\" (i.e., not detected in our irradiation experiments) molecules from five estuaries worldwide, to predict their photochemical reactivity. Results showed that numerous molecules with strong photolability can be captured solely by the ML model. Moreover, comparing DOM photochemical reactivity in five estuaries revealed that the riverine DOM chemistry largely determines their subsequent photochemical transformation. We offer an expandable and renewable approach based on ML to compatibly integrate existing irradiation experiments and shed insight into DOM transformation and degradation processes.", 
    "abstract": "Artificial intelligence (AI) has the potential to simplify and optimize various steps of the brachytherapy workflow, and this literature review aims to provide an overview of the work done in this field.\nWe conducted a literature search in June 2022 on PubMed, Embase, and Cochrane for papers that proposed AI applications in brachytherapy.\nA total of 80 papers satisfied inclusion/exclusion criteria. These papers were categorized as follows: segmentation (24), registration and image processing (6), preplanning (13), dose prediction and treatment planning (11), applicator/catheter/needle reconstruction (16), and quality assurance (10). AI techniques ranged from classical models such as support vector machines and decision tree-based learning to newer techniques such as U-Net and deep reinforcement learning, and were applied to facilitate small steps of a process (e.g., optimizing applicator selection) or even automate the entire step of the workflow (e.g., end-to-end preplanning). Many of these algorithms demonstrated human-level performance and offer significant improvements in speed.\nAI has potential to augment, automate, and/or accelerate many steps of the brachytherapy workflow. We recommend that future studies adhere to standard reporting guidelines. We also stress the importance of using larger sample sizes and reporting results using clinically interpretable measures.", 
    "abstract": "Pharmacy service is to provide individualized pharmaceutical care for patients, which should follow the current evidence-based pharmacy, and constantly verify the evidence and then produce new evidence. In pharmaceutical care, differences are often found in the efficacy and adverse reactions of drugs among individuals, even within individuals, which are closely related to patient's genetics, liver and kidney functions, disease states, and drug interactions. Back in the 1980s, therapeutic drug monitoring (TDM) has been applied to routinely monitor the blood drug concentration of patients taking antiepileptic drugs or immunosuppressants after transplantation to provide individualized dosage recommendations and accumulate a large amount of pharmacokinetic (PK)/pharmacodynamic (PD) data. As individualized pharmaceutical care proceeds, the concept of precision medicine was introduced into pharmacy services in combination with evidence-based pharmacy, PK/PD theories and big data to further promote the TDM technology and drugs, and carry out pharmacogenomics analysis. The TDM and pharmacogenomics have been applied gradually to the fields of antimicrobial, antitumor and antipsychotic drugs and immunosuppressants. Based on the concept of precision pharmacy, we adpoted approaches including PK/PD, quantitative pharmacology, population pharmacokinetics, and big data machine learning to provide more personalized pharmacy services, which is mainly for special patients, such as critical patients, patients with interaction risk of multiple drugs, patients with liver and renal insufficiency, pregnant women, children and elderly patients. As the service pattern of precision pharmacy has been constructed and constantly improved, better evidence in clinical practice will be produced to provide patients with better precision pharmacy service.", 
    "abstract": "Identifying predictors of readmissions after mitral valve transcatheter edge-to-edge repair (MV-TEER) is essential for risk stratification and optimization of clinical outcomes.\nWe investigated the performance of machine learning [ML] algorithms vs. logistic regression in predicting readmissions after MV-TEER.\nWe utilized the National-Readmission-Database to identify patients who underwent MV-TEER between 2015 and 2018. The database was randomly split into training (70\u00a0%) and testing (30\u00a0%) sets. Lasso regression was used to remove non-informative variables and rank informative ones. The top 50 informative predictors were tested using 4 ML models: ML-logistic regression [LR], Naive Bayes [NB], random forest [RF], and artificial neural network [ANN]/For comparison, we used a traditional statistical method (principal component analysis logistic regression PCA-LR).\nA total of 9425 index hospitalizations for MV-TEER were included. Overall, the 30-day readmission rate was 14.6\u00a0%, and heart failure was the most common cause of readmission (32\u00a0%). The readmission cohort had a higher burden of comorbidities (median Elixhauser score 5 vs. 3) and frailty score (3.7 vs. 2.9), longer hospital stays (3 vs. 2\u00a0days), and higher rates of non-home discharges (17.4\u00a0% vs. 8.5\u00a0%). The traditional PCA-LR model yielded a modest predictive value (area under the curve [AUC] 0.615 [0.587-0.644]). Two ML algorithms demonstrated superior performance than the traditional PCA-LR model; ML-LR (AUC 0.692 [0.667-0.717]), and NB (AUC 0.724 [0.700-0.748]). RF (AUC 0.62 [0.592-0.677]) and ANN (0.65 [0.623-0.677]) had modest performance.\nMachine learning algorithms may provide a useful tool for predicting readmissions after MV-TEER using administrative databases.", 
    "abstract": "The ability of T cells to undergo robust cell division in response to antigenic stimulation is essential for competent T cell function. However, this ability is reduced with aging and contributes to increased susceptibility to infectious diseases, cancers, and other diseases among older adults. To better understand T cell aging, improved measurements of age-related cellular changes in T cells are necessary. The recent development of machine learning (ML)-assisted transcriptome-based quantification of individual CD8", 
    "abstract": "The recurrence of cancer following chemotherapy treatment is a major cause of death across solid and hematologic cancers. In B-cell acute lymphoblastic leukemia (B-ALL), relapse after initial chemotherapy treatment leads to poor patient outcomes. Here we test the hypothesis that chemotherapy-treated versus control B-ALL cells can be characterized based on cellular physical phenotypes. To quantify physical phenotypes of chemotherapy-treated leukemia cells, we use cells derived from B-ALL patients that are treated for 7\u00a0days with a standard multidrug chemotherapy regimen of vincristine, dexamethasone, and L-asparaginase (VDL). We conduct physical phenotyping of VDL-treated versus control cells by tracking the sequential deformations of single cells as they flow through a series of micron-scale constrictions in a microfluidic device; we call this method Quantitative Cyclical Deformability Cytometry. Using automated image analysis, we extract time-dependent features of deforming cells including cell size and transit time (TT) with single-cell resolution. Our findings show that VDL-treated B-ALL cells have faster TTs and transit velocity than control cells, indicating that VDL-treated cells are more deformable. We then test how effectively physical phenotypes can predict the presence of VDL-treated cells in mixed populations of VDL-treated and control cells using machine learning approaches. We find that TT measurements across a series of sequential constrictions can enhance the classification accuracy of VDL-treated cells in mixed populations using a variety of classifiers. Our findings suggest the predictive power of cell physical phenotyping as a complementary prognostic tool to detect the presence of cells that survive chemotherapy treatment. Ultimately such complementary physical phenotyping approaches could guide treatment strategies and therapeutic interventions. Insight box Cancer cells that survive chemotherapy treatment are major contributors to patient relapse, but the ability to predict recurrence remains a challenge. Here we investigate the physical properties of leukemia cells that survive treatment with chemotherapy drugs by deforming individual cells through a series of micron-scale constrictions in a microfluidic channel. Our findings reveal that leukemia cells that survive chemotherapy treatment are more deformable than control cells. We further show that machine learning algorithms applied to physical phenotyping data can predict the presence of cells that survive chemotherapy treatment in a mixed population. Such an integrated approach using physical phenotyping and machine learning could be valuable to guide patient treatments.", 
    "abstract": "Microbes play a central role in coral reef health. However, the relative importance of physical-chemical and biological processes in the control of microbial biomass are unknown. Here, we applied machine learning to analyze a large dataset of biological, physical, and chemical parameters (N\u202f=\u202f665 coral reef seawater samples) to understand the factors that modulate microbial abundance in the water of Abrolhos reefs, the largest and richest coral reefs of the Southwest Atlantic. Random Forest (RF) and Boosted Regression Tree (BRT) models indicated that hydrodynamic forcing, Dissolved Organic Carbon (DOC), and Total Nitrogen (TN) were the most important predictors of microbial abundance. The possible cumulative effects of higher temperatures, longer seawater residence time, higher nutrient concentration, and lower coral and fish biomass observed in coastal reefs resulted in higher microbial abundance, potentially impacting coral resilience against stressors.", 
    "abstract": "Continuous optimization of atrioventricular (AV)-delay for CRT is mainly performed by electrical means.\nDevelopment of an estimation model of cardiac function that uses a piezoelectric microphone embedded in a pulse generator to guide CRT optimization.\nElectrocardiogram, left ventricular pressure (LVP) and heart sounds were simultaneously collected during CRT implantation procedures. A piezoelectric alarm-transducer embedded in a modified CRT device facilitated recording of heart sounds in patients undergoing a pacing protocol with different AV-delays. Machine-learning (ML) was employed to produce a decision-tree ensemble model capable of estimating absolute maximal LVP (LVP\nIn the dataset of \u223c30,000 heartbeats, ML indicated S1-, S2-amplitude and S1-integral (S1-energy for LVdP/dt\nHeart sound sensors embedded in a CRT device, powered by a ML-algorithm provide a reliable assessment of optimal AV-delays and absolute LVP", 
    "abstract": "A hybrid energy cycle (HEC) based on biomass gasification can be suggested as an efficient, modern and low-carbon energy power plant. In the current article, a thermodynamic-conceptual design of a HEC based on biomass and solar energies has been developed in order to generate electric power, heat and hydrogen energy. The planned HEC consists of six main units: two electric energy production units, a heat recovery unit (HRU), a hydrogen energy generation cycle based on water electrolysis, a thermal power generation unit (based on LFR field), and a biofuel production unit (based on biomass gasification process). Conceptual analysis is based on the development of energy, exergy and exergoeconomic assessments. Besides that, the reduction rate of pollutant emission through the planned HEC compared to conventional power plants is presented. In the planned HEC, when hydrogen energy is not needed, excess hydrogen is feed into the combustion chamber to improve system performance and reduce the need for natural gas. Accordingly, the rate of polluting gases emitted from the cycle can be mitigated due to the reduction of fossil fuels consumption. Further, based on the machine learning technique (MLT), the level of biofuel produced from the mentioned process is estimated. In this regard, two algorithms (i.e., Support vector machine and Gaussian process regression) have been employed to develop the prediction model. The findings indicated that the considered HEC can produce about 10.2\u202fMW of electricity, 153\u202fkW of thermal power, and 71.8\u202fkmol/h of hydrogen energy. In both training and testing sets, the Support vector machine model exhibits better behavior compared the two Gaussian process regression model. Based on machine learning technique, with increasing gasification pressure, the level of biofuel obtained from the process does not increase significantly.", 
    "abstract": "Background In the past four to five decades, the field of swallowing science has made significant strides in the evaluation and treatment of swallowing disorders (dysphagia). Despite these strides, several gaps in knowledge remain and optimal approaches for dysphagia management have yet to be established. Part of this hindrance stems from our relatively limited understanding of the complex underlying swallowing mechanisms which further limits our ability to examine how these mechanisms may be altered in patients with dysphagia, and how to optimally target them in therapy. To overcome this hindrance, it is critical that we develop sensitive new tools and methods that will allow for the precise and personalized examination of patients' complex swallowing control and neurophysiological changes, and for the direct targeting of this control to improve treatment effectiveness. Summary Herein the advantages and limitations of current approaches in the study of swallowing biomechanics and central and peripheral swallowing control mechanisms are first summarized. Then, two examples of recent technological advances developed by the author's multidisciplinary team are described, including an integrative MRI sequence that allows for the simultaneous examination of oropharyngeal swallow and brain activity (SimulScan), and a novel wearable surface electromyography sensor technology (i-Phagia) designed for swallowing rehabilitation monitoring. The current state, limitations and future applications of both technologies are discussed. Upon optimization and validation, such technological advancements can offer unprecedented opportunities to gain direct and precise insights on the swallowing mechanism. Information gained from these and similar new technologies can act as a catalyst for the future development of optimized personalized dysphagia care. By leveraging advances in current methods, multidisciplinary collaborations, and new Digital Age technologies, the field of dysphagia can take the next giant leap forward in improving clinical care and patient lives. Key Messages \u2022 There is a critical need to develop sensitive new tools and methods that will allow for the precise and personalized examination of the complex swallowing mechanism, and lead to the development of physiology-based and more effective interventions. \u2022 The Digital Age is the ideal time to begin leveraging the technological advancements of fields such as imaging, electrophysiology, wearables, and machine learning to advance dysphagia research and practice. \u2022 A new integrative MRI sequence and a novel wearable surface electromyography sensor technology developed by the author's team are presented, as examples of recent technological advances that can play an important role in the future of personalized dysphagia care.", 
    "abstract": "The value of informal sources in increasing the timeliness of disease outbreak detection and providing detailed epidemiological information in the early warning and preparedness context is recognized. This study evaluates machine learning methods for classifying information from animal disease-related news at a fine-grained level (i.e., epidemiological topic). We compare two textual representations, the bag-of-words method and a distributional approach, i.e., word embeddings. Both representations performed well for binary relevance classification (F-measure of 0.839 and 0.871, respectively). Bag-of-words representation was outperformed by word embedding representation for classifying sentences into fine-grained epidemiological topics (F-measure of 0.745). Our results suggest that the word embedding approach is of interest in the context of low-frequency classes in a specialized domain. However, this representation did not bring significant performance improvements for binary relevance classification, indicating that the textual representation should be adapted to each classification task.", 
    "abstract": "Patients with known heart failure (HF) present to emergency departments (ED) with a plethora of symptoms. Although symptom clusters have been suggested as prognostic features, accurately triaging HF patients is a longstanding challenge.\nWe sought to use machine learning to identify subtle phenotypes of patient symptoms and evaluate their diagnostic and prognostic value among HF patients seeking emergency care.\nThis was a secondary analysis of a prospective cohort study of consecutive patients seen in the ED for chest pain or equivalent symptoms. Independent reviewers extracted clinical data from charts, including nine categories of subjective symptoms reported during initial evaluation. The diagnostic outcome was acute HF exacerbation and prognostic outcome was 30-day major adverse cardiac events (MACE). Outcomes were adjudicated by two independent reviewers. K-means clustering was used to derive latent patient symptom clusters, and their associations with outcomes were assessed using multivariate logistic regression.\nSample included 438 patients (age 65\u00b114 years; 45% female, 49% Black, 18% HF exacerbation, 32% MACE). K-means clustering identified three presentation phenotypes: patients with dyspnea only (Cluster A, 40%); patients with indigestion, with or without dyspnea (Cluster B, 23%); patients with neither dyspnea nor indigestion (Cluster C, 37%). Compared to Cluster C, indigestion was a significant predictor of acute HF exacerbation (OR=1.8, 95%CI=1.0-3.4) and 30-day MACE (OR=1.8, 95%CI=1.0-3.1), independent of age, sex, race, and other comorbidities.\nIndigestion symptoms in patients with known HF signify excess risk of adverse events, suggesting that these patients should be triaged as high-risk during initial ED evaluation.", 
    "abstract": "Contrast-induced nephropathy (CIN) is a postprocedural complication associated with increased morbidity and mortality. An important risk factor for development of CIN is renal impairment. Identification of patients at risk for acute renal failure will allow physicians to make appropriate decisions to minimize the incidence of CIN. We developed a machine learning model to stratify risk of acute renal failure that may assist in mitigating risk for CIN in patients with peripheral artery disease (PAD) undergoing endovascular interventions.\nWe utilized the American College of Surgeons National Surgical Quality Improvement Program database to extract clinical and laboratory information associated with 14,444 patients who underwent lower extremity endovascular procedures between 2011 and 2018. Using 11,604 cases from 2011 to 2017 for training and 2840 cases from 2018 for testing, we developed a random forest model to predict risk of 30-day acute renal failure following infra-inguinal endovascular procedures.\nEight variables were identified as contributing optimally to model predictions, the most important being diabetes, preoperative BUN, and claudication. Using these variables, the model achieved an area under the receiver-operating characteristic (AU-ROC) curve of 0.81, accuracy of 0.83, sensitivity of 0.67, and specificity of 0.74. The model performed equally well on white and nonwhite patients (Delong p-value\u00a0=\u00a00.955) and patients age\u00a0<\u00a065 and patients age\u00a0\u2265\u00a065 (Delong p-value\u00a0=\u00a00.659).\nWe develop a model that fairly and accurately stratifies 30-day acute renal failure risk in patients undergoing lower extremity endovascular procedures for PAD. This model may assist in identifying patients who may benefit from strategies to prevent CIN.", 
    "abstract": "Multi-facility nuclear sites with research reactors have several environmental area gamma monitors in a network as a part of their surveillance capability. However, the routine release of low levels of ", 
    "abstract": "Alzheimer's Disease (AD) is a complex clinical phenotype with unprecedented social and economic tolls on an ageing global population. Real-world data (RWD) from electronic health records (EHRs) offer opportunities to accelerate precision drug development and scale epidemiological research on AD. A precise characterization of AD cohorts is needed to address the noise abundant in RWD.\nWe conducted a retrospective cohort study to develop and test computational models for AD cohort identification using clinical data from 8 Massachusetts healthcare systems. We mined temporal representations from EHR data using the transitive sequential pattern mining algorithm (tSPM) to train and validate our models. We then tested our models against a held-out test set from a review of medical records to adjudicate the presence of AD. We trained two classes of Machine Learning models, using Gradient Boosting Machine (GBM), to compare the utility of AD diagnosis records versus the tSPM temporal representations (comprising sequences of diagnosis and medication observations) from electronic medical records for characterizing AD cohorts.\nIn a group of 4985 patients, we identified 219 tSPM temporal representations (i.e., transitive sequences) of medical records for constructing the best classification models. The models with sequential features improved AD classification by a magnitude of 3-16 percent over the use of AD diagnosis codes alone. The computed cohort included 663 patients, 35 of whom had no record of AD. Six groups of tSPM sequences were identified for characterizing the AD cohorts.\nWe present sequential patterns of diagnosis and medication codes from electronic medical records, as digital markers of Alzheimer's Disease. Classification algorithms developed on sequential patterns can replace standard features from EHRs to enrich phenotype modelling.\nNational Institutes of Health: the National Institute on Aging (RF1AG074372) and the National Institute of Allergy and Infectious Diseases (R01AI165535).", 
    "abstract": "Adsorption of CO", 
    "abstract": "The COVID-19 pandemic has had a negative impact on the mental health of the population. Many studies reported high levels of psychological distress and rising rates of suicidal ideation (SI). Data on a range of psychometric scales from 1790 respondents were collected in Slovenia through an online survey between July 2020 and January 2021. As a worrying percentage (9.7%) of respondents reported having SI within the last month, the goal of this study was to estimate the presence of SI, as indicated by the Suicidal Ideation Attributes Scale (SIDAS). The estimation was based on the change of habits, demographic features, strategies for coping with stress, and satisfaction with three most important aspects of life (relationships, finances, and housing). This could both help recognize the telltale factors indicative of SI and potentially identify people at risk. The factors were specifically selected to be discreet about suicide, likely sacrificing some accuracy in return. We tried four machine learning algorithms: binary logistic regression, random forest, XGBoost, and support vector machines. Logistic regression, random forest, and XGBoost models achieved comparable performance with the highest area under the receiver operating characteristic curve of 0.83 on previously unseen data. We found an association between various subscales of Brief-COPE and SI; Self-Blame was especially indicative of the presence of SI, followed by increase in Substance Use, low Positive Reframing, Behavioral Disengagement, dissatisfaction with relationships and lower age. The results showed that the presence of SI can be estimated with reasonable specificity and sensitivity based on the proposed indicators. This suggests that the indicators we examined have a potential to be developed into a quick screening tool that would assess suicidality indirectly, without unnecessary exposure to direct questions on suicidality. As with any screening tool, subjects identified as being at risk, should be further clinically examined.", 
    "abstract": "Isocitrate dehydrogenase (IDH) is one of the most important genotypes in patients with glioma because it can affect treatment planning. Machine learning-based methods have been widely used for prediction of IDH status (denoted as IDH prediction). However, learning discriminative features for IDH prediction remains challenging because gliomas are highly heterogeneous in MRI. In this paper, we propose a multi-level feature exploration and fusion network (MFEFnet) to comprehensively explore discriminative IDH-related features and fuse different features at multiple levels for accurate IDH prediction in MRI. First, a segmentation-guided module is established by incorporating a segmentation task and is used to guide the network in exploiting features that are highly related to tumors. Second, an asymmetry magnification module is used to detect T2-FLAIR mismatch sign from image and feature levels. The T2-FLAIR mismatch-related features can be magnified from different levels to increase the power of feature representations. Finally, a dual-attention feature fusion module is introduced to fuse and exploit the relationships of different features from intra- and inter-slice feature fusion levels. The proposed MFEFnet is evaluated on a multi-center dataset and shows promising performance in an independent clinical dataset. The interpretability of the different modules is also evaluated to illustrate the effectiveness and credibility of the method. Overall, MFEFnet shows great potential for IDH prediction.", 
    "abstract": "Air pollution remains a major threat to cardiovascular health and most acute myocardial infarction (AMI) deaths occur at home. However, currently established knowledge on the deleterious effect of air pollution on AMI has been limited to routinely monitored air pollutants and overlooked the place of death. In this study, we examined the association between short-term residential exposure to China's routinely monitored and unmonitored air pollutants and the risk of AMI deaths at home. A time-stratified case-crossover analysis was undertaken to associate short-term residential exposure to air pollution with 0.1 million AMI deaths at home in Jiangsu Province (China) during 2016-2019. Individual-level residential exposure to five unmonitored and monitored air pollutants including PM", 
    "abstract": "Current artificial intelligence studies for supporting CT screening tasks depend on either supervised learning or detecting anomalies. However, the former involves a heavy annotation workload owing to requiring many slice-wise annotations (ground truth labels); the latter is promising, but while it reduces the annotation workload, it often suffers from lower performance. This study presents a novel weakly supervised anomaly detection (WSAD) algorithm trained based on scan-wise normal and anomalous annotations to provide better performance than conventional methods while reducing annotation workload.\nBased on surveillance video anomaly detection methodology, feature vectors representing each CT slice were trained on an AR-Net-based convolutional network using a dynamic multiple-instance learning loss and a center loss function. The following two publicly available CT datasets were retrospectively analyzed: the RSNA brain hemorrhage dataset (normal scans: 12,862; scans with intracranial hematoma: 8882) and COVID-CT set (normal scans: 282; scans with COVID-19: 95).\nAnomaly scores of each slice were successfully predicted despite inaccessibility to any slice-wise annotations. Slice-level area under the curve (AUC), sensitivity, specificity, and accuracy from the brain CT dataset were 0.89, 0.85, 0.78, and 0.79, respectively. The proposed method reduced the number of annotations in the brain dataset by 97.1% compared to an ordinary slice-level supervised learning method.\nThis study demonstrated a significant annotation reduction in identifying anomalous CT slices compared to a supervised learning approach. The effectiveness of the proposed WSAD algorithm was verified through higher AUC than existing anomaly detection techniques.", 
    "abstract": "We aimed to develop and validate caries prognosis models in primary and permanent teeth after 2 and 10 y of follow-up through a machine learning (ML) approach, using predictors collected in early childhood. Data from a 10-y prospective cohort study conducted in southern Brazil were analyzed. Children aged 1 to 5 y were first examined in 2010 and reassessed in 2012 and 2020 regarding caries development. Dental caries was assessed using the Caries Detection and Assessment System (ICDAS) criteria. Demographic, socioeconomic, psychosocial, behavioral, and clinical factors were collected. ML algorithms decision tree, random forest, and extreme gradient boosting (XGBoost) were employed, along with logistic regression. The discrimination and calibration of models were verified in independent sets. From 639 children included at the baseline, we reassessed 467 (73.3%) and 428 (66.9%) children in 2012 and 2020, respectively. For all models, the area under receiver operating characteristic curve (AUC) at training and testing was above 0.70 for predicting caries in primary teeth after 2-y follow-up, with caries severity at the baseline being the strongest predictor. After 10 y, the SHAP algorithm based on XGBoost achieved an AUC higher than 0.70 in the testing set and indicated caries experience, nonuse of fluoridated toothpaste, parent education, higher frequency of sugar consumption, low frequency of visits to the relatives, and poor parents' perception of their children's oral health as top predictors for caries in permanent teeth. In conclusion, the implementation of ML shows potential for determining caries development in both primary and permanent teeth using easy-to-collect predictors in early childhood.", 
    "abstract": "Protein-protein interactions (PPIs) have been often considered undruggable targets although they are attractive for the discovery of new therapeutics. The spread of artificial intelligence and machine learning complemented with experimental methods is likely to change the perspectives of protein-protein modulator research. Noteworthy, some novel low molecular weight (LMW) and short peptide modulators of PPIs are already in clinical trials for the treatment of relevant diseases.\nThis review focuses on the main molecular properties of protein-protein interfaces and on key concepts pertaining to the modulation of PPIs. The authors survey recently reported state-of-the-art methods dealing with the rational design of PPI modulators and highlight the role of several computer-based approaches.\nInterfering specifically with large protein interfaces is still an open challenge. The initial concerns about the unfavorable physicochemical properties of many of these modulators are nowadays less acute with several molecules lying beyond the rule of 5, orally available and successful in clinical trials. As the cost of biologics interfering with PPIs is very high, it would seem reasonable to put more effort, both in academia and the private sectors, on actively developing novel low molecular weight compounds and short peptides to perform this task.", 
    "abstract": "The incidence of acute myocardial infarction (AMI) in hemodialysis (HD) patients is high and the prognosis is extremely poor. However, the potential connection between HD and AMI, and its regulatory mechanisms remain unclear. In this study, the gene expression profiles of HD (GSE15072) and AMI (GSE66360) were downloaded from the Gene Expression Omnibus database, common differentially expressed genes (DEGs) were obtained using the limma R package, the biological functions were analyzed according to Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analyses, machine learning was conducted to identify hub genes. Receiver operating characteristic curves and gene set enrichment analyses were used to explore the characters and biological function of hub genes, networks were used for candidate identification of transcription factor (TF), microRNA (miRNA), and drug. After a total of 255 common DEGs were selected, GO and KEGG analyses indicated that neutrophil extracellular trap (NET) may be a potential connection between HD and AMI, LILRB2, S100A12, CYBB, ITGAM, and PPIF were finally identified as hub genes. The area under curve of LILRB2, S100A12, and PPIF was higher than 0.8 in both datasets. Networks show the relationship between hub genes, TF, and miRNA, also the relationship between potential drugs and protein. In conclusion, NETs may be the potential connection between AMI and HD. The potential hub gene, signaling pathways, and drugs provided by this study may contribute to future AMI prevention and intervention in HD patients.", 
    "abstract": "One of the primary challenges in human genetics is determining the functional impact of single nucleotide variants (SNVs) and insertion and deletions (InDels), whether coding or noncoding. In the past, methods have been created to detect disease-related single amino acid changes, but only some can assess the influence of noncoding variations. CADD is the most commonly used and advanced algorithm for predicting the diverse effects of genome variations. It employs a combination of sequence conservation and functional features derived from the ENCODE project data. To use CADD, a large set of pre-calculated information must be downloaded during the installation process. To streamline the variant annotation process, we developed PhD-SNPg, a machine-learning tool that is easy to install and lightweight, relying solely on sequence-based features. Here we present an updated version, trained on a larger dataset, that can also predict the impact of the InDel variations. Despite its simplicity, PhD-SNPg performs similarly to CADD, making it ideal for rapid genome interpretation and as a benchmark for tool development.", 
    "abstract": "Traditional Chinese medicine (TCM) is the treasure of China, and the quality control of TCM is of crucial importance. In recent years, with the quick rise of artificial intelligence (AI) and the rapid development of hyperspectral imaging (HSI) technology, the combination of the two has been widely used in the quality evaluation of TCM. Machine learning (ML) is the core wisdom of AI, and its progress in rapid analysis and higher accuracy improves the potential of applying HSI to the field of TCM. This article reviewed five aspects of ML applied to hyperspectral data analysis of TCM: partition of data set, data preprocessing, data dimension reduction, qualitative or quantitative models, and model performance measurement. The different algorithms proposed by researchers for quality assessment of TCM were also compared. Finally, the challenges in the analysis of hyperspectral images for TCM were summarized, and the future works were prospected.", 
    "abstract": "As found in the health studies literature, the levels of climate association between epidemiological diseases have been found to vary across regions. Therefore, it seems reasonable to allow for the possibility that relationships might vary spatially within regions. We implemented the geographically weighted random forest (GWRF) machine learning method to analyze ecological disease patterns caused by spatially non-stationary processes using a malaria incidence dataset for Rwanda. We first compared the geographically weighted regression (WGR), the global random forest (GRF), and the geographically weighted random forest (GWRF) to examine the spatial non-stationarity in the non-linear relationships between malaria incidence and their risk factors. We used the Gaussian areal kriging model to disaggregate the malaria incidence at the local administrative cell level to understand the relationships at a fine scale since the model goodness of fit was not satisfactory to explain malaria incidence due to the limited number of sample values. Our results show that in terms of the coefficients of determination and prediction accuracy, the geographical random forest model performs better than the GWR and the global random forest model. The coefficients of determination of the geographically weighted regression (R2), the global RF (R2), and the GWRF (R2) were 4.74, 0.76, and 0.79, respectively. The GWRF algorithm achieves the best result and reveals that risk factors (rainfall, land surface temperature, elevation, and air temperature) have a strong non-linear relationship with the spatial distribution of malaria incidence rates, which could have implications for supporting local initiatives for malaria elimination in Rwanda.", 
    "abstract": "Autism is a neurodevelopmental disorder that cannot be completely cured, but early intervention during childhood can improve outcomes. Identifying autism spectrum disorder (ASD) has relied on subjective detection methods that involve questionnaires, medical professionals, and therapists and are subject to observer variability. The need for early diagnosis and the limitations of subjective detection methods has led researchers to explore machine learning-based approaches, such as Random Forests, K-Nearest Neighbors, Naive Bayes, and Support Vector Machines, to predict ASD meltdowns. In recent years, deep learning techniques have gained traction for early ASD detection. This study evaluates the performance of various deep learning networks, including AlexNet, VGG16, and ResNet50, using 5 cepstral coefficient features for ASD detection. The main contributions of this study are the utilization of Cepstral Coefficients in the processing stage to construct spectrograms and the modification of the AlexNet architecture for precise classification. Experimental observations indicate that the AlexNet with Linear Frequency Cepstral Coefficients (LFCC) yields the highest accuracy of 85.1%, while a customized AlexNet with LFCC achieves 90% accuracy.", 
    "abstract": "The negative genetic impacts of gene flow from domestic to wild populations can be dependent on the degree of domestication and exacerbated by the magnitude of pre-existing genetic differences between wild populations and the domestication source. Recent evidence of European ancestry within North American aquaculture Atlantic salmon (Salmo salar) has elevated the potential impact of escaped farmed salmon on often at-risk wild North American salmon populations. Here, we compare the ability of single nucleotide polymorphism (SNP) and microsatellite (SSR) marker panels of different sizes (7-SSR, 100-SSR and 220K-SNP) to detect introgression of European genetic information into North American wild and aquaculture populations. Linear regression comparing admixture predictions for a set of individuals common to the three datasets showed that the 100-SSR panel and 7-SSR panels replicated the full 220K-SNP-based admixture estimates with low accuracy (r", 
    "abstract": "Machine learning has proven useful in analyzing complex biological data and has greatly influenced the course of research in structural biology and precision medicine. Deep neural network models oftentimes fail to predict the structure of complex proteins and are heavily dependent on experimentally determined structures for their training and validation. Single-particle cryogenic electron microscopy (cryoEM) is also advancing the understanding of biology and will be needed to complement these models by continuously supplying high-quality experimentally validated structures for improvements in prediction quality. In this perspective, the significance of structure prediction methods is highlighted, but\u00a0the authors also ask, what if these programs cannot accurately predict a protein structure important for preventing disease? The role of cryoEM is discussed to help fill the gaps left by artificial intelligence predictive models in resolving targetable proteins and protein complexes that will pave the way for personalized therapeutics.", 
    "abstract": "Serial block face scanning electron microscopy (SBF-SEM), also referred to as serial block-face electron microscopy, is an advanced ultrastructural imaging technique that enables three-dimensional visualization that provides largerx- and y-axis ranges than other volumetric EM techniques. While SEM is first introduced in the 1930s, SBF-SEM is developed as a novel method to resolve the 3D architecture of neuronal networks across large volumes with nanometer resolution by Denk and Horstmann in 2004.\u00a0Here, the authors provide an accessible overview of the advantages and challenges associated with SBF-SEM. Beyond this, the applications of SBF-SEM in biochemical domains as well as potential future clinical applications are briefly reviewed. Finally, the alternative forms of artificial intelligence-based segmentation which may contribute to devising a feasible workflow involving SBF-SEM, are also considered.", 
    "abstract": "Carotid atherosclerosis (CAS), an important factor in the development of stroke, is a major public health concern. The aim of this study was to establish and validate machine learning (ML) models for early screening of CAS using routine health check-up indicators in northeast China.\nA total of 69,601 health check-up records from the health examination center of the First Hospital of China Medical University (Shenyang, China) were collected between 2018 and 2019. For the 2019 records, 80% were assigned to the training set and 20% to the testing set. The 2018 records were used as the external validation dataset. Ten ML algorithms, including decision tree (DT), K-nearest neighbors (KNN), logistic regression (LR), naive Bayes (NB), random forest (RF), multiplayer perceptron (MLP), extreme gradient boosting machine (XGB), gradient boosting decision tree (GBDT), linear support vector machine (SVM-linear), and non-linear support vector machine (SVM-nonlinear), were used to construct CAS screening models. The area under the receiver operating characteristic curve (auROC) and precision-recall curve (auPR) were used as measures of model performance. The SHapley Additive exPlanations (SHAP) method was used to demonstrate the interpretability of the optimal model.\nA total of 6315 records of patients undergoing carotid ultrasonography were collected; of these, 1632, 407, and 1141 patients were diagnosed with CAS in the training, internal validation, and external validation datasets, respectively. The GBDT model achieved the highest performance metrics with auROC of 0.860 (95% CI 0.839-0.880) in the internal validation dataset and 0.851 (95% CI 0.837-0.863) in the external validation dataset. Individuals with diabetes or those over 65\u00a0years of age showed low negative predictive value. In the interpretability analysis, age was the most important factor influencing the performance of the GBDT model, followed by sex and non-high-density lipoprotein cholesterol.\nThe ML models developed could provide good performance for CAS identification using routine health check-up indicators and could hopefully be applied in scenarios without ethnic and geographic heterogeneity for CAS prevention.", 
    "abstract": "Imaginary worlds are present and often central in many of the most culturally successful modern narrative fictions, be it in novels (e.g., Harry Potter), movies (e.g., Star Wars), video games (e.g., The Legend of Zelda), graphic novels (e.g., One Piece) and TV series (e.g., Game of Thrones). We propose that imaginary worlds are popular because they activate exploratory preferences that evolved to help us navigate the real world and find new fitness-relevant information. Therefore, we hypothesize that the attraction to imaginary worlds is intrinsically linked to the desire to explore novel environments and that both are influenced by the same underlying factors. Notably, the inter-individual and cross-cultural variability of the preference for imaginary worlds should follow the inter-individual and cross-cultural variability of exploratory preferences (with the personality trait Openness-to-experience, age, sex, and ecological conditions). We test these predictions with both experimental and computational methods. For experimental tests, we run a pre-registered online experiment about movie preferences (N\u2009=\u2009230). For computational tests, we leverage two large cultural datasets, namely the Internet Movie Database (N\u2009=\u20099424 movies) and the Movie Personality Dataset (N\u2009=\u20093.5 million participants), and use machine-learning algorithms (i.e., random forest and topic modeling). In all, consistent with how the human preference for spatial exploration adaptively varies, we provide empirical evidence that imaginary worlds appeal more to more explorative people, people higher in Openness-to-experience, younger individuals, males, and individuals living in more affluent environments. We discuss the implications of these findings for our understanding of the cultural evolution of narrative fiction and, more broadly, the evolution of human exploratory preferences.", 
    "doi": "10.1038/s41598-023-35151-2\n10.1017/S0140525X21000923\n10.1111/j.2044-8295.1954.tb01243.x\n10.1037/rev0000175\n10.1016/j.cobeha.2020.05.014\n10.1016/j.cognition.2017.12.014\n10.1016/j.tics.2013.09.001\n10.1016/j.neuron.2015.09.010\n10.1016/j.conb.2018.11.003\n10.1037/a0038199\n10.1073/pnas.1318616111\n10.3389/fpsyg.2022.786770\n10.1016/j.tics.2003.11.002\n10.1016/j.jrp.2017.11.011\n10.1207/s15516709cog0000_50\n10.1037/a0020666\n10.1016/j.tics.2014.10.004\n10.1177/0013916509341244\n10.1111/nyas.14757\n10.1016/j.cognition.2022.105119\n10.1037/xge0000749\n10.1016/j.neuron.2006.06.021\n10.1038/nn.2342\n10.1016/S0893-6080(02)00048-5\n10.1016/j.neuron.2006.07.017\n10.1016/j.neuropsychologia.2009.01.015\n10.1017/S1930297500005659\n10.1177/0013916582141001\n10.1177/0013916506298796\n10.1177/0013916589215001\n10.1098/rstb.2007.2098\n10.1016/j.cobeha.2020.10.001\n10.1037/dec0000033\n10.1038/s41559-019-0938-7\n10.1016/j.tree.2004.10.006\n10.1111/j.1467-7687.2012.01182.x\n10.1016/j.cub.2005.09.016\n10.1007/s00265-006-0172-6\n10.1016/j.tics.2010.01.001\n10.1111/j.1745-6924.2009.01138.x\n10.1016/j.evolhumbehav.2016.04.001\n10.1037/0003-066X.61.6.622\n10.1016/j.jrp.2011.04.002\n10.1111/j.1467-6494.1992.tb00970.x\n10.1016/j.copsyc.2015.08.021\n10.1038/s41562-020-00989-3\n10.1098/rstb.2010.0061\n10.1177/09567976211031571\n10.1177/0022022106297299\n10.1037/pspp0000395\n10.1037/a0037128\n10.1037/0021-9010.86.3.513\n10.1111/jopy.12387\n10.1093/scan/nsu041\n10.2190/H8H6-QYKR-KEU8-GAQ0\n10.1038/s41598-022-19694-4\n10.1016/j.paid.2020.110187\n10.1007/s12110-015-9253-4\n10.1016/j.paid.2015.05.023\n10.1016/j.cogdev.2019.100800\n10.1027/1614-0001/a000302\n10.1348/000712606X117612\n10.1111/desc.13153\n10.1016/S0191-8869(02)00062-4\n10.1177/030573569302100105\n10.1002/1099-0984(200011/12)14:6<553::AID-PER384>3.0.CO;2-H\n10.1177/0305735616658957\n10.1037/pspp0000150\n10.1016/j.ijintrel.2009.02.004\n10.1016/j.cognition.2020.104327\n10.1111/desc.13026\n10.1037/0033-2909.116.1.75\n10.1111/cdep.12084\n10.1098/rstb.2012.0122\n10.1002/1520-6505(2000)9:43.0.CO;2-7\n10.1093/geronb/gbp035\n10.3389/fnins.2013.00053/abstract\n10.1016/j.cogpsych.2020.101276\n10.1016/j.cobeha.2020.05.012\n10.1177/1541931215591018\n10.1037/dev0000777\n10.1016/S0010-0277(03)00098-2\n10.1073/pnas.1700811114\n10.1016/j.cognition.2013.12.010\n10.1177/0956797617693005\n10.1177/0956797619863663\n10.1007/s10433-020-00567-6\n10.1111/desc.13075\n10.1111/desc.12532\n10.1177/0963721419887361\n10.1196/annals.1440.010\n10.1111/cdep.12310\n10.1016/j.neubiorev.2016.07.034\n10.1037/a0012897\n10.1093/geronb/55.1.P4\n10.5964/ps.6009\n10.1002/per.2247\n10.1017/S0140525X98221248\n10.1016/j.cognition.2003.09.007\n10.1016/0003-3472(89)90121-8\n10.1016/S0003-3472(80)80103-5\n10.1098/rspb.2014.1476\n10.1016/S1090-5138(00)00036-2\n10.2307/1130467\n10.1037/0033-2909.117.2.250\n10.1037/bul0000191\n10.1037/0735-7036.118.2.217\n10.2307/622576\n10.1086/668168\n10.1016/j.pnpbp.2006.01.010\n10.1073/pnas.1811032115\n10.1111/j.1469-7610.2004.00349.x\n10.1177/1362361398023008\n10.1023/A:1005653411471\n10.1016/j.paid.2015.03.026\n10.1016/j.paid.2018.03.026\n10.1016/j.evolhumbehav.2011.10.005\n10.1017/S0140525X1800211X\n10.1086/685644\n10.1002/dev.21293\n10.1016/j.evolhumbehav.2021.02.003\n10.1016/j.evolhumbehav.2021.04.001\n10.1038/s41598-020-60576-4\n10.1016/j.anbehav.2017.10.005\n10.1098/rstb.2015.0183\n10.1093/beheco/arv045\n10.1046/j.1439-0310.2002.00773.x\n10.1016/j.bbr.2017.02.038\n10.1007/s10826-018-1130-4\n10.1073/pnas.2109373119\n10.1109/TAFFC.2018.2808349\n10.1037/a0034084\n10.1016/j.paid.2004.11.002\n10.1016/j.dr.2008.09.001\n10.1017/S0140525X22000048\n10.1525/aa.1998.100.4.876\n10.1163/156853707X171829\n10.1037/a0027906\n10.1556/JCEP.3.2005.1.3\n10.1080/10509208.2011.575658\n10.1037/h0099308\n10.1080/00224490409552217\n10.1037/a0027918\n10.1017/S0140525X21002296\n10.5621/sciefictstud.40.1.0116\n10.1080/14649373.2012.636875\n10.3390/rel11010025\n10.1016/j.intell.2012.11.003\n10.1016/j.paid.2020.110461\n10.1016/j.paid.2020.110397\n10.1016/j.jrp.2006.02.001\n10.1016/j.jrp.2009.04.011\n10.1186/1477-7525-7-15\n10.1027/1015-5759/a000283\n10.1037/a0022403\n10.1023/A:1010933404324\n10.1007/BF02289263", 
    "abstract": "Acute respiratory infections (ARIs) are common in children. We developed machine learning models to predict pediatric ARI pathogens at admission.\nWe included hospitalized children with respiratory infections between 2010 and 2018. Clinical features were collected within 24\u00a0h of admission to construct models. The outcome of interest was the prediction of 6 common respiratory pathogens, including adenovirus, influenza virus types A and B, parainfluenza virus (PIV), respiratory syncytial virus (RSV), and Mycoplasma pneumoniae (MP). Model performance was estimated using area under the receiver operating characteristic curve (AUROC). Feature importance was measured using Shapley Additive exPlanation (SHAP) values.\nA total of 12,694 admissions were included. Models trained with 9 features (age, event pattern, fever, C-reactive protein, white blood cell count, platelet count, lymphocyte ratio, peak temperature, peak heart rate) achieved the best performance (AUROC: MP 0.87, 95% CI 0.83-0.90; RSV 0.84, 95% CI 0.82-0.86; adenovirus 0.81, 95% CI 0.77-0.84; influenza A 0.77, 95% CI 0.73-0.80; influenza B 0.70, 95% CI 0.65-0.75; PIV 0.73, 95% CI 0.69-0.77). Age was the most important feature to predict MP, RSV and PIV infections. Event patterns were useful for influenza virus prediction, and C-reactive protein had the highest SHAP value for adenovirus infections.\nWe demonstrate how artificial intelligence can assist clinicians identify potential pathogens associated with pediatric ARIs upon admission. Our models provide explainable results that could help optimize the use of diagnostic testing. Integrating our models into clinical workflows may lead to improved patient outcomes and reduce unnecessary medical costs.", 
    "abstract": "Pancreatic cystic lesions are frequently identified on cross-sectional imaging. As many of these are presumed branch-duct intraductal papillary mucinous neoplasms, these lesions generate much anxiety for the patients and clinicians, often necessitating long-term follow-up imaging and even unnecessary surgical resections. However, the incidence of pancreatic cancer is overall low for patients with incidental pancreatic cystic lesions. Radiomics and deep learning are advanced tools of imaging analysis that have attracted much attention in addressing this unmet need, however, current publications on this topic show limited success and large-scale research is needed.", 
    "abstract": "Diabetes mellitus (DM) can lead to diabetic ulcers (DUs), which are the most severe complications. Due to the need for more accurate patient classifications and diagnostic models, treatment and management strategies for DU patients still need improvement. The difficulty of diabetic wound healing is caused closely related to biological metabolism and immune chemotaxis reaction dysfunction. Therefore, the purpose of our study is to identify metabolic biomarkers in patients with DU and construct a molecular subtype-specific prognostic model that is highly accurate and robust. RNA-sequencing data for DU samples were obtained from the Gene Expression Omnibus (GEO) database. DU patients and normal individuals were compared regarding the expression of metabolism-related genes (MRGs). Then, a novel diagnostic model based on MRGs was constructed with the random forest algorithm, and classification performance was evaluated utilizing receiver operating characteristic (ROC) analysis. The biological functions of MRGs-based subtypes were investigated using consensus clustering analysis. A principal component analysis (PCA) was conducted to determine whether MRGs could distinguish between subtypes. We also examined the correlation between MRGs and immune infiltration. Lastly, qRT-PCR was utilized\u00a0to validate the expression of the hub MRGs with clinical validations and animal experimentations. Firstly, 8 metabolism-related hub genes were obtained by random forest algorithm, which could distinguish the DUs from normal samples\u00a0validated by the ROC curves. Secondly, DU samples could be consensus clustered into three molecular classifications by MRGs, verified by PCA analysis. Thirdly, associations between MRGs and immune infiltration were confirmed, with LYN and Type 1 helper cell significantly positively correlated; RHOH and TGF-\u03b2 family remarkably negatively correlated. Finally, clinical validations and animal experiments of DU skin tissue samples showed that\u00a0the expressions of metabolic hub genes in the DU groups were considerably upregulated, including GLDC, GALNT6, RHOH, XDH, MMP12, KLK6, LYN, and CFB. The current study proposed an auxiliary MRGs-based DUs model while proposing MRGs-based molecular clustering and confirmed the association with immune infiltration, facilitating the diagnosis and management of DU patients and designing individualized treatment plans.", 
    "abstract": "Early cancer diagnosis plays a critical role in improving treatment outcomes and increasing survival rates for certain cancers. NIR spectroscopy offers a rapid and cost-effective approach to evaluate the optical properties of tissues at the microvessel level and provides valuable molecular insights. The integration of NIR spectroscopy with advanced data-driven algorithms in portable instruments has made it a cutting-edge technology for medical applications. NIR spectroscopy is a simple, non-invasive and affordable analytical tool that complements expensive imaging modalities such as functional magnetic resonance imaging, positron emission tomography and computed tomography. By examining tissue absorption, scattering, and concentrations of oxygen, water, and lipids, NIR spectroscopy can reveal inherent differences between tumor and normal tissue, often revealing specific patterns that help stratify disease. In addition, the ability of NIR spectroscopy to assess tumor blood flow, oxygenation, and oxygen metabolism provides a key paradigm for its application in cancer diagnosis. This review evaluates the effectiveness of NIR spectroscopy in the detection and characterization of disease, particularly in cancer, with or without the incorporation of chemometrics and machine learning algorithms. The report highlights the potential of NIR spectroscopy technology to significantly improve discrimination between benign and malignant tumors and accurately predict treatment outcomes. In addition, as more medical applications are studied in large patient cohorts, consistent advances in clinical implementation can be expected, making NIR spectroscopy a valuable adjunct technology for cancer therapy management. Ultimately, the integration of NIR spectroscopy into cancer diagnostics promises to improve prognosis by providing critical new insights into cancer patterns and physiology.", 
    "abstract": "Eyes provide valuable information for neurological diagnosis. So far, the use of diagnostic devices to analyze eye movement is limited. We explored whether the analysis of eye movements can be efficacious. Patients with Parkinson's disease (PD) (n=29), spinocerebellar degeneration (SCD) (21), progressive supranuclear palsy (PSP) (19), and control individuals (19) participated in this study. The patients read aloud two sets of sentences displayed on a monitor: one was displayed horizontally, and the other vertically. Parameters such as eye movement speed, travel distance, and fixation/saccade ratio were extracted, and comparisons between groups were performed. Maneuvers of eye movements were also subjected to image classification using deep learning. Reading velocity and fixation/saccade ratio were altered in the PD group, and the SCD group exhibited ineffective eye movements due to dysmetria and nystagmus. Vertical gaze parameters showed aberrant values in the PSP group. Vertical written sentences were more sensitive than horizontal ones in detecting these abnormalities. In the regression analysis, vertical reading indicated a high accuracy in identifying each group. The machine learning analysis showed more than 90% accuracy in distinguishing between the control and SCD groups and between the SCD and PSP groups. Analyzing eye movements is useful and easily applicable.", 
    "abstract": "The urban on-road CO", 
    "abstract": "This study proposes a multi-objective, non-dominated, imperialist competitive algorithm (NSICA) to solve optimal feature selection problems. The NSICA is a multi-objective and discrete version of the original Imperialist Competitive Algorithm (ICA) that utilizes the competition between colonies and imperialists to solve optimization problems. This study focused on solving challenges such as discretization and elitism by modifying the original operations and using a non-dominated sorting approach. The proposed algorithm is independent of the application, and with customization, it could be employed to solve any feature selection problem. We evaluated the algorithm's efficiency using it as a feature selection system for diagnosing cardiac arrhythmias. The Pareto optimal selected features from NSICA were utilized to classify arrhythmias in binary and multi-class forms based on three essential objectives: accuracy, number of features, and false negativity. We applied NSICA to an ECG-based arrhythmia classification dataset from the UCI machine learning repository. The evaluation results indicate the efficiency of the proposed algorithm compared to other state-of-the-art algorithms.", 
    "abstract": "Although intake of specific macronutrients has been associated with sleep parameters, interventional evidence is lacking. Therefore, this\u00a0randomized trial was conducted\u00a0to examine how a more unhealthy high-fat/high-sugar (HFHS) diet impacts sleep in humans.\nIn a crossover study, 15 healthy young men consumed two isocaloric diets in random order for a week: an HFHS and a low-fat/low-sugar diet. Following each diet, in-lab sleep was recorded using polysomnography during a full night of sleep and during recovery sleep after extended wakefulness. Sleep duration, macrostructure, and microstructure (oscillatory pattern and slow waves) were investigated using machine learning-based algorithms.\nSleep duration did not differ across the diets based on actigraphy and the in-lab polysomnography. Sleep macrostructure was similar after 1\u2009week on each diet. Compared with the low-fat/low-sugar diet, consumption of the HFHS diet resulted in reduced delta power, delta to beta ratio, and slow wave amplitude but increased alpha and theta power during deep sleep. During recovery sleep, similar sleep oscillatory changes were observed.\nShort-term consumption of a more unhealthy diet alters sleep oscillatory features that regulate the restorative properties of sleep. Whether such changes can mediate adverse health outcomes associated with consumption of an unhealthier diet warrants investigation.", 
    "abstract": "Parkinson's disease (PD) is characterized by slowed movements, speech disorders, an inability to control muscle movements, and tremors in the hands and feet. In the early stages of PD, the changes in these motor signs are very vague, so an objective and accurate diagnosis is difficult. The disease is complex, progressive, and very common. There are more than 10 million people worldwide suffering from PD. In this study, an EEG-based deep learning model was proposed for the automatic detection of PD to support experts. The EEG dataset comprises signals recorded by the University of Iowa from 14 PD patients and 14 healthy controls. First of all, the power spectral density values (PSDs) \u200b\u200bof the frequencies between 1 and 49\u00a0Hz of the EEG signals were calculated separately using periodogram, welch, and multitaper spectral analysis methods. 49 feature vectors were extracted for each of the three different experiments. Then, the performances of support vector machine, random forest, k-nearest neighbor, and bidirectional long-short-term memory (BiLSTM) algorithms were compared using the PSDs feature vectors. After the comparison, the model integrating welch spectral analysis and the BiLSTM algorithm showed the highest performance as a result of the experiments. The deep learning model achieved satisfactory performance with 0.965 specificity, 0.994 sensitivity, 0.964 precision, 0.978 f1-score, 0.958 Matthews correlation coefficient, and 97.92% accuracy. The study is a promising attempt to detect PD from EEG signals and it also provides evidence that deep learning algorithms are more effective than machine learning algorithms for EEG signal analysis.", 
    "abstract": "Surgical data science is an emerging field focused on quantitative analysis of pre-, intra-, and postoperative patient data (Maier-Hein et al. in Med Image Anal 76: 102306, 2022). Data science approaches can decompose complex procedures, train surgical novices, assess outcomes of actions, and create predictive models of surgical outcomes (Marcus et al. in Pituitary 24: 839-853, 2021; R\u00f8adsch et al. in Nat Mach Intell, 2022). Surgical videos contain\u00a0powerful signals of events that may impact patient outcomes. A\u00a0necessary step\u00a0before the deployment of supervised machine learning methods\u00a0is the development of labels for objects and\u00a0anatomy. We describe a complete method for annotating videos of transsphenoidal surgery.\nEndoscopic video recordings of transsphenoidal pituitary tumor removal surgeries were collected from a multicenter research collaborative. These videos were anonymized and stored in a cloud-based platform. Videos were uploaded to an online annotation platform. Annotation framework was developed based on a literature review and surgical\u00a0observations to ensure proper understanding of the tools, anatomy, and steps present. A user guide was developed to trained annotators to ensure standardization.\nA fully annotated video of a transsphenoidal pituitary tumor removal surgery was produced. This annotated video included over 129,826 frames. To prevent any missing annotations, all frames were later reviewed by highly experienced annotators and a surgeon reviewer. Iterations to annotated videos allowed for the creation of an annotated video complete with labeled surgical tools, anatomy, and phases. In addition, a user guide was developed for the training of novice annotators, which\u00a0provides information about the annotation software to ensure the production of standardized annotations.\nA standardized and reproducible workflow for managing surgical video data is a necessary prerequisite to surgical data science applications. We developed a standard methodology for annotating\u00a0surgical videos that may\u00a0facilitate the quantitative analysis of videos using machine learning applications. Future work will demonstrate the clinical relevance and impact of this workflow by developing\u00a0process modeling and\u00a0outcome predictors.", 
    "abstract": "Oxaliplatin-based chemotherapy is the first-line treatment for colorectal cancer (CRC). Long noncoding RNAs (lncRNAs) have been implicated in chemotherapy sensitivity. This study aimed to identify lncRNAs related to oxaliplatin sensitivity and predict the prognosis of CRC patients underwent oxaliplatin-based chemotherapy.\nData from the Genomics of Drug Sensitivity in Cancer (GDSC) was used to screen for lncRNAs related to oxaliplatin sensitivity. Four machine learning algorithms (LASSO, Decision tree, Random-forest, and support vector machine) were applied to identify the key lncRNAs. A predictive model for oxaliplatin sensitivity and a prognostic model based on key lncRNAs were established. The published datasets, and cell experiments were used to verify the predictive value.\nA total of 805 tumor cell lines from GDSC were divided into oxaliplatin sensitive (top 1/3) and resistant (bottom 1/3) groups based on their IC50 values, and 113 lncRNAs, which were differentially expressed between the two groups, were selected and incorporated into four machine learning algorithms, and seven key lncRNAs were identified. The predictive model exhibited good predictions for oxaliplatin sensitivity. The prognostic model exhibited high performance in patients with CRC who underwent oxaliplatin-based chemotherapies. Four lncRNAs, including C20orf197, UCA1, MIR17HG, and MIR22HG, displayed consistent responses to oxaliplatin treatment in the validation analysis.\nCertain lncRNAs were associated with oxaliplatin sensitivity and predicted the response to oxaliplatin treatment. The prognostic models established based on the key lncRNAs could predict the prognosis of patients given oxaliplatin-based chemotherapy.", 
    "abstract": "The recent progress in molecular biology generates an increasing interest in investigating molecular biomarkers as markers of response to treatments. The present work is motivated by a study, where the objective was to explore the potential of the molecular biomarkers of renin-angiotensin-aldosterone system (RAAS) to identify the undertaken antihypertensive treatments in the general population. Population-based studies offer an opportunity to assess the effectiveness of treatments in real-world scenarios. However, lack of quality documentation, especially when electronic health record linkage is unavailable, leads to inaccurate reporting and classification bias.\nWe present a machine learning clustering technique to determine the potential of measured RAAS biomarkers for the identification of undertaken treatments in the general population. The biomarkers were simultaneously determined through a novel mass-spectrometry analysis in 800 participants of the Cooperative Health Research In South Tyrol (CHRIS) study with documented antihypertensive treatments. We assessed the agreement, sensitivity and specificity of the resulting clusters against known treatment types. Through the lasso penalized regression, we identified clinical characteristics associated with the biomarkers, accounting for the effects of cluster and treatment classifications.\nWe identified three well-separated clusters: cluster 1 (n\u2009=\u2009444) preferentially including individuals not receiving RAAS-targeting drugs; cluster 2 (n\u2009=\u2009235) identifying angiotensin type 1 receptor blockers (ARB) users (weighted kappa \u03ba\nUnsupervised clustering of angiotensin-based biomarkers is a viable technique to identify individuals on specific antihypertensive treatments, pointing to a potential application of the biomarkers as useful clinical diagnostic tools even outside of a controlled clinical setting.", 
    "abstract": "Continuous, comfortable, convenient (C3), and accurate blood pressure (BP) measurement and monitoring are needed for early diagnosis of various cardiovascular diseases. To supplement the limited C3 BP measurement of existing cuff-based BP technologies, though they may achieve reliable accuracy, cuffless BP measurement technologies, such as pulse transit/arrival time, pulse wave analysis, and image processing, have been studied to obtain C3 BP measurement. One of the recent cuffless BP measurement technologies, innovative machine-learning and artificial intelligence-based technologies that can estimate BP by extracting BP-related features from photoplethysmography (PPG)-based waveforms have attracted interdisciplinary attention of the medical and computer scientists owing to their handiness and effectiveness for both C3 and accurate, i.e., C3A, BP measurement. However, C3A BP measurement remains still unattainable because the accuracy of the existing PPG-based BP methods was not sufficiently justified for subject-independent and highly varying BP, which is a typical case in practice. To circumvent this issue, a novel convolutional neural network(CNN)- and calibration-based model (PPG2BP-Net) was designed by using a comparative paired one-dimensional CNN structure to estimate highly varying intrasubject BP. To this end, approximately [Formula: see text], [Formula: see text], and [Formula: see text] of 4185 cleaned, independent subjects from 25,779 surgical cases were used for training, validating, and testing the proposed PPG2BP-Net, respectively and exclusively (i.e., subject-independent modelling). For quantifying the intrasubject BP variation from an initial calibration BP, a novel 'standard deviation of subject-calibration centring (SDS)' metric is proposed wherein high SDS represents high intrasubject BP variation from the calibration BP and vice versa. PPG2BP-Net achieved accurately estimated systolic and diastolic BP values despite high intrasubject variability. In 629-subject data acquired after 20 minutes following the A-line (arterial line) insertion, low error mean and standard deviation of [Formula: see text] and [Formula: see text] for highly varying A-line systolic and diastolic BP values, respectively, where their SDSs are 15.375 and 8.745. This study moves one step forward in developing the C3A cuffless BP estimation devices that enable the push and agile pull services.", 
    "abstract": "Primary Sj\u00f6gren's syndrome (pSS) is a chronic, systemic autoimmune disease mostly affecting the exocrine glands. This debilitating condition is complex and specific treatments remain unavailable. There is a need for the development of novel diagnostic models for early screening. Four gene profiling datasets were downloaded from the Gene Expression Omnibus database. The 'limma' software package was used to identify differentially expressed genes (DEGs). A random forest-supervised classification algorithm was used to screen disease-specific genes, and three machine learning algorithms, including artificial neural networks (ANN), random forest (RF), and support vector machines (SVM), were used to build a pSS diagnostic model. The performance of the model was measured using its area under the receiver operating characteristic curve. Immune cell infiltration was investigated using the CIBERSORT algorithm. A total of 96 DEGs were identified. By utilizing a RF classifier, a set of 14 signature genes that are pivotal in transcription regulation and disease progression in pSS were identified. Through the utilization of training and testing datasets, diagnostic models for pSS were successfully designed using ANN, RF, and SVM, resulting in AUCs of 0.972, 1.00, and 0.9742, respectively. The validation set yielded AUCs of 0.766, 0.8321, and 0.8223. It was the RF model that produced the best prediction performance out of the three models tested. As a result, an early predictive model for pSS was successfully developed with high diagnostic performance, providing a valuable resource for the screening and early diagnosis of pSS.", 
    "abstract": "New satellite remote sensing and machine learning techniques offer untapped possibilities to monitor global biodiversity with unprecedented speed and precision. These efficiencies promise to reveal novel ecological insights at spatial scales which are germane to the management of populations and entire ecosystems. Here, we present a robust transferable deep learning pipeline to automatically locate and count large herds of migratory ungulates (wildebeest and zebra) in the Serengeti-Mara ecosystem using fine-resolution (38-50\u2009cm) satellite imagery. The results achieve accurate detection of nearly 500,000 individuals across thousands of square kilometers and multiple habitat types, with an overall F1-score of 84.75% (Precision: 87.85%, Recall: 81.86%). This research demonstrates the capability of satellite remote sensing and machine learning techniques to automatically and accurately count very large populations of terrestrial mammals across a highly heterogeneous landscape. We also discuss the potential for satellite-derived species detections to advance basic understanding of animal behavior and ecology.", 
    "abstract": "Poor dynamic balance and impaired gait adaptation to different contexts are hallmarks of people with neurological disorders (PwND), leading to difficulties in daily life and increased fall risk. Frequent assessment of dynamic balance and gait adaptability is therefore essential for monitoring the evolution of these impairments and/or the long-term effects of rehabilitation. The modified dynamic gait index (mDGI) is a validated clinical test specifically devoted to evaluating gait facets in clinical settings under a physiotherapist's supervision. The need of a clinical environment, consequently, limits the number of assessments. Wearable sensors are increasingly used to measure balance and locomotion in real-world contexts and may permit an increase in monitoring frequency. This study aims to provide a preliminary test of this opportunity by using nested cross-validated machine learning regressors to predict the mDGI scores of 95 PwND via inertial signals collected from short steady-state walking bouts derived from the 6-minute walk test. Four different models were compared, one for each pathology (multiple sclerosis, Parkinson's disease, and stroke) and one for the pooled multipathological cohort. Model explanations were computed on the best-performing solution; the model trained on the multipathological cohort yielded a median (interquartile range) absolute test error of 3.58 (5.38) points. In total, 76% of the predictions were within the mDGI's minimal detectable change of 5 points. These results confirm that steady-state walking measurements provide information about dynamic balance and gait adaptability and can help clinicians identify important features to improve upon during rehabilitation. Future developments will include training of the method using short steady-state walking bouts in real-world settings, analysing the feasibility of this solution to intensify performance monitoring, providing prompt detection of worsening/improvements, and complementing clinical assessments.", 
    "abstract": "Bacteremia is a life-threatening complication of infectious diseases. Bacteremia can be predicted using machine learning (ML) models, but these models have not utilized cell population data (CPD).\nThe derivation cohort from emergency department (ED) of China Medical University Hospital (CMUH) was used to develop the model and was prospectively validated in the same hospital. External validation was performed using cohorts from ED of Wei-Gong Memorial Hospital (WMH) and Tainan Municipal An-Nan Hospital (ANH). Adult patients who underwent complete blood count (CBC), differential count (DC), and blood culture tests were enrolled in the present study. The ML model was developed using CBC, DC, and CPD to predict bacteremia from positive blood cultures obtained within 4\u00a0h before or after the acquisition of CBC/DC blood samples.\nThis study included 20,636 patients from CMUH, 664 from WMH, and 1622 patients from ANH. Another 3143 patients were included in the prospective validation cohort of CMUH. The CatBoost model achieved an area under the receiver operating characteristic curve of 0.844 in the derivation cross-validation, 0.812 in the prospective validation, 0.844 in the WMH external validation, and 0.847 in the ANH external validation. The most valuable predictors of bacteremia in the CatBoost model were the mean conductivity of lymphocytes, nucleated red blood cell count, mean conductivity of monocytes, and neutrophil-to-lymphocyte ratio.\nML model that incorporated CBC, DC, and CPD showed excellent performance in predicting bacteremia among adult patients with suspected bacterial infections and blood culture sampling in emergency departments.", 
    "abstract": "Machine learning is a powerful tool that is increasingly being used in many research areas, including neuroscience. The recent development of new algorithms and network architectures, especially in the field of deep learning, has made machine learning models more reliable and accurate and useful for the biomedical research sector. By minimizing the effort necessary to extract valuable features from datasets, they can be used to find trends in data automatically and make predictions about future data, thereby improving the reproducibility and efficiency of research. One application is the automatic evaluation of micrograph images, which is of great value in neuroscience research. While the development of novel models has enabled numerous new research applications, the barrier to use these new algorithms has also decreased by the integration of deep learning models into known applications such as microscopy image viewers. For researchers unfamiliar with machine learning algorithms, the steep learning curve can hinder the successful implementation of these methods into their workflows. This review explores the use of machine learning in neuroscience, including its potential applications and limitations, and provides some guidance on how to select a fitting framework to use in real-life research projects.", 
    "abstract": "Palm oil fuel ash (POFA) has limited use as a fertilizer, while contribute effectively to the environmental contamination and health risks. Petroleum sludge poses a serious effect on the ecological environment and human health. The present work aimed to present a novel encapsulation process with POFA binder for treating petroleum sludge. Among 16 polycyclic aromatic hydrocarbons, four compounds were selected for the optimization of encapsulation process due to their high risk as carcinogenic substrates. Percentage PS (10-50%) and curing days (7-28 days) factors were used in the optimization process. The leaching test of PAHs was assessed using a GC-MS. The best operating parameters to minimize PAHs leaching from solidified cubes with OPC and10% POFA were recorded with 10% PS and after 28 days, at which PAH leaching was 4.255 and 0.388\u00a0ppm with R", 
    "abstract": "Late radiation-induced hematuria can develop in prostate cancer patients undergoing radiotherapy and can negatively impact the quality-of-life of survivors. If a genetic component of risk could be modeled, this could potentially be the basis for modifying treatment for high-risk patients. We therefore investigated whether a previously developed machine learning-based modeling method using genome-wide common single nucleotide polymorphisms (SNPs) can stratify patients in terms of the risk of radiation-induced hematuria.\nWe applied a two-step machine learning algorithm that we previously developed for genome-wide association studies called pre-conditioned random forest regression (PRFR). PRFR includes a pre-conditioning step, producing adjusted outcomes, followed by random forest regression modeling. Data was from germline genome-wide SNPs for 668 prostate cancer patients treated with radiotherapy. The cohort was stratified only once, at the outset of the modeling process, into two groups: a training set (2/3 of samples) for modeling and a validation set (1/3 of samples). Post-modeling bioinformatics analysis was conducted to identify biological correlates plausibly associated with the risk of hematuria.\nThe PRFR method achieved significantly better predictive performance compared to other alternative methods (all p < 0.05). The odds ratio between the high and low risk groups, each of which consisted of 1/3 of samples in the validation set, was 2.87 (p=0.029), implying a clinically useful level of discrimination. Bioinformatics analysis identified six key proteins encoded by CTNND2, GSK3B, KCNQ2, NEDD4L, PRKAA1, and TXNL1 genes as well as four statistically significant biological process networks previously shown to be associated with the bladder and urinary tract.\nThe risk of hematuria is significantly dependent on common genetic variants. The PRFR algorithm resulted in a stratification of prostate cancer patients at differential risk levels of post-radiotherapy hematuria. Bioinformatics analysis identified important biological processes involved in radiation-induced hematuria.", 
    "abstract": "Machine learning (ML) was used to predict specific methane yields (SMY) with a dataset of 14 features from lignocellulosic biomass (LB) characteristics and operating conditions of completely mixed reactors under continuous feeding mode. The random forest (RF) model was best suited for predicting SMY with a coefficient of determination (R", 
    "abstract": "Hydrochar has become a popular product for immobilizing heavy metals in water bodies. However, the relationships between the preparation conditions, hydrochar properties, adsorption conditions, heavy metal types, and the maximum adsorption capacity (Q", 
    "abstract": "Epigenetic regulations of immune responses are essential for cancer development and growth. As a critical step, comprehensive and rigorous explorations of m6A methylation are important to determine its prognostic significance, tumor microenvironment (TME) infiltration characteristics and underlying relationship with glioblastoma (GBM).\nTo evaluate m6A modification patterns in GBM, we conducted unsupervised clustering to determine the expression levels of GBM-related m6A regulatory factors and performed differential analysis to obtain m6A-related genes. Consistent clustering was used to generate m6A regulators cluster A and B. Machine learning algorithms were implemented for identifying TME features and predicting the response of GBM patients receiving immunotherapy.\nIt is found that the m6A regulatory factor significantly regulates the mutation of GBM and TME. Based on Europe, America, and China data, we established m6Ascore through the m6A model. The model accurately predicted the results of 1206 GBM patients from the discovery cohort. Additionally, a high m6A score was associated with poor prognoses. Significant TME features were found among the different m6A score groups, which demonstrated positive correlations with biological functions (i.e., EMT2) and immune checkpoints.\nm6A modification was important to characterize the tumorigenesis and TME infiltration in GBM. The m6Ascore provided GBM patients with valuable and accurate prognosis and prediction of clinical response to various treatment modalities, which could be useful to guide patient treatments.", 
    "abstract": "Catastrophic forgetting (CF) happens whenever a neural network overwrites past knowledge while being trained on new tasks. Common techniques to handle CF include regularization of the weights (using, e.g., their importance on past tasks), and rehearsal strategies, where the network is constantly re-trained on past data. Generative models have also been applied for the latter, in order to have endless sources of data. In this paper, we propose a novel method that combines the strengths of regularization and generative-based rehearsal approaches. Our generative model consists of a normalizing flow (NF), a probabilistic and invertible neural network, trained on the internal embeddings of the network. By keeping a single NF throughout the training process, we show that our memory overhead remains constant. In addition, exploiting the invertibility of the NF, we propose a simple approach to regularize the network's embeddings with respect to past tasks. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, with bounded computational power and memory overheads.", 
    "abstract": "Epigenetic modifications are implicated in the onset and progression of obstructive sleep apnea (OSA) and its complications through their bidirectional relationship with long-term chronic intermittent hypoxia (IH). However, the exact role of epigenetic acetylation in OSA is unclear. Here we explored the relevance and impact of acetylation-related genes in OSA by identifying molecular subtypes modified by acetylation in OSA patients. Twenty-nine significantly differentially expressed acetylation-related genes were screened in a training dataset (GSE135917). Six common signature genes were identified using the lasso and support vector machine algorithms, with the powerful SHAP algorithm used to judge the importance of each identified feature. DSCC1, ACTL6A, and SHCBP1 were best calibrated and discriminated OSA patients from normal in both training and validation (GSE38792) datasets. Decision curve analysis showed that patients could benefit from a nomogram model developed using these variables. Finally, a consensus clustering approach characterized OSA patients and analyzed the immune signatures of each subgroup. OSA patients were divided into two acetylation patterns (higher acetylation scores in Group B than in Group A) that differed significantly in terms of immune microenvironment infiltration. This is the first study to reveal the expression patterns and key role played by acetylation in OSA, laying the foundation for OSA epitherapy and refined clinical decision-making.", 
    "abstract": "Since the 1970s, 2 lines of White Leghorn chickens, HAS and LAS, have been continuously divergently selected for 5-day postinjection antibody titer to injection with sheep red blood cells (SRBC). Antibody response is a complex genetic trait and characterizing differences in gene expression could facilitate better understanding of physiological changes due to selection and antigen exposure. At 41 d of age, randomly selected HAS and LAS chickens, which had been coraised from hatch, were either injected with SRBC (HASI and LASI) or kept as the noninjected cohort (HASN and LASN). Five days later, all were euthanized, and samples collected from the jejunum for RNA isolation and sequencing. Resulting gene expression data were analyzed combining traditional statistics with machine learning to obtain signature gene lists for functional analysis. Differences in ATP production and cellular processes were observed in the jejunum between lines and following SRBC injection. HASN vs. LASN exhibited upregulation of ATP production, immune cell motility, and inflammation. LASI exhibits upregulation of ATP production and protein synthesis vs. LASN, reflective of what was observed in HASN vs. LASN. In contrast, no corresponding upregulation of ATP production was observed in HASI vs. HASN, and most other cellular processes appear inhibited. Without exposure to SRBC, gene expression in the jejunum indicates HAS generates more ATP than LAS, suggesting HAS maintains a \"primed\" system; and gene expression of HASI vs. HASN further suggests this basal ATP production is sufficient for robust antibody responses. Conversely, LASI vs. LASN jejunal gene expression implies a physiological need for increased ATP production with only minimal correlating antibody production. The results of this experiment provide insight into energetic resource needs and allocations in the jejunum in response to genetic selection and antigen exposure in HAS and LAS which may help explain phenotypic differences observed in antibody response.", 
    "abstract": "Schizophrenia has the main symptom of psychosis which is characterized by speech incoherence due to thought process disturbance. Before schizophrenia, there is a prodromal phase of psychosis in adolescence. Early recognition of this phase is important to prevent the development of symptoms into a severe mental disorder. Machine learning technology can be used to predict thought process disturbance through syntactic and semantic analysis of speech. This study aims to describe the differences in syntactic and semantic analysis in prodromal psychosis and normal adolescents. The research subjects consisted of 70 adolescents aged 14-19 years which were divided into 2 groups. Based on the results of the Prodromal Questionnaire-Brief (PQ-B) Indonesian version, the subjects were split into two groups: prodromal and normal. All participants were voice-recorded during interviews using an open-ended qualitative questionnaire. Syntactic and semantic analysis was carried out on all data which amounted to 1017 phrase segments and classified by machine learning. This is the first study in Indonesia to compare the analysis of syntactic and semantic aspects in prodromal psychosis and normal adolescent populations. There were significant differences in syntactic and semantic analysis between groups of adolescents with prodromal psychosis and normal adolescents at the minimum value of coherence and frequency of use of nouns, personal pronouns, subordinate conjunctions, adjectives, prepositions, and proper nouns.", 
    "abstract": "Pharmacogenomics studies how genes influence a person's response to treatment. When complex phenotypes are influenced by multiple genetic variations with little effect, a single piece of genetic information is often insufficient to explain this variability. The application of machine learning (ML) in pharmacogenomics holds great potential; namely, it can be used to unravel complicated genetic relationships that could explain response to therapy. In this study, ML techniques were used to investigate the relationship between genetic variations affecting more than 60 candidate genes and carboplatin-, taxane-, and bevacizumab-induced toxicities in 171 patients with ovarian cancer enrolled in the MITO-16A/MaNGO-OV2A trial. Single nucleotide polymorphism (SNP) profiles were examined using ML to find and prioritise those associated with drug-induced toxicities, specifically hypertension, hematological toxicity, non-hematological toxicity, and proteinuria. The Boruta algorithm was used in cross-validation to determine the significance of SNPs in predicting toxicities. Important SNPs were then used to train eXtreme gradient boosting models. During cross-validation, the models achieved reliable performance with a Matthews correlation coefficient ranging from 0.375 to 0.410. A total of 43 SNPs critical for predicting toxicity were identified. For each toxicity, key SNPs were used to create a polygenic toxicity risk score that effectively divided individuals into high-risk and low-risk categories. In particular, compared with low-risk individuals, high-risk patients were 28- fold more likely to develop hypertension. The proposed method provided insightful data to improve precision medicine for ovarian cancer patients, which may be useful for reducing toxicities and improving toxicity management.", 
    "abstract": "The pathophysiological mechanisms at work in Parkinson's disease (PD) patients with freezing of gait (FOG) remain poorly understood. Functional connectivity density (FCD) could provide an unbiased way to analyse connectivity across the brain. In this study, a total of 23 PD patients with FOG (PD FOG\u2009+\u2009patients), 26 PD patients without FOG (PD FOG- patients), and 22 healthy controls (HCs) were recruited, and their resting-state functional magnetic resonance imaging (rs-fMRI) images were collected. FCD mapping was first performed to identify differences between groups. Pearson correlation analysis was used to explore relationships between FCD values and the severity of FOG. Then, a machine learning model was employed to classify each pair of groups. PD FOG\u2009+\u2009patients showed significantly increased short-range FCD in the precuneus, cingulate gyrus, and fusiform gyrus and decreased long-range FCD in the frontal gyrus, temporal gyrus, and cingulate gyrus. Short-range FCD values in the middle temporal gyrus and inferior temporal gyrus were positively correlated with FOG questionnaire (FOGQ) scores, and long-range FCD values in the middle frontal gyrus were negatively correlated with FOGQ scores. Using FCD in abnormal regions as input, a support vector machine (SVM) classifier can achieve classification with good performance. The mean accuracy values were 0.895 (PD FOG\u2009+\u2009vs. HC), 0.966 (PD FOG- vs. HC), and 0.897 (PD FOG\u2009+\u2009vs. PD FOG-). This study demonstrates that PD FOG\u2009+\u2009patients showed altered short- and long-range FCD in several brain regions involved in action planning and control, motion processing, emotion, cognition, and object recognition.", 
    "abstract": "Harmful algal blooms (HABs) are a natural phenomenon caused by outbreaks of algae, resulting in serious problems for aquatic ecosystems and the coastal environment. ", 
    "abstract": "Homologous recombination repair (HRR) plays an important role in cancer development, drug resistance, and immune escape, but the role of HRR genes in primary lung cancer (PLC) after previous malignancies is unclear.\nWe used HRR-related score constrcted by HRR genes to classify patients into two groups and compared clinical progression, differential genes, and their functions between them. Then, we constructed a prognostic risk model based on HRR-related score and screened key differentially expressed genes. We evaluated the potential roles, mutational information, and immune correlations of key genes. Finally, we compared the long-term prognosis and immune correlations of different prognostic risk subgroups.\nWe found that HRR-related score was associated with T-stage, immunotherapy sensitivity, and prognosis of PLC after previous malignancies. Differential genes between HRR-related low-score and high-score groups are mainly involved in DNA replication and repair processes, such as the cell cycle. We identified three key genes, ABO, SERPINE2, and MYC, by machine learning, and MYC had the highest amplification mutation frequency. We verified that the key gene-based prognostic model can better assess the prognosis of patients. The risk score of the prognostic model was associated with immune microenvironment and efficacy of immunotherapy.\nOverall, we identified three key genes ABO, SERPINE2, and MYC associated with HRR status in PLC after previous malignancies. The risk model based on key genes is associated with immune microenvironment and can well predict the prognosis for PLC after previous malignancies.", 
    "abstract": "Breast cancer survivors often experience recurrence or a second primary cancer. We developed an automated approach to predict the occurrence of any second breast cancer (SBC) using patient-level data and explored the generalizability of the models with an external validation data source. Breast cancer patients from the cancer registry of Zurich, Zug, Schaffhausen, Schwyz (N\u2009=\u20093213; training dataset) and the cancer registry of Ticino (N\u2009=\u20091073; external validation dataset), diagnosed between 2010 and 2018, were used for model training and validation, respectively. Machine learning (ML) methods, namely a feed-forward neural network (ANN), logistic regression, and extreme gradient boosting (XGB) were employed for classification. The best-performing model was selected based on the receiver operating characteristic (ROC) curve. Key characteristics contributing to a high SBC risk were identified. SBC was diagnosed in 6% of all cases. The most important features for SBC prediction were age at incidence, year of birth, stage, and extent of the pathological primary tumor. The ANN model had the highest area under the ROC curve with 0.78 (95% confidence interval [CI] 0.750.82) in the training data and 0.70 (95% CI 0.61-0.79) in the external validation data. Investigating the generalizability of different ML algorithms, we found that the ANN generalized better than the other models on the external validation data. This research is a first step towards the development of an automated tool that could assist clinicians in the identification of women at high risk of developing an SBC and potentially preventing it.", 
    "abstract": "CHARGE syndrome, due to CHD7 pathogenic variations, is an autosomal dominant disorder characterized by a large spectrum of severity. Despite the great number of variations reported, no clear genotype-to-phenotype correlation has been reported. Unsupervised machine learning and clustering was undertaken using a retrospective cohort of 42 patients, after deep radiologic and clinical phenotyping, to establish genotype-phenotype correlation for CHD7-related CHARGE syndrome. It resulted in three clusters showing phenotypes of different severities. While no clear genotype-phenotype correlation appeared within the first two clusters, a single patient was outlying the cohort data (cluster 3) with the most atypical phenotype and the most distal frameshift variant in the gene. We added two other patients with similar distal pathogenic variants and observed a tendency toward mild and/or atypical phenotypes. We hypothesized that this finding could potentially be related to escaping nonsense mediated RNA decay, but found no evidence of such decay in vivo for any of the CHD7 pathogenic variation tested. This indicates that this milder phenotype may rather result from the production of a protein retaining all functional domains.", 
    "abstract": "Exposure-response (E-R) is a key aspect of pharmacometrics analysis that supports drug dose selection. Currently, there is a lack of understanding of the technical considerations necessary for drawing unbiased estimates from data. Due to recent advances in machine learning (ML) explainability methods, ML has garnered significant interest for causal inference. To this end, we used simulated datasets with known E-R \"ground truth\" to generate a set of good practices for the development of ML models required to avoid introducing biases when performing causal inference. These practices include the use of causal diagrams to enable the careful consideration of model variables by which to obtain desired E-R relationship insights, keeping a strict separation of data for model-training and for inference generation to avoid biases, hyperparameter tuning to improve the reliability of models, and estimating proper confidence intervals around inferences using a bootstrap sampling with replacement strategy. We computationally confirm the benefits of the proposed ML workflow by using a simulated dataset with nonlinear and non-monotonic exposure-response relationships.", 
    "abstract": "Advances in machine learning (ML) and the availability of protein sequences via high-throughput sequencing techniques have transformed the ability to design novel diagnostic and therapeutic proteins. ML allows protein engineers to capture complex trends hidden within protein sequences that would otherwise be difficult to identify in the context of the immense and rugged protein fitness landscape. Despite this potential, there persists a need for guidance during the training and evaluation of ML methods over sequencing data. Two key challenges for training discriminative models and evaluating their performance include handling severely imbalanced datasets (e.g., few high-fitness proteins among an abundance of non-functional proteins) and selecting appropriate protein sequence representations (numerical encodings). Here, we present a framework for applying ML over assay-labeled datasets to elucidate the capacity of sampling techniques and protein encoding methods to improve binding affinity and thermal stability prediction tasks. For protein sequence representations, we incorporate two widely used methods (One-Hot encoding and physiochemical encoding) and two language-based methods (next-token prediction, UniRep; masked-token prediction, ESM). Elaboration on performance is provided over protein fitness, protein size, and sampling techniques. In addition, an ensemble of protein representation methods is generated to discover the contribution of distinct representations and improve the final prediction score. We then implement multiple criteria decision analysis (MCDA; TOPSIS with entropy weighting), using multiple metrics well-suited for imbalanced data, to ensure statistical rigor in ranking our methods. Within the context of these datasets, the synthetic minority oversampling technique (SMOTE) outperformed undersampling while encoding sequences with One-Hot, UniRep, and ESM representations. Moreover, ensemble learning increased the predictive performance of the affinity-based dataset by 4% compared to the best single-encoding candidate (F1-score = 97%), while ESM alone was rigorous enough in stability prediction (F1-score = 92%).", 
    "abstract": "Anticipating and understanding cancers' need for specific gene activities is key for novel therapeutic development. Here we utilized DepMap, a cancer gene dependency screen, to demonstrate that machine learning combined with network biology can produce robust algorithms that both predict what genes a cancer is dependent on and what network features coordinate such gene dependencies. Using network topology and biological annotations, we constructed four groups of novel engineered machine learning features that produced high accuracies when predicting binary gene dependencies. We found that in all examined cancer types, F1 scores were greater than 0.90, and model accuracy remained robust under multiple hyperparameter tests. We then deconstructed these models to identify tumor type-specific coordinators of gene dependency and identified that in certain cancers, such as thyroid and kidney, tumors' dependencies are highly predicted by gene connectivity. In contrast, other histologies relied on pathway-based features such as lung, where gene dependencies were highly predictive by associations with cell death pathway genes. In sum, we show that biologically informed network features can be a valuable and robust addition to predictive pharmacology models while simultaneously providing mechanistic insights.", 
    "abstract": "In bioequivalence, the maximum plasma concentration (Cmax) is traditionally used as a metric for the absorption rate, despite the fact that there are several concerns. The idea of \"average slope\" (AS) was recently introduced as an alternative metric to reflect absorption rate. This study aims to further extend the previous findings and apply an in silico approach to investigate the kinetic sensitivity of AS and Cmax. This computational analysis was applied to the C-t data of hydrochlorothiazide, donepezil, and amlodipine, which exhibit different absorption kinetics. Principal component analysis (PCA) was applied to uncover the relationships between all bioequivalence metrics. Monte Carlo simulations of bioequivalence trials were performed to investigate sensitivity. The appropriate programming codes were written in Python for the PCA and in MATLAB", 
    "abstract": "In this work, we implemented an approximate algorithm for calculating nonadiabatic coupling matrix elements (NACMEs) of a polyatomic system with ab initio methods and machine learning (ML) models. Utilizing this algorithm, one can calculate NACMEs using only the information of potential energy surfaces (PESs), i.e., energies, and gradients as well as Hessian matrix elements. We used a realistic system, namely CH", 
    "abstract": "The emergence of multiresistant bacteria and the shortage of antibacterials in the drug pipeline creates the need to search for novel agents. Evolution drives the optimization of the structure of marine natural products to act as antibacterial agents. Polyketides are a vast and structurally diverse family of compounds that have been isolated from different marine microorganisms. Within the different polyketides, benzophenones, diphenyl ethers, anthraquinones, and xanthones have shown promising antibacterial activity. In this work, a dataset of 246 marine polyketides has been identified. In order to characterize the chemical space occupied by these marine polyketides, molecular descriptors and fingerprints were calculated. Molecular descriptors were analyzed according to the scaffold, and principal component analysis was performed to identify the relationships among the different descriptors. Generally, the identified marine polyketides are unsaturated, water-insoluble compounds. Among the different polyketides, diphenyl ethers tend to be more lipophilic and non-polar than the remaining classes. Molecular fingerprints were used to group the polyketides according to their molecular similarity into clusters. A total of 76 clusters were obtained, with a loose threshold for the Butina clustering algorithm, highlighting the large structural diversity of the marine polyketides. The large structural diversity was also evidenced by the visualization trees map assembled using the tree map (TMAP) unsupervised machine-learning method. The available antibacterial activity data were examined in terms of bacterial strains, and the activity data were used to rank the compounds according to their antibacterial potential. This potential ranking was used to identify the most promising compounds (four compounds) which can inspire the development of new structural analogs with better potency and absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties.", 
    "abstract": "Ion mobility-mass spectrometry (IM-MS) is a powerful separation technique providing an additional dimension of separation to support the enhanced separation and characterization of complex components from the tissue metabolome and medicinal herbs. The integration of machine learning (ML) with IM-MS can overcome the barrier to the lack of reference standards, promoting the creation of a large number of proprietary collision cross section (CCS) databases, which help to achieve the rapid, comprehensive, and accurate characterization of the contained chemical components. In this review, advances in CCS prediction using ML in the past 2 decades are summarized. The advantages of ion mobility-mass spectrometers and the commercially available ion mobility technologies with different principles (e.g., time dispersive, confinement and selective release, and space dispersive) are introduced and compared. The general procedures involved in CCS prediction based on ML (acquisition and optimization of the independent and dependent variables, model construction and evaluation, etc.) are highlighted. In addition, quantum chemistry, molecular dynamics, and CCS theoretical calculations are also described. Finally, the applications of CCS prediction in metabolomics, natural products, foods, and the other research fields are reflected.", 
    "abstract": "Machine learning has achieved remarkable success across a broad range of scientific and engineering disciplines, particularly its use for predicting native protein structures from sequence information alone. However, biomolecules are inherently dynamic, and there is a pressing need for accurate predictions of dynamic structural ensembles across multiple functional levels. These problems range from the relatively well-defined task of predicting conformational dynamics around the native state of a protein, which traditional molecular dynamics (MD) simulations are particularly adept at handling, to generating large-scale conformational transitions connecting distinct functional states of structured proteins or numerous marginally stable states within the dynamic ensembles of intrinsically disordered proteins. Machine learning has been increasingly applied to learn low-dimensional representations of protein conformational spaces, which can then be used to drive additional MD sampling or directly generate novel conformations. These methods promise to greatly reduce the computational cost of generating dynamic protein ensembles, compared to traditional MD simulations. In this review, we examine recent progress in machine learning approaches towards generative modeling of dynamic protein ensembles and emphasize the crucial importance of integrating advances in machine learning, structural data, and physical principles to achieve these ambitious goals.", 
    "abstract": "Nowadays, machining products, especially by turning methods, are more and more popular and require high-quality. With the development of science and technology, especially numerical computing technology and control technology, the application of these technological achievements to improve productivity and product quality has become increasingly essential. This study applies a simulation method considering the affecting factors of the vibration of the tool and the surface quality of the workpiece during turning. The study simulated and analyzed the characteristics of the cutting force and oscillation of the toolholder when stabilizing; at the same time, the study also simulated the behavior of the toolholder under the effect of cutting force and determined the finished surface quality through simulation. Additionally, the study utilized a machine learning model to examine the relationship between the toolholder length, cutting speed, feed rate, wavelength and surface roughness. The study found that tool hardness is the most crucial factor, and if the toolholder length exceeds the critical length, it leads to a rapid increase in roughness. In this study, the critical toolholder length was determined to be 60 mm, and this resulted in a corresponding surface roughness (Rz) of approximately 20 \u00b5m.", 
    "abstract": "Microfluidics attracts much attention due to its multiple advantages such as high throughput, rapid analysis, low sample volume, and high sensitivity. Microfluidics has profoundly influenced many fields including chemistry, biology, medicine, information technology, and other disciplines. However, some stumbling stones (miniaturization, integration, and intelligence) strain the development of industrialization and commercialization of microchips. The miniaturization of microfluidics means fewer samples and reagents, shorter times to results, and less footprint space consumption, enabling a high throughput and parallelism of sample analysis. Additionally, micro-size channels tend to produce laminar flow, which probably permits some creative applications that are not accessible to traditional fluid-processing platforms. The reasonable integration of biomedical/physical biosensors, semiconductor microelectronics, communications, and other cutting-edge technologies should greatly expand the applications of current microfluidic devices and help develop the next generation of lab-on-a-chip (LOC). At the same time, the evolution of artificial intelligence also gives another strong impetus to the rapid development of microfluidics. Biomedical applications based on microfluidics normally bring a large amount of complex data, so it is a big challenge for researchers and technicians to analyze those huge and complicated data accurately and quickly. To address this problem, machine learning is viewed as an indispensable and powerful tool in processing the data collected from micro-devices. In this review, we mainly focus on discussing the integration, miniaturization, portability, and intelligence of microfluidics technology.", 
    "abstract": "Gesture recognition has found widespread applications in various fields, such as virtual reality, medical diagnosis, and robot interaction. The existing mainstream gesture-recognition methods are primarily divided into two categories: inertial-sensor-based and camera-vision-based methods. However, optical detection still has limitations such as reflection and occlusion. In this paper, we investigate static and dynamic gesture-recognition methods based on miniature inertial sensors. Hand-gesture data are obtained through a data glove and preprocessed using Butterworth low-pass filtering and normalization algorithms. Magnetometer correction is performed using ellipsoidal fitting methods. An auxiliary segmentation algorithm is employed to segment the gesture data, and a gesture dataset is constructed. For static gesture recognition, we focus on four machine learning algorithms, namely support vector machine (SVM), backpropagation neural network (BP), decision tree (DT), and random forest (RF). We evaluate the model prediction performance through cross-validation comparison. For dynamic gesture recognition, we investigate the recognition of 10 dynamic gestures using Hidden Markov Models (HMM) and Attention-Biased Mechanisms for Bidirectional Long- and Short-Term Memory Neural Network Models (Attention-BiLSTM). We analyze the differences in accuracy for complex dynamic gesture recognition with different feature datasets and compare them with the prediction results of the traditional long- and short-term memory neural network model (LSTM). Experimental results demonstrate that the random forest algorithm achieves the highest recognition accuracy and shortest recognition time for static gestures. Moreover, the addition of the attention mechanism significantly improves the recognition accuracy of the LSTM model for dynamic gestures, with a prediction accuracy of 98.3%, based on the original six-axis dataset.", 
    "abstract": "Flash memory-based computing-in-memory (CIM) architectures have gained popularity due to their remarkable performance in various computation tasks of data processing, including machine learning, neuron networks, and scientific calculations. Especially in the partial differential equation (PDE) solver that has been widely utilized in scientific calculations, high accuracy, processing speed, and low power consumption are the key requirements. This work proposes a novel flash memory-based PDE solver to implement PDE with high accuracy, low power consumption, and fast iterative convergence. Moreover, considering the increasing current noise in nanoscale devices, we investigate the robustness against the noise in the proposed PDE solver. The results show that the noise tolerance limit of the solver can reach more than five times that of the conventional Jacobi CIM solver. Overall, the proposed flash memory-based PDE solver offers a promising solution for scientific calculations that require high accuracy, low power consumption, and good noise immunity, which could help to develop flash-based general computing.", 
    "abstract": "The leaching of minerals is one of the main unit operations in the metal dissolution process, and in turn it is a process that generates fewer environmental liabilities compared to pyrometallurgical processes. As an alternative to conventional leaching methods, the use of microorganisms in mineral treatment processes has become widespread in recent decades, due to advantages such as the non-production of emissions or pollution, energy savings, low process costs, products compatible with the environment, and increases in the benefit of low-grade mining deposits. The purpose of this work is to introduce the theoretical foundations associated with modeling the process of bioleaching, mainly the modeling of mineral recovery rates. The different models are collected from models based on conventional leaching dynamics modeling, based on the shrinking core model, where the oxidation process is controlled by diffusion, chemically, or by film diffusion until bioleaching models based on statistical analysis are presented, such as the surface response methodology or the application of machine learning algorithms. Although bioleaching modeling (independent of modeling techniques) of industrial (or large-scale mined) minerals is a fairly developed area, bioleaching modeling applied to rare earth elements is a field with great growth potential in the coming years, as in general bioleaching has the potential to be a more sustainable and environmentally friendly mining method than traditional mining methods.", 
    "abstract": "The accurate estimation of rock strength is an essential task in almost all rock-based projects, such as tunnelling and excavation. Numerous efforts to create indirect techniques for calculating unconfined compressive strength (UCS) have been attempted. This is often due to the complexity of collecting and completing the abovementioned lab tests. This study applied two advanced machine learning techniques, including the extreme gradient boosting trees and random forest, for predicting the UCS based on non-destructive tests and petrographic studies. Before applying these models, a feature selection was conducted using a Pearson's Chi-Square test. This technique selected the following inputs for the development of the gradient boosting tree (XGBT) and random forest (RF) models: dry density and ultrasonic velocity as non-destructive tests, and mica, quartz, and plagioclase as petrographic results. In addition to XGBT and RF models, some empirical equations and two single decision trees (DTs) were developed to predict UCS values. The results of this study showed that the XGBT model outperforms the RF for UCS prediction in terms of both system accuracy and error. The linear correlation of XGBT was 0.994, and its mean absolute error was 0.113. In addition, the XGBT model outperformed single DTs and empirical equations. The XGBT and RF models also outperformed KNN (R = 0.708), ANN (R = 0.625), and SVM (R = 0.816) models. The findings of this study imply that the XGBT and RF can be employed efficiently for predicting the UCS values.", 
    "abstract": "", 
    "abstract": "", 
    "abstract": "", 
    "abstract": "The emergency department (ED) is often overburdened, due to the high influx of patients and limited availability of attending physicians. This situation highlights the need for improvement in the management of, and assistance provided in the ED. A key point for this purpose is the identification of patients with the highest risk, which can be achieved using machine learning predictive models. The objective of this study is to conduct a systematic review of predictive models used to detect ward admissions from the ED. The main targets of this review are the best predictive algorithms, their predictive capacity, the studies' quality, and the predictor variables.\nThis review is based on PRISMA methodology. The information has been searched in PubMed, Scopus and Google Scholar databases. Quality assessment has been performed using the QUIPS tool.\nThrough the advanced search, a total of 367 articles were found, of which 14 were of interest that met the inclusion criteria. Logistic regression is the most used predictive model, achieving AUC values between 0.75-0.92. The two most used variables are the age and ED triage category.\nartificial intelligence models can contribute to improving the quality of care in the ED and reducing the burden on healthcare systems.", 
    "abstract": "to predict vestibular schwannoma (VS) response to radiosurgery by applying machine learning (ML) algorithms on radiomic features extracted from pre-treatment magnetic resonance (MR) images.\npatients with VS treated with radiosurgery in two Centers from 2004 to 2016 were retrospectively evaluated. Brain T1-weighted contrast-enhanced MR images were acquired before and at 24 and 36 months after treatment. Clinical and treatment data were collected contextually. Treatment responses were assessed considering the VS volume variation based on pre- and post-radiosurgery MR images at both time points. Tumors were semi-automatically segmented and radiomic features were extracted. Four ML algorithms (Random Forest, Support Vector Machine, Neural Network, and extreme Gradient Boosting) were trained and tested for treatment response (i.e., increased or non-increased tumor volume) using nested cross-validation. For training, feature selection was performed using the Least Absolute Shrinkage and Selection Operator, and the selected features were used as input to separately build the four ML classification algorithms. To overcome class imbalance during training, Synthetic Minority Oversampling Technique was used. Finally, trained models were tested on the corresponding held out set of patients to evaluate balanced accuracy, sensitivity, and specificity.\n108 patients treated with Cyberknife\nradiomics may predict VS response to radiosurgery avoiding long-term follow-up as well as unnecessary treatment.", 
    "abstract": "(1) Background: The therapeutic mechanisms underlying psychotherapeutic interventions for individuals with treatment-resistant schizophrenia are mostly unknown. One of these treatment techniques is avatar therapy (AT), in which the patient engages in immersive sessions while interacting with an avatar representing their primary persistent auditory verbal hallucination. The aim of this study was to conduct an unsupervised machine-learning analysis of verbatims of treatment-resistant schizophrenia patients that have followed AT. The second aim of the study was to compare the data clusters obtained from the unsupervised machine-learning analysis with previously conducted qualitative analysis. (2) Methods: A k-means algorithm was performed over the immersive-session verbatims of 18 patients suffering from treatment-resistant schizophrenia who followed AT to cluster interactions of the avatar and the patient. Data were pre-processed using vectorization and data reduction. (3): Results: Three clusters of interactions were identified for the avatar's interactions whereas four clusters were identified for the patient's interactions. (4) Conclusion: This study was the first attempt to conduct unsupervised machine learning on AT and provided a quantitative insight into the inner interactions that take place during immersive sessions. The use of unsupervised machine learning could yield a better understanding of the type of interactions that take place in AT and their clinical implications.", 
    "abstract": "Proteomics instrumentation and the corresponding bioinformatics tools have evolved at a rapid pace in the last 20 years, whereas the exploitation of deep learning techniques in proteomics is on the horizon. The ability to revisit proteomics raw data, in particular, could be a valuable resource for machine learning applications seeking new insight into protein expression and functions of previously acquired data from different instruments under various lab conditions. We map publicly available proteomics repositories (such as ProteomeXchange) and relevant publications to extract MS/MS data to form one large database that contains the patient history and mass spectrometric data acquired for the patient sample. The extracted mapped dataset should enable the research to overcome the issues attached to the dispersions of proteomics data on the internet, which makes it difficult to apply emerging new bioinformatics tools and deep learning algorithms. The workflow proposed in this study enables a linked large dataset of heart-related proteomics data, which could be easily and efficiently applied to machine learning and deep learning algorithms for futuristic predictions of heart diseases and modeling. Data scraping and crawling offer a powerful tool to harvest and prepare the training and test datasets; however, the authors advocate caution because of ethical and legal issues, as well as the need to ensure the quality and accuracy of the data that are being collected.", 
    "abstract": "Clear epigenetic signatures were found in hypertensive and pre-hypertensive patients using DNA methylation data and neural networks in a classification algorithm. It is shown how by selecting an appropriate subset of CpGs it is possible to achieve a mean accuracy classification of 86% for distinguishing control and hypertensive (and pre-hypertensive) patients using only 2239 CpGs. Furthermore, it is also possible to obtain a statistically comparable model achieving an 83% mean accuracy using only 22 CpGs. Both of these approaches represent a substantial improvement over using the entire amount of available CpGs, which resulted in the neural network not generating accurate classifications. An optimization approach is followed to select the CpGs to be used as the base for a model distinguishing between hypertensive and pre-hypertensive individuals. It is shown that it is possible to find methylation signatures using machine learning techniques, which can be applied to distinguish between control (healthy) individuals, pre-hypertensive individuals and hypertensive individuals, illustrating an associated epigenetic impact. Identifying epigenetic signatures might lead to more targeted treatments for patients in the future.", 
    "abstract": "Heart failure with preserved ejection fraction (HFpEF) is a heterogeneous clinical syndrome with multiple underlying mechanisms and comorbidities that leads to a variety of clinical phenotypes. The identification and characterization of these phenotypes are essential for better understanding the precise pathophysiology of HFpEF, identifying appropriate treatment strategies, and improving patient outcomes. Despite accumulating data showing the potentiality of artificial intelligence (AI)-based phenotyping using clinical, biomarker, and imaging information from multiple dimensions in HFpEF management, contemporary guidelines and consensus do not incorporate these in daily practice. In the future, further studies are required to authenticate and substantiate these findings in order to establish a more standardized approach for clinical implementation.", 
    "abstract": "preoperative risk assessment of gastrointestinal stromal tumors (GISTS) is required for optimal and personalized treatment planning. Radiomics features are promising tools to predict risk assessment. The purpose of this study is to develop and validate an artificial intelligence classification algorithm, based on CT features, to define GIST's prognosis as determined by the Miettinen classification.\npatients with histological diagnosis of GIST and CT studies were retrospectively enrolled. Eight morphologic and 30 texture CT features were extracted from each tumor and combined to obtain three models (morphologic, texture and combined). Data were analyzed using a machine learning classification (WEKA). For each classification process, sensitivity, specificity, accuracy and area under the curve were evaluated. Inter- and intra-reader agreement were also calculated.\n52 patients were evaluated. In the validation population, highest performances were obtained by the combined model (SE 85.7%, SP 90.9%, ACC 88.8%, and AUC 0.954) followed by the morphologic (SE 66.6%, SP 81.8%, ACC 76.4%, and AUC 0.742) and texture (SE 50%, SP 72.7%, ACC 64.7%, and AUC 0.613) models. Reproducibility was high of all manual evaluations.\nthe AI-based radiomics model using a CT feature demonstrates good predictive performance for preoperative risk stratification of GISTs.", 
    "abstract": "The concept of chronic kidney disease (CKD) originated in the 2000s, and an estimated 850 million patients are currently suffering from health threats from different degrees of CKD. However, it is unclear whether the existing CKD care systems are optimal for improving patient prognosis and outcomes, so this review summarizes the burden, existing care models, effectiveness, challenges, and developments of CKD care. Even under the general care principles, there are still significant gaps in our understanding of the causes of CKD, prevention or care resources, and care burdens between countries worldwide. Receiving care from multidisciplinary teams rather than only a nephrologist shows potential profits in comprehensive and preferable outcomes. In addition, we propose a novel CKD care structure that combines modern technologies, biosensors, longitudinal data visualization, machine learning algorithms, and mobile care. The novel care structure could simultaneously change the care process, significantly reduce human contact, and make the vulnerable population less likely to be exposed to infectious diseases such as COVID-19. The information offered should be beneficial, allowing us to rethink future CKD care models and applications to reach the goals of health equality and sustainability.", 
    "abstract": "Hallux valgus, a frequently seen foot deformity, requires early detection to prevent it from becoming more severe. It is a medical economic problem, so a means of quickly distinguishing it would be helpful. We designed and investigated the accuracy of an early version of a tool for screening hallux valgus using machine learning. The tool would ascertain whether patients had hallux valgus by analyzing pictures of their feet. In this study, 507 images of feet were used for machine learning. Image preprocessing was conducted using the comparatively simple pattern A (rescaling, angle adjustment, and trimming) and slightly more complicated pattern B (same, plus vertical flip, binary formatting, and edge emphasis). This study used the VGG16 convolutional neural network. Pattern B machine learning was more accurate than pattern A. In our early model, Pattern A achieved 0.62 for accuracy, 0.56 for precision, 0.94 for recall, and 0.71 for F1 score. As for Pattern B, the scores were 0.79, 0.77, 0.96, and 0.86, respectively. Machine learning was sufficiently accurate to distinguish foot images between feet with hallux valgus and normal feet. With further refinement, this tool could be used for the easy screening of hallux valgus.", 
    "abstract": "The efficacy of electroconvulsive therapy (ECT) in the treatment of adolescents with treatment-refractory depression is still unsatisfactory, and the individual differences are large. It is not clear which factors are related to the treatment effect. Resting-state fMRI may be a good tool to predict the clinical efficacy of this treatment, and it is helpful to identify the most suitable population for this treatment.\nForty treatment-refractory depression adolescents were treated by ECT and evaluated using HAMD and BSSI scores before and after treatment, and were then divided into a treatment response group and a non-treatment group according to the reduction rate of the HAMD scale. We extracted the ALFF, fALFF, ReHo, and functional connectivity of patients as predicted features after a two-sample \nTwenty-seven patients achieved a clinical response; symptoms of depression and suicidal ideation were significantly improved after treatment with ECT, which was reflected in a significant decrease in the scores of HAMD and BSSI (\nThe local brain function in the insula, superior parietal gyrus, and angular gyrus as well as characteristic changes in the functional connectivity of cortical-limbic circuits may serve as potential markers for efficacy judgment of ECT and help to provide optimized individual treatment strategies for adolescents with depression and suicidal ideation in the early stages of treatment.", 
    "abstract": "Near-infrared (NIR) imaging with indocyanine green (ICG) has proven to be useful in general, visceral, and transplant surgery. However, most studies have performed only qualitative assessments. Therefore, a systematic overview of all studies performing quantitative indocyanine green evaluation in general, visceral, and transplant surgeries should be conducted. Free term and medical subject heading (MeSH) term searches were performed in the Medline and Cochrane databases until October 2022. The main categories of ICG quantification were esophageal surgery (24.6%), reconstructive surgery (24.6%), and colorectal surgery (21.3%). Concordantly, anastomotic leak (41%) was the main endpoint, followed by the assessment of flap perfusion (23%) and the identification of structures and organs (14.8%). Most studies examined open surgery (67.6%) or laparoscopic surgery (23.1%). The analysis was mainly carried out using manufacturer software (44.3%) and open-source software (15.6%). The most frequently analyzed parameter was intensity over time for blood flow assessment, followed by intensity alone or intensity-to-background ratios for structure and organ identification. Intraoperative ICG quantification could become more important with the increasing impact of robotic surgery and machine learning algorithms for image and video analysis.", 
    "abstract": "(1) In the present study, we used data comprising patient medical histories from a panel of primary care practices in Germany to predict post-COVID-19 conditions in patients after COVID-19 diagnosis and to evaluate the relevant factors associated with these conditions using machine learning methods. (2) Methods: Data retrieved from the IQVIA", 
    "abstract": "In 2016, the SOFA score was proposed as the main evaluation system for diagnosis in the definition of sepsis 3.0, and the SOFA score has become a new research focus in sepsis. Some people are skeptical about diagnosing sepsis using the SOFA score. Experts and scholars from different regions have proposed different, modified versions of SOFA score to make up for the related problems with the use of the SOFA score in the diagnosis of sepsis. While synthesizing the different improved versions of SOFA proposed by experts and scholars in various regions, this paper also summarizes the relevant definitions of sepsis put forward in recent years to build a clear, improved application framework of SOFA score. In addition, the comparison between machine learning and SOFA scores related to sepsis is described and discussed in the article. Taken together, by summarizing the application of the improved SOFA score proposed in recent years in the related definition of sepsis, we believe that the SOFA score is still an effective means of diagnosing sepsis, but in the process of the continuous refinement and development of sepsis in the future, the SOFA score needs to be further refined and improved to provide more accurate coping strategies for different patient populations or application directions regarding sepsis. Against the big data background, machine learning has immeasurable value and significance, but its future applications should add more humanistic references and assistance.", 
    "abstract": "(1) Background: The probability of technical success in percutaneous coronary intervention (PCI) for chronic total occlusion (CTO) represents essential information for specifying the priority of PCI for treatment selection in patients with CTO. However, the predictabilities of existing scores based on conventional regression analysis remain modest, leaving room for improvements in model discrimination. Recently, machine learning (ML) techniques have emerged as highly effective methods for prediction and decision-making in various disciplines. We therefore investigated the predictability of ML models for technical results of CTO-PCI and compared their performances to the results from existing scores, including J-CTO, CL, and CASTLE scores. (2) Methods: This analysis used data from the Japanese CTO-PCI expert registry, which enrolled 8760 consecutive patients undergoing CTO-PCI. The performance of prediction models was assessed using the area under the receiver operating curve (ROC-AUC). (3) Results: Technical success was achieved in 7990 procedures, accounting for an overall success rate of 91.2%. The best ML model, extreme gradient boosting (XGBoost), outperformed the conventional prediction scores with ROC-AUC (XGBoost 0.760 [95% confidence interval {CI}: 0.740-0.780] vs. J-CTO 0.697 [95%CI: 0.675-0.719], CL 0.662 [95%CI: 0.639-0.684], CASTLE 0.659 [95%CI: 0.636-0.681]; ", 
    "abstract": "Perineural invasion is a prevalent pathological finding in head and neck squamous cell carcinoma and a risk factor for unfavorable survival. An adequate diagnosis of perineural invasion by pathologic examination is limited due to the availability of tumor samples from surgical resection, which can arise in cases of definitive nonsurgical treatment. To address this medical need, we established a random forest prediction model for the risk assessment of perineural invasion, including occult perineural invasion, and characterized distinct cellular and molecular features based on our new and extended classification. RNA sequencing data of head and neck squamous cell carcinoma from The Cancer Genome Atlas were used as a training cohort to identify differentially expressed genes that are associated with perineural invasion. A random forest classification model was established based on these differentially expressed genes and was validated by inspection of H&E-stained whole image slides. Differences in epigenetic regulation and the mutational landscape were detected by an integrative analysis of multiomics data and single-cell RNA-sequencing data were analyzed. We identified a 44-gene expression signature related to perineural invasion and enriched for genes mainly expressed in cancer cells according to single-cell RNA-sequencing data. A machine learning model was trained based on the expression pattern of the 44-gene set with the unique feature to predict occult perineural invasion. This extended classification model enabled a more accurate analysis of alterations in the mutational landscape and epigenetic regulation by DNA methylation as well as quantitative and qualitative differences in the cellular composition in the tumor microenvironment between head and neck squamous cell carcinoma with or without perineural invasion. In conclusion, the newly established model could not only complement histopathologic examination as an additional diagnostic tool but also guide the identification of new drug targets for therapeutic intervention in future clinical trials with head and neck squamous cell carcinoma patients at a higher risk for treatment failure due to perineural invasion.", 
    "abstract": "Non-coding RNA (ncRNA) classes take over important housekeeping and regulatory functions and are quite heterogeneous in terms of length, sequence conservation and secondary structure. High-throughput sequencing reveals that the expressed novel ncRNAs and their classification are important to understand cell regulation and identify potential diagnostic and therapeutic biomarkers. To improve the classification of ncRNAs, we investigated different approaches of utilizing primary sequences and secondary structures as well as the late integration of both using machine learning models, including different neural network architectures. As input, we used the newest version of RNAcentral, focusing on six ncRNA classes, including lncRNA, rRNA, tRNA, miRNA, snRNA and snoRNA. The late integration of graph-encoded structural features and primary sequences in our ", 
    "abstract": "Non-enzymatic thiol addition into the \u03b1,\u03b2-unsaturated carbonyl system is associated with several biological effects. In vivo, the reactions can form small-molecule thiol (e.g., glutathione) or protein thiol adducts. The reaction of two synthetic (4'-methyl- and 4'-methoxy substituted) cyclic chalcone analogs with reduced glutathione (GSH) and ", 
    "abstract": "This study's relevance lies in the need to assess the role of socioeconomic, medical, and demographic factors on working-age population mortality in Russia. The purpose of this study is to substantiate the methodological tools for the assessment of the partial contribution of the most important factors that determine the dynamics of the mortality of the working-age population. Our hypothesis is that the factors determining the socioeconomic situation in the country affect the level and dynamics of mortality of the working-age population, but to a different extent in each separate period. To analyse the impact of the factors, we used official Rosstat data for the period from 2005 to 2021. We used the data that reflect the dynamics of socioeconomic and demographic indicators, including the dynamics of mortality of the working-age population in Russia as a whole and in its 85 regions. First, we selected 52 indicators of socioeconomic development and then grouped them into four factor blocks (working conditions, health care, life security, living standards). To reduce the level of statistical noise, we carried out a correlation analysis, which allowed us to narrow down the list to 15 key indicators with the strongest association with the mortality rate of the working-age population. The total period of 2005-2021 was divided into five segments of 3-4 years each, characterising the picture of the socioeconomic state of the country during the period under consideration. The socioeconomic approach used in the study made it possible to assess the extent to which the mortality rate was influenced by the indicators adopted for analysis. The results of this study show that over the whole period, life security (48%) and working conditions (29%) contributed most to the level and dynamics of mortality in the working-age population, while factors determining living standards and the state of the healthcare system accounted for much smaller shares (14% and 9%, respectively). The methodological apparatus of this study is based on the application of methods of machine learning and intelligent data analysis, which allowed us to identify the main factors and their share in the total influence on the mortality rate of the working-age population. The results of this study show the need to monitor the impact of socioeconomic factors on the dynamics and mortality rate of the working-age population in order to improve the effectiveness of social programme. When developing and adjusting government programmes to reduce mortality in the working-age population, the degree of influence of these factors should be taken into account.", 
    "abstract": "Loneliness is an issue of public health significance. Longitudinal studies indicate that feelings of loneliness are prevalent and were exacerbated by the Coronavirus Disease 2019 (COVID-19) pandemic. With the advent of new media, more people are turning to social media platforms such as Twitter and Reddit as well as online forums, e.g., loneliness forums, to seek advice and solace regarding their health and well-being. The present study therefore aimed to investigate the public messaging on loneliness via an unsupervised machine learning analysis of posts made by organisations on Twitter. We specifically examined tweets put out by organisations (companies, agencies or common interest groups) as the public may view them as more credible information as opposed to individual opinions. A total of 68,345 unique tweets in English were posted by organisations on Twitter from 1 January 2012 to 1 September 2022. These tweets were extracted and analysed using unsupervised machine learning approaches. BERTopic, a topic modelling technique that leverages state-of-the-art natural language processing, was applied to generate interpretable topics around the public messaging of loneliness and highlight the key words in the topic descriptions. The topics and topic labels were then reviewed independently by all study investigators for thematic analysis. Four key themes were uncovered, namely, the experience of loneliness, people who experience loneliness, what exacerbates loneliness and what could alleviate loneliness. Notably, a significant proportion of the tweets centred on the impact of the COVID-19 pandemic on loneliness. While current online interactions are largely descriptive of the complex and multifaceted problem of loneliness, more targeted prosocial messaging appears to be lacking to combat the causes of loneliness brought up in public messaging.", 
    "abstract": "Since 2016, there has been a substantial rise in e-cigarette (vaping) dependence among young people. In this prospective cohort study, we aimed to identify the different predictors of vaping dependence over 3 months among adolescents who were baseline daily and non-daily vapers. We recruited ever-vaping Canadian residents aged 16-25 years on social media platforms and asked them to complete a baseline survey in November 2020. A validated vaping dependence score (0-23) summing up their responses to nine questions was calculated at the 3-month follow-up survey. Separate lasso regression models were developed to identify predictors of higher 3-month vaping dependence score among baseline daily and non-daily vapers. Of the 1172 participants, 643 (54.9%) were daily vapers with a mean age of 19.6 \u00b1 2.6 years and 76.4% (", 
    "abstract": "The objective of the study is to explore the factors that influence the ", 
    "abstract": "To solve the research-practice gap and take one step forward toward using big data with real-world evidence, the present study aims to adopt a novel method using machine learning to pool findings from meta-analyses and predict the change of countermovement jump. The data were collected through a total of 124 individual studies included in 16 recent meta-analyses. The performance of four selected machine learning algorithms including support vector machine, random forest (RF) ensemble, light gradient boosted machine, and the neural network using multi-layer perceptron was compared. The RF yielded the highest accuracy (mean absolute error: 0.071 cm; R", 
    "abstract": "The most common cause of mortality and disability globally right now is cholangiocarcinoma, one of the worst forms of cancer that may affect people. When cholangiocarcinoma develops, the DNA of the bile duct cells is altered. Cholangiocarcinoma claims the lives of about 7000 individuals annually. Women pass away less often than men. Asians have the greatest fatality rate. Following Whites (20%) and Asians (22%), African Americans (45%) saw the greatest increase in cholangiocarcinoma mortality between 2021 and 2022. For instance, 60-70% of cholangiocarcinoma patients have local infiltration or distant metastases, which makes them unable to receive a curative surgical procedure. Across the board, the median survival time is less than a year. Many researchers work hard to detect cholangiocarcinoma, but this is after the appearance of symptoms, which is late detection. If cholangiocarcinoma progression is detected at an earlier stage, then it will help doctors and patients in treatment. Therefore, an ensemble deep learning model (EDLM), which consists of three deep learning algorithms-long short-term model (LSTM), gated recurrent units (GRUs), and bi-directional LSTM (BLSTM)-is developed for the early identification of cholangiocarcinoma. Several tests are presented, such as a 10-fold cross-validation test (10-FCVT), an independent set test (IST), and a self-consistency test (SCT). Several statistical techniques are used to evaluate the proposed model, such as accuracy (Acc), sensitivity (Sn), specificity (Sp), and Matthew's correlation coefficient (MCC). There are 672 mutations in 45 distinct cholangiocarcinoma genes among the 516 human samples included in the proposed study. The IST has the highest Acc at 98%, outperforming all other validation approaches.", 
    "abstract": "The rapidly evolving high-throughput sequencing (HTS) technologies generate voluminous genomic and metagenomic sequences, which can help classify the microbial communities with high accuracy in many ecosystems. Conventionally, the rule-based binning techniques are used to classify the contigs or scaffolds based on either sequence composition or sequence similarity. However, the accurate classification of the microbial communities remains a major challenge due to massive data volumes at hand as well as a requirement of efficient binning methods and classification algorithms. Therefore, we attempted here to implement iterative K-Means clustering for the initial binning of metagenomics sequences and applied various machine learning algorithms (MLAs) to classify the newly identified unknown microbes. The cluster annotation was achieved through the BLAST program of NCBI, which resulted in the grouping of assembled scaffolds into five classes, i.e., bacteria, archaea, eukaryota, viruses and others. The annotated cluster sequences were used to train machine learning algorithms (MLAs) to develop prediction models to classify unknown metagenomic sequences. In this study, we used metagenomic datasets of samples collected from the Ganga (Kanpur and Farakka) and the Yamuna (Delhi) rivers in India for clustering and training the MLA models. Further, the performance of MLAs was evaluated by 10-fold cross validation. The results revealed that the developed model based on the Random Forest had a superior performance compared to the other considered learning algorithms. The proposed method can be used for annotating the metagenomic scaffolds/contigs being complementary to existing methods of metagenomic data analysis. An offline predictor source code with the best prediction model is available at (https://github.com/Nalinikanta7/metagenomics).", 
    "abstract": "(1) ", 
    "abstract": "Genomic selection (GS) is revolutionizing plant breeding. However, because it is a predictive methodology, a basic understanding of statistical machine-learning methods is necessary for its successful implementation. This methodology uses a reference population that contains both the phenotypic and genotypic information of genotypes to train a statistical machine-learning method. After optimization, this method is used to make predictions of candidate lines for which only genotypic information is available. However, due to a lack of time and appropriate training, it is difficult for breeders and scientists of related fields to learn all the fundamentals of prediction algorithms. With smart or highly automated software, it is possible for these professionals to appropriately implement any state-of-the-art statistical machine-learning method for its collected data without the need for an exhaustive understanding of statistical machine-learning methods and programing. For this reason, we introduce state-of-the-art statistical machine-learning methods using the Sparse Kernel Methods (SKM) R library, with complete guidelines on how to implement seven statistical machine-learning methods that are available in this library for genomic prediction (", 
    "abstract": "Hereditary transthyretin amyloidosis with polyneuropathy (ATTRv) is an adult-onset multisystemic disease, affecting the peripheral nerves, heart, gastrointestinal tract, eyes, and kidneys. Nowadays, several treatment options are available; thus, avoiding misdiagnosis is crucial to starting therapy in early disease stages. However, clinical diagnosis may be difficult, as the disease may present with unspecific symptoms and signs. We hypothesize that the diagnostic process may benefit from the use of machine learning (ML).\n397 patients referring to neuromuscular clinics in 4 centers from the south of Italy with neuropathy and at least 1 more red flag, as well as undergoing genetic testing for ATTRv, were considered. Then, only probands were considered for analysis. Hence, a cohort of 184 patients, 93 with positive and 91 (age- and sex-matched) with negative genetics, was considered for the classification task. The XGBoost (XGB) algorithm was trained to classify positive and negative \ndiabetes, gender, unexplained weight loss, cardiomyopathy, bilateral carpal tunnel syndrome (CTS), ocular symptoms, autonomic symptoms, ataxia, renal dysfunction, lumbar canal stenosis, and history of autoimmunity were used for the model training. The XGB model showed an accuracy of 0.707 \u00b1 0.101, a sensitivity of 0.712 \u00b1 0.147, a specificity of 0.704 \u00b1 0.150, and an AUC-ROC of 0.752 \u00b1 0.107. Using the SHAP explanation, it was confirmed that unexplained weight loss, gastrointestinal symptoms, and cardiomyopathy showed a significant association with the genetic diagnosis of ATTRv, while bilateral CTS, diabetes, autoimmunity, and ocular and renal involvement were associated with a negative genetic test.\nOur data show that ML might potentially be a useful instrument to identify patients with neuropathy that should undergo genetic testing for ATTRv. Unexplained weight loss and cardiomyopathy are relevant red flags in ATTRv in the south of Italy. Further studies are needed to confirm these findings.", 
    "abstract": "We present a framework for electroencephalography (EEG)-based classification between patients with Alzheimer's Disease (AD) and robust normal elderly (RNE) via a graph theory approach using visibility graphs (VGs). This EEG VG approach is motivated by research that has demonstrated differences between patients with early stage AD and RNE using various features of EEG oscillations or cognitive event-related potentials (ERPs). In the present study, EEG signals recorded during a word repetition experiment were wavelet decomposed into 5 sub-bands (\u03b4,\u03b8,\u03b1,\u03b2,\u03b3). The raw and band-specific signals were then converted to VGs for analysis. Twelve graph features were tested for differences between the AD and RNE groups, and ", 
    "abstract": "(1) Background: there is an urgent clinical need for rapid and effective antidepressants. (2) Methods: We employed proteomics to profile proteins in two animal models (", 
    "abstract": "Nonverbal learning disability (NVLD) is a neurodevelopmental disorder characterized by deficits in visuospatial processing but spared verbal competencies. Neurocognitive markers may provide confirmatory evidence for characterizing NVLD as a separate neurodevelopmental disorder. Visuospatial performance and high-density electroencephalography (EEG) were measured in 16 NLVD and in 16 typically developing (TD) children. Cortical source modeling was applied to assess resting-state functional connectivity (rs-FC) in spatial attention networks (dorsal (DAN) and ventral attention networks (VAN)) implicated in visuospatial abilities. A machine-learning approach was applied to investigate whether group membership could be predicted from rs-FC maps and if these connectivity patterns were predictive of visuospatial performance. Graph theoretical measures were applied to nodes inside each network. EEG rs-FC maps in the gamma and beta band differentiated children with and without NVLD, with increased but more diffuse and less efficient functional connections bilaterally in the NVLD group. While rs-FC of the left DAN in the gamma range predicted visuospatial scores for TD children, in the NVLD group rs-FC of the right DAN in the delta range predicted impaired visuospatial performance, confirming that NVLD is a disorder with a predominant dysfunction in right hemisphere connectivity patterns.", 
    "abstract": "As the field of sensor-based rehabilitation continues to expand, it is important to gain a comprehensive understanding of its current research landscape. This study aimed to conduct a bibliometric analysis to identify the most influential authors, institutions, journals, and research areas in this field.\nA search of the Web of Science Core Collection was performed using keywords related to sensor-based rehabilitation in neurological diseases. The search results were analyzed with CiteSpace software using bibliometric techniques, including co-authorship analysis, citation analysis, and keyword co-occurrence analysis.\nBetween 2002 and 2022, 1103 papers were published on the topic, with slow growth from 2002 to 2017, followed by a rapid increase from 2018 to 2022. The United States was the most active country, while the Swiss Federal Institute of Technology had the highest number of publications among institutions. \nThis study provides a comprehensive overview of the current state of sensor-based rehabilitation research in neurological diseases, highlighting the most influential authors, journals, and research themes. The findings can help researchers and practitioners to identify emerging trends and opportunities for collaboration and can inform the development of future research directions in this field.", 
    "abstract": "Viruses infect millions of people worldwide each year, and some can lead to cancer or increase the risk of cancer. As viruses have highly mutable genomes, new viruses may emerge in the future, such as COVID-19 and influenza. Traditional virology relies on predefined rules to identify viruses, but new viruses may be completely or partially divergent from the reference genome, rendering statistical methods and similarity calculations insufficient for all genome sequences. Identifying DNA/RNA-based viral sequences is a crucial step in differentiating different types of lethal pathogens, including their variants and strains. While various tools in bioinformatics can align them, expert biologists are required to interpret the results. Computational virology is a scientific field that studies viruses, their origins, and drug discovery, where machine learning plays a crucial role in extracting domain- and task-specific features to tackle this challenge. This paper proposes a genome analysis system that uses advanced deep learning to identify dozens of viruses. The system uses nucleotide sequences from the NCBI GenBank database and a BERT tokenizer to extract features from the sequences by breaking them down into tokens. We also generated synthetic data for viruses with small sample sizes. The proposed system has two components: a scratch BERT architecture specifically designed for DNA analysis, which is used to learn the next codons unsupervised, and a classifier that identifies important features and understands the relationship between genotype and phenotype. Our system achieved an accuracy of 97.69% in identifying viral sequences.", 
    "abstract": "The rapid detection of chestnut quality is a critical aspect of chestnut processing. However, traditional imaging methods pose a challenge for chestnut-quality detection due to the absence of visible epidermis symptoms. This study aims to develop a quick and efficient detection method using hyperspectral imaging (HSI, 935-1720 nm) and deep learning modeling for qualitative and quantitative identification of chestnut quality. Firstly, we used principal component analysis (PCA) to visualize the qualitative analysis of chestnut quality, followed by the application of three pre-processing methods to the spectra. To compare the accuracy of different models for chestnut-quality detection, traditional machine learning models and deep learning models were constructed. Results showed that deep learning models were more accurate, with FD-LSTM achieving the highest accuracy of 99.72%. Moreover, the study identified important wavelengths for chestnut-quality detection at around 1000, 1400 and 1600 nm, to improve the efficiency of the model. The FD-UVE-CNN model achieved the highest accuracy of 97.33% after incorporating the important wavelength identification process. By using the important wavelengths as input for the deep learning network model, recognition time decreased on average by 39 s. After a comprehensive analysis, FD-UVE-CNN was deter-mined to be the most effective model for chestnut-quality detection. This study suggests that deep learning combined with HSI has potential for chestnut-quality detection, and the results are encouraging.", 
    "abstract": "Fish industry operators have to process fish that arrive at various ", 
    "abstract": "Epigenetics studies heritable or inheritable mechanisms that regulate gene expression rather than altering the DNA sequence. However, no research has investigated the link between TME-related genes (TRGs) and epigenetic-related genes (ERGs) in GC.\nA complete review of genomic data was performed to investigate the relationship between the epigenesis tumor microenvironment (TME) and machine learning algorithms in GC.\nFirstly, TME-related differential expression of genes (DEGs) performed non-negative matrix factorization (NMF) clustering analysis and determined two clusters (C1 and C2). Then, Kaplan-Meier curves for overall survival (OS) and progression-free survival (PFS) rates suggested that cluster C1 predicted a poorer prognosis. The Cox-LASSO regression analysis identified eight hub genes (\nThe study identified some hub genes that could be useful in predicting prognosis and management in GC.", 
    "abstract": "Machine learning-based Network Intrusion Detection Systems (NIDS) are designed to protect networks by identifying anomalous behaviors or improper uses. In recent years, advanced attacks, such as those mimicking legitimate traffic, have been developed to avoid alerting such systems. Previous works mainly focused on improving the anomaly detector itself, whereas in this paper, we introduce a novel method, Test-Time Augmentation for Network Anomaly Detection (TTANAD), which utilizes test-time augmentation to enhance anomaly detection from the data side. TTANAD leverages the temporal characteristics of traffic data and produces temporal test-time augmentations on the monitored traffic data. This method aims to create additional points of view when examining network traffic during inference, making it suitable for a variety of anomaly detector algorithms. Our experimental results demonstrate that TTANAD outperforms the baseline in all benchmark datasets and with all examined anomaly detection algorithms, according to the Area Under the Receiver Operating Characteristic (AUC) metric.", 
    "abstract": "Aftershocks of earthquakes can destroy many urban infrastructures and exacerbate the damage already inflicted upon weak structures. Therefore, it is important to have a method to forecast the probability of occurrence of stronger earthquakes in order to mitigate their effects. In this work, we applied the NESTORE machine learning approach to Greek seismicity from 1995 to 2022 to forecast the probability of a strong aftershock. Depending on the magnitude difference between the mainshock and the strongest aftershock, NESTORE classifies clusters into two types, Type A and Type B. Type A clusters are the most dangerous clusters, characterized by a smaller difference. The algorithm requires region-dependent training as input and evaluates performance on an independent test set. In our tests, we obtained the best results 6 h after the mainshock, as we correctly forecasted 92% of clusters corresponding to 100% of Type A clusters and more than 90% of Type B clusters. These results were also obtained thanks to an accurate analysis of cluster detection in a large part of Greece. The successful overall results show that the algorithm can be applied in this area. The approach is particularly attractive for seismic risk mitigation due to the short time required for forecasting.", 
    "abstract": "Partial Information Decomposition (PID) is a body of work within information theory that allows one to quantify the information that several random variables provide about another random variable, either individually (unique information), redundantly (shared information), or only jointly (synergistic information). This review article aims to provide a survey of some recent and emerging applications of partial information decomposition in algorithmic fairness and explainability, which are of immense importance given the growing use of machine learning in high-stakes applications. For instance, PID, in conjunction with causality, has enabled the disentanglement of the non-exempt disparity which is the part of the overall disparity that is not due to critical job necessities. Similarly, in federated learning, PID has enabled the quantification of tradeoffs between local and global disparities. We introduce a taxonomy that highlights the role of PID in algorithmic fairness and explainability in three main avenues: (i) Quantifying the legally non-exempt disparity for auditing or training; (ii) Explaining contributions of various features or data points; and (iii) Formalizing tradeoffs among different disparities in federated learning. Lastly, we also review techniques for the estimation of PID measures, as well as discuss some challenges and future directions.", 
    "abstract": "Ionic liquids are good candidates as the main component of safe electrolytes for high-energy lithium-ion batteries. The identification of a reliable algorithm to estimate the electrochemical stability of ionic liquids can greatly speed up the discovery of suitable anions able to sustain high potentials. In this work, we critically assess the linear dependence of the anodic limit from the HOMO level of 27 anions, whose performances have been experimentally investigated in the previous literature. A limited ", 
    "abstract": "In this paper we predict Bitcoin movements by utilizing a machine-learning framework. We compile a dataset of 24 potential explanatory variables that are often employed in the finance literature. Using daily data from 2nd of December 2014 to July 8th 2019, we build forecasting models that utilize past Bitcoin values, other cryptocurrencies, exchange rates and other macroeconomic variables. Our empirical results suggest that the traditional logistic regression model outperforms the linear support vector machine and the random forest algorithm, reaching an accuracy of 66%. Moreover, based on the results, we provide evidence that points to the rejection of weak form efficiency in the Bitcoin market.", 
    "abstract": "Basketball is a popular sport worldwide, and many researchers have utilized various machine learning models to predict the outcome of basketball games. However, prior research has primarily focused on traditional machine learning models. Furthermore, models that rely on vector inputs tend to ignore the intricate interactions between teams and the spatial structure of the league. Therefore, this study aimed to apply graph neural networks to basketball game outcome prediction, by transforming structured data into unstructured graphs, to represent the interactions between teams in the 2012-2018 NBA season dataset. Initially, the study used a homogeneous network and undirected graph to build a team representation graph. The constructed graph was fed into a graph convolutional network, which yielded an average success rate of 66.90% in predicting the outcome of games. To improve the prediction success rate, feature extraction based on the random forest algorithm was combined with the model. The fused model yielded the best results, and the prediction accuracy was improved to 71.54%. Additionally, the study compared the results of the developed model with previous studies and the baseline model. Our proposed method considers the spatial structure of teams and the interaction between teams, resulting in superior performance in basketball game outcome prediction. The results of this study provide valuable insights for basketball performance prediction research.", 
    "abstract": "This work applies concepts from algorithmic probability to Boolean and quantum combinatorial logic circuits. The relations among the statistical, algorithmic, computational, and circuit complexities of states are reviewed. Thereafter, the probability of states in the circuit model of computation is defined. Classical and quantum gate sets are compared to select some characteristic sets. The reachability and expressibility in a space-time-bounded setting for these gate sets are enumerated and visualized. These results are studied in terms of computational resources, universality, and quantum behavior. The article suggests how applications like geometric quantum machine learning, novel quantum algorithm synthesis, and quantum artificial general intelligence can benefit by studying circuit probabilities.", 
    "abstract": "The influenza virus and the novel beta coronavirus (SARS-CoV-2) have similar transmission characteristics, and it is very difficult to distinguish them clinically. With the development of information technologies, novel opportunities have arisen for the application of intelligent software systems in disease diagnosis and patient triage.\nA cross-sectional study was conducted on 268 infants: 133 infants with a SARS-CoV-2 infection and 135 infants with an influenza virus infection. In total, 10 hematochemical variables were used to construct an automated machine learning model.\nAn accuracy range from 53.8% to 60.7% was obtained by applying support vector machine, random forest, k-nearest neighbors, logistic regression, and neural network models. Alternatively, an automated model convincingly outperformed other models with an accuracy of 98.4%. The proposed automated algorithm recommended a random tree model, a randomization-based ensemble method, as the most appropriate for the given dataset.\nThe application of automated machine learning in clinical practice can contribute to more objective, accurate, and rapid diagnosis of SARS-CoV-2 and influenza virus infections in children.", 
    "abstract": "Diabetes in humans is a rapidly expanding chronic disease and a major crisis in modern societies. The classification of diabetics is a challenging and important procedure that allows the interpretation of diabetic data and diagnosis. Missing values in datasets can impact the prediction accuracy of the methods for the diagnosis. Due to this, a variety of machine learning techniques has been studied in the past. This research has developed a new method using machine learning techniques for diabetes risk prediction. The method was developed through the use of clustering and prediction learning techniques. The method uses Singular Value Decomposition for missing value predictions, a Self-Organizing Map for clustering the data, STEPDISC for feature selection, and an ensemble of Deep Belief Network classifiers for diabetes mellitus prediction. The performance of the proposed method is compared with the previous prediction methods developed by machine learning techniques. The results reveal that the deployed method can accurately predict diabetes mellitus for a set of real-world datasets.", 
    "abstract": "One of the most prevalent chronic conditions that can result in permanent vision loss is diabetic retinopathy (DR). Diabetic retinopathy occurs in five stages: no DR, and mild, moderate, severe, and proliferative DR. The early detection of DR is essential for preventing vision loss in diabetic patients. In this paper, we propose a method for the detection and classification of DR stages to determine whether patients are in any of the non-proliferative stages or in the proliferative stage. The hybrid approach based on image preprocessing and ensemble features is the foundation of the proposed classification method. We created a convolutional neural network (CNN) model from scratch for this study. Combining Local Binary Patterns (LBP) and deep learning features resulted in the creation of the ensemble features vector, which was then optimized using the Binary Dragonfly Algorithm (BDA) and the Sine Cosine Algorithm (SCA). Moreover, this optimized feature vector was fed to the machine learning classifiers. The SVM classifier achieved the highest classification accuracy of 98.85% on a publicly available dataset, i.e., Kaggle EyePACS. Rigorous testing and comparisons with state-of-the-art approaches in the literature indicate the effectiveness of the proposed methodology.", 
    "abstract": "The assessment of alveolar bone loss, a crucial element of the periodontium, plays a vital role in the diagnosis of periodontitis and the prognosis of the disease. In dentistry, artificial intelligence (AI) applications have demonstrated practical and efficient diagnostic capabilities, leveraging machine learning and cognitive problem-solving functions that mimic human abilities. This study aims to evaluate the effectiveness of AI models in identifying alveolar bone loss as present or absent across different regions. To achieve this goal, alveolar bone loss models were generated using the PyTorch-based YOLO-v5 model implemented via CranioCatch software, detecting periodontal bone loss areas and labeling them using the segmentation method on 685 panoramic radiographs. Besides general evaluation, models were grouped according to subregions (incisors, canines, premolars, and molars) to provide a targeted evaluation. Our findings reveal that the lowest sensitivity and F1 score values were associated with total alveolar bone loss, while the highest values were observed in the maxillary incisor region. It shows that artificial intelligence has a high potential in analytical studies evaluating periodontal bone loss situations. Considering the limited amount of data, it is predicted that this success will increase with the provision of machine learning by using a more comprehensive data set in further studies.", 
    "abstract": "The early diagnosis of infectious diseases is demanded by digital healthcare systems. Currently, the detection of the new coronavirus disease (COVID-19) is a major clinical requirement. For COVID-19 detection, deep learning models are used in various studies, but the robustness is still compromised. In recent years, deep learning models have increased in popularity in almost every area, particularly in medical image processing and analysis. The visualization of the human body's internal structure is critical in medical analysis; many imaging techniques are in use to perform this job. A computerized tomography (CT) scan is one of them, and it has been generally used for the non-invasive observation of the human body. The development of an automatic segmentation method for lung CT scans showing COVID-19 can save experts time and can reduce human error. In this article, the CRV-NET is proposed for the robust detection of COVID-19 in lung CT scan images. A public dataset (SARS-CoV-2 CT Scan dataset), is used for the experimental work and customized according to the scenario of the proposed model. The proposed modified deep-learning-based U-Net model is trained on a custom dataset with 221 training images and their ground truth, which was labeled by an expert. The proposed model is tested on 100 test images, and the results show that the model segments COVID-19 with a satisfactory level of accuracy. Moreover, the comparison of the proposed CRV-NET with different state-of-the-art convolutional neural network models (CNNs), including the U-Net Model, shows better results in terms of accuracy (96.67%) and robustness (low epoch value in detection and the smallest training data size).", 
    "abstract": "The growth of biomedical engineering has made depression diagnosis via electroencephalography (EEG) a trendy issue. The two significant challenges to this application are EEG signals' complexity and non-stationarity. Additionally, the effects caused by individual variances may hamper the generalization of detection systems. Given the association between EEG signals and particular demographics, such as gender and age, and the influences of these demographic characteristics on the incidence of depression, it would be preferable to include demographic factors during EEG modeling and depression detection. The main objective of this work is to develop an algorithm that can recognize depression patterns by studying EEG data. Following a multiband analysis of such signals, machine learning and deep learning techniques were used to detect depression patients automatically. EEG signal data are collected from the multi-modal open dataset MODMA and employed in studying mental diseases. The EEG dataset contains information from a traditional 128-electrode elastic cap and a cutting-edge wearable 3-electrode EEG collector for widespread applications. In this project, resting EEG readings of 128 channels are considered. According to CNN, training with 25 epoch iterations had a 97% accuracy rate. The patient's status has to be divided into two basic categories: major depressive disorder (MDD) and healthy control. Additional MDD include the following six classes: obsessive-compulsive disorders, addiction disorders, conditions brought on by trauma and stress, mood disorders, schizophrenia, and the anxiety disorders discussed in this paper are a few examples of mental illnesses. According to the study, a natural combination of EEG signals and demographic data is promising for the diagnosis of depression.", 
    "abstract": "Alzheimer's disease (AD) is a complex genetic disorder that affects the brain and has been the focus of many bioinformatics research studies. The primary objective of these studies is to identify and classify genes involved in the progression of AD and to explore the function of these risk genes in the disease process. The aim of this research is to identify the most effective model for detecting biomarker genes associated with AD using several feature selection methods. We compared the efficiency of feature selection methods with an SVM classifier, including mRMR, CFS, the Chi-Square Test, F-score, and GA. We calculated the accuracy of the SVM classifier using validation methods such as 10-fold cross-validation. We applied these feature selection methods with SVM to a benchmark AD gene expression dataset consisting of 696 samples and 200 genes. The results indicate that the mRMR and F-score feature selection methods with SVM classifier achieved a high accuracy of around 84%, with a number of genes between 20 and 40. Furthermore, the mRMR and F-score feature selection methods with SVM classifier outperformed the GA, Chi-Square Test, and CFS methods. Overall, these findings suggest that the mRMR and F-score feature selection methods with SVM classifier are effective in identifying biomarker genes related to AD and could potentially lead to more accurate diagnosis and treatment of the disease.", 
    "abstract": "Cervical cancer is known as a major health problem globally, with high mortality as well as incidence rates. Over the years, there have been significant advancements in cervical cancer detection techniques, leading to improved accuracy, sensitivity, and specificity. This article provides a chronological review of cervical cancer detection techniques, from the traditional Pap smear test to the latest computer-aided detection (CAD) systems. The traditional method for cervical cancer screening is the Pap smear test. It consists of examining cervical cells under a microscope for abnormalities. However, this method is subjective and may miss precancerous lesions, leading to false negatives and a delayed diagnosis. Therefore, a growing interest has been in shown developing CAD methods to enhance cervical cancer screening. However, the effectiveness and reliability of CAD systems are still being evaluated. A systematic review of the literature was performed using the Scopus database to identify relevant studies on cervical cancer detection techniques published between 1996 and 2022. The search terms used included \"(cervix OR cervical) AND (cancer OR tumor) AND (detect* OR diagnosis)\". Studies were included if they reported on the development or evaluation of cervical cancer detection techniques, including traditional methods and CAD systems. The results of the review showed that CAD technology for cervical cancer detection has come a long way since it was introduced in the 1990s. Early CAD systems utilized image processing and pattern recognition techniques to analyze digital images of cervical cells, with limited success due to low sensitivity and specificity. In the early 2000s, machine learning (ML) algorithms were introduced to the CAD field for cervical cancer detection, allowing for more accurate and automated analysis of digital images of cervical cells. ML-based CAD systems have shown promise in several studies, with improved sensitivity and specificity reported compared to traditional screening methods. In summary, this chronological review of cervical cancer detection techniques highlights the significant advancements made in this field over the past few decades. ML-based CAD systems have shown promise for improving the accuracy and sensitivity of cervical cancer detection. The Hybrid Intelligent System for Cervical Cancer Diagnosis (HISCCD) and the Automated Cervical Screening System (ACSS) are two of the most promising CAD systems. Still, deeper validation and research are required before being broadly accepted. Continued innovation and collaboration in this field may help enhance cervical cancer detection as well as ultimately reduce the disease's burden on women worldwide.", 
    "abstract": "Predicting length of stay (LoS) and understanding its underlying factors is essential to minimizing the risk of hospital-acquired conditions, improving financial, operational, and clinical outcomes, and better managing future pandemics. The purpose of this study was to forecast patients' LoS using a deep learning model and to analyze cohorts of risk factors reducing or prolonging LoS. We employed various preprocessing techniques, SMOTE-N to balance data, and a TabTransformer model to forecast LoS. Finally, the Apriori algorithm was applied to analyze cohorts of risk factors influencing hospital LoS. The TabTransformer outperformed the base machine learning models in terms of F1 score (0.92), precision (0.83), recall (0.93), and accuracy (0.73) for the discharged dataset and F1 score (0.84), precision (0.75), recall (0.98), and accuracy (0.77) for the deceased dataset. The association mining algorithm was able to identify significant risk factors/indicators belonging to laboratory, X-ray, and clinical data, such as elevated LDH and D-dimer levels, lymphocyte count, and comorbidities such as hypertension and diabetes. It also reveals what treatments have reduced the symptoms of COVID-19 patients, leading to a reduction in LoS, particularly when no vaccines or medication, such as Paxlovid, were available.", 
    "abstract": "Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), causing a disease called COVID-19, is a class of acute respiratory syndrome that has considerably affected the global economy and healthcare system. This virus is diagnosed using a traditional technique known as the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. However, RT-PCR customarily outputs a lot of false-negative and incorrect results. Current works indicate that COVID-19 can also be diagnosed using imaging resolutions, including CT scans, X-rays, and blood tests. Nevertheless, X-rays and CT scans cannot always be used for patient screening because of high costs, radiation doses, and an insufficient number of devices. Therefore, there is a requirement for a less expensive and faster diagnostic model to recognize the positive and negative cases of COVID-19. Blood tests are easily performed and cost less than RT-PCR and imaging tests. Since biochemical parameters in routine blood tests vary during the COVID-19 infection, they may supply physicians with exact information about the diagnosis of COVID-19. This study reviewed some newly emerging artificial intelligence (AI)-based methods to diagnose COVID-19 using routine blood tests. We gathered information about research resources and inspected 92 articles that were carefully chosen from a variety of publishers, such as IEEE, Springer, Elsevier, and MDPI. Then, these 92 studies are classified into two tables which contain articles that use machine Learning and deep Learning models to diagnose COVID-19 while using routine blood test datasets. In these studies, for diagnosing COVID-19, Random Forest and logistic regression are the most widely used machine learning methods and the most widely used performance metrics are accuracy, sensitivity, specificity, and AUC. Finally, we conclude by discussing and analyzing these studies which use machine learning and deep learning models and routine blood test datasets for COVID-19 detection. This survey can be the starting point for a novice-/beginner-level researcher to perform on COVID-19 classification.", 
    "abstract": "Bibliometric analysis is a widely used technique for analyzing large quantities of academic literature and evaluating its impact in a particular academic field. In this paper bibliometric analysis has been used to analyze the academic research on arrhythmia detection and classification from 2005 to 2022. We have followed PRISMA 2020 framework to identify, filter and select the relevant papers. This study has used the Web of Science database to find related publications on arrhythmia detection and classification. \"Arrhythmia detection\", \"arrhythmia classification\" and \"arrhythmia detection and classification\" are three keywords for gathering the relevant articles. 238 publications in total were selected for this research. In this study, two different bibliometric techniques, \"performance analysis\" and \"science mapping\", were applied. Different bibliometric parameters such as publication analysis, trend analysis, citation analysis, and networking analysis have been used to evaluate the performance of these articles. According to this analysis, the three countries with the highest number of publications and citations are China, the USA, and India in terms of arrhythmia detection and classification. The three most significant researchers in this field are those named U. R. Acharya, S. Dogan, and P. Plawiak. Machine learning, ECG, and deep learning are the three most frequently used keywords. A further finding of the study indicates that the popular topics for arrhythmia identification are machine learning, ECG, and atrial fibrillation. This research provides insight into the origins, current status, and future direction of arrhythmia detection research.", 
    "abstract": "This paper presents a robust colon cancer diagnosis method based on the feature selection method. The proposed method for colon disease diagnosis can be divided into three steps. In the first step, the images' features were extracted based on the convolutional neural network. Squeezenet, Resnet-50, AlexNet, and GoogleNet were used for the convolutional neural network. The extracted features are huge, and the number of features cannot be appropriate for training the system. For this reason, the metaheuristic method is used in the second step to reduce the number of features. This research uses the grasshopper optimization algorithm to select the best features from the feature data. Finally, using machine learning methods, colon disease diagnosis was found to be accurate and successful. Two classification methods are applied for the evaluation of the proposed method. These methods include the decision tree and the support vector machine. The sensitivity, specificity, accuracy, and F1Score have been used to evaluate the proposed method. For Squeezenet based on the support vector machine, we obtained results of 99.34%, 99.41%, 99.12%, 98.91% and 98.94% for sensitivity, specificity, accuracy, precision, and F1Score, respectively. In the end, we compared the suggested recognition method's performance to the performances of other methods, including 9-layer CNN, random forest, 7-layer CNN, and DropBlock. We demonstrated that our solution outperformed the others.", 
    "abstract": "Breast cancer is responsible for the deaths of thousands of women each year. The diagnosis of breast cancer (BC) frequently makes the use of several imaging techniques. On the other hand, incorrect identification might occasionally result in unnecessary therapy and diagnosis. Therefore, the accurate identification of breast cancer can save a significant number of patients from undergoing unnecessary surgery and biopsy procedures. As a result of recent developments in the field, the performance of deep learning systems used for medical image processing has showed significant benefits. Deep learning (DL) models have found widespread use for the aim of extracting important features from histopathologic BC images. This has helped to improve the classification performance and has assisted in the automation of the process. In recent times, both convolutional neural networks (CNNs) and hybrid models of deep learning-based approaches have demonstrated impressive performance. In this research, three different types of CNN models are proposed: a straightforward CNN model (1-CNN), a fusion CNN model (2-CNN), and a three CNN model (3-CNN). The findings of the experiment demonstrate that the techniques based on the 3-CNN algorithm performed the best in terms of accuracy (90.10%), recall (89.90%), precision (89.80%), and f1-Score (89.90%). In conclusion, the CNN-based approaches that have been developed are contrasted with more modern machine learning and deep learning models. The application of CNN-based methods has resulted in a significant increase in the accuracy of the BC classification.", 
    "abstract": "The complete blood count (CBC) is a highly requested test that is generally restricted to centralized laboratories, which are limited by high cost, being maintenance-demanding, and requiring costly equipment. The Hilab System (HS) is a small, handheld hematological platform that uses microscopy and chromatography techniques, combined with machine learning (ML) and artificial intelligence (AI), to perform a CBC test. This platform uses ML and AI techniques to add higher accuracy and reliability to the results besides allowing for faster reporting. For clinical and flagging capability evaluation of the handheld device, the study analyzed 550 blood samples of patients from a reference institution for oncological diseases. The clinical analysis encompassed the data comparison between the Hilab System and a conventional hematological analyzer (Sysmex XE-2100) for all CBC analytes. The flagging capability study compared the microscopic findings from the Hilab System and the standard blood smear evaluation method. The study also assessed the sample collection source (venous or capillary) influences. The Pearson correlation, Student ", 
    "abstract": "Although handcrafted radiomics features (RF) are commonly extracted via radiomics software, employing deep features (DF) extracted from deep learning (DL) algorithms merits significant investigation. Moreover, a \"tensor'' radiomics paradigm where various flavours of a given feature are generated and explored can provide added value. We aimed to employ conventional and tensor DFs, and compare their outcome prediction performance to conventional and tensor RFs.\n408 patients with head and neck cancer were selected from TCIA. PET images were first registered to CT, enhanced, normalized, and cropped. We employed 15 image-level fusion techniques (e.g., dual tree complex wavelet transform (DTCWT)) to combine PET and CT images. Subsequently, 215 RFs were extracted from each tumor in 17 images (or flavours) including CT only, PET only, and 15 fused PET-CT images through the standardized-SERA radiomics software. Furthermore, a 3 dimensional autoencoder was used to extract DFs. To predict the binary progression-free-survival-outcome, first, an end-to-end CNN algorithm was employed. Subsequently, we applied conventional and tensor DFs vs. RFs as extracted from each image to three sole classifiers, namely multilayer perceptron (MLP), random-forest, and logistic regression (LR), linked with dimension reduction algorithms.\nDTCWT fusion linked with CNN resulted in accuracies of 75.6 \u00b1 7.0% and 63.4 \u00b1 6.7% in five-fold cross-validation and external-nested-testing, respectively. For the tensor RF-framework, polynomial transform algorithms + analysis of variance feature selector (ANOVA) + LR enabled 76.67 \u00b1 3.3% and 70.6 \u00b1 6.7% in the mentioned tests. For the tensor DF framework, PCA + ANOVA + MLP arrived at 87.0 \u00b1 3.5% and 85.3 \u00b1 5.2% in both tests.\nThis study showed that tensor DF combined with proper machine learning approaches enhanced survival prediction performance compared to conventional DF, tensor and conventional RF, and end-to-end CNN frameworks.", 
    "abstract": "Intrauterine fetal demise in women during pregnancy is a major contributing factor in prenatal mortality and is a major global issue in developing and underdeveloped countries. When an unborn fetus passes away in the womb during the 20th week of pregnancy or later, early detection of the fetus can help reduce the chances of intrauterine fetal demise. Machine learning models such as Decision Trees, Random Forest, SVM Classifier, KNN, Gaussian Na\u00efve Bayes, Adaboost, Gradient Boosting, Voting Classifier, and Neural Networks are trained to determine whether the fetal health is Normal, Suspect, or Pathological. This work uses 22 features related to fetal heart rate obtained from the Cardiotocogram (CTG) clinical procedure for 2126 patients. Our paper focuses on applying various cross-validation techniques, namely, K-Fold, Hold-Out, Leave-One-Out, Leave-P-Out, Monte Carlo, Stratified K-fold, and Repeated K-fold, on the above ML algorithms to enhance them and determine the best performing algorithm. We conducted exploratory data analysis to obtain detailed inferences on the features. Gradient Boosting and Voting Classifier achieved 99% accuracy after applying cross-validation techniques. The dataset used has the dimension of 2126 \u00d7 22, and the label is multiclass classified as Normal, Suspect, and Pathological condition. Apart from incorporating cross-validation strategies on several machine learning algorithms, the research paper focuses on Blackbox evaluation, which is an Interpretable Machine Learning Technique used to understand the underlying working mechanism of each model and the means by which it picks features to train and predict values.", 
    "abstract": "We aimed to predict Montreal Cognitive Assessment (MoCA) scores in Parkinson's disease patients at year 4 using handcrafted radiomics (RF), deep (DF), and clinical (CF) features at year 0 (baseline) applied to hybrid machine learning systems (HMLSs).\n297 patients were selected from the Parkinson's Progressive Marker Initiative (PPMI) database. The standardized SERA radiomics software and a 3D encoder were employed to extract RFs and DFs from single-photon emission computed tomography (DAT-SPECT) images, respectively. The patients with MoCA scores over 26 were indicated as normal; otherwise, scores under 26 were indicated as abnormal. Moreover, we applied different combinations of feature sets to HMLSs, including the Analysis of Variance (ANOVA) feature selection, which was linked with eight classifiers, including Multi-Layer Perceptron (MLP), K-Neighbors Classifier (KNN), Extra Trees Classifier (ETC), and others. We employed 80% of the patients to select the best model in a 5-fold cross-validation process, and the remaining 20% were employed for hold-out testing.\nFor the sole usage of RFs and DFs, ANOVA and MLP resulted in averaged accuracies of 59 \u00b1 3% and 65 \u00b1 4% for 5-fold cross-validation, respectively, with hold-out testing accuracies of 59 \u00b1 1% and 56 \u00b1 2%, respectively. For sole CFs, a higher performance of 77 \u00b1 8% for 5-fold cross-validation and a hold-out testing performance of 82 + 2% were obtained from ANOVA and ETC. RF+DF obtained a performance of 64 \u00b1 7%, with a hold-out testing performance of 59 \u00b1 2% through ANOVA and XGBC. Usage of CF+RF, CF+DF, and RF+DF+CF enabled the highest averaged accuracies of 78 \u00b1 7%, 78 \u00b1 9%, and 76 \u00b1 8% for 5-fold cross-validation, and hold-out testing accuracies of 81 \u00b1 2%, 82 \u00b1 2%, and 83 \u00b1 4%, respectively.\nWe demonstrated that CFs vitally contribute to predictive performance, and combining them with appropriate imaging features and HMLSs can result in the best prediction performance.", 
    "abstract": "Detection of early clinical keratoconus (KCN) is a challenging task, even for expert clinicians. In this study, we propose a deep learning (DL) model to address this challenge. We first used Xception and InceptionResNetV2 DL architectures to extract features from three different corneal maps collected from 1371 eyes examined in an eye clinic in Egypt. We then fused features using Xception and InceptionResNetV2 to detect subclinical forms of KCN more accurately and robustly. We obtained an area under the receiver operating characteristic curves (AUC) of 0.99 and an accuracy range of 97-100% to distinguish normal eyes from eyes with subclinical and established KCN. We further validated the model based on an independent dataset with 213 eyes examined in Iraq and obtained AUCs of 0.91-0.92 and an accuracy range of 88-92%. The proposed model is a step toward improving the detection of clinical and subclinical forms of KCN.", 
    "abstract": "This study aimed to develop a comprehensive approach for assessing fresh ejaculate from Muscovy duck (", 
    "abstract": "As climate change progresses rapidly, biodiversity declines, and ecosystems shift, it is becoming increasingly difficult to document dynamic populations, track fluctuations, and predict responses to climate change. Concurrently, publicly available databases and tools are improving scientific accessibility, increasing collaboration, and generating more data than ever before. One of the most successful projects is iNaturalist, an AI-driven social network doubling as a public database designed to allow citizen scientists to report personal biodiversity reports with accuracy. iNaturalist is especially useful for the research of rare, dangerous, and charismatic organisms, but requires better integration into the marine system. Despite their abundance and ecological relevance, there are few long-term, high-sample datasets for jellyfish, which makes management difficult. To provide some high-sample datasets and demonstrate the utility of publicly collected data, we synthesized two global datasets for ten genera of jellyfishes in the order Rhizostomeae containing 8412 curated datapoints from both iNaturalist (", 
    "abstract": "Here, we report on our study of plasma lipidomics profiles of patients with type 1 diabetes (T1DM) and explore potential associations. One hundred and seven patients with T1DM were consecutively recruited. Ultrasound imaging of peripheral arteries was performed using a high image resolution B-mode ultrasound system. Untargeted lipidomics analysis was performed using UHPLC coupled to qTOF/MS. The associations were evaluated using machine learning algorithms. SM(32:2) and ether lipid species (PC(O-30:1)/PC(P-30:0)) were significantly and positively associated with subclinical atherosclerosis (SA). This association was further confirmed in patients with overweight/obesity (specifically with SM(40:2)). A negative association between SA and lysophosphatidylcholine species was found among lean subjects. Phosphatidylcholines (PC(40:6) and PC(36:6)) and cholesterol esters (ChoE(20:5)) were associated positively with intima-media thickness both in subjects with and without overweight/obesity. In summary, the plasma antioxidant molecules SM and PC differed according to the presence of SA and/or overweight status in patients with T1DM. This is the first study showing the associations in T1DM, and the findings may be useful in the targeting of a personalized approach aimed at preventing cardiovascular disease in these patients.", 
    "abstract": "", 
    "abstract": "Neurodegenerative diseases (NDDs), which are chronic and progressive diseases, are a growing health concern. Among the therapeutic methods, stem-cell-based therapy is an attractive approach to NDD treatment owing to stem cells' characteristics such as their angiogenic ability, anti-inflammatory, paracrine, and anti-apoptotic effects, and homing ability to the damaged brain region. Human bone-marrow-derived mesenchymal stem cells (hBM-MSCs) are attractive NDD therapeutic agents owing to their widespread availability, easy attainability and in vitro manipulation and the lack of ethical issues. Ex vivo hBM-MSC expansion before transplantation is essential because of the low cell numbers in bone marrow aspirates. However, hBM-MSC quality decreases over time after detachment from culture dishes, and the ability of hBM-MSCs to differentiate after detachment from culture dishes remains poorly understood. Conventional analysis of hBM-MSCs characteristics before transplantation into the brain has several limitations. However, omics analyses provide more comprehensive molecular profiling of multifactorial biological systems. Omics and machine learning approaches can handle big data and provide more detailed characterization of hBM-MSCs. Here, we provide a brief review on the application of hBM-MSCs in the treatment of NDDs and an overview of integrated omics analysis of the quality and differentiation ability of hBM-MSCs detached from culture dishes for successful stem cell therapy.", 
    "abstract": "Tracking vital signs accurately is critical for triaging a patient and ensuring timely therapeutic intervention. The patient's status is often clouded by compensatory mechanisms that can mask injury severity. The compensatory reserve measurement (", 
    "abstract": "One problem in the quantitative assessment of biomechanical impairments in Parkinson's disease patients is the need for scalable and adaptable computing systems. This work presents a computational method that can be used for motor evaluations of pronation-supination hand movements, as described in item 3.6 of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The presented method can quickly adapt to new expert knowledge and includes new features that use a self-supervised training approach. The work uses wearable sensors for biomechanical measurements. We tested a machine-learning model on a dataset of 228 records with 20 indicators from 57 PD patients and eight healthy control subjects. The test dataset's experimental results show that the method's precision rates for the pronation and supination classification task achieved up to 89% accuracy, and the F1-scores were higher than 88% in most categories. The scores present a root mean squared error of 0.28 when compared to expert clinician scores. The paper provides detailed results for pronation-supination hand movement evaluations using a new analysis method when compared to the other methods mentioned in the literature. Furthermore, the proposal consists of a scalable and adaptable model that includes expert knowledge and affectations not covered in the MDS-UPDRS for a more in-depth evaluation.", 
    "abstract": "Since the onset of the COVID-19 pandemic in early 2020, the importance of timely and effective assessment of mental well-being has increased dramatically. Machine learning (ML) algorithms and artificial intelligence (AI) techniques can be harnessed for early detection, prognostication and prediction of negative psychological well-being states.\nWe used data from a large, multi-site cross-sectional survey consisting of 17 universities in Southeast Asia. This research work models mental well-being and reports on the performance of various machine learning algorithms, including generalized linear models, k-nearest neighbor, na\u00efve Bayes, neural networks, random forest, recursive partitioning, bagging, and boosting.\nRandom Forest and adaptive boosting algorithms achieved the highest accuracy for identifying negative mental well-being traits. The top five most salient features associated with predicting poor mental well-being include the number of sports activities per week, body mass index, grade point average (GPA), sedentary hours, and age.\nBased on the reported results, several specific recommendations and suggested future work are discussed. These findings may be useful to provide cost-effective support and modernize mental well-being assessment and monitoring at the individual and university level.", 
    "abstract": "A machine learning method for classifying lung ultrasound is proposed here to provide a point of care tool for supporting a safe, fast, and accurate diagnosis that can also be useful during a pandemic such as SARS-CoV-2. Given the advantages (e.g., safety, speed, portability, cost-effectiveness) provided by the ultrasound technology over other examinations (e.g., X-ray, computer tomography, magnetic resonance imaging), our method was validated on the largest public lung ultrasound dataset. Focusing on both accuracy and efficiency, our solution is based on an efficient adaptive ensembling of two EfficientNet-b0 models reaching 100% of accuracy, which, to our knowledge, outperforms the previous state-of-the-art models by at least 5%. The complexity is restrained by adopting specific design choices: ensembling with an adaptive combination layer, ensembling performed on the deep features, and minimal ensemble using two weak models only. In this way, the number of parameters has the same order of magnitude of a single EfficientNet-b0 and the computational cost (FLOPs) is reduced at least by 20%, doubled by parallelization. Moreover, a visual analysis of the saliency maps on sample images of all the classes of the dataset reveals where an inaccurate weak model focuses its attention versus an accurate one.", 
    "abstract": "A brain-computer interface (BCI) allows users to control external devices through brain activity. Portable neuroimaging techniques, such as near-infrared (NIR) imaging, are suitable for this goal. NIR imaging has been used to measure rapid changes in brain optical properties associated with neuronal activation, namely fast optical signals (FOS) with good spatiotemporal resolution. However, FOS have a low signal-to-noise ratio, limiting their BCI application. Here FOS were acquired with a frequency-domain optical system from the visual cortex during visual stimulation consisting of a rotating checkerboard wedge, flickering at 5 Hz. We used measures of photon count (Direct Current, DC light intensity) and time of flight (phase) at two NIR wavelengths (690 nm and 830 nm) combined with a machine learning approach for fast estimation of visual-field quadrant stimulation. The input features of a cross-validated support vector machine classifier were computed as the average modulus of the wavelet coherence between each channel and the average response among all channels in 512 ms time windows. An above chance performance was obtained when differentiating visual stimulation quadrants (left vs. right or top vs. bottom) with the best classification accuracy of ~63% (information transfer rate of ~6 bits/min) when classifying the superior and inferior stimulation quadrants using DC at 830 nm. The method is the first attempt to provide generalizable retinotopy classification relying on FOS, paving the way for the use of FOS in real-time BCI.", 
    "abstract": "Pain assessment is a complex task largely dependent on the patient's self-report. Artificial intelligence (AI) has emerged as a promising tool for automating and objectifying pain assessment through the identification of pain-related facial expressions. However, the capabilities and potential of AI in clinical settings are still largely unknown to many medical professionals. In this literature review, we present a conceptual understanding of the application of AI to detect pain through facial expressions. We provide an overview of the current state of the art as well as the technical foundations of AI/ML techniques used in pain detection. We highlight the ethical challenges and the limitations associated with the use of AI in pain detection, such as the scarcity of databases, confounding factors, and medical conditions that affect the shape and mobility of the face. The review also highlights the potential impact of AI on pain assessment in clinical practice and lays the groundwork for further study in this area.", 
    "abstract": "The length of the standing long jump (SLJ) is widely recognized as an indicator of developmental motor competence or sports conditional performance. This work aims at defining a methodology to allow athletes/coaches to easily measure it using the inertial measurement units embedded on a smartphone. A sample group of 114 trained young participants was recruited and asked to perform the instrumented SLJ task. A set of features was identified based on biomechanical knowledge, then Lasso regression allowed the identification of a subset of predictors of the SLJ length that was used as input of different optimized machine learning architectures. Results obtained from the use of the proposed configuration allow an estimate of the SLJ length with a Gaussian Process Regression model with a RMSE of 0.122 m in the test phase, Kendall's \u03c4 < 0.1. The proposed models give homoscedastic results, meaning that the error of the models does not depend on the estimated quantity. This study proved the feasibility of using low-cost smartphone sensors to provide an automatic and objective estimate of SLJ performance in ecological settings.", 
    "abstract": "Postural deficits such as hyperlordosis (hollow back) or hyperkyphosis (hunchback) are relevant health issues. Diagnoses depend on the experience of the examiner and are, therefore, often subjective and prone to errors. Machine learning (ML) methods in combination with explainable artificial intelligence (XAI) tools have proven useful for providing an objective, data-based orientation. However, only a few works have considered posture parameters, leaving the potential for more human-friendly XAI interpretations still untouched. Therefore, the present work proposes an objective, data-driven ML system for medical decision support that enables especially human-friendly interpretations using counterfactual explanations (CFs). The posture data for 1151 subjects were recorded by means of stereophotogrammetry. An expert-based classification of the subjects regarding the presence of hyperlordosis or hyperkyphosis was initially performed. Using a Gaussian progress classifier, the models were trained and interpreted using CFs. The label errors were flagged and re-evaluated using confident learning. Very good classification performances for both hyperlordosis and hyperkyphosis were found, whereby the re-evaluation and correction of the test labels led to a significant improvement (M", 
    "abstract": "Marker-based Optical Motion Capture (OMC) systems and associated musculoskeletal (MSK) modelling predictions offer non-invasively obtainable insights into muscle and joint loading at an in vivo level, aiding clinical decision-making. However, an OMC system is lab-based, expensive, and requires a line of sight. Inertial Motion Capture (IMC) techniques are widely-used alternatives, which are portable, user-friendly, and relatively low-cost, although with lesser accuracy. Irrespective of the choice of motion capture technique, one typically uses an MSK model to obtain the kinematic and kinetic outputs, which is a computationally expensive tool increasingly well approximated by machine learning (ML) methods. Here, an ML approach is presented that maps experimentally recorded IMC input data to the human upper-extremity MSK model outputs computed from ('gold standard') OMC input data. Essentially, this proof-of-concept study aims to predict higher-quality MSK outputs from the much easier-to-obtain IMC data. We use OMC and IMC data simultaneously collected for the same subjects to train different ML architectures that predict OMC-driven MSK outputs from IMC measurements. In particular, we employed various neural network (NN) architectures, such as Feed-Forward Neural Networks (FFNNs) and Recurrent Neural Networks (RNNs) (vanilla, Long Short-Term Memory, and Gated Recurrent Unit) and a comprehensive search for the best-fit model in the hyperparameters space in both subject-exposed (SE) as well as subject-naive (SN) settings. We observed a comparable performance for both FFNN and RNN models, which have a high degree of agreement (ravg,SE,FFNN=0.90\u00b10.19, ravg,SE,RNN=0.89\u00b10.17, ravg,SN,FFNN=0.84\u00b10.23, and ravg,SN,RNN=0.78\u00b10.23) with the desired OMC-driven MSK estimates for held-out test data. The findings demonstrate that mapping IMC inputs to OMC-driven MSK outputs using ML models could be instrumental in transitioning MSK modelling from 'lab to field'.", 
    "abstract": "Abnormal bite force is an important risk factor for oral and maxillofacial disorders, which is a critical dilemma that dentists face every day without effective solutions. Therefore, it is of great clinical significance to develop a wireless bite force measurement device and explore quantitative measurement methods to help find effective strategies for improving occlusal diseases. This study designed the open window carrier of a bite force detection device through 3D printing technology, and then the stress sensors were integrated and embedded into a hollow structure. The sensor system mainly consisted of a pressure signal acquisition module, a main control module, and a server terminal. A machine learning algorithm will be leveraged for bite force data processing and parameter configuration in the future. This study implemented a sensor prototype system from scratch to fully evaluate each component of the intelligent device. The experimental results showed reasonable parameter metrics for the device carrier and demonstrated the feasibility of the proposed scheme for bite force measurement. An intelligent and wireless bite force device with a stress sensor system is a promising approach to occlusal disease diagnosis and treatment.", 
    "abstract": "Zinc (Zn) is an essential element that influences many cellular functions. Depending on bioavailability, Zn can cause both deficiency and toxicity. Zn bioavailability is influenced by water hardness. Therefore, water quality analysis for health-risk assessment should consider both Zn concentration and water hardness. However, exposure media selection for traditional toxicology tests are set to defined hardness levels and do not represent the diverse water chemistry compositions observed in nature. Moreover, these tests commonly use whole organism endpoints, such as survival and reproduction, which require high numbers of test animals and are labor intensive. Gene expression stands out as a promising alternative to provide insight into molecular events that can be used for risk assessment. In this work, we apply machine learning techniques to classify the Zn concentrations and water hardness from ", 
    "abstract": "Epigenetic dysregulation is essential to the tumorigenesis of oral squamous cell carcinoma (OSCC). SET and MYND domain-containing protein 3 (SMYD3), a histone lysine methyltransferase, is implicated in gene transcription regulation and tumor development. However, the roles of SMYD3 in OSCC initiation are not fully understood. The present study investigated the biological functions and mechanisms involved in the SMYD3-mediated tumorigenesis of OSCC utilizing bioinformatic approaches and validation assays with the aim of informing the development of targeted therapies for OSCC.\n429 chromatin regulators were screened by a machine learning approach and aberrant expression of SMYD3 was found to be closely associated with OSCC formation and poor prognosis. Data profiling of single-cell and tissue demonstrated that upregulated SMYD3 significantly correlated with aggressive clinicopathological features of OSCC. Alterations in copy number and DNA methylation patterns may contribute to SMYD3 overexpression. Functional experimental results suggested that SMYD3 enhanced cancer cell stemness and proliferation in vitro and tumor growth in vivo. SMYD3 was observed to bind to the High Mobility Group AT-Hook 2 (HMGA2) promoter and elevated tri-methylation of histone H3 lysine 4 at the corresponding site was responsible for transactivating HMGA2. SMYD3 also was positively linked to HMGA2 expression in OSCC samples. Furthermore, treatment with the SMYD3 chemical inhibitor BCI-121 exerted anti-tumor effects.\nHistone methyltransferase activity and transcription-potentiating function of SMYD3 were found to be essential for tumorigenesis and the SMYD3-HMGA2 is a potential therapeutic target in OSCC.", 
    "abstract": "Each year, many help seekers in need contact health helplines for mental support. It is crucial that they receive support immediately, and that waiting times are minimal. In order to minimize delay, helplines must have adequate staffing levels, especially during peak hours. This has raised the need for means to predict the call and chat volumes ahead of time accurately. Motivated by this, in this paper, we analyze real-life data to develop models for accurately forecasting call volumes, for both phone and chat conversations for online mental health support.\nThis research was conducted on real call and chat data (adequately anonymized) provided by 113 Suicide Prevention (Over ons | 113 Zelfmoordpreventie)\u00a0(throughout referred to as '113'), the online helpline for suicide prevention in the Netherlands. Chat and phone call data were analyzed to better understand the important factors that influence the call arrival process. These factors were then used as input to several Machine Learning (ML) models to forecast the number of call and chat arrivals. Next to that, senior counselors of the helpline completed a web-based questionnaire after each shift to assess their perception of the workload.\nThis study has led to several remarkable and key insights. First, the most important factors that determine the call volumes for the helpline are the trend, and weekly and daily cyclic patterns (cycles), while monthly and yearly cycles were found to be non-significant predictors for the number of phone and chat conversations. Second, media events that were included in this study only have limited-and only short-term-impact on the call volumes. Third, so-called (S)ARIMA models are shown to lead to the most accurate prediction in the case of short-term forecasting, while simple linear models work best for long-term forecasting. Fourth, questionnaires filled in by senior counselors show that the experienced workload is mainly correlated to the number of chat conversations compared to phone calls.\n(S)ARIMA models can best be used to forecast the number of daily chats and phone calls with a MAPE of less than 10 in short-term forecasting. These models perform better than other models showing that the number of arrivals depends on historical data. These forecasts can be used as support for planning the number of counselors needed. Furthermore, the questionnaire data show that the workload experienced by senior counselors is more dependent on the number of chat arrivals and less on the number of available agents, showing the value of insight into the arrival process of conversations.", 
    "abstract": "A quantitative assessment of pulmonary edema is important because the clinical severity can range from mild impairment to life threatening. A quantitative surrogate measure, although invasive, for pulmonary edema is the extravascular lung water index (EVLWI) extracted from the transpulmonary thermodilution (TPTD). Severity of edema from chest X-rays, to date is based on the subjective classification of radiologists. In this work, we use machine learning to quantitatively predict the severity of pulmonary edema from chest radiography.\nWe retrospectively included 471 X-rays from 431 patients who underwent chest radiography and TPTD measurement within 24\u00a0h at our intensive care unit. The EVLWI extracted from the TPTD was used as a quantitative measure for pulmonary edema. We used a deep learning approach and binned the data into two, three, four and five classes increasing the resolution of the EVLWI prediction from the X-rays.\nThe accuracy, area under the receiver operating characteristic curve (AUROC) and Mathews correlation coefficient (MCC) in the binary classification models (EVLWI\u2009<\u200915,\u2009\u2265\u200915) were 0.93 (accuracy), 0.98 (AUROC) and 0.86(MCC). In the three multiclass models, the accuracy ranged between 0.90 and 0.95, the AUROC between 0.97 and 0.99 and the MCC between 0.86 and 0.92.\nDeep learning can quantify pulmonary edema as measured by EVLWI with high accuracy.", 
    "abstract": "This study explored Veteran and family member perspectives on factors that drive post-traumatic stress disorder (PTSD) therapy engagement within constructs of the Andersen model of behavioral health service utilization. Despite efforts by the Department of Veterans Affairs (VA) to increase mental health care access, the proportion of Veterans with PTSD who engage in PTSD therapy remains low. Support for therapy from family members and friends could improve Veteran therapy use.\nWe applied a multiple methods approach using data from VA administrative data and semi-structured individual interviews with Veterans and their support partners who applied to the VA Caregiver Support Program. We integrated findings from a machine learning analysis of quantitative data with findings from a qualitative analysis of the semi-structured interviews.\nIn quantitative models, Veteran medical need for health care use most influenced treatment initiation and retention. However, qualitative data suggested mental health symptoms combined with positive Veteran and support partner treatment attitudes motivated treatment engagement. Veterans indicated their motivation to seek treatment increased when family members perceived treatment to be of high value. Veterans who experienced poor continuity of VA care, group, and virtual treatment modalities expressed less care satisfaction. Prior marital therapy use emerged as a potentially new facilitator of PTSD treatment engagement that warrants more exploration.\nOur multiple methods findings represent Veteran and support partner perspectives and show that amid Veteran and organizational barriers to care, attitudes and support of family members and friends still matter. Family-oriented services and intervention could be a gateway to increase Veteran PTSD therapy engagement.", 
    "abstract": "In most mammals and particularly in mice, chemical communication relies on the detection of ethologically relevant fitness-related cues from other individuals. In mice, urine is the primary source of these signals, so we employed proteomics and metabolomics to identify key components of chemical signalling. We show that there is a correspondence between urinary volatiles and proteins in the representation of genetic background, sex and environment in two house mouse subspecies Mus musculus musculus and M. m. domesticus. We found that environment has a strong influence upon proteomic and metabolomic variation and that volatile mixtures better represent males while females have surprisingly more sex-biased proteins. Using machine learning and combined-omics techniques, we identified mixtures of metabolites and proteins that are associated with biological features.", 
    "abstract": "A major environmental problem on a global scale is the contamination of water by dyes, particularly from industrial effluents. Consequently, wastewater treatment from various industrial wastes is crucial to restoring environmental quality. Dye is an important class of organic pollutants that are considered harmful to both people and aquatic habitats. The textile industry has become more interested in agricultural-based adsorbents, particularly in adsorption. The biosorption of Methylene blue (MB) dye from aqueous solutions by the wheat straw (T. aestivum) biomass was evaluated in this study. The biosorption process parameters were optimized using the response surface methodology (RSM) approach with a face-centred central composite design (FCCCD). Using a 10\u00a0mg/L concentration MB dye, 1.5\u00a0mg of biomass, an initial pH of 6, and a contact time of 60\u00a0min at 25\u00a0\u00b0C, the maximum MB dye removal percentages (96%) were obtained. Artificial neural network (ANN) modelling techniques are also employed to stimulate and validate the process, and their efficacy and ability to predict the reaction (removal efficiency) were assessed. The existence of functional groups, which are important binding sites involved in the process of MB biosorption, was demonstrated using Fourier Transform Infrared Spectroscopy (FTIR) spectra. Moreover, a scan electron microscope (SEM) revealed that fresh, shiny particles had been absorbed on the surface of the T. aestivum following the biosorption procedure. The bio-removal of MB from wastewater effluents has been demonstrated to be possible using T. aestivum biomass as a biosorbent. It is also a promising biosorbent that is economical, environmentally friendly, biodegradable, and cost-effective.", 
    "abstract": "This study aimed to develop a machine learning-based clinical decision support system for emergency departments based on the decision-making framework of physicians. We extracted 27 fixed and 93 observation features using data on vital signs, mental status, laboratory results, and electrocardiograms during emergency department stay. Outcomes included intubation, admission to the intensive care unit, inotrope or vasopressor administration, and in-hospital cardiac arrest. eXtreme gradient boosting algorithm was used to learn and predict each outcome. Specificity, sensitivity, precision, F1 score, area under the receiver operating characteristic curve (AUROC), and area under the precision-recall curve were assessed. We analyzed 303,345 patients with 4,787,121 input data, resampled into 24,148,958 1\u00a0h-units. The models displayed a discriminative ability to predict outcomes (AUROC\u2009>\u20090.9), and the model with lagging 6 and leading 0 displayed the highest value. The AUROC curve of in-hospital cardiac arrest had the smallest change, with increased lagging for all outcomes. With inotropic use, intubation, and intensive care unit\u00a0admission, the range of AUROC curve change with the leading 6 was the highest according to different amounts of previous information (lagging). In this study, a human-centered approach to emulate the clinical decision-making process of emergency physicians has been adopted to enhance the use of the system. Machine learning-based clinical decision support systems customized according to clinical situations can help improve the quality of care.", 
    "abstract": "For endocrine-positive Her2 negative breast cancer patients at an early stage, the benefit of adding chemotherapy to adjuvant endocrine therapy is not still confirmed. Several genomic tests are available on the market but are very expensive. Therefore, there is the urgent need to explore novel reliable and less expensive prognostic tools in this setting. In this paper, we shown a machine learning survival model to estimate Invasive Disease-Free Events trained on clinical and histological data commonly collected in clinical practice. We collected clinical and cytohistological outcomes of 145 patients referred to Istituto Tumori \"Giovanni Paolo II\". Three machine learning survival models are compared with the Cox proportional hazards regression according to time-dependent performance metrics evaluated in cross-validation. The c-index at 10\u00a0years obtained by random survival forest, gradient boosting, and component-wise gradient boosting is stabled with or without feature selection at approximately 0.68 in average respect to 0.57 obtained to Cox model. Moreover, machine learning survival models have accurately discriminated low- and high-risk patients, and so a large group which can be spared additional chemotherapy to hormone therapy. The preliminary results obtained by including only clinical determinants are encouraging. The integrated use of data already collected in clinical practice for routine diagnostic investigations, if properly analyzed, can reduce time and costs of the genomic tests.", 
    "abstract": "The accuracy of reliability models is one of the most problematic issues that must be considered for the life of electronic assemblies, particularly those used for critical applications. The reliability of electronics is limited by the fatigue life of interconnected solder materials, which is influenced by many factors. This paper provides a method to build a robust machine-learning reliability model to predict the life of solder joints in common applications. The impacts of combined fatigue and creep stresses on solder joints are also investigated in this paper. The common alloy used in solder joint fabrication is SAC305 (Sn-Ag-Cu). The test vehicle includes individual solder joints of SAC305 alloy assembled on a printed circuit board. The effects of testing temperature, stress amplitude, and creep dwell time on the life of solder joints were considered. A two-parameter Weibull distribution was utilized to analyze the fatigue life. Inelastic work and plastic strain were extracted from the stress-strain curves. Then, Artificial Neural Networks (ANNs) were used to build a machine learning model to predict characteristic life obtained from the Weibull analysis. The inelastic work and plastic stains were also considered in the ANN model. Fuzzy logic was used to combine the process parameters and fatigue properties and to construct the final life prediction model. Then a relationship equation between the comprehensive output measure obtained from the fuzzy system and the life was determined using a nonlinear optimizer. The results indicated that increasing the stress level, testing temperature, and creep dwell time decreases reliability. The case of long creep dwell time at elevated temperatures is worst in terms of impact on reliability. Finally, a single robust reliability model was computed as a function of the fatigue properties and process parameters. A significant enhancement of the prediction model was achieved compared to the stress-life equations.", 
    "abstract": "The number of magnetic resonance imaging (MRI) studies on neuronal correlates of catatonia has dramatically increased in the last 10\u00a0years, but conclusive findings on white matter (WM) tracts alterations underlying catatonic symptoms are still lacking. Therefore, we conduct an interdisciplinary longitudinal MRI study (whiteCAT) with two main objectives: First, we aim to enroll 100 psychiatric patients with and 50 psychiatric patients without catatonia according to ICD-11 who will undergo a deep phenotyping approach with an extensive battery of demographic, psychopathological, psychometric, neuropsychological, instrumental and diffusion MRI assessments at baseline and 12\u00a0weeks follow-up. So far, 28 catatonia patients and 40 patients with schizophrenia or other primary psychotic disorders or mood disorders without catatonia have been studied cross-sectionally. 49 out of 68 patients have completed longitudinal assessment, so far. Second, we seek to develop and implement a new method for semi-automatic fiber tract delineation using active learning. By training supportive machine learning algorithms on the fly that are custom tailored to the respective analysis pipeline used to obtain the tractogram as well as the WM tract of interest, we plan to streamline and speed up this tedious and error-prone task while at the same time increasing reproducibility and robustness of the extraction process. The goal is to develop robust neuroimaging biomarkers of symptom severity and therapy outcome based on WM tracts underlying catatonia. If our MRI study is successful, it will be the largest longitudinal study to date that has investigated WM tracts in catatonia patients.", 
    "abstract": "Studies of voluntary visual-spatial attention have used attention-directing cues, such as arrows, to induce or instruct observers to focus selective attention on relevant locations in visual space in order to detect or discriminate subsequent target stimuli. In everyday vision, however, voluntary attention is influenced by a host of factors, most of which are quite different from the laboratory paradigms that utilize attention-directing cues. These factors include priming, experience, reward, meaning, motivations, and high-level behavioral goals. Attention that is endogenously directed in the absence of external attention-directing cues has been referred to as self-initiated attention, or as in our prior work, as \"willed attention\" where volunteers decide where to attend in respond to a prompt to do so. Here, we used a novel paradigm that eliminated external influences (i.e., attention-directing cues and prompts) about where and/or when spatial attention should be directed. Using machine learning decoding methods, we showed that the well-known lateralization of EEG alpha power during spatial attention was also present during purely self-generated attention. By eliminating explicit cues or prompts that affect the allocation of voluntary attention, this work advances our understanding of the neural correlates of attentional control, and provides steps toward the development of EEG-based brain-computer interfaces that tap into human intentions.", 
    "abstract": "Machine learning has been used to analyse heart failure subtypes, but not across large, distinct, population-based datasets, across the whole spectrum of causes and presentations, or with clinical and non-clinical validation by different machine learning methods. Using our published framework, we aimed to discover heart failure subtypes and validate them upon population representative data.\nIn this external, prognostic, and genetic validation study we analysed individuals aged 30 years or older with incident heart failure from two population-based databases in the UK (Clinical Practice Research Datalink [CPRD] and The Health Improvement Network [THIN]) from 1998 to 2018. Pre-heart failure and post-heart failure factors (n=645) included demographic information, history, examination, blood laboratory values, and medications. We identified subtypes using four unsupervised machine learning methods (K-means, hierarchical, K-Medoids, and mixture model clustering) with 87 of 645 factors in each dataset. We evaluated subtypes for (1) external validity (across datasets); (2) prognostic validity (predictive accuracy for 1-year mortality); and (3) genetic validity (UK Biobank), association with polygenic risk score (PRS) for heart failure-related traits (n=11), and single nucleotide polymorphisms (n=12).\nWe included 188\u2008800, 124\u2008262, and 9573 individuals with incident heart failure from CPRD, THIN, and UK Biobank, respectively, between Jan 1, 1998, and Jan 1, 2018. After identifying five clusters, we labelled heart failure subtypes as (1) early onset, (2) late onset, (3) atrial fibrillation related, (4) metabolic, and (5) cardiometabolic. In the external validity analysis, subtypes were similar across datasets (c-statistics: THIN model in CPRD ranged from 0\u00b779 [subtype 3] to 0\u00b794 [subtype 1], and CPRD model in THIN ranged from 0\u00b779 [subtype 1] to 0\u00b792 [subtypes 2 and 5]). In the prognostic validity analysis, 1-year all-cause mortality after heart failure diagnosis (subtype 1 0\u00b720 [95% CI 0\u00b714-0\u00b725], subtype 2 0\u00b746 [0\u00b743-0\u00b749], subtype 3 0\u00b761 [0\u00b757-0\u00b764], subtype 4 0\u00b711 [0\u00b707-0\u00b716], and subtype 5 0\u00b737 [0\u00b732-0\u00b741]) differed across subtypes in CPRD and THIN data, as did risk of non-fatal cardiovascular diseases and all-cause hospitalisation. In the genetic validity analysis the atrial fibrillation-related subtype showed associations with the related PRS. Late onset and cardiometabolic subtypes were the most similar and strongly associated with PRS for hypertension, myocardial infarction, and obesity (p<0\u00b70009). We developed a prototype app for routine clinical use, which could enable evaluation of effectiveness and cost-effectiveness.\nAcross four methods and three datasets, including genetic data, in the largest study of incident heart failure to date, we identified five machine learning-informed subtypes, which might inform aetiological research, clinical risk prediction, and the design of heart failure trials.\nEuropean Union Innovative Medicines Initiative-2.", 
    "abstract": null, 
    "abstract": null, 
    "abstract": "Drug combination therapy has become a common strategy for the treatment of complex diseases. There is an urgent need for computational methods to efficiently identify appropriate drug combinations owing to the high cost of experimental screening. In recent years, deep learning has been widely used in the field of drug discovery. Here, we provide a comprehensive review on deep-learning-based drug combination prediction algorithms from multiple aspects. Current studies highlight the flexibility of this technology in integrating multimodal data and the ability to achieve state-of-art performance; it is expected that deep-learning-based prediction of drug combinations should play an important part in future drug discovery.", 
    "abstract": "DrugRepurposing Online is a database of well-curated literature examples of drug repurposing, structured by reference to compounds and indications, via a generalisation layer (within specific datasets) of mechanism. References are categorised by level of relevance to human application to assist users in prioritising repurposing hypotheses. Users can search freely between any two of the three categories in either direction; results can then be extended to the third category. The concatenation of two (or more) direct relationships to create an indirect, hypothetical new repurposing relationship is intended to offer novel and non-obvious opportunities that can be both patented and efficiently developed. A natural language processing (NLP) powered search capability extends the opportunities from the hand-curated foundation to identify further opportunities.", 
    "abstract": "In prior studies, central pulmonary embolism (PE) was associated with high clot burden and was considered an independent predictor for thrombolysis. Further information about predictors of adverse outcomes in these patients is needed for better risk stratification. The objective is to describe independent predictors of adverse clinical outcomes in patients with central PE.\nLarge retrospective, observational, and single-center study of hospitalized patients with central PE. Data were gathered on demographics, comorbidities, clinical features on admission, imaging, treatments, and outcomes. Multivariable standard and Least Absolute Shrinkage and Selection Operator (LASSO) machine learning logistic regressions and sensitivity analyses were used to analyze factors associated with a composite of adverse clinical outcomes, including vasopressor use, mechanical ventilation, and inpatient mortality.\nA total of 654 patients had central PE. The mean age was 63.1 years, 59% were women, and 82% were African American. The composite adverse outcome was observed in 18% (n\u202f=\u202f115) of patients. Serum creatinine elevation (odds ratio [OR]\u202f=\u202f1.37, 95% CI\u202f=\u202f1.20-1.57; p\u202f=\u202f0.0001), white blood cell (WBC) count elevation (OR\u202f=\u202f1.10, 95% CI\u202f=\u202f1.05-1.15; p\u202f<\u202f0.001), higher simplified pulmonary embolism severity index (sPESI) score (OR\u202f=\u202f1.47, 95% CI\u202f=\u202f1.18-1.84; p\u202f=\u202f0.001), serum troponin elevation (OR\u202f=\u202f1.26, 95% CI 1.02-1.56; p\u202f=\u202f0.03), and respiratory rate increase (OR\u202f=\u202f1.03, 95% CI\u202f=\u202f1.0-1.05; p\u202f=\u202f0.02) were independent predictors of adverse clinical outcomes.\nAmong patients with central PE, higher sPESI score, WBC count elevation, serum creatinine elevation, serum troponin elevation, and respiratory rate increase were independent predictors of adverse clinical outcomes. Right ventricular dysfunction on imaging and saddle PE location did not predict adverse outcomes.", 
    "abstract": "Mild traumatic brain injury (mTBI) is characterized as brain microstructural damage, which may cause a wide range of brain functional disturbances and emotional problems. Brain network analysis based on machine learning is an important means of neuroimaging research. Obtaining the most discriminating functional connection is of great significance to analyze the pathological mechanism of mTBI.\nTo better obtain the most discriminating features of functional connection networks, this study proposes a hierarchical feature selection pipeline (HFSP) composed of Variance Filtering (VF), Lasso, and Principal Component Analysis (PCA). Ablation experiments indicate that each module plays a positive role in classification, validating the robustness and reliability of the HFSP. Furthermore, the HFSP is compared with recursive feature elimination (RFE), elastic net (EN), and locally linear embedding (LLE), verifying its superiority. In addition, this study also utilizes random forest (RF), SVM, Bayesian, linear discriminant analysis (LDA), and logistic regression (LR) as classifiers to evaluate the generalizability of HFSP.\nThe results show that the indexes obtained from RF are the highest, with accuracy = 89.74%, precision = 91.26%, recall = 89.74%, and F1 score = 89.42%. The HFSP selects 25 pairs of the most discriminating functional connections, mainly distributed in the frontal lobe, occipital lobe, and cerebellum. Nine brain regions show the largest node degree.\nThe number of samples is small. This study only includes acute mTBI.\nThe HFSP is a useful tool for extracting discriminating functional connections and may contribute to the diagnostic processes..", 
    "abstract": null, 
    "abstract": "Dynamic radiographic measurements of 3-Dimensional (3-D) total knee arthroplasty (TKA) kinematics have provided important information for implant design and surgical technique for over 30 years. However, current methods of measuring TKA kinematics are too cumbersome, inaccurate, or time-consuming for practical clinical application. Even state-of-the-art techniques require human-supervision to obtain clinically reliable kinematics. Eliminating human supervision could potentially make this technology practical for clinical use.\nWe demonstrate a fully autonomous pipeline for quantifying 3D-TKA kinematics from single-plane radiographic imaging. First, a convolutional neural network (CNN) segmented the femoral and tibial implants from the image. Second, those segmented images were compared to pre-computed shape libraries for initial pose estimates. Lastly, a numerical optimization routine aligned 3D implant contours and fluoroscopic images to obtain the final implant poses.\nThe autonomous technique reliably produces kinematic measurements comparable to human-supervised measures, with root-mean-squared differences of less than 0.7 mm and 4\u00b0 for our test data, and 0.8 mm and 1.7\u00b0 for external validation studies.\nA fully autonomous method to measure 3D-TKA kinematics from single-plane radiographic images produces results equivalent to a human-supervised method, and may soon make it practical to perform these measurements in a clinical setting.", 
    "abstract": "Cerebral small vessel disease (SVD) is common during ageing and can present as stroke, cognitive decline, neurobehavioural symptoms, or functional impairment. SVD frequently coexists with neurodegenerative disease, and can exacerbate cognitive and other symptoms and affect activities of daily living. Standards for Reporting Vascular Changes on Neuroimaging 1 (STRIVE-1) categorised and standardised the diverse features of SVD that are visible on structural MRI. Since then, new information on these established SVD markers and novel MRI sequences and imaging features have emerged. As the effect of combined SVD imaging features becomes clearer, a key role for quantitative imaging biomarkers to determine sub-visible tissue damage, subtle abnormalities visible at high-field strength MRI, and lesion-symptom patterns, is also apparent. Together with rapidly emerging machine learning methods, these metrics can more comprehensively capture the effect of SVD on the brain than the structural MRI features alone and serve as intermediary outcomes in clinical trials and future routine practice. Using a similar approach to that adopted in STRIVE-1, we updated the guidance on neuroimaging of vascular changes in studies of ageing and neurodegeneration to create STRIVE-2.", 
    "abstract": "Cyclic peptides have emerged as a promising class of therapeutics. However, their ", 
    "abstract": "Growing evidence links long-term air pollution exposure with renal function. However, little research has been conducted on the combined effects of air pollutant mixture on renal function and multiple mediation effects of metabolic risk factors. This study enrolled 8996 adults without chronic kidney disease (CKD) at baseline from the CHCN-BTH cohort study. Three-year exposure to air pollutants [particulate matter \u2264\u00a02.5\u00a0\u00b5m (PM", 
    "abstract": "The aims of this study were to determine whether paranasal sinus volumetric measurements differ according to sex, age group, and right-left side and to determine the rate of sexual dimorphism using discriminant function analysis and machine learning algorithms. The study included paranasal computed tomography images of 100 live individuals of known sex and age. The paranasal sinuses were marked using semiautomatic segmentation and their volumes and densities were measured. Sex determination using discriminant analyses and machine learning algorithms was performed. Males had higher mean volumes of all paranasal sinuses than females (P < 0.05); however, there were no statistically significant differences between age groups or sides (P > 0.05). The paranasal sinus volumes of females were more dysmorphic during sex determination. The frontal sinus volume had the highest accuracy, whereas the sphenoid sinus volume was the least dysmorphic. In this study, although there was moderate sexual dimorphism in paranasal sinus volumes, the use of machine learning methods increased the accuracy of sex estimation. We believe that sex estimation rates will be significantly higher in future studies that combine linear measurements, volumetric measurements, and machine-learning algorithms.", 
    "abstract": "Retrospective cohort study.\nTo determine prognostic factors for the progression of osteoporotic vertebral fracture (OVF) following conservative treatment.\nFew studies have evaluated factors associated with progressive collapse of OVFs. Furthermore, machine learning has not been applied in this context.\nThe study involved the progression of collapse (PC) and non-PC groups based on a compression rate of 15%. Clinical data, fracture site, OVF shape, Cobb angle, and anterior wedge angle of the fractured vertebra were evaluated. The presence of intravertebral cleft and the type of bone marrow signal change were analyzed using magnetic resonance imaging. Multivariate logistic regression analysis was performed to identify prognostic factors. In machine learning methods, decision tree (DT) and random forest (RF) models were used.\nThere were no significant differences in clinical data between the groups. The proportion of fracture shape (P<0.001) and bone marrow signal change (P=0.01) were significantly different between the groups. Moderate wedge shape was frequently observed in the non-PC group (31.7%), whereas the normative shape was most common in the PC group (54.7%). The Cobb angle and anterior wedge angle at diagnosis of OVFs were higher in the non-PC group (13.2\u00b110.9; P=0.001, 14.3\u00b16.6; P<0.001) than in the PC group (10.3\u00b111.8, 10.4\u00b15.5). The bone marrow signal change at the superior aspect of the vertebra was more frequently found in the PC group (42.5%) than in the non-PC group (34.9%). Machine learning revealed that vertebral shape at initial diagnosis was a main predictor of progressive vertebral collapse.\nThe initial shape of the vertebra and bone edema pattern on MRI appear to be useful prognostic factors for the progression of collapse in OVFs.", 
    "abstract": "Despite being researched for decades, shape-shifting molecular crystals have yet to claim their spot as an actuating materials class among the primary functional materials. While the process for developing and commercializing materials can be lengthy, it inevitably starts with building an extensive knowledge base, which for molecular crystal actuators remains scattered and disjointed. Using machine learning for the first time, we identify inherent features and structure-function relationships that fundamentally impact the mechanical response of molecular crystal actuators. Our model can factor in different crystal properties in tandem and decipher their intersectional and combined effects on each actuation performance. This analysis is an open invitation to utilize interdisciplinary expertise in translating the current basic research on molecular crystal actuators into technology-based development that promotes large-scale experimentation and prototyping.", 
    "abstract": "Hepatocyte intrinsic clearance (CL", 
    "abstract": "PNCK, or CAMK1b, is an understudied kinase of the calcium-calmodulin dependent kinase family which recently has been identified as a marker of cancer progression and survival in several large-scale multi-omics studies. The biology of PNCK and its relation to oncogenesis has also begun to be elucidated, with data suggesting various roles in DNA damage response, cell cycle control, apoptosis and HIF-1-alpha related pathways. To further explore PNCK as a clinical target, potent small-molecule molecular probes must be developed. Currently, there are no targeted small molecule inhibitors in pre-clinical or clinical studies for the CAMK family. Additionally, there exists no experimentally derived crystal structure for PNCK. We herein report a three-pronged chemical probe discovery campaign which utilized homology modeling, machine learning, virtual screening and molecular dynamics to identify small molecules with low-micromolar potency against PNCK activity from commercially available compound libraries. We report the discovery of a hit-series for the first targeted effort towards discovering PNCK inhibitors that will serve as the starting point for future medicinal chemistry efforts for hit-to-lead optimization of potent chemical probes.", 
    "abstract": "Machine learning tools have proven useful across biological disciplines, allowing researchers to draw conclusions from large datasets, and opening up new opportunities for interpreting complex and heterogeneous biological data. Alongside the rapid growth of machine learning, there have also been growing pains: some models that appear to perform well have later been revealed to rely on features of the data that are artifactual or biased; this feeds into the general criticism that machine learning models are designed to optimize model performance over the creation of new biological insights. A natural question arises: how do we develop machine learning models that are inherently interpretable or explainable? In this manuscript, we describe the SWIF(r) reliability score (SRS), a method building on the SWIF(r) generative framework that reflects the trustworthiness of the classification of a specific instance. The concept of the reliability score has the potential to generalize to other machine learning methods. We demonstrate the utility of the SRS when faced with common challenges in machine learning including: 1) an unknown class present in testing data that was not present in training data, 2) systemic mismatch between training and testing data, and 3) instances of testing data that have missing values for some attributes. We explore these applications of the SRS using a range of biological datasets, from agricultural data on seed morphology, to 22 quantitative traits in the UK Biobank, and population genetic simulations and 1000 Genomes Project data. With each of these examples, we demonstrate how the SRS can allow researchers to interrogate their data and training approach thoroughly, and to pair their domain-specific knowledge with powerful machine-learning frameworks. We also compare the SRS to related tools for outlier and novelty detection, and find that it has comparable performance, with the advantage of being able to operate when some data are missing. The SRS, and the broader discussion of interpretable scientific machine learning, will aid researchers in the biological machine learning space as they seek to harness the power of machine learning without sacrificing rigor and biological insight.", 
    "abstract": "Stepwise linear regression (SLR) is the most common approach to predicting activities of daily living at discharge with the Functional Independence Measure (FIM) in stroke patients, but noisy nonlinear clinical data decrease the predictive accuracies of SLR. Machine learning is gaining attention in the medical field for such nonlinear data. Previous studies reported that machine learning models, regression tree (RT), ensemble learning (EL), artificial neural networks (ANNs), support vector regression (SVR), and Gaussian process regression (GPR), are robust to such data and increase predictive accuracies. This study aimed to compare the predictive accuracies of SLR and these machine learning models for FIM scores in stroke patients.\nSubacute stroke patients (N = 1,046) who underwent inpatient rehabilitation participated in this study. Only patients' background characteristics and FIM scores at admission were used to build each predictive model of SLR, RT, EL, ANN, SVR, and GPR with 10-fold cross-validation. The coefficient of determination (R2) and root mean square error (RMSE) values were compared between the actual and predicted discharge FIM scores and FIM gain.\nMachine learning models (R2 of RT = 0.75, EL = 0.78, ANN = 0.81, SVR = 0.80, GPR = 0.81) outperformed SLR (0.70) to predict discharge FIM motor scores. The predictive accuracies of machine learning methods for FIM total gain (R2 of RT = 0.48, EL = 0.51, ANN = 0.50, SVR = 0.51, GPR = 0.54) were also better than of SLR (0.22).\nThis study suggested that the machine learning models outperformed SLR for predicting FIM prognosis. The machine learning models used only patients' background characteristics and FIM scores at admission and more accurately predicted FIM gain than previous studies. ANN, SVR, and GPR outperformed RT and EL. GPR could have the best predictive accuracy for FIM prognosis.", 
    "abstract": "Protein-Protein binding affinity reflects the binding strength between the binding partners. The prediction of protein-protein binding affinity is important for elucidating protein functions and also for designing protein-based therapeutics. The geometric characteristics such as area (both interface and surface areas) in the structure of a protein-protein complex play an important role in determining protein-protein interactions and their binding affinity. Here, we present a free web server for academic use, ", 
    "abstract": "Automatically solving math word problems (MWPs) is a challenging task for artificial intelligence (AI) and machine learning (ML) research, which aims to answer the problem with a mathematical expression. Many existing solutions simply model the MWP as a sequence of words, which is far from precise solving. To this end, we turn to how humans solve MWPs. Humans read the problem part-by-part and capture dependencies between words for a thorough understanding and infer the expression precisely in a goal-driven manner with knowledge. Moreover, humans can associate different MWPs to help solve the target with related experience. In this article, we present a focused study on an MWP solver by imitating such procedure. Specifically, we first propose a novel hierarchical math solver (HMS) to exploit semantics in one MWP. First, to imitate human reading habits, we propose a novel encoder to learn the semantics guided by dependencies between words following a hierarchical \"word-clause-problem\" paradigm. Next, we develop a goal-driven tree-based decoder with knowledge application to generate the expression. One step further, to imitate human associating different MWPs for related experience in problem-solving, we extend HMS to the Relation-enHanced Math Solver (RHMS) to utilize the relation between MWPs. First, to capture the structural similarity relation, we develop a meta-structure tool to measure the similarity based on the logical structure of MWPs and construct a graph to associate related MWPs. Then, based on the graph, we learn an improved solver to exploit related experience for higher accuracy and robustness. Finally, we conduct extensive experiments on two large datasets, which demonstrates the effectiveness of the two proposed methods and the superiority of RHMS.", 
    "abstract": "Artificial intelligence and machine learning have been increasingly used in the medical imaging field in the past few years. The evaluation of medical images is very subjective and complex, and therefore the application of artificial intelligence and deep learning methods to automatize the analysis process would be very beneficial. A lot of researchers have been applying these methods to image analysis diagnosis, developing software capable of assisting veterinary doctors or radiologists in their daily practice. This article details the main methodologies used to develop software applications on machine learning and how veterinarians with an interest in this field can benefit from such methodologies. The main goal of this study is to offer veterinary professionals a simple guide to enable them to understand the basics of artificial intelligence and machine learning and the concepts such as deep learning, convolutional neural networks, transfer learning, and the performance evaluation method. The language is adapted for medical technicians, and the work already published in this field is reviewed for application in the imaging diagnosis of different animal body systems: musculoskeletal, thoracic, nervous, and abdominal.", 
    "abstract": "With about 13,000 known species, ants are the most abundant venomous insects. Their venom consists of polypeptides, enzymes, alkaloids, biogenic amines, formic acid, and hydrocarbons. In this study, we investigated, using in silico techniques, the peptides composing a putative antimicrobial arsenal from the venom gland of the neotropical trap-jaw ant ", 
    "abstract": "Per- and polyfluoroalkyl substances (PFASs) are important and ubiquitous environmental contaminants worldwide. These novel contaminants can enter human bodies via various pathways, subsequently posing risks to the ecosystem and human health. The exposure of pregnant women to PFASs might pose risks to the health of mothers and the growth and development of fetuses. However, little information is available about the placental transfer of PFASs from mothers to fetuses and the related mechanisms through model simulation. In the present study, based upon a review of previously published literature, we initially summarized the exposure pathways of PFASs in pregnant women, factors affecting the efficiency of placental transfer, and mechanisms associated with placental transfer; outlined simulation analysis approaches using molecular docking and machine learning to reveal the mechanisms of placental transfer; and finally highlighted future research emphases that need to be focused on. Consequently, it was notable that the binding of PFASs to proteins during placental transfer could be simulated by molecular docking and that the placental transfer efficiency of PFASs could also be predicted by machine learning. Therefore, future research on the maternal-fetal transfer mechanisms of PFASs with the benefit of simulation analysis approaches is warranted to provide a scientific basis for the health effects of PFASs on newborns.", 
    "abstract": "Ecological risk assessment of combined polluted soil has been conducted mostly on the basis of the risk screening value (", 
    "abstract": "Data-based approaches are promising alternatives to the traditional analytical constitutive models for solid mechanics. Herein, we propose a Gaussian process (GP) based constitutive modeling framework, specifically focusing on planar, hyperelastic and incompressible soft tissues. The strain energy density of soft tissues is modeled as a GP, which can be regressed to experimental stress-strain data obtained from biaxial experiments. Moreover, the GP model can be weakly constrained to be convex. A key advantage of a GP-based model is that, in addition to the mean value, it provides a probability density (i.e. associated uncertainty) for the strain energy density. To simulate the effect of this uncertainty, a non-intrusive stochastic finite element analysis (SFEA) framework is proposed. The proposed framework is verified against an artificial dataset based on the Gasser-Ogden-Holzapfel model and applied to a real experimental dataset of a porcine aortic valve leaflet tissue. Results show that the proposed framework can be trained with limited experimental data and fits the data better than several existing models. The SFEA framework provides a straightforward way of using the experimental data and quantifying the resulting uncertainty in simulation-based predictions.", 
    "abstract": "", 
    "abstract": "The increasing precision of observations of the large-scale structure of the universe has created a problem for simulators: running the simulations necessary to interpret these observations has become impractical. Simulators have thus turned to machine learning (ML) algorithms instead. Though ML decreases computational expense, one might be worried about the use of ML for scientific investigations: How can algorithms that have repeatedly been described as black-boxes deliver scientific understanding? In this paper, I investigate how cosmologists employ ML, arguing that in this context, ML algorithms should not be considered black-boxes and can deliver genuine scientific understanding. Accordingly, understanding the methodological role of ML algorithms is crucial to understanding the types of questions they are capable of, and ought to be responsible for, answering.", 
    "abstract": "Accurate preoperative assessment of surgical difficulty is crucial to the success of the surgery and patient safety. This study aimed to evaluate the difficulty for endoscopic resection (ER) of gastric gastrointestinal stromal tumors (gGISTs) using multiple machine learning (ML) algorithms.\nFrom December 2010 to December 2022, 555 patients with gGISTs in multi-centers were retrospectively studied and assigned to a training, validation, and test cohort. A \nThe GBM model outperformed other models with an AUC of 0.894 in the validation and 0.791 in the test cohorts. Furthermore, the GBM model achieved the highest accuracy among these AutoML models, with 0.935 and 0.911 in the validation and test cohorts, respectively. In addition, it was found that tumor size and endoscopists' experience were the most prominent features that significantly impacted the AutoML model's performance in predicting the difficulty for ER of gGISTs.\nThe AutoML model based on the GBM algorithm can accurately predict the difficulty for ER of gGISTs before surgery.", 
    "abstract": "Collectively, rare genetic disorders affect a substantial portion of the world's population. In most cases, those affected face difficulties in receiving a clinical diagnosis and genetic characterization. The understanding of the molecular mechanisms of these diseases and the development of therapeutic treatments for patients are also challenging. However, the application of recent advancements in genome sequencing/analysis technologies and computer-aided tools for predicting phenotype-genotype associations can bring significant benefits to this field. In this review, we highlight the most relevant online resources and computational tools for genome interpretation that can enhance the diagnosis, clinical management, and development of treatments for rare disorders. Our focus is on resources for interpreting single nucleotide variants. Additionally, we present use cases for interpreting genetic variants in clinical settings and review the limitations of these results and prediction tools. Finally, we have compiled a curated set of core resources and tools for analyzing rare disease genomes. Such resources and tools can be utilized to develop standardized protocols that will enhance the accuracy and effectiveness of rare disease diagnosis.", 
    "abstract": "Artificial intelligence (AI) scholars and mediciners have reported AI systems that accurately detect medical imaging and COVID-19 in chest images. However, the robustness of these models remains unclear for the segmentation of images with nonuniform density distribution or the multiphase target. The most representative one is the Chan-Vese (CV) image segmentation model. In this paper, we demonstrate that the recent level set (LV) model has excellent performance on the detection of target characteristics from medical imaging relying on the filtering variational method based on the global medical pathology facture. We observe that the capability of the filtering variational method to obtain image feature quality is better than other LV models. This research reveals a far-reaching problem in medical-imaging AI knowledge detection. In addition, from the analysis of experimental results, the algorithm proposed in this paper has a good effect on detecting the lung region feature information of COVID-19 images and also proves that the algorithm has good adaptability in processing different images. These findings demonstrate that the proposed LV method should be seen as an effective clinically adjunctive method using machine-learning healthcare models.", 
    "abstract": "Cocoa cultivation is the basis for chocolate production; it has a unique aroma that makes it useful in the production of snacks and usable for cooking or baking. The maximum harvest period of cocoa is normally once or twice a year and spread over several months, depending on the country. Determining the best harvesting period for cocoa pods plays a major role in the export process and the pods quality. The degree of ripening of the pods affects the quality of the resulting beans. Also, unripe pods do not have enough sugar and may prevent proper bean fermentation. As for too-mature pods, they are usually dry, and their beans may germinate inside the pods, or they may develop a fungal disease and cannot be used. Computer-based determination of the ripeness of cocoa pods throughout image analysis could facilitate massive cocoa ripeness detection. Recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities for agricultural engineering and computer scientists to meet the demands of the manual. The need for diverse and representative sets of pod images is essential for developing and testing automatic cocoa pod maturity detection systems. In this perspective, we collected images of cocoa pods to set up a database of cocoa pods of the C\u00f4te d'Ivoire named CocoaMFDB. We performed a pre-processing step using the CLAHE algorithm to improve the quality of the images since the effect of the light was not controlled on our data set. CocoaMFDB allows the characterization of cocoa pods according to their maturity level and provides information on the pod family for each image. Our dataset comprises three large families, namely Amelonado, Angoleta, and Guiana, grouped into two maturity categories: the ripe and unripe pods. It is, therefore, perfect for developing and evaluating image analysis algorithms for future research.", 
    "abstract": "This research considered several applications of a coupled Internet of Things sensor network with Edge Computing (IoTEC) for improved environmental monitoring. Two pilot applications, covering environmental monitoring of vapor intrusion and system performance of wastewater-based algae cultivation, were designed to compare data latency, energy consumption, and economic cost between the IoTEC approach and the conventional sensor monitoring method. The results show that the IoTEC monitoring approach, compared with conventional IoT sensor networks, could significantly reduce data latency by 13%, and the amount of data transmission decreased by an average of 50%. In addition, the IoTEC method can increase the duration of power supply by 130%. Collectively, these improvements could lead to a compelling cost reduction of 55% - 82% per year for monitoring vapor intrusion at five houses, with more houses leading to more significant savings. Additionally, our results demonstrate the feasibility of deploying machine learning tools at edge servers for more advanced data processing and analysis.", 
    "abstract": "Alzheimer's disease (AD) is the most common neurodegenerative disease, imposing huge mental and economic burdens on patients and society. The specific molecular pathway(s) and biomarker(s) that distinguish AD from other neurodegenerative diseases and reflect the disease progression are still not well studied.\nFour frontal cortical datasets of AD were integrated to conduct differentially expressed genes (DEGs) and functional gene enrichment analyses. The transcriptional changes after the integrated frontal cortical datasets subtracting the cerebellar dataset of AD were further compared with frontal cortical datasets of frontotemporal dementia and Huntingdon's disease to identify AD-frontal-associated gene expression. Integrated bioinformatic analysis and machine-learning strategies were applied for screening and determining diagnostic biomarkers, which were further validated in another two frontal cortical datasets of AD by receiver operating characteristic (ROC) curves.\nSix hundred and twenty-six DEGs were identified as AD frontal associated, including 580 downregulated genes and 46 upregulated genes. The functional enrichment analysis revealed that immune response and oxidative stress were enriched in AD patients. Decorin (DCN) and regulator of G protein signaling 1 (RGS1) were screened as diagnostic biomarkers in distinguishing AD from frontotemporal dementia and Huntingdon's disease of AD. The diagnostic effects of DCN and RGS1 for AD were further validated in another two datasets of AD: the areas under the curve (AUCs) reached 0.8148 and 0.8262 in GSE33000, and 0.8595 and 0.8675 in GSE44770. There was a better value for AD diagnosis when combining performances of DCN and RGS1 with the AUCs of 0.863 and 0.869. Further, DCN mRNA level was correlated to CDR (Clinical Dementia Rating scale) score (\nDCN and RGS1 associated with the immune response may be useful biomarkers for diagnosing AD and distinguishing the disease from frontotemporal dementia and Huntingdon's disease. DCN mRNA level reflects the development of the disease.", 
    "abstract": "Community health worker (CHW)-led maternal health programs have contributed to increased facility-based deliveries and decreased maternal mortality in sub-Saharan Africa. The recent adoption of mobile devices in these programs provides an opportunity for real-time implementation of machine learning predictive models to identify women most at risk for home-based delivery. However, it is possible that falsified data could be entered into the model to get a specific prediction result - known as an \"adversarial attack\". The goal of this paper is to evaluate the algorithm's vulnerability to adversarial attacks.\nThe dataset used in this research is from the \nManipulating input variables affected prediction results. The variable with the greatest vulnerability was previous delivery location, with 55.65% of predicted classifications changing when applying adversarial attacks from previously delivered at a facility to previously delivered at home, and 37.63% of predicted classifications changing when applying adversarial attacks from previously delivered at home to previously delivered at a facility.\nThis paper investigates the vulnerability of an algorithm to predict facility-based delivery when facing adversarial attacks. By understanding the effect of adversarial attacks, programs can implement data monitoring strategies to assess for and deter these manipulations. Ensuring fidelity in algorithm deployment secures that CHWs target those women who are actually at high risk of delivering at home.", 
    "abstract": "This study utilized both experimental testing and machine learning (ML) strategies to assess the effectiveness of waste glass powder (WGP) on the compressive strength (CS) of cement mortar. The cement-to-sand ratio was kept 1:1 with a water-to-cement ratio of 0.25. The superplasticizer content was 4% by cement mass, and the proportion of silica fume was 15%, 20%, and 25% by cement mass in three different mixes. WGP was added to cement mortar at replacement contents from 0 to 15% for sand and cement with a 2.5% increment. Initially, using an experimental method, the CS of WGP-based cement mortar at the age of 28 days was calculated. The obtained data were then used to forecast the CS using ML techniques. For CS estimation, two ML approaches, namely decision tree and AdaBoost, were applied. The ML model's performance was assessed by calculating the coefficient of determination (R", 
    "abstract": "Zoonotic virus spillover in human hosts including outbreaks of Hantavirus and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) imposes a serious impact on the quality of life of patients. Recent studies provide a shred of evidence that patients with Hantavirus-caused hemorrhagic fever with renal syndrome (HFRS) are at risk of contracting SARS-CoV-2. Both RNA viruses shared a higher degree of clinical features similarity including dry cough, high fever, shortness of breath, and certain reported cases with multiple organ failure. However, there is currently no validated treatment option to tackle this global concern. This study is attributed to the identification of common genes and perturbed pathways by combining differential expression analysis with bioinformatics and machine learning approaches. Initially, the transcriptomic data of hantavirus-infected peripheral blood mononuclear cells (PBMCs) and SARS-CoV-2 infected PBMCs were analyzed through differential gene expression analysis for identification of common differentially expressed genes (DEGs). The functional annotation by enrichment analysis of common genes demonstrated immune and inflammatory response biological processes enriched by DEGs. The protein-protein interaction (PPI) network of DEGs was then constructed and six genes named RAD51, ALDH1A1, UBA52, CUL3, GADD45B, and CDKN1A were identified as the commonly dysregulated hub genes among HFRS and COVID-19. Later, the classification performance of these hub genes were evaluated using Random Forest (RF), Poisson Linear Discriminant Analysis (PLDA), Voom-based Nearest Shrunken Centroids (voomNSC), and Support Vector Machine (SVM) classifiers which demonstrated accuracy >70%, suggesting the biomarker potential of the hub genes. To our knowledge, this is the first study that unveiled biological processes and pathways commonly dysregulated in HFRS and COVID-19, which could be in the next future used for the design of personalized treatment to prevent the linked attacks of COVID-19 and HFRS.", 
    "abstract": "", 
    "abstract": "This study identified the risk factors for type 2 diabetes (T2D) and proposed a machine learning (ML) technique for predicting T2D. The risk factors for T2D were identified by multiple logistic regression (MLR) using p-value (p<0.05). Then, five ML-based techniques, including logistic regression, na\u00efve Bayes, J48, multilayer perceptron, and random forest (RF) were employed to predict T2D. This study utilized two publicly available datasets, derived from the National Health and Nutrition Examination Survey, 2009-2010 and 2011-2012. About 4922 respondents with 387 T2D patients were included in 2009-2010 dataset, whereas 4936 respondents with 373 T2D patients were included in 2011-2012. This study identified six risk factors (age, education, marital status, SBP, smoking, and BMI) for 2009-2010 and nine risk factors (age, race, marital status, SBP, DBP, direct cholesterol, physical activity, smoking, and BMI) for 2011-2012. RF-based classifier obtained 95.9% accuracy, 95.7% sensitivity, 95.3% F-measure, and 0.946 area under the curve.", 
    "abstract": "Contrary to the self-limiting nature of reversible bone marrow lesions, irreversible bone marrow lesions require early surgical intervention to prevent further morbidity. Thus, early discrimination of irreversible pathology is necessitated. The purpose of this study is to evaluate the efficacy of radiomics and machine learning regarding this topic.\nA database was scanned for patients who had undergone MRI of the hip for differential diagnosis of bone marrow lesions and had had follow-up images acquired within 8 weeks after the first imaging. Images that showed resolution of oedema were included in the reversible group. The remainders that showed progression into characteristic signs of osteonecrosis were included in the irreversible group. Radiomics was performed on the first MR images, calculating first- and second-order parameters. Support vector machine and random forest classifiers were performed using these parameters.\nThirty-seven patients (seventeen osteonecrosis) were included. A total of 185 ROIs were segmented. Fortyseven parameters were accepted as classifiers with an area under the curve value ranging from 0.586 to 0.718. Support vector machine yielded a sensitivity of 91.3% and a specificity of 85.1%. Random forest classifier yielded a sensitivity of 84.8% and a specificity of 76.7%. Area under curves were 0.921 for support vector machine and 0.892 for random forest classifier.\nRadiomics analysis could prove useful for discrimination of reversible and irreversible bone marrow lesions before the irreversible changes occur, which could prevent morbidities of osteonecrosis by guiding the decisionmaking process for management.", 
    "abstract": "Imaging Mueller polarimetry (IMP) appears as a promising technique for real-time delineation of healthy and neoplastic tissue during neurosurgery. The training of machine learning algorithms used for the image post-processing requires large data sets typically derived from the measurements of formalin-fixed brain sections. However, the success of the transfer of such algorithms from fixed to fresh brain tissue depends on the degree of alterations of polarimetric properties induced by formalin fixation (FF).\nComprehensive studies were performed on the FF induced changes in fresh pig brain tissue polarimetric properties.\nPolarimetric properties of pig brain were assessed in 30 coronal thick sections before and after FF using a wide-field IMP system. The width of the uncertainty region between gray and white matter was also estimated.\nThe depolarization increased by 5% in gray matter and remained constant in white matter following FF, whereas the linear retardance decreased by 27% in gray matter and by 28% in white matter after FF. The visual contrast between gray and white matter and fiber tracking remained preserved after FF. Tissue shrinkage induced by FF did not have a significant effect on the uncertainty region width.\nSimilar polarimetric properties were observed in both fresh and fixed brain tissues, indicating a high potential for transfer learning.", 
    "abstract": "Cost estimates for care for those with dementia and other cognitive impairments are rising globally, estimated to reach US $1 trillion by 2025. Lack of specialized personnel, infrastructure, diagnostic capabilities, and healthcare access impedes the timely identification of patients progressing to dementia, particularly in underserved populations. International healthcare infrastructure may be unable to handle existing cases in addition to a sudden increase due to undiagnosed cognitive impairment and dementia. Healthcare bioinformatics offers a potential route for quicker access to healthcare services; however, a better preparedness plan must be implemented now if expected demands are to be met. The most critical consideration for implementing artificial intelligence/machine learning (AI/ML) -driven clinical decision intelligence applications (CDIA) is ensuring patients and practitioners take action on the information provided.", 
    "abstract": "Compensatory movements usually occur in stroke survivors with hemiplegia, which is detrimental to recovery. This paper proposes a compensatory movement detection method based on near-infrared spectroscopy (NIRS) technology and verifies its feasibility using a machine learning algorithm. We present a differential-based signal improvement (DBSI) method to enhance NIRS signal quality and discuss its effect on improving detection performance.\nTen healthy subjects and six stroke survivors performed three common rehabilitation training tasks while the activation of six trunk muscles was recorded using NIRS sensors. After data preprocessing, DBSI was applied to the NIRS signals, and two time-domain features (mean and variance) were extracted. An SVM algorithm was used to test the effect of the NIRS signal on compensatory behavior detection.\nClassification results show that NIRS signals have good performance in compensatory detection, with accuracy rates of 97.76% in healthy subjects and 97.95% in stroke survivors. After using the DBSI method, the accuracy improved to 98.52% and 99.47%, respectively.\nCompared with other compensatory motion detection methods, our proposed method based on NIRS technology has better classification performance. The study highlights the potential of NIRS technology for improving stroke rehabilitation and warrants further investigation.", 
    "abstract": "Forkhead box M1 (FOXM1) is a member of the Forkhead box (Fox) transcription factor family. It regulates cell mitosis, cell proliferation, and genome stability. However, the relationship between the expression of FOXM1 and the levels of m6a modification, immune infiltration, glycolysis, and ketone body metabolism in HCC has yet to be fully elucidated.\nTranscriptome and somatic mutation profiles of HCC were downloaded from the TCGA database. Somatic mutations were analyzed by maftools R package and visualized in oncoplots. GO, KEGG and GSEA function enrichment was performed on FOXM1 co-expression using R. We used Cox regression and machine learning algorithms (CIBERSORT, LASSO, random forest, and SVM-RFE) to study the prognostic value of FOXM1 and immune infiltrating characteristic immune cells in HCC. The relationship between FOXM1 and m6A modification, glycolysis, and ketone body metabolism were analyzed by RNA-seq and CHIP-seq. The competing endogenous RNA (ceRNA) network construction relies on the multiMiR R package, ENCORI, and miRNET platforms.\nFOXM1 is highly expressed in HCC and is associated with a poorer prognosis. At the same time, the expression level of FOXM1 is significantly related to the T, N, and stage. Subsequently, based on the machine learning strategies, we found that the infiltration level of T follicular helper cells (Tfh) was a risk factor affecting the prognosis of HCC patients. The high infiltration of Tfh was significantly related to the poor overall survival rate of HCC. Besides, the CHIP-seq demonstrated that FOXM1 regulates m6a modification by binding to the promoter of IGF2BP3 and affects the glycolytic process by initiating the transcription of HK2 and PKM in HCC. A ceRNA network was successfully obtained, including FOXM1 - has-miR-125-5p - DANCR/MIR4435-2HG ceRNA network related to the prognosis of HCC.\nOur study implicates that the aberrant infiltration of Tfh associated with FOXM1 is a crucial prognostic factor for HCC patients. FOXM1 regulates genes related to m6a modification and glycolysis at the transcriptional level. Furthermore, the specific ceRNA network can be used as a potential therapeutic target for HCC.", 
    "abstract": "Heart failure (HF) is the final stage of many cardiovascular illnesses and the leading cause of death worldwide. At the same time, ischemic cardiomyopathy has replaced valvular heart disease and hypertension as the primary causes of heart failure. Cellular senescence in heart failure is currently receiving more attention. In this paper, we investigated the correlation between the immunological properties of myocardial tissue and the pathological mechanisms of cellular senescence during ischemic cardiomyopathy leading to heart failure (ICM-HF) using bioinformatics and machine learning methodologies. Our goals were to clarify the pathogenic causes of heart failure and find new treatment options. First, after obtaining GSE5406 from the Gene Expression Omnibus (GEO) database and doing limma analysis, differential genes (DEGs) among the ICM-HF and control groups were identified. We intersected these differential genes with cellular senescence-associated genes (CSAG) ", 
    "abstract": "A high tumor mutation burden (TMB) is known to drive the response to immune checkpoint inhibitors (ICI) and is associated with favorable prognoses. However, because it is a one-dimensional numerical representation of non-synonymous genetic alterations, TMB suffers from clinical challenges due to its equal quantification. Since not all mutations elicit the same antitumor rejection, the effect on immunity of neoantigens encoded by different types or locations of somatic mutations may vary. In addition, other typical genomic features, including complex structural variants, are not captured by the conventional TMB metric. Given the diversity of cancer subtypes and the complexity of treatment regimens, this paper proposes that tumor mutations capable of causing various degrees of immunogenicity should be calculated separately. TMB should therefore, be segmented into more exact, higher dimensional feature vectors to exhaustively measure the foreignness of tumors. We systematically reviewed patients' multifaceted efficacy based on a refined TMB metric, investigated the association between multidimensional mutations and integrative immunotherapy outcomes, and developed a convergent categorical decision-making framework, TMBserval (Statistical Explainable machine learning with Regression-based VALidation). TMBserval integrates a multiple-instance learning concept with statistics to create a statistically interpretable model that addresses the broad interdependencies between multidimensional mutation burdens and decision endpoints. TMBserval is a pan-cancer-oriented many-to-many nonlinear regression model with discrimination and calibration power. Simulations and experimental analyses using data from 137 actual patients both demonstrated that our method could discriminate between patient groups in a high-dimensional feature space, thereby rationally expanding the beneficiary population of immunotherapy.", 
    "abstract": "Microalgal biotechnology holds the potential for renewable biofuels, bioproducts, and carbon capture applications due to unparalleled photosynthetic efficiency and diversity. Outdoor open raceway pond (ORP) cultivation enables utilization of sunlight and atmospheric carbon dioxide to drive microalgal biomass synthesis for production of bioproducts including biofuels; however, environmental conditions are highly dynamic and fluctuate both diurnally and seasonally, making ORP productivity prediction challenging without time-intensive physical measurements and location-specific calibrations. Here, for the first time, we present an image-based deep learning method for the prediction of ORP productivity. Our method is based on parameter profile plot images of sensor parameters, including pH, dissolved oxygen, temperature, photosynthetically active radiation, and total dissolved solids. These parameters can be remotely monitored without physical interaction with ORPs. We apply the model to data we generated during the Unified Field Studies of the Algae Testbed Public-Private-Partnership (ATP", 
    "abstract": "Unplanned patient readmissions within 30 days of discharge pose a substantial challenge in Canadian health care economics. To address this issue, risk stratification, machine learning, and linear regression paradigms have been proposed as potential predictive solutions. Ensemble machine learning methods, such as stacked ensemble models with boosted tree algorithms, have shown promise for early risk identification in specific patient groups.\nThis study aims to implement an ensemble model with submodels for structured data, compare metrics, evaluate the impact of optimized data manipulation with principal component analysis on shorter readmissions, and quantitatively verify the causal relationship between expected length of stay (ELOS) and resource intensity weight (RIW) value for a comprehensive economic perspective.\nThis retrospective study used Python 3.9 and streamlined libraries to analyze data obtained from the Discharge Abstract Database covering 2016 to 2021. The study used 2 sub-data sets, clinical and geographical data sets, to predict patient readmission and analyze its economic implications, respectively. A stacking classifier ensemble model was used after principal component analysis to predict patient readmission. Linear regression was performed to determine the relationship between RIW and ELOS.\nThe ensemble model achieved precision and slightly higher recall (0.49 and 0.68), indicating a higher instance of false positives. The model was able to predict cases better than other models in the literature. Per the ensemble model, readmitted women and men aged 40 to 44 and 35 to 39 years, respectively, were more likely to use resources. The regression tables verified the causality of the model and confirmed the trend that patient readmission is much more costly than continued hospital stay without discharge for both the patient and health care system.\nThis study validates the use of hybrid ensemble models for predicting economic cost models in health care with the goal of reducing the bureaucratic and utility costs associated with hospital readmissions. The availability of robust and efficient predictive models, as demonstrated in this study, can help hospitals focus more on patient care while maintaining low economic costs. This study predicts the relationship between ELOS and RIW, which can indirectly impact patient outcomes by reducing administrative tasks and physicians' burden, thereby reducing the cost burdens placed on patients. It is recommended that changes to the general ensemble model and linear regressions be made to analyze new numerical data for predicting hospital costs. Ultimately, the proposed work hopes to emphasize the advantages of implementing hybrid ensemble models in forecasting health care economic cost models, empowering hospitals to prioritize patient care while simultaneously decreasing administrative and bureaucratic expenses.", 
    "abstract": "The objective of this study was to reveal the signs and symptoms for the classification of pediatric patients at risk of CKD using decision trees and extreme gradient boost models for predicting outcomes. A case-control study was carried out involving children with 376 chronic kidney disease (cases) and a control group of healthy children (n\u2009=\u2009376). A family member responsible for the children answered a questionnaire with variables potentially associated with the disease. Decision tree and extreme gradient boost models were developed to test signs and symptoms for the classification of children. As a result, the decision tree model revealed 6 variables associated with CKD, whereas twelve variables that distinguish CKD from healthy children were found in the \"XGBoost\". The accuracy of the \"XGBoost\" model (ROC AUC\u2009=\u20090.939, 95%CI: 0.911 to 0.977) was the highest, while the decision tree model was a little lower (ROC AUC\u2009=\u20090.896, 95%CI: 0.850 to 0.942). The cross-validation of results showed that the accuracy of the evaluation database model was like that of the training.\nIn conclusion, a dozen symptoms that are easy to be clinically verified emerged as risk indicators for chronic kidney disease. This information can contribute to increasing awareness of the diagnosis, mainly in primary care settings. Therefore, healthcare professionals can select patients for more detailed investigation, which will reduce the chance of wasting time and improve early disease detection.\n\u2022\u00a0Late diagnosis of chronic kidney disease in children is common, increasing morbidity. \u2022\u00a0Mass screening of the whole population is not cost-effective.\n\u2022\u00a0With two machine-learning methods, this study revealed 12 symptoms to aid early CKD diagnosis. \u2022\u00a0These symptoms are easily obtainable and can be useful mainly in primary care settings.", 
    "abstract": "Urban Green Infrastructure (UGI) provides ecosystem services such as cooling of temperatures and is majorly important for climate change adaptation. Green Volume (GV) describes the 3-D space occupied by vegetation and is highly useful for the assessment of UGI. This research uses Sentinel-2 (S-2) optical data, vegetation indices (VIs), Sentinel-1 (S-1) and PALSAR-2 (P-2) radar data to build machine learning models for yearly GV estimation on large scales. Our study compares random and stratified sampling of reference data, assesses the performance of different machine learning algorithms and tests model transferability by independent validation. The results indicate that stratified sampling of training data leads to improved accuracies when compared to random sampling. While the Gradient Tree Boost (GTB) and Random Forest (RF) algorithms show generally similar performance, Support Vector Machine (SVM) exhibits considerably greater model error. The results suggest RF to be the most robust classifier overall, achieving highest accuracies for independent and inter-annual validation. Furthermore, modelling GV based on S-2 features considerably outperforms using only S-1 or P-2 based features. Moreover, the study finds that underestimation of large GV magnitudes in urban forests constitutes the biggest source of model error. Overall, modelled GV explains around 79% of the variability in reference GV at 10\u2009m resolution and over 90% when aggregated to 100\u2009m resolution. The research shows that accurately modelling GV is possible using openly available satellite data. Resulting GV predictions can be useful for environmental management by providing valuable information for climate change adaptation, environmental monitoring and change detection.", 
    "abstract": null, 
    "abstract": "While economical and effective catalysts are required for sustainable hydrogen production, low-dimensional interfacial engineering techniques have been developed to improve the catalytic activity in the hydrogen evolution reaction (HER). In this study, we used density functional theory (DFT) calculations to measure the Gibbs free energy change (\u0394", 
    "abstract": "Trillions of diverse microbes reside in the gut and are deeply interwoven with the human physiological process, from food digestion, immune system maturation, and fighting invading pathogens, to drug metabolism. Microbial drug metabolism has a profound impact on drug absorption, bioavailability, stability, efficacy, and toxicity. However, our knowledge of specific gut microbial strains, and their genes that encode enzymes involved in the metabolism, is limited. The microbiome encodes over 3 million unique genes contributing to a huge enzymatic capacity, vastly expanding the traditional drug metabolic reactions that occur in the liver, manipulating their pharmacological effect, and, ultimately, leading to variation in drug response. For example, the microbial deactivation of anticancer drugs such as gemcitabine can lead to resistance to chemotherapeutics or the crucial role of microbes in modulating the efficacy of the anticancer drug, cyclophosphamide. On the other hand, recent findings show that many drugs can shape the composition, function, and gene expression of the gut microbial community, making it harder to predict the outcome of drug-microbiota interactions. In this review, we discuss the recent understanding of the multidirectional interaction between the host, oral medications, and gut microbiota, using traditional and machine-learning approaches. We analyze gaps, challenges, and future promises of personalized medicine that consider gut microbes as a crucial player in drug metabolism. This consideration will enable the development of personalized therapeutic regimes with an improved outcome, ultimately leading to precision medicine.", 
    "abstract": "Untargeted metabolomics is an important tool in studying health and disease and is employed in fields such as biomarker discovery and drug development, as well as precision medicine. Although significant technical advances were made in the field of mass-spectrometry driven metabolomics, instrumental drifts, such as fluctuations in retention time and signal intensity, remain a challenge, particularly in large untargeted metabolomics studies. Therefore, it is crucial to consider these variations during data processing to ensure high-quality data. Here, we will provide recommendations for an optimal data processing workflow using intrastudy quality control (QC) samples that identifies errors resulting from instrumental drifts, such as shifts in retention time and metabolite intensities. Furthermore, we provide an in-depth comparison of the performance of three popular batch-effect correction methods of different complexity. By using different evaluation metrics based on QC samples and a machine learning approach based on biological samples, the performance of the batch-effect correction methods were evaluated. Here, the method TIGER demonstrated the overall best performance by reducing the relative standard deviation of the QCs and dispersion-ratio the most, as well as demonstrating the highest area under the receiver operating characteristic with three different probabilistic classifiers (Logistic regression, Random Forest, and Support Vector Machine). In summary, our recommendations will help to generate high-quality data that are suitable for further downstream processing, leading to more accurate and meaningful insights into the underlying biological processes.", 
    "abstract": "Metabolic diseases have been related to the overdrinking of high-sugar content beverages. As a result, the demand for alternative formulations based on plant-based ingredients with health-promoting properties has increased during the last few years. Nonetheless, the design and production of effective formulations requires understanding the bioavailability of these compounds. For this purpose, a two-month longitudinal trial with 140 volunteers was conducted to measure the beneficial effects of a maqui-citrus beverage, rich in (poly)phenols. From data obtained by quantifying metabolites present in urine samples, biostatistical and machine learning (data imputation, feature selection, and clustering) methods were applied to assess whether a volunteer's sex and the sweetener added to the beverage (sucrose, sucralose, or stevia) affected the bioavailability of (poly)phenol metabolites. Several metabolites have been described as being differentially influenced: 3,4-dihydroxyphenylacetic acid and naringenin with its derivatives were positively influenced by stevia and men, while eriodictyol sulfate and homoeridictyol glucunoride concentrations were enhanced with stevia and women. By examining groups of volunteers created by clustering analysis, patterns in metabolites' bioavailability distribution as a function of sex and/or sweeteners (or even due to an uncontrolled factor) were also discovered. These results underline the potential of stevia as a (poly)phenol bioavailability enhancer. Furthermore, they also evidence sex affects the bioavailability of (poly)phenols, pointing at a sex-dependent metabolic pathway regulation.", 
    "abstract": "Overall, combating food waste necessitates a multifaceted approach that includes education, infrastructure, and policy change. By working together to implement these strategies, we can help reduce the negative impacts of food waste and create a more sustainable and equitable food system. The sustained supply of nutrient-rich agrifood commodities is seriously threatened by inefficiencies caused by agricultural losses, which must be addressed. As per the statistical data given by the Food and Agriculture Organisation (FAO) of the United Nations, nearly 33.33% of the food that is produced for utilization is wasted and frittered away on a global level, which can be estimated as a loss of 1.3 billion metric tons per annum, which includes 30% cereals, 20% dairy products 35% seafood and fish, 45% fruits and vegetables, and 20% of meat. This review summarizes the various types of waste originating from various segments of the food industry, such as fruits and vegetables, dairy, marine, and brewery, also focusing on their potential for developing commercially available value-added products such as bioplastics, bio-fertilizers, food additives, antioxidants, antibiotics, biochar, organic acids, and enzymes. The paramount highlights include food waste valorization, which is a sustainable yet profitable alternative to waste management, and harnessing Machine Learning and Artificial Intelligence technology to minimize food waste. Detail of sustainability and feasibility of food waste-derived metabolic chemical compounds, along with the market outlook and recycling of food wastes, have been elucidated in this review.", 
    "abstract": "The assessment, management, and prognostication of spinal cord injury (SCI) mainly rely upon observer-based ordinal scales measures. ", 
    "abstract": "This study aimed to analyze the associations of obstructive sleep apnea (OSA) with dental parameters while controlling for socio-demographics, health-related habits, and each of the diseases comprising metabolic syndrome (MetS), its consequences, and related conditions. We analyzed data from the dental, oral, and medical epidemiological (DOME) cross-sectional records-based study that combines comprehensive socio-demographic, medical, and dental databases of a nationally representative sample of military personnel for one year. Analysis included statistical and machine learning models. The study included 132,529 subjects; of these, 318 (0.2%) were diagnosed with OSA. The following parameters maintained a statistically significant positive association with OSA in the multivariate binary logistic regression analysis (descending order from highest to lowest OR): obesity (OR = 3.104 (2.178-4.422)), male sex (OR = 2.41 (1.25-4.63)), periodontal disease (OR = 2.01 (1.38-2.91)), smoking (OR = 1.45 (1.05-1.99)), and age (OR = 1.143 (1.119-1.168)). Features importance generated by the XGBoost machine learning algorithm were age, obesity, and male sex (located on places 1-3), which are well-known risk factors of OSA, as well as periodontal disease (fourth place) and delivered dental fillings (fifth place). The Area Under Curve (AUC) of the model was 0.868 and the accuracy was 0.92. Altogether, the findings supported the main hypothesis of the study, which was that OSA is linked to dental morbidity, in particular to periodontitis. The findings highlight the need for dental evaluation as part of the workup of OSA patients and emphasizes the need for dental and general medical authorities to collaborate by exchanging knowledge about dental and systemic morbidities and their associations. The study also highlights the necessity for a comprehensive holistic risk management strategy that takes systemic and dental diseases into account.", 
    "abstract": "Colorectal cancer (CRC) is one of the most common and lethal diseases among all types of cancer, and metabolites play a significant role in the development of this complex disease. This study aimed to identify potential biomarkers and targets in the diagnosis and treatment of CRC using high-throughput metabolomics. Metabolite data extracted from the feces of CRC patients and healthy volunteers were normalized with the median normalization and Pareto scale for multivariate analysis. Univariate ROC analysis, the ", 
    "abstract": "Separating carbon dioxide (CO", 
    "abstract": "Deep neural networks have gained popularity in the field of mammography. Data play an integral role in training these models, as training algorithms requires a large amount of data to capture the general relationship between the model's input and output. Open-access databases are the most accessible source of mammography data for training neural networks. Our work focuses on conducting a comprehensive survey of mammography databases that contain images with defined abnormal areas of interest. The survey includes databases such as INbreast, the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM), the OPTIMAM Medical Image Database (OMI-DB), and The Mammographic Image Analysis Society Digital Mammogram Database (MIAS). Additionally, we surveyed recent studies that have utilized these databases in conjunction with neural networks and the results they have achieved. From these databases, it is possible to obtain at least 3801 unique images with 4125 described findings from approximately 1842 patients. The number of patients with important findings can be increased to approximately 14,474, depending on the type of agreement with the OPTIMAM team. Furthermore, we provide a description of the annotation process for mammography images to enhance the understanding of the information gained from these datasets.", 
    "abstract": "Drowsiness-related car accidents continue to have a significant effect on road safety. Many of these accidents can be eliminated by alerting the drivers once they start feeling drowsy. This work presents a non-invasive system for real-time driver drowsiness detection using visual features. These features are extracted from videos obtained from a camera installed on the dashboard. The proposed system uses facial landmarks and face mesh detectors to locate the regions of interest where mouth aspect ratio, eye aspect ratio, and head pose features are extracted and fed to three different classifiers: random forest, sequential neural network, and linear support vector machine classifiers. Evaluations of the proposed system over the National Tsing Hua University driver drowsiness detection dataset showed that it can successfully detect and alarm drowsy drivers with an accuracy up to 99%.", 
    "abstract": "Annuloplasty rings are routinely used in mitral valve repair (MVr). However, accurate annuloplasty ring size selection is essential to obtain a favorable outcome. Moreover, ring sizing can be challenging in some patients and is highly influenced by surgeons' experience. This study investigated the utility of three-dimensional mitral valve (3D-MV) reconstruction models to predict annuloplasty ring size for MVr.\nA total of 150 patients undergoing minimally invasive MVr with annuloplasty ring due to Carpentier type II pathology and who were discharged with none/trace residual MR were included. 3D-MV reconstruction models were created with a semi-automated software package (4D MV Analysis; Tomtec Imaging Systems, Munich, Germany) to quantitate MV geometry. To predict the ring size, univariable and multivariable linear regression analyses were performed.\nBetween 3D-MV reconstruction values and implanted ring sizes, the highest correlation coefficients were provided by commissural width (CW) (0.839; p\u2009<\u20090.001), intertrigonal distance (ITD) (0.796; p\u2009<\u20090.001), annulus area (0.782; p\u2009<\u20090.001), anterior mitral leaflet area (0.767; p\u2009<\u20090.001), Anterior-posterior diameter (0.679; p\u2009<\u20090.001) and AML length (0.515; p\u2009<\u20090.001). In multivariable regression analysis, only CW and ITD were found to be independent predictors of annuloplasty ring size (R2=0.743; p\u2009<\u20090.001). The highest level of agreement was achieved with CW and ITD, and 76.6% of patients received a ring with no greater than one ring size difference from the predicted ring sizes.\n3D-MV reconstruction models can support surgeons in the decision-making process for annuloplasty ring sizing. The present study may be a first step towards accurate annuloplasty ring size prediction using multimodal Machine Learning decision support.", 
    "abstract": "As the world produces exabytes of data, there is a growing need to find new methods that are more suitable for dealing with complex datasets. Artificial intelligence (AI) has significant potential to impact the healthcare industry, which is already on the road to change with the digital transformation of vast quantities of information. The implementation of AI has already achieved success in the domains of molecular chemistry and drug discoveries. The reduction in costs and in the time needed for experiments to predict the pharmacological activities of new molecules is a milestone in science. These successful applications of AI algorithms provide hope for a revolution in healthcare systems. A significant part of artificial intelligence is machine learning (ML), of which there are three main types-supervised learning, unsupervised learning, and reinforcement learning. In this review, the full scope of the AI workflow is presented, with explanations of the most-often-used ML algorithms and descriptions of performance metrics for both regression and classification. A brief introduction to explainable artificial intelligence (XAI) is provided, with examples of technologies that have developed for XAI. We review important AI implementations in cardiology for supervised, unsupervised, and reinforcement learning and natural language processing, emphasizing the used algorithm. Finally, we discuss the need to establish legal, ethical, and methodical requirements for the deployment of AI models in medicine.", 
    "abstract": "As the world population continues to grow, there is a need to come up with alternative sources of feed and food to combat the existing challenge of food insecurity across the globe. The use of insects, particularly the black soldier fly (BSF) ", 
    "abstract": "The Asian tiger mosquito, ", 
    "abstract": "Multicomponent materials are microwave-absorbing (MA) materials composed of a variety of absorbents that are capable of reaching the property inaccessible for a single component. Discovering mostly valuable properties, however, often relies on semi-experience, as conventional multicomponent MA materials' design rules alone often fail in high-dimensional design spaces. Therefore, we propose performance optimization engineering to accelerate the design of multicomponent MA materials with desired performance in a practically infinite design space based on very sparse data. Our approach works as a closed-loop, integrating machine learning with the expanded Maxwell-Garnett model, electromagnetic calculations, and experimental feedback; aiming at different desired performances, Ni surface@carbon fiber (NiF) materials and NiF-based multicomponent (NMC) materials with target MA performance were screened and identified out of nearly infinite possible designs. The designed NiF and NMC fulfilled the requirements for the X- and Ku-bands at thicknesses of only 2.0 and 1.78 mm, respectively. In addition, the targets regarding S, C, and all bands (2.0-18.0 GHz) were also achieved as expected. This performance optimization engineering opens up a unique and effective way to design microwave-absorbing materials for practical application.", 
    "abstract": "Tuberculosis (TB) is among the more frequent causes of death in many countries. For pulmonary TB, early diagnosis greatly increases the efficiency of therapies. Although highly sensitive tests based on nucleic acid amplification tests (NAATs) and loop-mediated isothermal amplification (TB-LAMP) are available, smear microscopy is still the most widespread diagnostics method in most low-middle-income countries, and the true positive rate of smear microscopy is lower than 65%. Thus, there is a need to increase the performance of low-cost diagnosis. For many years, the use of sensors to analyze the exhaled volatile organic compounds (VOCs) has been proposed as a promising alternative for the diagnosis of several diseases, including tuberculosis. In this paper, the diagnostic properties of an electronic nose (EN) based on sensor technology previously used to identify tuberculosis have been tested on-field in a Cameroon hospital. The EN analyzed the breath of a cohort of subjects including pulmonary TB patients (46), healthy controls (38), and TB suspects (16). Machine learning analysis of the sensor array data allows for the identification of the pulmonary TB group with respect to healthy controls with 88% accuracy, 90.8% sensitivity, 85.7% specificity, and 0.88 AUC. The model trained with TB and healthy controls maintains its performance when it is applied to symptomatic TB suspects with a negative TB-LAMP. These results encourage the investigation of electronic noses as an effective diagnostic method for future inclusion in clinical practice.", 
    "abstract": "The implementation of artificial intelligence brings with it a great change in health care, however, there is a discrepancy about the perceptions and attitudes that dental students present towards these new technologies.\nThe study design was observational, descriptive, and cross-sectional. A total of 200 dental students who met the inclusion criteria were surveyed online. For the qualitative variables, descriptive statistical measures were obtained, such as absolute and relative frequencies. For the comparison of the main variables with the type of educational institution, sex and level of education, the chi-square test or Fisher's exact test was used according to the established assumptions with a level of statistical significance of \nThe results indicated that 86% of the students surveyed agreed that artificial intelligence will lead to great advances in dentistry. However, 45% of the participants disagreed that artificial intelligence would replace dentists in the future. In addition, the respondents agreed that the use of artificial intelligence should be part of undergraduate and postgraduate studies with 67% and 72% agreement rates respectively.\nThe attitudes and perceptions of the students indicate that 86% agreed that artificial intelligence will lead to great advances in dentistry. This suggests a bright future for the relationship between dentists and artificial intelligence.", 
    "abstract": "The drug discovery and research for an anti-COVID-19 drug has been ongoing despite repurposed drugs in the market. Over time, these drugs were discontinued due to side effects. The search for effective drugs is still under process. The role of Machine Learning (ML) is critical in the search for novel drug compounds. In the current work, using the equivariant diffusion model, we built novel compounds targeting the spike protein of SARS-CoV-2. Using the ML models, 196 de novo compounds were generated which had no hits on any major chemical databases. These novel compounds fulfilled all the criteria of ADMET properties to be lead-like and drug-like compounds. Of the 196 compounds, 15 were docked with high confidence in the target. These compounds were further subjected to molecular docking, the best compound having an IUPAC name of (4aS,4bR,8aS,8bS)-4a,8a-dimethylbiphenylene-1,4,5,8(4aH,4bH,8aH,8bH)-tetraone and a binding score of -6.930 kcal/mol. The principal compound is labeled as CoECG-M1. Density Function Theory (DFT) and Quantum optimization was carried out along with the study of ADMET properties. This suggests that the compound has potential drug-like properties. The docked complex was further subjected to MD simulations, GBSA, and metadynamics simulations to gain insights into the stability of binding. The model can be in the future modified to improve the positive docking rate.", 
    "abstract": "Memory problems are common among older adults with a history of alcohol use disorder (AUD). Employing a machine learning framework, the current study investigates the use of multi-domain features to classify individuals with and without alcohol-induced memory problems. A group of 94 individuals (ages 50-81 years) with alcohol-induced memory problems (the memory group) were compared with a matched control group who did not have memory problems. The random forests model identified specific features from each domain that contributed to the classification of the memory group vs. the control group (AUC = 88.29%). Specifically, individuals from the memory group manifested a predominant pattern of hyperconnectivity across the default mode network regions except for some connections involving the anterior cingulate cortex, which were predominantly hypoconnected. Other significant contributing features were: (i) polygenic risk scores for AUD, (ii) alcohol consumption and related health consequences during the past five years, such as health problems, past negative experiences, withdrawal symptoms, and the largest number of drinks in a day during the past twelve months, and (iii) elevated neuroticism and increased harm avoidance, and fewer positive \"uplift\" life events. At the neural systems level, hyperconnectivity across the default mode network regions, including the connections across the hippocampal hub regions, in individuals with memory problems may indicate dysregulation in neural information processing. Overall, the study outlines the importance of utilizing multidomain features, consisting of resting-state brain connectivity data collected ~18 years ago, together with personality, life experiences, polygenic risk, and alcohol consumption and related consequences, to predict the alcohol-related memory problems that arise in later life.", 
    "abstract": "The unbounded permutations of biological molecules, including proteins and their constituent peptides, present a dilemma in identifying the components of complex biosamples. Sequence search algorithms used to identify peptide spectra can be expanded to cover larger classes of molecules, including more modifications, isoforms, and atypical cleavage, but at the cost of false positives or false negatives due to the simplified spectra they compute from sequence records. Spectral library searching can help solve this issue by precisely matching experimental spectra to library spectra with excellent sensitivity and specificity. However, compiling spectral libraries that span entire proteomes is pragmatically difficult. Neural networks that predict complete spectra containing a full range of annotated and unannotated ions can be used to replace these simplified spectra with libraries of fully predicted spectra, including modified peptides. Using such a network, we created predicted spectral libraries that were used to rescore matches from a sequence search done over a large search space, including a large number of modifications. Rescoring improved the separation of true and false hits by 82%, yielding an 8% increase in peptide identifications, including a 21% increase in nonspecifically cleaved peptides and a 17% increase in phosphopeptides.", 
    "abstract": "The rise of antibiotic-resistant Mycobacterium tuberculosis (Mtb) has reduced the availability of medications for tuberculosis therapy, resulting in increased morbidity and mortality globally. Tuberculosis spreads from the lungs to other parts of the body, including the brain and spine. Developing a single drug can take several decades, making drug discovery costly and time-consuming. Machine learning algorithms like support vector machines (SVM), k-nearest neighbor (k-NN), random forest (RF) and Gaussian naive base (GNB) are fast and effective and are commonly used in drug discovery. These algorithms are ideal for the virtual screening of large compound libraries to classify molecules as active or inactive. For the training of the models, a dataset of 307 was downloaded from BindingDB. Among 307 compounds, 85 compounds were labeled as active, having an IC50 below 58\u2009mM, while 222 compounds were labeled inactive against thymidylate kinase, with 87.2% accuracy. The developed models were subjected to an external ZINC dataset of 136,564 compounds. Furthermore, we performed the 100-ns dynamic simulation and post trajectories analysis of compounds having good interaction and score in molecular docking. As compared to the standard reference compound, the top three hits revealed greater stability and compactness. In conclusion, our predicted hits can inhibit thymidylate kinase overexpression to combat Mycobacterium tuberculosis.Communicated by Ramaswamy H. Sarma.", 
    "abstract": "Nature is full of a bundle of medicinal substances and its product perceived as a prerogative structure to collaborate with protein drug targets. The natural product's (NPs) structure heterogeneity and eccentric characteristics inspired scientists to work on natural product-inspired medicine. To gear NP drug-finding artificial intelligence (AI) to confront and excavate unexplored opportunities. Natural product-inspired drug discoveries based on AI to act as an innovative tool for molecular design and lead discovery. Various models of machine learning produce quickly synthesizable mimetics of the natural products templates. The invention of novel natural products mimetics by computer-assisted technology provides a feasible strategy to get the natural product with defined bio-activities. AI's hit rate makes its high importance by improving trail patterns such as dose selection, trail life span, efficacy parameters, and biomarkers. Along these lines, AI methods can be a successful tool in a targeted way to formulate advanced medicinal applications for natural products. 'Prediction of future of natural product based drug discovery is not magic, actually its artificial intelligence'Communicated by Ramaswamy H. Sarma.", 
    "abstract": "The volume of ribonucleic acid (RNA)-seq data has increased exponentially, providing numerous new insights into various biological processes. However, due to significant practical challenges, such as data heterogeneity, it is still difficult to ensure the quality of these data when integrated. Although some quality control methods have been developed, sample consistency is rarely considered and these methods are susceptible to artificial factors. Here, we developed MassiveQC, an unsupervised machine learning-based approach, to automatically download and filter large-scale high-throughput data. In addition to the read quality used in other tools, MassiveQC also uses the alignment and expression quality as model features. Meanwhile, it is user-friendly since the cutoff is generated from self-reporting and is applicable to multimodal data. To explore its value, we applied MassiveQC to Drosophila RNA-seq data and generated a comprehensive transcriptome atlas across 28 tissues from embryogenesis to adulthood. We systematically characterized fly gene expression dynamics and found that genes with high expression dynamics were likely to be evolutionarily young and expressed at late developmental stages, exhibiting high nonsynonymous substitution rates and low phenotypic severity, and they were involved in simple regulatory programs. We also discovered that human and Drosophila had strong positive correlations in gene expression in orthologous organs, revealing the great potential of the Drosophila system for studying human development and disease.", 
    "abstract": "Computational analysis of RNA sequences constitutes a crucial step in the field of RNA biology. As in other domains of the life sciences, the incorporation of artificial intelligence and machine learning techniques into RNA sequence analysis has gained significant traction in recent years. Historically, thermodynamics-based methods were widely employed for the prediction of RNA secondary structures; however, machine learning-based approaches have demonstrated remarkable advancements in recent years, enabling more accurate predictions. Consequently, the precision of sequence analysis pertaining to RNA secondary structures, such as RNA-protein interactions, has also been enhanced, making a substantial contribution to the field of RNA biology. Additionally, artificial intelligence and machine learning are also introducing technical innovations in the analysis of RNA-small molecule interactions for RNA-targeted drug discovery and in the design of RNA aptamers, where RNA serves as its own ligand. This review will highlight recent trends in the prediction of RNA secondary structure, RNA aptamers and RNA drug discovery using machine learning, deep learning and related technologies, and will also discuss potential future avenues in the field of RNA informatics.", 
    "abstract": "Machine learning (ML) is an approach to artificial intelligence characterised by the use of algorithms that improve their own performance at a given task (e.g. classification or prediction) based on data and without being explicitly and fully instructed on how to achieve this. Surveillance systems for animal and zoonotic diseases depend upon effective completion of a broad range of tasks, some of them amenable to ML algorithms. As in other fields, the use of ML in animal and veterinary public health surveillance has greatly expanded in recent years. Machine learning algorithms are being used to accomplish tasks that have become attainable only with the advent of large data sets, new methods for their analysis and increased computing capacity. Examples include the identification of an underlying structure in large volumes of data from an ongoing stream of abattoir condemnation records, the use of deep learning to identify lesions in digital images obtained during slaughtering, and the mining of free text in electronic health records from veterinary practices for the purpose of sentinel surveillance. However, ML is also being applied to tasks that previously relied on traditional statistical data analysis. Statistical models have been used extensively to infer relationships between predictors and disease to inform risk-based surveillance, and increasingly, ML algorithms are being used for prediction and forecasting of animal diseases in support of more targeted and efficient surveillance. While ML and inferential statistics can accomplish similar tasks, they have different strengths, making one or the other more or less appropriate in a given context.\nL'apprentissage automatique (AA) est une approche de l'intelligence artificielle caract\u00e9ris\u00e9e par l'utilisation d'algorithmes qui am\u00e9liorent leurs propres performances sur une t\u00e2che donn\u00e9e (par exemple, la classification ou la pr\u00e9diction) sur la base de donn\u00e9es et sans avoir re\u00e7u d'instructions sp\u00e9cifiques ou compl\u00e8tes concernant la marche \u00e0 suivre. Les syst\u00e8mes de surveillance des maladies animales et des zoonoses sont tributaires de la mise en oeuvre efficace d'un large \u00e9ventail de t\u00e2ches, parmi lesquelles certaines sont susceptibles de fonctionner avec des algorithmes d'AA. Comme dans d'autres domaines, l'utilisation de l'AA s'est beaucoup d\u00e9velopp\u00e9e ces derni\u00e8res ann\u00e9es dans le secteur de la surveillance de la sant\u00e9 animale et de la sant\u00e9 publique v\u00e9t\u00e9rinaire. Les algorithmes d'AA sont utilis\u00e9s pour accomplir des t\u00e2ches qui ne sont devenues possibles que gr\u00e2ce \u00e0 l'arriv\u00e9e de grandes s\u00e9ries de donn\u00e9es, de nouvelles m\u00e9thodes d'analyse et de capacit\u00e9s informatiques accrues. Parmi les exemples, on peut citer la capacit\u00e9 \u00e0 d\u00e9celer une structure sous-jacente dans de grands volumes de donn\u00e9es provenant d'un flux continu de registres de saisies d'abattoirs, l'utilisation de l'apprentissage profond pour identifier les l\u00e9sions r\u00e9v\u00e9l\u00e9es par les images num\u00e9riques obtenues pendant l'abattage et l'extraction de texte libre \u00e0 partir des registres sanitaires \u00e9lectroniques des cabinets v\u00e9t\u00e9rinaires \u00e0 des fins de surveillance sentinelle. L'AA est cependant \u00e9galement appliqu\u00e9 dans des t\u00e2ches qui s'appuyaient pr\u00e9c\u00e9demment sur une analyse classique de donn\u00e9es statistiques. Les mod\u00e8les statistiques ont \u00e9t\u00e9 largement utilis\u00e9s pour d\u00e9duire des relations entre pr\u00e9dicteurs et maladie afin d'\u00e9tayer la surveillance fond\u00e9e sur le risque ; les algorithmes d'AA sont de plus en plus utilis\u00e9s pour pr\u00e9dire et pronostiquer des maladies animales en vue d'une surveillance plus cibl\u00e9e et efficace. S'il est vrai que l'AA et la statistique inf\u00e9rentielle peuvent accomplir des t\u00e2ches similaires, chaque approche pr\u00e9sente ses propres atouts et pourra se r\u00e9v\u00e9ler plus ou moins pertinente selon le contexte sp\u00e9cifique.\nEl aprendizaje autom\u00e1tico es una vertiente de la inteligencia artificial que se caracteriza por el uso de algoritmos capaces de mejorarse a s\u00ed mismos en la ejecuci\u00f3n de una determinada tarea (p.ej., procesos de clasificaci\u00f3n o predicci\u00f3n) con empleo de datos y sin necesidad de recibir instrucciones expl\u00edcitas y completas sobre la manera de lograrlo. Los sistemas de vigilancia de enfermedades animales y zoon\u00f3ticas dependen de la ejecuci\u00f3n eficaz de numerosas y muy diversas tareas, algunas de las cuales se prestan al uso de algoritmos de aprendizaje autom\u00e1tico. Al igual que en otros campos, la aplicaci\u00f3n del aprendizaje autom\u00e1tico en sanidad animal y salud p\u00fablica veterinaria se ha extendido sobremanera en los \u00faltimos a\u00f1os. Ahora se utilizan algoritmos de aprendizaje autom\u00e1tico para realizar tareas que solo han empezado a ser factibles con el advenimiento de ingentes conjuntos de datos, nuevos m\u00e9todos para analizarlos y una mayor capacidad de tratamiento inform\u00e1tico. Entre otros ejemplos, cabe citar la determinaci\u00f3n de la estructura subyacente de grandes vol\u00famenes de datos procedentes de un flujo continuo de registros de los descartes de matadero; la utilizaci\u00f3n del aprendizaje profundo para detectar lesiones en im\u00e1genes digitales obtenidas durante las operaciones de sacrificio, o el an\u00e1lisis del texto libre de registros sanitarios electr\u00f3nicos de procedimientos veterinarios con fines de vigilancia centinela. Con todo, el aprendizaje autom\u00e1tico se est\u00e1 aplicando tambi\u00e9n a tareas que anteriormente reposaban en el an\u00e1lisis estad\u00edstico cl\u00e1sico de los datos. Los modelos estad\u00edsticos han sido extensamente utilizados para inferir relaciones entre una enfermedad y uno u otro predictor y alimentar a partir de ah\u00ed la vigilancia basada en el riesgo. Por otro lado, cada vez m\u00e1s se vienen empleando algoritmos de aprendizaje autom\u00e1tico para predecir y anticipar enfermedades animales y conferir as\u00ed m\u00e1s eficacia y especificidad a las actividades de vigilancia. Aunque el aprendizaje autom\u00e1tico y la estad\u00edstica inferencial realizan tareas parecidas, sus puntos fuertes son distintos, con lo cual, en funci\u00f3n del contexto de que se trate, ser\u00e1 preferible recurrir a uno u otro m\u00e9todo.", 
    "abstract": "How do aberrations in widely expressed genes lead to tissue-selective hereditary diseases? Previous attempts to answer this question were limited to testing a few candidate mechanisms. To answer this question at a larger scale, we developed \"Tissue Risk Assessment of Causality by Expression\" (TRACE), a machine learning approach to predict genes that underlie tissue-selective diseases and selectivity-related features. TRACE utilized 4,744 biologically interpretable tissue-specific gene features that were inferred from heterogeneous omics datasets. Application of TRACE to 1,031 disease genes uncovered known and novel selectivity-related features, the most common of which was previously overlooked. Next, we created a catalog of tissue-associated risks for 18,927 protein-coding genes (https://netbio.bgu.ac.il/trace/). As proof-of-concept, we prioritized candidate disease genes identified in 48 rare-disease patients. TRACE ranked the verified disease gene among the patient's candidate genes significantly better than gene prioritization methods that rank by gene constraint or tissue expression. Thus, tissue selectivity combined with machine learning enhances genetic and clinical understanding of hereditary diseases.", 
    "abstract": "Motivation Accurate assignment of procedural codes has important medico-legal, academic, and economic purposes for healthcare providers. Procedural coding requires accurate documentation and exhaustive manual labour to interpret complex operation notes. Ophthalmology operation notes are highly specialised making the process time-consuming and challenging to implement. This study aimed to develop natural language processing (NLP) models trained by medical professionals to assign procedural codes based on the surgical report. The automation and accuracy of these models can reduce burden on healthcare providers and generate reimbursements that reflect the operation performed. Description A retrospective analysis of ophthalmological operation notes from two metropolitan hospitals over a 12-month period was conducted. Procedural codes according to the Medicare Benefits Schedule (MBS) were applied. XGBoost, decision tree, Bidirectional Encoder Representations from Transformers (BERT) and logistic regression models were developed for classification experiments. Experiments involved both multi-label or binary classification, and the best performing model was used on the hold-out test dataset. Results: There were 1000 operation notes included in the study. Following manual review, the five most common procedures were cataract surgery (374 cases), vitrectomy (298 cases), laser therapy (149 cases), trabeculectomy (56 cases), and intravitreal injections (49 cases). Across the entire dataset, current coding was correct in 53.9% of cases. The BERT model had the highest classification accuracy (88.0%) in the multi-label classification on these the five procedures. The total reimbursements achieved by the machine learning algorithm was $184,689.45 ($923.45 per case) compared with the gold standard of $214,527.50 ($1,072.64 per case). Conclusions: Our study demonstrates accurate classification of ophthalmic operation notes into MBS coding categories with NLP technology. Combining human and machine-led approaches involves using NLP to screen operation notes to code procedures, with human review for further scrutiny. This technology can the assignment of correct MBS codes with greater accuracy. Further research and application in this area can facilitate accurate logging of unit activity leading to reimbursements for healthcare providers. Increased accuracy of procedural coding can play an important role in training and education, study of disease epidemiology and improve research ways to optimise patient outcomes.", 
    "abstract": "Neonatal-hypoxic ischemic encephalopathy (HIE) is the leading cause of acquired neonatal brain injury with the risk of developing serious neurologic sequelae and death. An accurate and robust prediction of short- and long-term outcomes may provide clinicians and families with fundamental evidence for their decision-making, the design of treatment strategies, and the discussion of developmental intervention plans after discharge. Diffusion tensor imaging (DTI) is one of the most powerful neuroimaging tools with which to predict the prognosis of neonatal HIE by providing microscopic features that cannot be assessed by conventional MRI. DTI provides various scalar measures that represent the properties of the tissue, such as fractional anisotropy (FA) and mean diffusivity (MD). Since the characteristics of the diffusion of water molecules represented by these measures are affected by the microscopic cellular and extracellular environment, such as the orientation of structural components and cell density, they are often used to study the normal developmental trajectory of the brain and as indicators of various tissue damage, including HIE-related pathologies, such as cytotoxic edema, vascular edema, inflammation, cell death, and Wallerian degeneration. Previous studies have demonstrated widespread alteration in DTI measurements in severe cases of HIE and more localized changes in neonates with mild-to-moderate HIE. In an attempt to establish cutoff values to predict the occurrence of neurological sequelae, MD and FA measured in the corpus callosum (CC), thalamus, basal ganglia, corticospinal tract (CST), and frontal white matter have proven to have an excellent ability to predict severe neurological outcomes. In addition, a recent study has suggested that a data-driven, unbiased approach using machine-learning techniques on features obtained from whole-brain image quantification may accurately predict the prognosis of HIE, including for mild-to-moderate cases. Further efforts are needed to overcome current challenges, such as MRI infrastructure, diffusion modeling methods, and data harmonization for clinical application. In addition, external validation of predictive models is essential for clinical application of DTI to prognostication.", 
    "abstract": null, 
    "abstract": "Patients with insulo-Sylvian gliomas continue to present with severe morbidity in cognitive functions primarily due to neurosurgeons' lack of familiarity with non-traditional brain networks. We sought to identify the frequency of invasion and proximity of gliomas to portions of these networks.\nWe retrospectively analyzed data from 45 patients undergoing glioma surgery centered in the insular lobe. Tumors were categorized based on their proximity and invasiveness of non-traditional cognitive networks and traditionally eloquent structures. Diffusion tensor imaging tractography was completed by creating a personalized brain atlas using Quicktome to determine eloquent and non-eloquent networks in each patient. Additionally, we prospectively collected neuropsychological data on 7 patients to compare tumor-network involvement with change in cognition. Lastly, 2 prospective patients had their surgical plan influenced by network mapping determined by Quicktome.\nForty-four of 45 patients demonstrated tumor involvement (<\u20091\u00a0cm proximity or invasion) with components of non-traditional brain networks involved in cognition such as the salience network (SN, 60%) and the central executive network (CEN, 56%). Of the seven prospective patients, all had tumors involved with the SN, CEN (5/7, 71%), and language network (5/7, 71%). The mean scores of MMSE and MOCA before surgery were 18.71\u2009\u00b1\u20096.94 and 17.29\u2009\u00b1\u20096.26, respectively. The two cases who received preoperative planning with Quicktome had a postoperative performance that was anticipated.\nNon-traditional brain networks involved in cognition are encountered during surgical resection of insulo-Sylvian gliomas. Quicktome can improve the understanding of the presence of these networks and allow for more informed surgical decisions based on patient functional goals.", 
    "abstract": "Pandemics such as the COVID-19 pandemic and other severe health care disruptions endanger individuals to miss essential care. Machine learning models that predict which patients are at greatest risk of missing care visits can help health administrators prioritize retentions efforts towards patients with the most need. Such approaches may be especially useful for efficiently targeting interventions for health systems overburdened during states of emergency.\nWe use data on missed health care visits from over 55,500 respondents of the Survey of Health, Ageing and Retirement in Europe (SHARE) COVID-19 surveys (June - August 2020 and June - August 2021) with longitudinal data from waves 1-8 (April 2004 - March 2020). We compare the performance of four machine learning algorithms (stepwise selection, lasso, random forest, and neural networks) to predict missed health care visits during the first COVID-19 survey based on common patient characteristics available to most health care providers. We test the prediction accuracy, sensitivity, and specificity of the selected models for the first COVID-19 survey by employing 5-fold cross-validation, and test the out-of-sample performance of the models by applying them to the data from the second COVID-19 survey.\nWithin our sample, 15.5% of the respondents reported any missed essential health care visit due to the COVID-19 pandemic. All four machine learning methods perform similarly in their predictive power. All models have an area under the curve (AUC) of around 0.61, outperforming random prediction. This performance is sustained for data from the second COVID-19 wave one year later, with an AUC of 0.59 for men and 0.61 for women. When classifying all men (women) with a predicted risk of 0.135 (0.170) or higher as being at risk of missing care, the neural network model correctly identifies 59% (58%) of the individuals with missed care visits, and 57% (58%) of the individuals without missed care visits. As the sensitivity and specificity of the models are strongly related to the risk threshold used to classify individuals, the models can be calibrated depending on users' resource constraints and targeting approach.\nPandemics such as COVID-19 require rapid and efficient responses to reduce disruptions in health care. Based on characteristics available to health administrators or insurance providers, simple machine learning algorithms can be used to efficiently target efforts to reduce missed essential care.", 
    "abstract": "This study used machine learning techniques to evaluate cardiovascular disease risk factors (CVD) and the relationship between sex and these risk factors. The objective was pursued in the context of CVD being a major global cause of death and the need for accurate identification of risk factors for timely diagnosis and improved patient outcomes. The researchers conducted a literature review to address previous studies' limitations in using machine learning to assess CVD risk factors.\nThis study analyzed data from 1024 patients to identify the significant CVD risk factors based on sex. The data comprising 13 features, such as demographic, lifestyle, and clinical factors, were obtained from the UCI repository and preprocessed to eliminate missing information. The analysis was performed using principal component analysis (PCA) and latent class analysis (LCA) to determine the major CVD risk factors and to identify any homogeneous subgroups between male and female patients. Data analysis was performed using XLSTAT Software. This software provides a comprehensive suite of tools for Data Analysis, Machine Learning, and Statistical Solutions for MS Excel.\nThis study showed significant sex differences in CVD risk factors. 8 out of 13 risk factors affecting male and female patients found that males and females share 4 of the eight risk factors. Identified latent profiles of CVD patients, suggesting the presence of subgroups among CVD patients. These findings provide valuable insights into the impact of sex differences on CVD risk factors. Moreover, they have important implications for healthcare professionals, who can use this information to develop individualized prevention and treatment plans. The results highlight the need for further research to elucidate these disparities better and develop more effective CVD prevention measures.\nThe study explored the sex differences in the CVD risk factors and the presence of subgroups among CVD patients using ML techniques. The results revealed sex-specific differences in risk factors and the existence of subgroups among CVD patients, thus providing essential insights for personalized prevention and treatment plans. Hence, further research is necessary to understand these disparities better and improve CVD prevention.", 
    "abstract": "Both in-hospital cardiac arrest (IHCA) and out-of-hospital cardiac arrest (OHCA) have higher incidence and lower survival rates. Predictors of in-hospital mortality for intensive care unit (ICU) admitted cardiac arrest (CA) patients remain unclear.\nThe Medical Information Mart for Intensive Care IV (MIMIC-IV) database was used to perform a retrospective study. Patients meeting the inclusion criteria were identified from the MIMIC-IV database and randomly divided into training set (n\u2009=\u20091206, 70%) and validation set (n\u2009=\u2009516, 30%). Candidate predictors consisted of the demographics, comorbidity, vital signs, laboratory test results, scoring systems, and treatment information on the first day of ICU admission. Independent risk factors for in-hospital mortality were screened using the least absolute shrinkage and selection operator (LASSO) regression model and the extreme gradient boosting (XGBoost) in the training set. Multivariate logistic regression analysis was used to build prediction models in training set, and then validated in validation set. Discrimination, calibration and clinical utility of these models were compared using the area under the curve (AUC) of the receiver operating characteristic (ROC) curves, calibration curves and decision curve analysis (DCA). After pairwise comparison, the best performing model was chosen to build a nomogram.\nAmong the 1722 patients, in-hospital mortality was 53.95%. In both sets, the LASSO, XGBoost,the logistic regression(LR) model and the National Early Warning Score 2 (NEWS 2) models showed acceptable discrimination. In pairwise comparison, the prediction effectiveness was higher with the LASSO,XGBoost and LR models than the NEWS 2 model (p\u2009<\u20090.001). The LASSO,XGBoost and LR models also showed good calibration. The LASSO model was chosen as our final model for its higher net benefit and wider threshold range. And the LASSO model was presented as the nomogram.\nThe LASSO model enabled good prediction of in-hospital mortality in ICU admission CA patients, which may be widely used in clinical decision-making.", 
    "abstract": "The Q-matrix, which specifies the relationship between items and attributes, is a crucial component of cognitive diagnostic models (CDMs). A precisely specified Q-matrix allows for valid cognitive diagnostic assessments. In practice, a Q-matrix is usually developed by domain experts, and noted as being subjective and potentially containing misspecifications which can decrease the classification accuracy of examinees. To overcome this, some promising validation methods have been proposed, such as the general discrimination index (GDI) method and the Hull method. In this article, we propose four new methods for Q-matrix validation based on random forest and feed-forward neural network techniques. Proportion of variance accounted for (PVAF) and coefficient of determination (i.e., the McFadden pseudo-R", 
    "abstract": "Melanoma is the most lethal of all skin cancers. This necessitates the need for a machine learning-driven skin cancer detection system to help medical professionals with early detection. We propose an integrated multi-modal ensemble framework that combines deep convolution neural representations with extracted lesion characteristics and patient meta-data. This study intends to integrate transfer-learned image features, global and local textural information, and patient data using a custom generator to diagnose skin cancer accurately. The architecture combines multiple models in a weighted ensemble strategy, which was trained and validated on specific and distinct datasets, namely, HAM10000, BCN20000\u2009+\u2009MSK, and the ISIC2020 challenge datasets. They were evaluated on the mean values of precision, recall or sensitivity, specificity, and balanced accuracy metrics. Sensitivity and specificity play a major role in diagnostics. The model achieved sensitivities of 94.15%, 86.69%, and 86.48% and specificity of 99.24%, 97.73%, and 98.51% for each dataset, respectively. Additionally, the accuracy on the malignant classes of the three datasets was 94%, 87.33%, and 89%, which is significantly higher than the physician recognition rate. The results demonstrate that our weighted voting integrated ensemble strategy outperforms existing models and could serve as an initial diagnostic tool for skin cancer.", 
    "abstract": "Given the high incidence and prevalence of myopia, the current healthcare system is struggling to handle the task of myopia management, which is worsened by home quarantine during the ongoing COVID-19 pandemic. The utilization of artificial intelligence (AI) in ophthalmology is thriving, yet not enough in myopia. AI can serve as a solution for the myopia pandemic, with application potential in early identification, risk stratification, progression prediction, and timely intervention. The datasets used for developing AI models are the foundation and determine the upper limit of performance. Data generated from clinical practice in managing myopia can be categorized into clinical data and imaging data, and different AI methods can be used for analysis. In this review, we comprehensively review the current application status of AI in myopia with an emphasis on data modalities used for developing AI models. We propose that establishing large public datasets with high quality, enhancing the model's capability of handling multimodal input, and exploring novel data modalities could be of great significance for the further application of AI for myopia.", 
    "abstract": "Acinetobacter baumannii is a nosocomial Gram-negative pathogen that often displays multidrug resistance. Discovering new antibiotics against A. baumannii has proven challenging through conventional screening approaches. Fortunately, machine learning methods allow for the rapid exploration of chemical space, increasing the probability of discovering new antibacterial molecules. Here we screened ~7,500 molecules for those that inhibited the growth of A. baumannii in vitro. We trained a neural network with this growth inhibition dataset and performed in silico predictions for structurally new molecules with activity against A. baumannii. Through this approach, we discovered abaucin, an antibacterial compound with narrow-spectrum activity against A. baumannii. Further investigations revealed that abaucin perturbs lipoprotein trafficking through a mechanism involving LolE. Moreover, abaucin could control an A. baumannii infection in a mouse wound model. This work highlights the utility of machine learning in antibiotic discovery and describes a promising lead with targeted activity against a challenging Gram-negative pathogen.", 
    "abstract": "The sources and properties of dissolved organic matter (DOM) in two lakes with different non-point source inputs were investigated by combining conventional three-dimensional fluorescence spectroscopy methods with a self-organizing map (SOM). To assess the DOM humification level, the representative neurons 1, 11, 25, and 36 were assessed. The SOM model showed that the DOM humification level of the Gaotang Lake (GT) which has a mainly agricultural non-point source input was significantly higher than that of the Yaogao Reservoir (YG) which has a mainly terrestrial source input (P < 0.01). The GT DOM mainly came from factors such as agricultural-related farm compost and decaying plants, while the YG DOM originated from human activities around the lake. The source characteristics of the YG DOM are obvious, with a high level of biological activity. Five representative areas in the fluorescence regional integral (FRI) were compared. The comparison showed that during the flat water period, the GT water column showed more terrestrial characteristics, even though the humus-like fractions in the DOM of both lakes were derived from microbial decomposition. Principal component analysis (PCA) showed that the agricultural lake water DOM (GT) was dominated by humus components, while the urban lake water DOM (YG) was dominated by authigenic sources.", 
    "abstract": "Dysfunctional pancreatic islet beta cells are a hallmark of type 2 diabetes (T2D), but a comprehensive understanding of the underlying mechanisms, including gene dysregulation, is lacking. Here we integrate information from measurements of chromatin accessibility, gene expression and function in single beta cells with genetic association data to nominate disease-causal gene regulatory changes in T2D. Using machine learning on chromatin accessibility data from 34 nondiabetic, pre-T2D and T2D donors, we identify two transcriptionally and functionally distinct beta cell subtypes that undergo an abundance shift during T2D progression. Subtype-defining accessible chromatin is enriched for T2D risk variants, suggesting a causal contribution of subtype identity to T2D. Both beta cell subtypes exhibit activation of a stress-response transcriptional program and functional impairment in T2D, which is probably induced by the T2D-associated metabolic environment. Our findings demonstrate the power of multimodal single-cell measurements combined with machine learning for characterizing mechanisms of complex diseases.", 
    "abstract": "The research aims to classify alluvial fans' morphometric properties using the SOM algorithm. It also determines the relationship between morphometric characteristics and erosion rate and lithology using the GMDH algorithm. For this purpose, alluvial fans of 4 watersheds in Iran are extracted semi-automatically using GIS and digital elevation model (DEM) analysis. The relationships between 25 morphometric features of these watersheds, the amount of erosion, and formation material are investigated using the self-organizing map (SOM) method. Principal component analysis (PCA), Greedy, Best first, Genetic search, Random search as feature selection algorithms are used to select the most important parameters affecting erosion and formation material. The group method of data handling (GMDH) algorithm is employed to predict erosion and formation material based on morphometries. The results indicated that the semi-automatic method in GIS could detect alluvial fans. The SOM algorithm determined that the morphometric factors affecting the formation material were fan length, minimum height of fan, and minimum fan slope. The main factors affecting erosion were fan area (A", 
    "abstract": "Organ donation is not meeting demand, and yet 30-60% of potential donors are potentially not identified. Current systems rely on manual identification and referral to an Organ Donation Organization (ODO). We hypothesized that developing an automated screening system based on machine learning could reduce the proportion of missed potentially eligible organ donors. Using routine clinical data and laboratory time-series, we retrospectively developed and tested a neural network model to automatically identify potential organ donors. We first trained a convolutive autoencoder that learned from the longitudinal changes of over 100 types of laboratory results. We then added a deep neural network classifier. This model was compared to a simpler logistic regression model. We observed an AUROC of 0.966 (CI 0.949-0.981) for the neural network and 0.940 (0.908-0.969) for the logistic regression model. At a prespecified cutoff, sensitivity and specificity were similar between both models at 84% and 93%. Accuracy of the neural network model was robust across donor subgroups and remained stable in a prospective simulation, while the logistic regression model performance declined when applied to rarer subgroups and in the prospective simulation. Our findings support using machine learning models to help with the identification of potential organ donors using routinely collected clinical and laboratory data.", 
    "abstract": "To compare the MRI texture profile of acetabular subchondral bone in normal, asymptomatic cam positive, and symptomatic cam-FAI hips and determine the accuracy of a machine learning model for discriminating between the three hip classes.\nA case-control, retrospective study was performed including 68 subjects (19 normal, 26 asymptomatic cam, 23 symptomatic cam-FAI). Acetabular subchondral bone of unilateral hip was contoured on 1.5\u00a0T MR images. Nine first-order 3D histogram and 16\u00a0s-order texture features were evaluated using specialized texture analysis software. Between-group differences were assessed using Kruskal-Wallis and Mann-Whitney U tests, and differences in proportions compared using chi-square and Fisher's exact tests. Gradient-boosted ensemble methods of decision trees were created and trained to discriminate between the three groups of hips, with percent accuracy calculated.\nSixty-eight subjects (median age 32 (28-40), 60 male) were evaluated. Significant differences among all three groups were identified with first-order (4 features, all p\u2009\u2264\u20090.002) and second-order (11 features, all p\u2009\u2264\u20090.002) texture analyses. First-order texture analysis could differentiate between control and cam positive hip groups (4 features, all p\u2009\u2264\u20090.002). Second-order texture analysis could additionally differentiate between asymptomatic cam and symptomatic cam-FAI groups (10 features, all p\u2009\u2264\u20090.02). Machine learning models demonstrated high classification accuracy of 79% (SD 16) for discriminating among all three groups.\nNormal, asymptomatic cam positive, and cam-FAI hips can be discriminated based on their MRI texture profile of subchondral bone using descriptive statistics and machine learning algorithms.\nTexture analysis can be performed on routine MR images of the hip and used to identify early changes in bone architecture, differentiating morphologically abnormal from normal hips, prior to onset of symptoms.\n\u2022 MRI texture analysis is a technique for extracting quantitative data from routine MRI images. \u2022 MRI texture analysis demonstrates that there are different bone profiles between normal hips and those with femoroacetabular impingement. \u2022 Machine learning models can be used in conjunction with MRI texture analysis to accurately differentiate between normal hips and those with femoroacetabular impingement.", 
    "abstract": "Object detection models commonly focus on utilizing the visible spectrum via Red-Green-Blue (RGB) imagery. Due to various limitations with this approach in low visibility settings, there is growing interest in fusing RGB with thermal Long Wave Infrared (LWIR) (7.5-13.5\u00a0\u00b5m) images to increase object detection performance. However, we still lack baseline performance metrics evaluating RGB, LWIR and RGB-LWIR fused object detection machine learning models, especially from air-based platforms. This study undertakes such an evaluation, finding that a blended RGB-LWIR model generally exhibits superior performance compared to independent RGB or LWIR approaches. For example, an RGB-LWIR blend only performs 1-5% behind the RGB approach in predictive power across various altitudes and periods of clear visibility. Yet, RGB fusion with a thermal signature overlay provides edge redundancy and edge emphasis, both which are vital in supporting edge detection machine learning algorithms (especially in low visibility environments). This approach has the ability to improve object detection performance for a range of use cases in industrial, consumer, government, and military applications. This research greatly contributes to the study of multispectral object detection by quantifying key factors affecting model performance from drone platforms (including distance, time-of-day and sensor type). Finally, this research additionally contributes a novel open labeled training dataset of 6300 images for RGB, LWIR, and RGB-LWIR fused imagery, collected from air-based platforms, enabling further multispectral machine-driven object detection research.", 
    "abstract": "Transcranial Direct Current Stimulation (tDCS) is a non-invasive neuromodulation technique with a wide variety of clinical and research applications. As increasingly acknowledged, its effectiveness is subject dependent, which may lead to time consuming and cost ineffective treatment development phases. We propose the combination of electroencephalography (EEG) and unsupervised learning for the stratification and prediction of individual responses to tDCS. A randomized, sham-controlled, double-blind crossover study design was conducted within a clinical trial for the development of pediatric treatments based on tDCS. The tDCS stimulation (sham and active) was applied either in the left dorsolateral prefrontal cortex or in the right inferior frontal gyrus. Following the stimulation session, participants performed 3 cognitive tasks to assess the response to the intervention: the Flanker Task, N-Back Task and Continuous Performance Test (CPT). We used data from 56 healthy children and adolescents to implement an unsupervised clustering approach that stratify participants based on their resting-state EEG spectral features before the tDCS intervention. We then applied a correlational analysis to characterize the clusters of EEG profiles in terms of participant's difference in\u00a0the behavioral outcome (accuracy and response time) of the cognitive tasks when performed after a tDCS-sham or a tDCS-active session. Better behavioral performance following the active tDCS session compared to the sham tDCS session is considered a positive intervention response, whilst the reverse is considered a negative one. Optimal results in terms of validity measures was obtained for 4 clusters. These results show that specific EEG-based digital phenotypes can be associated to particular responses. While one cluster presents neurotypical EEG activity, the remaining clusters present non-typical EEG characteristics, which seem to be associated with a positive response. Findings suggest that unsupervised machine learning can be successfully used to stratify and eventually predict responses of individuals to\u00a0a tDCS treatment.", 
    "abstract": "We provide a sequence of analysis-ready optical underwater images from the Clarion-Clipperton Zone (CCZ) of the Pacific Ocean. The images were originally recorded using a towed camera sledge that photographed a seabed covered with polymetallic manganese-nodules, at an average water depth of 4,250 meters. The original degradation in visual quality and inconsistent scale among individual raw images due to different altitude implies that they are not scientifically comparable in their original form. Here, we present analysis-ready images that have already been pre-processed to account for this degradation. We also provide accompanying metadata for each image, which includes their geographic coordinates, depth\u00a0of the seafloor, absolute scale (cm/pixel), and seafloor habitat class obtained from a previous study. The provided images are thus directly usable by the marine scientific community e.g., to train machine learning models for seafloor substrate classification and megafauna detection.", 
    "abstract": "Investigation of the neurobiology of depression in humans depends on animal models that attempt to mimic specific features of the human disorder. However, frequently-used paradigms based on social stress cannot be easily applied to female mice which has led to a large sex bias in preclinical studies of depression. Furthermore, most studies focus on one or only a few behavioral assessments, with time and practical considerations prohibiting a comprehensive evaluation. In this study, we demonstrate that predator stress effectively induced depression-like behaviors in both male and female mice. By comparing predator stress and social defeat models, we observed that the former elicited a higher level of behavioral despair and the latter elicited more robust social avoidance. Furthermore, the use of machine learning (ML)-based spontaneous behavioral classification can distinguish mice subjected to one type of stress from another, and from non-stressed mice. We show that related patterns of spontaneous behaviors correspond to depression status as measured by canonical depression-like behaviors, which illustrates that depression-like symptoms can be predicted by ML-classified behavior patterns. Overall, our study confirms that the predator stress induced phenotype in mice is a good reflection of several important aspects of depression in humans and illustrates that ML-supported analysis can simultaneously evaluate multiple behavioral alterations in different animal models of depression, providing a more unbiased and holistic approach for the study of neuropsychiatric disorders.", 
    "abstract": "Surface Pourbaix diagrams are critical to understanding the stability of nanomaterials in electrochemical environments. Their construction based on density functional theory is, however, prohibitively expensive for real-scale systems, such as several nanometer-size nanoparticles (NPs). Herein, with the aim of accelerating the accurate prediction of adsorption energies, we developed a bond-type embedded crystal graph convolutional neural network (BE-CGCNN) model in which four bonding types were treated differently. Owing to the enhanced accuracy of the bond-type embedding approach, we demonstrate the construction of reliable Pourbaix diagrams for very large-size NPs involving up to 6525 atoms (approximately 4.8 nm in diameter), which enables the exploration of electrochemical stability over various NP sizes and shapes. BE-CGCNN-based Pourbaix diagrams well reproduce the experimental observations with increasing NP size. This work suggests a method for accelerated Pourbaix diagram construction for real-scale and arbitrarily shaped NPs, which would significantly open up an avenue for electrochemical stability studies.", 
    "abstract": "The presence of blebs increases the rupture risk of intracranial aneurysms (IAs).\nTo evaluate whether cross-sectional bleb formation models can identify aneurysms with focalized enlargement in longitudinal series.\nHemodynamic, geometric, and anatomical variables derived from computational fluid dynamics models of 2265 IAs from a cross-sectional dataset were used to train machine learning (ML) models for bleb development. ML algorithms, including logistic regression, random forest, bagging method, support vector machine, and K-nearest neighbors, were validated using an independent cross-sectional dataset of 266 IAs. The models' ability to identify aneurysms with focalized enlargement was evaluated using a separate longitudinal dataset of 174 IAs. Model performance was quantified by the area under the receiving operating characteristic curve (AUC), the sensitivity and specificity, positive predictive value, negative predictive value, F1 score, balanced accuracy, and misclassification error.\nThe final model, with three hemodynamic and four geometrical variables, along with aneurysm location and morphology, identified strong inflow jets, non-uniform wall shear stress with high peaks, larger sizes, and elongated shapes as indicators of a higher risk of focal growth over time. The logistic regression model demonstrated the best performance on the longitudinal series, achieving an AUC of 0.9, sensitivity of 85%, specificity of 75%, balanced accuracy of 80%, and a misclassification error of 21%.\nModels trained with cross-sectional data can identify aneurysms prone to future focalized growth with good accuracy. These models could potentially be used as early indicators of future risk in clinical practice.", 
    "abstract": "The fecal metabolome is affected by diet and includes metabolites generated by human and microbial metabolism. Advances in -omics technologies and analytic approaches have allowed researchers to identify metabolites and better utilize large data sets to generate usable information. One promising aspect of these advancements is the ability to determine objective biomarkers of food intake.\nWe aimed to utilize a multivariate, machine learning approach to identify metabolite biomarkers that accurately predict food intake.\nData were aggregated from 5 controlled feeding studies in adults that tested the impact of specific foods (almonds, avocados, broccoli, walnuts, barley, and oats) on the gastrointestinal microbiota. Fecal samples underwent GC-MS metabolomic analysis; 344 metabolites were detected in preintervention samples, whereas 307 metabolites were detected postintervention. After removing metabolites that were only detected in either pre- or postintervention and those undetectable in \u226580% of samples in all study groups, changes in 96 metabolites relative concentrations (treatment postintervention minus preintervention) were utilized in random forest models to 1) examine the relation between food consumption and fecal metabolome changes and 2) rank the fecal metabolites by their predictive power (i.e., feature importance score).\nUsing the change in relative concentration of 96 fecal metabolites, 6 single-food random forest models for almond, avocado, broccoli, walnuts, whole-grain barley, and whole-grain oats revealed prediction accuracies between 47% and 89%. When comparing foods with one another, almond intake was differentiated from walnut intake with 91% classification accuracy.\nOur findings reveal promise in utilizing fecal metabolites as objective complements to certain self-reported food intake estimates. Future research on other foods at different doses and dietary patterns is needed to identify biomarkers that can be applied in feeding study compliance and clinical settings.", 
    "abstract": "Valvular heart disease (VHD) is a morbid condition in which timely identification and evidence-based treatments can lead to improved outcomes. Artificial intelligence broadly refers to the ability for computers to perform tasks and problem solve like the human mind. Studies applying AI to VHD have used a variety of structured (eg, sociodemographic, clinical) and unstructured (eg, electrocardiogram, phonocardiogram, and echocardiograms) and machine learning modeling approaches. Additional researches in diverse populations, including prospective clinical trials, are needed to evaluate the effectiveness and value of AI-enabled medical technologies in clinical care for patients with VHD.", 
    "abstract": "A correct and prompt diagnosis of coronary artery disease (CAD) is a crucial component of disease management to reduce the risk of death and improve the quality of life in patients with CAD. Currently, the American College of Cardiology (ACC)/American Heart Association (AHA) and the European Society of Cardiology (ESC) guidelines recommend selecting an appropriate pre-diagnosis test for an individual patient according to the CAD probability. The purpose of this study was to develop a practical pre-test probability (PTP) for obstructive CAD in patients with chest pain using machine learning (ML); also, the performance of ML-PTP for CAD is compared to the final result of coronary angiography (CAG).\nWe used a database from a single-center, prospective, all-comer registry designed to reflect real-world practice since 2004. All subjects underwent invasive CAG at Korea University Guro Hospital in Seoul, South Korea. We used logistic regression algorithms, random forest (RF), supporting vector machine, and K-nearest neighbor classification for the ML models. The dataset was divided into two consecutive sets according to the registration period to validate the ML models. ML training for PTP and internal validation used the first dataset registered between 2004 and 2012 (8631 patients). The second dataset registered between 2013 and 2014 (1546 patients) was used for external validation. The primary endpoint was obstructive CAD. Obstructive CAD was defined as having a stenosis diameter of >70% on the quantitative CAG of the main epicardial coronary artery.\nWe derived an ML-based model consisting of three different models according to the subject used to obtain the information, such as the patient himself (dataset 1), the community's first medical center (dataset 2), and doctors (dataset 3). The performance range of the ML-PTP models as the non-invasive test had C-statistics of 0.795 to 0.984 compared to the result of invasive testing via CAG in patients with chest pain. The training ML-PTP models were adjusted to have 99% sensitivity for CAD so as not to miss actual CAD patients. In the testing dataset, the best accuracy of the ML-PTP model was 45.7% using dataset 1, 47.2% using dataset 2, and 92.8% using dataset 3 and the RF algorithm. The CAD prediction sensitivity was 99.0%, 99.0%, and 98.0%, respectively.\nWe successfully developed a high-performance model of ML-PTP for CAD which is expected to reduce the need for non-invasive tests in chest pain. However, since this PTP model is derived from data of a single medical center, multicenter verification is required to use it as a PTP recommended by the major American societies and the ESC.", 
    "abstract": "In return for their nutritional properties and broad availability, cereal crops have been associated with different alimentary disorders and symptoms, with the majority of the responsibility being attributed to gluten. Therefore, the research of gluten-related literature data continues to be produced at ever-growing rates, driven in part by the recent exploratory studies that link gluten to non-traditional diseases and the popularity of gluten-free diets, making it increasingly difficult to access and analyse practical and structured information. In this sense, the accelerated discovery of novel advances in diagnosis and treatment, as well as exploratory studies, produce a favourable scenario for disinformation and misinformation.\nAligned with, the European Union strategy \"Delivering on EU Food Safety and Nutrition in 2050\" which emphasizes the inextricable links between imbalanced diets, the increased exposure to unreliable sources of information and misleading information, and the increased dependency on reliable sources of information; this paper presents GlutKNOIS, a public and interactive literature-based database that reconstructs and represents the experimental biomedical knowledge extracted from the gluten-related literature. The developed platform includes different external database knowledge, bibliometrics statistics and social media discussion to propose a novel and enhanced way to search, visualise and analyse potential biomedical and health-related interactions in relation to the gluten domain.\nFor this purpose, the presented study applies a semi-supervised curation workflow that combines natural language processing techniques, machine learning algorithms, ontology-based normalization and integration approaches, named entity recognition methods, and graph knowledge reconstruction methodologies to process, classify, represent and analyse the experimental findings contained in the literature, which is also complemented by data from the social discussion.\nand Conclusions: In this sense, 5,814 documents were manually annotated and 7,424 were fully automatically processed to reconstruct the first online gluten-related knowledge database of evidenced health-related interactions that produce health or metabolic changes based on the literature. In addition, the automatic processing of the literature combined with the knowledge representation methodologies proposed has the potential to assist in the revision and analysis of years of gluten research. The reconstructed knowledge base is public and accessible at https://sing-group.org/glutknois/.", 
    "abstract": "In the anaerobic digestion (AD) process there are some difficulties in maintaining process stability due to the complexity of the system. The variability of the raw material coming to the facility, temperature fluctuations and pH changes as a result of microbial processes cause process instability and require continuous monitoring and control. Increasing continuous monitoring, and internet of things applications within the scope of Industry 4.0 in AD facilities can provide process stability control and early intervention. In this study, five different machine learning (ML) algorithms (RF, ANN, KNN, SVR, and XGBoost) were used to describe and predict the correlation between operational parameters and biogas production quantities collected from a real-scale anaerobic digestion plant. The KNN algorithm had the lowest accuracy in predicting total biogas production over time, while the RF model had the highest prediction accuracy of all prediction models. The RF method produced the best prediction, with an R", 
    "abstract": "White matter hyperintensities (WMH) of assumed vascular origin are common in elderly individuals and are closely associated with cognitive decline. However, the underlying neural mechanisms of WMH-related cognitive impairment remain unclear. After strict screening, 59 healthy controls (HC, n\u00a0=\u00a059), 51 patients with WMH and normal cognition (WMH-NC, n\u00a0=\u00a051) and 68 patients with WMH and mild cognitive impairment (WMH-MCI, n\u00a0=\u00a068) were included in the final analyses. All individuals underwent multimodal magnetic resonance imaging (MRI) and cognitive evaluations. We investigated the neural mechanism underlying WMH-related cognitive impairment based on static and dynamic functional network connectivity (sFNC and dFNC) approaches. Finally, the support vector machine (SVM) method was performed to identify WMH-MCI individuals. The sFNC analysis indicated that functional connectivity within the visual network (VN) could mediate the impairment of information processing speed related to WMH (indirect effect: 0.24; 95% CI: 0.03, 0.88 and indirect effect: 0.05; 95% CI: 0.001, 0.14). WMH may regulate the dFNC between the higher-order cognitive network and other networks and enhance the dynamic variability between the left frontoparietal network (lFPN) and the VN to compensate for the decline in high-level cognitive functions. The SVM model achieved good prediction ability for WMH-MCI patients based on the above characteristic connectivity patterns. Our findings shed light on the dynamic regulation of brain network resources to maintain cognitive processing in individuals with WMH. Crucially, dynamic reorganization of brain networks could be regarded as a potential neuroimaging biomarker for identifying WMH-related cognitive impairment.", 
    "abstract": "The current conflicting neuroimaging findings of insomnia disorder (ID) may be attributed to heterogeneity in ID. The present study aims to clarify the high heterogeneity in ID and examine the objective neurobiological subtypes of ID by using a novel machine learning method based on gray matter volumes (GMVs). We recruited 56 patients with ID and 73 healthy controls (HCs). The T1-weighted anatomical images were obtained for each participant. We investigated whether the ID has higher interindividual heterogeneity in GMVs. Then, we used a heterogeneous machine learning algorithm by discriminative analysis (HYDRA) to identify subtypes of ID with features of brain regional GMVs. We found that patients with ID have higher interindividual variability than HCs. HYDRA identified two distinct and reliable neuroanatomical subtypes of ID. Two subtypes showed significantly different aberrance in GMVs compared with HCs. Specifically, subtype 1 exhibited widespread decreased GMVs in some brain regions, including the right inferior temporal gyrus, left superior temporal gyrus, left precuneus, right middle cingulate, and right supplementary motor area. Subtype 2 only demonstrated increased GMVs in the right superior temporal gyrus. Additionally, the GMVs of altered brain regions in subtype 1 were significantly correlated with daytime functioning, but in subtype 2, they were significantly correlated with sleep disturbance. These results explain conflicting neuroimaging findings and propose a potential objective neurobiological classification contributing to ID's precise clinical diagnosis and treatment.", 
    "abstract": "Transcranial magnetic stimulation (TMS) can modulate neural activity by evoking action potentials in cortical neurons. TMS neural activation can be predicted by coupling subject-specific head models of the TMS-induced electric field (E-field) to populations of biophysically realistic neuron models; however, the significant computational cost associated with these models limits their utility and eventual translation to clinically relevant applications.\nTo develop computationally efficient estimators of the activation thresholds of multi-compartmental cortical neuron models in response to TMS-induced E-field distributions.\nMulti-scale models combining anatomically accurate finite element method (FEM) simulations of the TMS E-field with layer-specific representations of cortical neurons were used to generate a large dataset of activation thresholds. 3D convolutional neural networks (CNNs) were trained on these data to predict thresholds of model neurons given their local E-field distribution. The CNN estimator was compared to an approach using the uniform E-field approximation to estimate thresholds in the non-uniform TMS-induced E-field.\nThe 3D CNNs estimated thresholds with mean absolute percent error (MAPE) on the test dataset below 2.5% and strong correlation between the CNN predicted and actual thresholds for all cell types (R\n3D CNNs can estimate rapidly and accurately the TMS activation thresholds of biophysically realistic neuron models using sparse samples of the local E-field, enabling simulating responses of large neuron populations or parameter space exploration on a personal computer.", 
    "abstract": "In this study, we combine machine learning and geospatial interpolations to create a two-dimensional high-resolution ozone concentration fields over the South Coast Air Basin for the entire year of 2020. Three spatial interpolation methods (bicubic, IDW, and ordinary kriging) were employed. The predicted ozone concentration fields were constructed using 15 building sites, and random forest regression was employed to test predictability of 2020 data based on input data from past years. Spatially interpolated ozone concentrations were evaluated at twelve sites that were independent of the actual spatial interpolations to find the most suitable method for SoCAB. Ordinary kriging interpolation had the best performance overall for 2020: concentrations were overestimated for Anaheim, Compton, LA North Main Street, LAX, Rubidoux, and San Gabriel sites and underestimated for Banning, Glendora, Lake Elsinore, and Mira Loma sites. The model performance improved from the West to the East, exhibiting better predictions for inland sites. The model is best at interpolating ozone concentrations inside the sampling region (bounded by the building sites), with R", 
    "abstract": "Identifying small molecule protein-protein interaction modulators (PPIMs) is a highly promising and meaningful research direction for drug discovery, cancer treatment, and other fields. In this study, we developed a stacking ensemble computational framework, SELPPI, based on a genetic algorithm and tree-based machine learning method for effectively predicting new modulators targeting protein-protein interactions. More specifically, extremely randomized trees (ExtraTrees), adaptive boosting (AdaBoost), random forest (RF), cascade forest, light gradient boosting machine (LightGBM), and extreme gradient boosting (XGBoost) were used as basic learners. Seven types of chemical descriptors were taken as the input characteristic parameters. Primary predictions were obtained with each basic learner-descriptor pair. Then, the 6 methods mentioned above were used as meta learners and trained on the primary prediction in turn. The most efficient method was utilized as the meta learner. Finally, the genetic algorithm was used to select the optimal primary prediction output as the input of the meta learner for secondary prediction to obtain the final result. We systematically evaluated our model on the pdCSM-PPI datasets. To our knowledge, our model outperformed all existing models, which demonstrates its great power.", 
    "abstract": "Generalized anxiety disorder (GAD) is the least studied among anxiety disorders. Therefore, we aimed to compare the cervical blood flow velocities using doppler ultrasonography in untreated chronic GAD patients and healthy individuals.\nIn this study, thirty-eight GAD patients were enrolled. And thirty-eight healthy volunteers were recruited as control participants. The common carotid artery (CCA), internal carotid artery (ICA), and vertebral artery (VA) of both sides were explored. Also, we trained machine learning models based on cervical arteries characteristics to diagnose GAD patients.\nPatients with chronic untreated GAD showed a significant increase in peak systolic velocity (PSV) bilaterally in the CCA and the ICA (P value < 0.05). In GAD patients, the end-diastolic velocity (EDV) of bilateral CCA, VA, and left ICA was significantly decreased. The Resistive Index (RI) showed a significant increase in all patients with GAD. Moreover, the Support Vector Machine (SVM) model showed the best accuracy in identifying anxiety disorder.\nGAD is associated with hemodynamic alterations of extracranial cervical arteries. With a larger sample size and more generalized data, it is possible to make a robust machine learning-based model for GAD diagnosis.", 
    "abstract": "Artificial intelligence (AI) and machine learning are increasingly utilized across healthcare. More recently, there has been a rise in the use AI within research, particularly through novel conversational AI platforms, such as ChatGPT. In this Controversies paper, we discuss the advantages, limitations, and future directions for ChatGPT and other forms of conversational AI in research and scholarly dissemination.", 
    "abstract": "Assessments of health-related quality of life (HRQoL) play an important role in transition to palliative care for women with metastatic breast cancer. We developed machine learning (ML) algorithms to analyse longitudinal HRQoL data and identify patients who may benefit from palliative care due to disease progression.\nWe recruited patients from two institutions and administered the EuroQoL Visual Analog Scale (EQ-VAS) via an online platform over a 6-month period. We trained a regularised regression algorithm using 10-fold cross-validation to determine if a patient was at high or low risk of disease progression based on changes in the EQ-VAS scores using data of one institution and validated the performance on data of the other institution. Progression-free survival (PFS) was the end-point. We conducted Kaplan-Meier and Cox regression analysis adjusted for clinical risk factors.\nOf 179 patients, 98 (54.7%) had progressive disease after a median follow-up of 14weeks. Using EQ-VAS scores collected at weeks 1-6 to predict disease progression at week 12, in the validation set (n\u00a0=\u00a063), PFS was significantly lower in the intelligent EQ-VAS high-risk versus low-risk group: median PFS 7 versus 10weeks, log-rank P\u00a0<\u00a00.038). Intelligent EQ-VAS had the strongest association with PFS (adjusted hazard ratio 2.69, 95% confidence interval 1.17-6.18, P\u00a0=\u00a00.02).\nML algorithms can analyse changes in longitudinal HRQoL data to identify patients with disease progression earlier than standard follow-up methods. Intelligent EQ-VAS scores were identified as independent prognostic factor. Future studies may validate these results to remotely monitor patients.", 
    "abstract": null, 
    "abstract": "According with \"Numbers of cancer in Italy. 2021\" mortality is decreasing for both the genders (-10% for men, -8% for women) in Italy. However, this trend is not uniform and seems stable in the Southern regions. Analyses of oncological care in Campania Region highlighted some structural critical issues and delays, which did not guarantee an efficient and effective use of the available economic resources. So, the Campania region established in September 2016 the Campania oncological network (Roc) addressed to prevention, diagnosis, treatment and rehabilitation of tumours through the establishment of multidisciplinary oncological groups (Gom). In February 2020, the ValPeRoc project was launched with the aim of periodically and progressively evaluating the Roc's performance both for the clinical services and for the economic aspects.\nIn five Goms (colon, ovary, lung, prostate, bladder) active in some Roc hospitals, the pre-Gom time elapsing between the date of diagnosis and the date of the first Gom meeting and the Gom time elapsing between the date of the first Gom meeting and the date of the treatment decision were measured. Gom times longer than 28 days were defined as high. The risk of high Gom time was analyzed with a Bart-type machine learning algorithm, considering the set of regressors (features) available to classify patients.\nThe results on the test set (54 patients) report an accuracy of 0.68. The classification technique reported a good fit for colon Gom (93%) and an over-classification for lung Gom. The study of the marginal effects showed a higher risk for those who had a previous therapeutic act and for lung Gom.\nWithin the Goms took in consideration the proposed statistical technique showed that, depending on each Gom, correctly classified about 70% of individuals on risk of delaying permanence within the Roc. The ValPeRoc project evaluates Roc activity for the first time through a replicable analysis of patient pathway times from diagnosis to the act of treatment. Specifically, the times analyzed measure the quality of the regional health care system.", 
    "abstract": "Systematic reviews (SRs) are essential tools for synthesising the available scientific evidence on a given topic, and in some healthcare fields they represent the core for public health decisions according to the principles of evidence-based medicine. However, keeping up to date with the volume of scientific production is not always easy given the estimated annual increase in scientific publications of 4.10%. Indeed, SRs take a long time, with an average time of eleven months from design to submission to a scientific journal; to make more efficient this process and timely achieve evidence collection, systems such as living systematic reviews and artificial intelligence tools have been developed for the automation of SRs. These tools can be divided into three categories: visualisation tools, active learning tools and automated tools with Natural Language Processing (NLP). Nlp makes it possible to reduce the time spent and human error, for example, in the screening of primary studies; there are already many tools that apply to all stages of a SR, currently the most widely used are those with \"human-in-the-loop\" where the reviewer is involved in the various steps to verify the goodness of the work performed by the model. At this time of transition in SRs, new approaches are emerging and are increasingly appreciated by the community of reviewers; leaving some more basic but also error-prone tasks to machine learning tools can increase the efficiency of the reviewer and the overall quality of the review itself.", 
    "abstract": "The exposome concept arises from the need to integrate different disciplines of public health and environmental sciences, mainly including environmental epidemiology, exposure science, and toxicology. The role of the exposome is to understand how the totality of an individual's exposures throughout the lifetime can impact human health. The etiology of a health condition is rarely explained by a single exposure. Therefore, examining the human exposome as a whole becomes relevant to simultaneously consider multiple risk factors and more accurately estimate concurrent causes of different health outcomes. Generally, the exposome is explained through three domains: general external exposome, specific external exposome, and internal exposome. The general external exposome includes measurable population-level exposures such as air pollution or meteorological factors. The specific external exposome includes information on individual exposures, such as lifestyle factors, typically obtained from questionnaires. Meanwhile, the internal exposome encompasses multiple biological responses to external factors, detected through molecular and omics analyses. Additionally, in recent decades, the socio-exposome theory has emerged, where all exposures are studied as a phenomenon dependent on the interaction between socioeconomic factors that vary depending on the context, allowing the identification of mechanisms that lead to health inequalities. The considerable production of data in exposome studies has led researchers to face new methodological and statistical challenges, introducing various approaches to estimate the effect of the exposome on health. Among the most common are regression models (Exposome-Wide Association Study - ExWAS), dimensionality reduction and exposure grouping techniques, and machine learning methods. The significant conceptual and methodological innovation of the exposome for a more holistic evaluation of the risks associated with human health is continuously expanding and will require further investigations related to the application of information obtained from studies into prevention and public health policies.", 
    "abstract": "The \"millennial\" epidemiologists, born between the beginning of the 80s and the end of the 90s, are the generation that most of all, today, lives between the present and the future of this discipline. This issue of Recenti Progressi in Medicina aims to talk about what young (and no longer young) epidemiologists and public health researchers are dealing with and to reflect on the most relevant topics in our field, with an eye to the future. Starting from the profile of the \"millennial\" epidemiologists in Italy and the topics on which they work, the issue develops through three parts dealing with relevant topics for the present and the future of Public health. The first part deals with the important issue of finding a balance between the protection of personal data and the protection of health through a dialogue between researchers, jurists and citizens. The second part aims to clarify the issue of big data and its implications for producing health. The third part touches on four relevant topics for the perspectives of epidemiology through reflections and application examples of machine learning, integration between pharmacoepidemiology and environmental epidemiology, health prevention and promotion involving citizens and other stakeholders, and epidemiology of mental health. In a constantly changing world, challenges for those who work to produce health are not lacking, as is the determination to face them. With this issue, we hope to contribute to the awareness of who we are and our potential, to help millennials (but not only) find their place in epidemiology, today and tomorrow.", 
    "abstract": "Gastric cancer (GC) is among the leading causes of cancer-related deaths worldwide. The discovery of robust diagnostic biomarkers for GC remains a challenge. This study sought to identify biomarker candidates for GC by integrating machine learning (ML) and bioinformatics approaches. Transcriptome profiles of patients with GC were analyzed to identify differentially expressed genes between the tumor and adjacent normal tissues. Subsequently, we constructed protein-protein interaction networks so as to find the significant hub genes. Along with the bioinformatics integration of ML methods such as support vector machine, the recursive feature elimination was used to select the most informative genes. The analysis unraveled 160 significant genes, with 88 upregulated and 72 downregulated, 10 hub genes, and 12 features from the variable selection method. The integrated analyses found that ", 
    "abstract": "The ability to predict survival accurately in patients with osseous metastatic disease of the extremities is vital for patient counseling and guiding surgical intervention. We, the Skeletal Oncology Research Group (SORG), previously developed a machine-learning algorithm (MLA) based on data from 1999 to 2016 to predict 90-day and 1-year survival of surgically treated patients with extremity bone metastasis. As treatment regimens for oncology patients continue to evolve, this SORG MLA-driven probability calculator requires temporal reassessment of its accuracy.\nDoes the SORG-MLA accurately predict 90-day and 1-year survival in patients who receive surgical treatment for a metastatic long-bone lesion in a more recent cohort of patients treated between 2016 and 2020?\nBetween 2017 and 2021, we identified 674 patients 18 years and older through the ICD codes for secondary malignant neoplasm of bone and bone marrow and CPT codes for completed pathologic fractures or prophylactic treatment of an impending fracture. We excluded 40% (268 of 674) of patients, including 18% (118) who did not receive surgery; 11% (72) who had metastases in places other than the long bones of the extremities; 3% (23) who received treatment other than intramedullary nailing, endoprosthetic reconstruction, or dynamic hip screw; 3% (23) who underwent revision surgery, 3% (17) in whom there was no tumor, and 2% (15) who were lost to follow-up within 1 year. Temporal validation was performed using data on 406 patients treated surgically for bony metastatic disease of the extremities from 2016 to 2020 at the same two institutions where the MLA was developed. Variables used to predict survival in the SORG algorithm included perioperative laboratory values, tumor characteristics, and general demographics. To assess the models' discrimination, we computed the c-statistic, commonly referred to as the area under the receiver operating characteristic (AUC) curve for binary classification. This value ranged from 0.5 (representing chance-level performance) to 1.0 (indicating excellent discrimination) Generally, an AUC of 0.75 is considered high enough for use in clinical practice. To evaluate the agreement between predicted and observed outcomes, a calibration plot was used, and the calibration slope and intercept were calculated. Perfect calibration would result in a slope of 1 and intercept of 0. For overall performance, the Brier score and null-model Brier score were determined. The Brier score can range from 0 (representing perfect prediction) to 1 (indicating the poorest prediction). Proper interpretation of the Brier score necessitates a comparison with the null-model Brier score, which represents the score for an algorithm that predicts a probability equal to the population prevalence of the outcome for each patient. Finally, a decision curve analysis was conducted to compare the potential net benefit of the algorithm with other decision-support methods, such as treating all or none of the patients. Overall, 90-day and 1-year mortality were lower in the temporal validation cohort than in the development cohort (90 day: 23% versus 28%; p < 0.001, and 1 year: 51% versus 59%; p<0.001).\nOverall survival of the patients in the validation cohort improved from 28% mortality at the 90-day timepoint in the cohort on which the model was trained to 23%, and 59% mortality at the 1-year timepoint to 51%. The AUC was 0.78 (95% CI 0.72 to 0.82) for 90-day survival and 0.75 (95% CI 0.70 to 0.79) for 1-year survival, indicating the model could distinguish the two outcomes reasonably. For the 90-day model, the calibration slope was 0.71 (95% CI 0.53 to 0.89), and the intercept was -0.66 (95% CI -0.94 to -0.39), suggesting the predicted risks were overly extreme, and that in general, the risk of the observed outcome was overestimated. For the 1-year model, the calibration slope was 0.73 (95% CI 0.56 to 0.91) and the intercept was -0.67 (95% CI -0.90 to -0.43). With respect to overall performance, the model's Brier scores for the 90-day and 1-year models were 0.16 and 0.22. These scores were higher than the Brier scores of internal validation of the development study (0.13 and 0.14) models, indicating the models' performance has declined over time.\nThe SORG MLA to predict survival after surgical treatment of extremity metastatic disease showed decreased performance on temporal validation. Moreover, in patients undergoing innovative immunotherapy, the possibility of mortality risk was overestimated in varying severity. Clinicians should be aware of this overestimation and discount the prediction of the SORG MLA according to their own experience with this patient population. Generally, these results show that temporal reassessment of these MLA-driven probability calculators is of paramount importance because the predictive performance may decline over time as treatment regimens evolve. The SORG-MLA is available as a freely accessible internet application at https://sorg-apps.shinyapps.io/extremitymetssurvival/.Level of Evidence Level III, prognostic study.", 
    "abstract": "The burden of diabetic retinopathy (DR) is increasing, and the sensitive biomarkers of the disease were not enough. Studies have found that the metabolic profile, such as amino acid (AA) and acylcarnitine (AcylCN), in the early stages of DR patients might have changed, indicating the potential of metabolites to become new biomarkers. We are amid to construct a metabolite-based prediction model for DR risk. This study was conducted on type 2 diabetes (T2D) patients with or without DR. Logistic regression and extreme gradient boosting (XGBoost) prediction models were constructed using the traditional clinical features and the screening features, respectively. Assessing the predictive power of the models in terms of both discrimination and calibration, the optimal model was interpreted using the Shapley Additive exPlanations (SHAP) to quantify the effect of features on prediction. Finally, the XGBoost model incorporating AA and AcylCN variables had the best comprehensive evaluation (ROCAUC = 0.82, PRAUC = 0.44, Brier score = 0.09). C18\u2009:\u20091OH lower than 0.04\u2009", 
    "abstract": "Depression is a prevalent disorder worldwide, with potentially severe implications. It contributes significantly to an increased risk of diseases associated with multiple risk factors. Early accurate diagnosis of depressive symptoms is a critical first step toward management, intervention, and prevention. Various nutritional and dietary compounds have been suggested to be involved in the onset, maintenance, and severity of depressive disorders. Despite the challenges to better understanding the association between nutritional risk factors and the occurrence of depression, assessing the interplay of these markers through supervised machine learning remains to be fully explored.\nThis study aimed to determine the ability of machine learning-based decision support methods to identify the presence of depression using publicly available health data from the Korean National Health and Nutrition Examination Survey. Two exploration techniques, namely, uniform manifold approximation and projection and Pearson correlation, were performed for explanatory analysis among datasets. A grid search optimization with cross-validation was performed to fine-tune the models for classifying depression with the highest accuracy. Several performance measures, including accuracy, precision, recall, F1 score, confusion matrix, areas under the precision-recall and receiver operating characteristic curves, and calibration plot, were used to compare classifier performances. We further investigated the importance of the features provided: visualized interpretation using ELI5, partial dependence plots, and local interpretable using model-agnostic explanations and Shapley additive explanation for the prediction at both the population and individual levels.\nThe best model achieved an accuracy of 86.18% for XGBoost and an area under the curve of 84.96% for the random forest model in original dataset and the XGBoost algorithm with an accuracy of 86.02% and an area under the curve of 85.34% in the quantile-based dataset. The explainable results revealed a complementary observation of the relative changes in feature values, and, thus, the importance of emergent depression risks could be identified.\nThe strength of our approach is the large sample size used for training with a fine-tuned model. The machine learning-based analysis showed that the hyper-tuned model has empirically higher accuracy in classifying patients with depressive disorder, as evidenced by the set of interpretable experiments, and can be an effective solution for disease control.", 
    "abstract": "Cutaneous melanoma (CM) is one of the malignant tumors with a relative high lethality. Necroptosis is a novel programmed cell death that participates in anti-tumor immunity and tumor prognosis. Necroptosis has been found to play an important role in tumors like CM. However, the necroptosis-associated lncRNAs' potential prognostic value in CM has not been identified.\nThe RNA sequencing data collected from The Cancer Genome Atlas (TCGA) and Genotype-Tissue Expression Project (GTEx) was utilized to identify differentially expressed genes in CM. By using the univariate Cox regression analysis and machine learning LASSO algorithm, a prognostic risk model had been built depending on 5 necroptosis-associated lncRNAs and was verified by internal validation. The performance of this prognostic model was assessed by the receiver operating characteristic curves. A nomogram was constructed and verified by calibration. Furthermore, we also performed sub-group K-M analysis to explore the 5 lncRNAs' expression in different clinical stages. Function enrichment had been analyzed by GSEA and ssGSEA. In addition, qRT-PCR was performed to verify the five lncRNAs' expression level in CM cell line (A2058 and A375) and normal keratinocyte cell line (HaCaT).\nWe constructed a prognostic model based on five necroptosis-associated lncRNAs (AC245041.1, LINC00665, AC018553.1, LINC01871, and AC107464.3) and divided patients into high-risk group and low-risk group depending on risk scores. A predictive nomogram had been built to be a prognostic indicator to clinical factors. Functional enrichment analysis showed that immune functions had more relationship and immune checkpoints were more activated in low-risk group than that in high-risk group. Thus, the low-risk group would have a more sensitive response to immunotherapy.\nThis risk score signature could be used to divide CM patients into low- and high-risk groups, and facilitate treatment strategy decision making that immunotherapy is more suitable for those in low-risk group, providing a new sight for CM prognostic evaluation.", 
    "abstract": "Ischemic stroke (IS), resulting from the occlusion of the cerebral artery and subsequent interruption of blood flow, represents a major and critical threat to public health. Oxidative stress (OS) has been confirmed to play a role in the IS pathological process and neural death. Understanding the essential role of OS-related genes in ischemic stroke is critical to understanding the current perception of the pathophysiological process in IS. Herein, by integrating three IS datasets (GSE16561, GSE22255, and GSE58294), we divided IS samples into the low- and high-OS groups by calculating the OS score identified by the oxidative stress gene set. The functional enrichment analysis of differentially expressed genes (DEGs) between the low- and high-OS groups indicated that DEGs were associated with hypoxia, the inflammatory response, and oxidative phosphorylation pathways. Furthermore, nine hub genes (namely TLR1, CXCL1, MMP9, TLR4, IL1R2, EGR1, FOS, CXCL10, and DUSP1) were identified through the Girvan-Newman algorithm and cytoHubba algorithms. Nine hub genes were highly expressed in IS samples and positively related to neutrophils and macrophages. Drug-sensitive analysis targeting hub genes defined allopurinol and nickel sulfate as potential candidates for impairing the neural death caused by oxidative stress in IS. Finally, we employed five machine learning methods to check the efficacy of the predictive model identified by nine hub genes. The results showed that our model had superior power for predicting the OS activity of IS patients. TLR4 was found to have excellent diagnostic value and a wide-spectrum interaction with other hub genes. Our research emphasized the impact of oxidative stress on ischemic stroke, which supports the idea that antioxidants hold great promise in ischemic stroke therapy.", 
    "abstract": "Although outpatient psychodynamic psychotherapy is effective, there has been no improvement in treatment success in recent years. One way to improve psychodynamic treatment could be the use of machine learning to design treatments tailored to the individual patient's needs. In the context of psychotherapy, machine learning refers mainly to various statistical methods, which aim to predict outcomes (e.g., drop-out) of future patients as accurately as possible. We therefore searched various literature for all studies using machine learning in outpatient psychodynamic psychotherapy research to identify current trends and objectives.\nFor this systematic review, we applied the Preferred Reporting Items for systematic Reviews and Meta-Analyses Guidelines.\nIn total, we found four studies that used machine learning in outpatient psychodynamic psychotherapy research. Three of these studies were published between 2019 and 2021.\nWe conclude that machine learning has only recently made its way into outpatient psychodynamic psychotherapy research and researchers might not yet be aware of its possible uses. Therefore, we have listed a variety of perspectives on how machine learning could be used to increase treatment success of psychodynamic psychotherapies. In doing so, we hope to give new impetus to outpatient psychodynamic psychotherapy research on how to use machine learning to address previously unsolved problems.", 
    "abstract": "Tracking and measuring national carbon footprints is key to achieving the ambitious goals set by the Paris Agreement on carbon emissions. According to statistics, more than 10% of global transportation carbon emissions result from shipping. However, accurate tracking of the emissions of the small boat segment is not well established. Past research looked into the role played by small boat fleets in terms of greenhouse gases, but this has relied either on high-level technological and operational assumptions or the installation of global navigation satellite system sensors to understand how this vessel class behaves. This research is undertaken mainly in relation to fishing and recreational boats. With the advent of open-access satellite imagery and its ever-increasing resolution, it can support innovative methodologies that could eventually lead to the quantification of greenhouse gas emissions. Our work used deep learning algorithms to detect small boats in three cities in the Gulf of California in Mexico. The work produced a methodology named BoatNet that can detect, measure and classify small boats with leisure boats and fishing boats even under low-resolution and blurry satellite images, achieving an accuracy of 93.9% with a precision of 74.0%. Future work should focus on attributing a boat activity to fuel consumption and operational profile to estimate small boat greenhouse gas emissions in any given region.", 
    "abstract": null, 
    "abstract": "", 
    "abstract": "In today's digital world, information is growing along with the expansion of Internet usage worldwide. As a consequence, bulk of data is generated constantly which is known to be \"Big Data\". One of the most evolving technologies in twenty-first century is Big Data analytics, it is promising field for extracting knowledge from very large datasets and enhancing benefits while lowering costs. Due to the enormous success of big data analytics, the healthcare sector is increasingly shifting toward adopting these approaches to diagnose diseases. Due to the recent boom in medical big data and the development of computational methods, researchers and practitioners have gained the ability to mine and visualize medical big data on a larger scale. Thus, with the aid of integration of big data analytics in healthcare sectors, precise medical data analysis is now feasible with early sickness detection, health status monitoring, patient treatment, and community services is now achievable. With all these improvements, a deadly disease COVID is considered in this comprehensive review with the intention of offering remedies utilizing big data analytics. The use of big data applications is vital to managing pandemic conditions, such as predicting outbreaks of COVID-19 and identifying cases and patterns of spread of COVID-19. Research is still being done on leveraging big data analytics to forecast COVID-19. But precise and early identification of COVID disease is still lacking due to the volume of medical records like dissimilar medical imaging modalities. Meanwhile, Digital imaging has now become essential to COVID diagnosis, but the main challenge is the storage of massive volumes of data. Taking these limitations into account, a comprehensive analysis is presented in the systematic literature review (SLR) to provide a deeper understanding of big data in the field of COVID-19.", 
    "abstract": "The main objective of this research is to develop a sustainable stock quantitative investing model based on Machine Learning and Economic Value-Added techniques for optimizing investment strategies. Quantitative stock selection and algorithmic trading are the two features of the model. Principal component analysis and economic value-added criteria are used in quantitative stock model for efficiently stocks selection, which may repeatedly select valuable stocks. Machine learning techniques such as Moving Average Convergence, Stochastic Indicators and Long-Short Term Memory are used in algorithmic trading. One of the first attempts, the Economic Value-Added indicators are used to appraise stocks in this study. Furthermore, the application of EVA in stock selection is exposed. Illustration of the proposed model has been done on United States stock market and finding shows that Long-Short Term Memory (LSTM) networks can more accurately forecast future stock values. The proposed strategy is feasible in all market situations, with a return that is significantly larger than the market return. As a result, the proposed approach can not only assist the market in returning to rational investing, but also assist investors in obtaining significant returns that are both realistic and valuable.", 
    "abstract": null, 
    "abstract": "The widespread use of algorithms for prediction-based decisions urges us to consider the question of what it means for a given act or practice to be discriminatory. Building upon work by Kusner and colleagues in the field of machine learning, we propose a counterfactual condition as a necessary requirement on discrimination. To demonstrate the philosophical relevance of the proposed condition, we consider two prominent accounts of discrimination in the recent literature, by Lippert-Rasmussen and Hellman respectively, that do not logically imply our condition and show that they face important objections. Specifically, Lippert-Rasmussen's definition proves to be over-inclusive, as it classifies some acts or practices as discriminatory when they are not, whereas Hellman's account turns out to lack explanatory power precisely insofar as it does not countenance a counterfactual condition on discrimination. By defending the necessity of our counterfactual condition, we set the conceptual limits for justified claims about the occurrence of discriminatory acts or practices in society, with immediate applications to the ethics of algorithmic decision-making.", 
    "abstract": "", 
    "abstract": "Chronic musculoskeletal pain is a prevalent condition impacting around 20% of people globally; resulting in patients living with pain, fatigue, restricted social and employment capacity, and reduced quality of life. Interdisciplinary multimodal pain treatment programs have been shown to provide positive outcomes by supporting patients modify their behavior and improve pain management through focusing attention on specific patient valued goals rather than fighting pain.\nGiven the complex nature of chronic pain there is no single clinical measure to assess outcomes from multimodal pain programs. Using Centre for Integral Rehabilitation data from 2019-2021 (\nIndividual algorithm performance ranged from 0.49 to 0.65 AUC reflecting characteristic outcome variation across patients, and unbalanced training data with high positive proportions of up to 86% for some measures. As expected, no single outcome provided a reliable indicator, however the complete set of algorithms established a stratified prognostic patient profile. Patient level validation achieved consistent prognostic assessment of outcomes for 75.3% of the study group (\nThese results indicate that although no single algorithm was individually conclusive, the complete stratified profile consistently identified patient outcomes. Our predictive profile provides promising positive contribution for clinicians and patients to assist with personalized assessment and goal setting, program engagement and improved patient outcomes.", 
    "abstract": "This study aims to construct a machine learning model that can recognize preoperative, intraoperative, and postoperative high-risk indicators and predict the onset of venous thromboembolism (VTE) in patients.\nA total of 1239 patients diagnosed with gastric cancer were enrolled in this retrospective study, among whom 107 patients developed VTE after surgery. We collected 42 characteristic variables of gastric cancer patients from the database of Wuxi People's Hospital and Wuxi Second People's Hospital between 2010 and 2020, including patients' demographic characteristics, chronic medical history, laboratory test characteristics, surgical information, and patients' postoperative conditions. Four machine learning algorithms, namely, extreme gradient boosting (XGBoost), random forest (RF), support vector machine (SVM), and k-nearest neighbor (KNN), were employed to develop predictive models. We also utilized Shapley additive explanation (SHAP) for model interpretation and evaluated the models using k-fold cross-validation, receiver operating characteristic (ROC) curves, calibration curves, decision curve analysis (DCA), and external validation metrics.\nThe XGBoost algorithm demonstrated superior performance compared to the other three prediction models. The area under the curve (AUC) value for XGBoost was 0.989 in the training set and 0.912 in the validation set, indicating high prediction accuracy. Furthermore, the AUC value of the external validation set was 0.85, signifying good extrapolation of the XGBoost prediction model. The results of SHAP analysis revealed that several factors, including higher body mass index (BMI), history of adjuvant radiotherapy and chemotherapy, T-stage of the tumor, lymph node metastasis, central venous catheter use, high intraoperative bleeding, and long operative time, were significantly associated with postoperative VTE.\nThe machine learning algorithm XGBoost derived from this study enables the development of a predictive model for postoperative VTE in patients after radical gastrectomy, thereby assisting clinicians in making informed clinical decisions.", 
    "abstract": "Spinal cord injury (SCI) is a severe central nervous system injury that leads to significant sensory and motor impairment. Copper, an essential trace element in the human body, plays a vital role in various biological functions and is strictly regulated by copper chaperones and transporters. Cuproptosis, a novel type of metal ion-induced cell death, is distinct from iron deprivation. Copper deprivation is closely associated with mitochondrial metabolism and mediated by protein fatty acid acylation.\nIn this study, we investigated the effects of cuproptosis-related genes (CRGs) on disease progression and the immune microenvironment in acute spinal cord injury (ASCI) patients. We obtained the gene expression profiles of peripheral blood leukocytes from ASCI patients using the Gene Expression Omnibus (GEO) database. We performed differential gene analysis, constructed protein-protein interaction networks, conducted weighted gene co-expression network analysis (WGCNA), and built a risk model.\nOur analysis revealed that dihydrolipoamide dehydrogenase (DLD), a regulator of copper toxicity, was significantly associated with ASCI, and DLD expression was significantly upregulated after ASCI. Furthermore, gene ontology (GO) enrichment analysis and gene set variation analysis (GSVA) showed abnormal activation of metabolism-related processes. Immune infiltration analysis indicated a significant decrease in T cell numbers in ASCI patients, while M2 macrophage numbers were significantly increased and positively correlated with DLD expression.\nIn summary, our study demonstrated that DLD affects the ASCI immune microenvironment by promoting copper toxicity, leading to increased peripheral M2 macrophage polarization and systemic immunosuppression. Thus, DLD has potential as a promising biomarker for ASCI, providing a foundation for future clinical interventions.", 
    "abstract": "A high-stakes event is an extreme risk with a low probability of occurring, but severe consequences (e.g., life-threatening conditions or economic collapse). The accompanying lack of information is a source of high-stress pressure and anxiety for emergency medical services authorities. Deciding on the best proactive plan and action in this environment is a complicated process, which calls for intelligent agents to automatically produce knowledge in the manner of human-like intelligence. Research in high-stakes decision-making systems has increasingly focused on eXplainable Artificial Intelligence (XAI), but recent developments in prediction systems give little prominence to explanations based on human-like intelligence. This work investigates XAI based on cause-and-effect interpretations for supporting high-stakes decisions. We review recent applications in the first aid and medical emergency fields based on three perspectives: available data, desirable knowledge, and the use of intelligence. We identify the limitations of recent AI, and discuss the potential of XAI for dealing with such limitations. We propose an architecture for high-stakes decision-making driven by XAI, and highlight likely future trends and directions.", 
    "abstract": "Metastasis of cancer is directly related to death in almost all cases, however a lot is yet to be understood about this process. Despite advancements in the available radiological investigation techniques, not all cases of Distant Metastasis (DM) are diagnosed at initial clinical presentation. Also, there are currently no standard biomarkers of metastasis. Early, accurate diagnosis of DM is however crucial for clinical decision making, and planning of appropriate management strategies. Previous works have achieved little success in attempts to predict DM from either clinical, genomic, radiology, or histopathology data. In this work we attempt a multimodal approach to predict the presence of DM in cancer patients by combining gene expression data, clinical data and histopathology images. We tested a novel combination of Random Forest (RF) algorithm with an optimization technique for gene selection, and investigated if gene expression pattern in the primary tissues of three cancer types (Bladder Carcinoma, Pancreatic Adenocarcinoma, and Head and Neck Squamous Carcinoma) with DM are similar or different. Gene expression biomarkers of DM identified by our proposed method outperformed Differentially Expressed Genes (DEGs) identified by the DESeq2 software package in the task of predicting presence or absence of DM. Genes involved in DM tend to be more cancer type specific rather than general across all cancers. Our results also indicate that multimodal data is more predictive of metastasis than either of the three unimodal data tested, and genomic data provides the highest contribution by a wide margin. The results re-emphasize the importance for availability of sufficient image data when a weakly supervised training technique is used. Code is made available at: https://github.com/rit-cui-lab/Multimodal-AI-for-Prediction-of-Distant-Metastasis-in-Carcinoma-Patients.", 
    "abstract": "Aerobic glycolysis is a process that metabolizes glucose under aerobic conditions, finally producing pyruvate, lactic acid, and ATP for tumor cells. Nevertheless, the overall significance of glycolysis-related genes in colorectal cancer and how they affect the immune microenvironment have not been investigated.\nBy combining the transcriptome and single-cell analysis, we summarize the various expression patterns of glycolysis-related genes in colorectal cancer. Three glycolysis-associated clusters (GAC) were identified with distinct clinical, genomic, and tumor microenvironment (TME). By mapping GAC to single-cell RNA sequencing analysis (scRNA-seq), we next discovered that the immune infiltration profile of GACs was similar to that of bulk RNA sequencing analysis (bulk RNA-seq). In order to determine the kind of GAC for each sample, we developed the GAC predictor using markers of single cells and GACs that were most pertinent to clinical prognostic indications. Additionally, potential drugs for each GAC were discovered using different algorithms.\nGAC1 was comparable to the immune-desert type, with a low mutation probability and a relatively general prognosis; GAC2 was more likely to be immune-inflamed/excluded, with more immunosuppressive cells and stromal components, which also carried the risk of the poorest prognosis; Similar to the immune-activated type, GAC3 had a high mutation rate, more active immune cells, and excellent therapeutic potential.\nIn conclusion, we combined transcriptome and single-cell data to identify new molecular subtypes using glycolysis-related genes in colorectal cancer based on machine-learning methods, which provided therapeutic direction for colorectal patients.", 
    "abstract": "Sepsis remains a complex condition with incomplete understanding of its pathogenesis. Further research is needed to identify prognostic factors, risk stratification tools, and effective diagnostic and therapeutic targets.\nThree GEO datasets (GSE54514, GSE65682, and GSE95233) were used to explore the potential role of mitochondria-related genes (MiRGs) in sepsis. WGCNA and two machine learning algorithms (RF and LASSO) were used to identify the feature of MiRGs. Consensus clustering was subsequently carried out to determine the molecular subtypes for sepsis. CIBERSORT algorithm was conducted to assess the immune cell infiltration of samples. A nomogram was also established to evaluate the diagnostic ability of feature biomarkers via \"rms\" package.\nThree different expressed MiRGs (DE-MiRGs) were identified as sepsis biomarkers. A significant difference in the immune microenvironment landscape was observed between healthy controls and sepsis patients. Among the DE-MiRGs, \nBy digging the role of these pivotal genes in immune cell infiltration, we gained a better understanding of the molecular immune mechanism in sepsis and identified potential intervention and treatment strategies.", 
    "abstract": "Recent developments in machine learning have greatly facilitated the design of proteins with improved properties. However, accurately assessing the contributions of an individual or multiple amino acid mutations to overall protein stability to select the most promising mutants remains a challenge. Knowing the specific types of amino acid interactions that improve energetic stability is crucial for finding favorable combinations of mutations and deciding which mutants to test experimentally. In this work, we present an interactive workflow for assessing the energetic contributions of single and multi-mutant designs of proteins. The energy breakdown guided protein design (ENDURE) workflow includes several key algorithms, including per-residue energy analysis and the sum of interaction energies calculations, which are performed using the Rosetta energy function, as well as a residue depth analysis, which enables tracking the energetic contributions of mutations occurring in different spatial layers of the protein structure. ENDURE is available as a web application that integrates easy-to-read summary reports and interactive visualizations of the automated energy calculations and helps users selecting protein mutants for further experimental characterization. We demonstrate the effectiveness of the tool in identifying the mutations in a designed polyethylene terephthalate (PET)-degrading enzyme that add up to an improved thermodynamic stability. We expect that ENDURE can be a valuable resource for researchers and practitioners working in the field of protein design and optimization. ENDURE is freely available for academic use at: http://endure.kuenzelab.org.", 
    "abstract": "Deep learning has been widely used for plant disease recognition in smart agriculture and has proven to be a powerful tool for image classification and pattern recognition. However, it has limited interpretability for deep features. With the transfer of expert knowledge, handcrafted features provide a new way for personalized diagnosis of plant diseases. However, irrelevant and redundant features lead to high dimensionality. In this study, we proposed a swarm intelligence algorithm for feature selection [salp swarm algorithm for feature selection (SSAFS)] in image-based plant disease detection. SSAFS is employed to determine the ideal combination of handcrafted features to maximize classification success while minimizing the number of features. To verify the effectiveness of the developed SSAFS algorithm, we conducted experimental studies using SSAFS and 5 metaheuristic algorithms. Several evaluation metrics were used to evaluate and analyze the performance of these methods on 4 datasets from the UCI machine learning repository and 6 plant phenomics datasets from PlantVillage. Experimental results and statistical analyses validated the outstanding performance of SSAFS compared to existing state-of-the-art algorithms, confirming the superiority of SSAFS in exploring the feature space and identifying the most valuable features for diseased plant image classification. This computational tool will allow us to explore an optimal combination of handcrafted features to improve plant disease recognition accuracy and processing time.", 
    "abstract": "The global Covid-19 pandemic has forced countries to impose strict lockdown restrictions and mandatory stay-at-home orders with varying impacts on individual's health. Combining a data-driven machine learning paradigm and a statistical approach, our previous paper documented a U-shaped pattern in levels of self-perceived loneliness in both the UK and Greek populations during the first lockdown (17 April to 17 July 2020). The current paper aimed to test the robustness of these results by focusing on data from the first and second lockdown waves in the UK. We tested a) the impact of the chosen model on the identification of the most time-sensitive variable in the period spent in lockdown. Two new machine learning models - namely, support vector regressor (SVR) and multiple linear regressor (MLR) were adopted to identify the most time-sensitive variable in the UK dataset from Wave 1 (n = 435). In the second part of the study, we tested b) whether the pattern of self-perceived loneliness found in the first UK national lockdown was generalisable to the second wave of the UK lockdown (17 October 2020 to 31 January 2021). To do so, data from Wave 2 of the UK lockdown (n = 263) was used to conduct a graphical inspection of the week-by-week distribution of self-perceived loneliness scores. In both SVR and MLR models, depressive symptoms resulted to be the most time-sensitive variable during the lockdown period. Statistical analysis of depressive symptoms by week of lockdown resulted in a U-shaped pattern between weeks 3 and 7 of Wave 1 of the UK national lockdown. Furthermore, although the sample size by week in Wave 2 was too small to have a meaningful statistical insight, a graphical U-shaped distribution between weeks 3 and 9 of lockdown was observed. Consistent with past studies, these preliminary results suggest that self-perceived loneliness and depressive symptoms may be two of the most relevant symptoms to address when imposing lockdown restrictions.", 
    "abstract": "An intuitive and generalisable approach to spatial-temporal feature extraction for high-density (HD) functional Near-Infrared Spectroscopy (fNIRS) brain-computer interface (BCI) is proposed, demonstrated here using Frequency-Domain (FD) fNIRS for motor-task classification. Enabled by the HD probe design, layered topographical maps of Oxy/deOxy Haemoglobin changes are used to train a 3D convolutional neural network (CNN), enabling simultaneous extraction of spatial and temporal features. The proposed spatial-temporal CNN is shown to effectively exploit the spatial relationships in HD fNIRS measurements to improve the classification of the functional haemodynamic response, achieving an average F1 score of 0.69 across seven subjects in a mixed subjects training scheme, and improving subject-independent classification as compared to a standard temporal CNN.", 
    "abstract": "Tuberculosis (TB), caused by the bacterium \nThe TB datasets were downloaded from the GEO database. Three machine learning models, namely LASSO, RF, and SVM-RFE, were used to identify the key characteristic genes related to inflammation during the progression of LTBI to ATB. The expression and diagnostic accuracy of these characteristic genes were subsequently verified. These genes were then used to develop diagnostic nomograms. In addition, single-cell expression clustering analysis, immune cell expression clustering analysis, GSVA analysis, immune cell correlation, and immune checkpoint correlation of characteristic genes were conducted. Furthermore, the upstream shared miRNA was predicted, and a miRNA-genes network was constructed. Candidate drugs were also analyzed and predicted.\nIn comparison to LTBI, a total of 96 upregulated and 26 downregulated genes related to the inflammatory response were identified in ATB. These characteristic genes have demonstrated excellent diagnostic performance and significant correlation with many immune cells and immune sites. The results of the miRNA-genes network analysis suggested a potential role of hsa-miR-3163 in the molecular mechanism of LTBI progressing into ATB. Moreover, retinoic acid may offer a potential avenue for the prevention of LTBI progression to ATB and for the treatment of ATB.\nOur research has identified key inflammatory response-related genes that are characteristic of LTBI progression to ATB and hsa-miR-3163 as a significant node in the molecular mechanism of this progression. Our analyses have demonstrated the excellent diagnostic performance of these characteristic genes and their significant correlation with many immune cells and immune checkpoints. The CD274 immune checkpoint presents a promising target for the prevention and treatment of ATB. Furthermore, our findings suggest that retinoic acid may have a role in preventing LTBI from progressing to ATB and in treating ATB. This study provides a new perspective for differential diagnosis of LTBI and ATB and may uncover potential inflammatory immune mechanisms, biomarkers, therapeutic targets, and effective drugs in the progression of LTBI into ATB.", 
    "abstract": "The influence of pediatricians on parental acceptance of COVID-19 vaccine for children has not been well studied. We designed a survey to estimate the impact of pediatricians' recommendations on caregivers' vaccine acceptance while accounting for participants' socio-demographic and personal characteristics. The secondary objectives were to compare childhood vaccination rates among different age groups and categorize caregivers' concerns about vaccinating young (under-five) children. Overall, the study aimed to provide insight into potential pro-vaccination strategies that could integrate pediatricians to alleviate parental vaccine hesitancy.\nWe conducted an online cross-sectional survey study using Redcap, in August 2022. We enquired COVID-19 vaccination status of the children in the family (\u2265five years). The survey questionnaire included socio-demographic and personal characteristics: age, race, sex, education, financial status, residence, healthcare worker, COVID-19 vaccination status and side effects, children's influenza vaccination status, and pediatricians' recommendations (1-5 scale). Logistic regression and neural network models were used to estimate the influence of socio-demographic determinants on children's vaccine status and build predictors' ranking.\nThe participants (\nPediatricians' affirmative recommendation was significantly associated with caregivers' COVID-19 vaccine acceptance for children while accounting for participants' socio-demographic characteristics. Notably, vaccine acceptance was lower among younger compared to older children, and caregivers' uncertainty about vaccine safety for under-five children was prevalent. Thus, pro-vaccination strategies might incorporate pediatricians to alleviate parental concerns and optimize poor vaccination rate among under-five children.", 
    "abstract": "A synthetic population is a simplified microscopic representation of an actual population. Statistically representative at the population level, it provides valuable inputs to simulation models (especially agent-based models) in research areas such as transportation, land use, economics, and epidemiology. This article describes the datasets from the Synthetic Sweden Mobility (", 
    "abstract": "Successful weaning from mechanical ventilation is important for patients admitted to intensive care units. However, models for predicting real-time weaning outcomes remain inadequate. Therefore, this study aimed to develop a machine-learning model for predicting successful extubation only using time-series ventilator-derived parameters with good accuracy.\nPatients with mechanical ventilation admitted to the Yuanlin Christian Hospital in Taiwan between August 2015 and November 2020 were retrospectively included. A dataset with ventilator-derived parameters was obtained before extubation. Recursive feature elimination was applied to select the most important features. Machine-learning models of logistic regression, random forest (RF), and support vector machine were adopted to predict extubation outcomes. In addition, the synthetic minority oversampling technique (SMOTE) was employed to address the data imbalance problem. The area under the receiver operating characteristic (AUC), F1 score, and accuracy, along with the 10-fold cross-validation, were used to evaluate prediction performance.\nIn this study, 233 patients were included, of whom 28 (12.0%) failed extubation. The six ventilatory variables per 180\u2009s dataset had optimal feature importance. RF exhibited better performance than the others, with an AUC value of 0.976 (95% confidence interval [CI], 0.975-0.976), accuracy of 94.0% (95% CI, 93.8-94.3%), and an F1 score of 95.8% (95% CI, 95.7-96.0%). The difference in performance between the RF and the original and SMOTE datasets was small.\nThe RF model demonstrated a good performance in predicting successful extubation in mechanically ventilated patients. This algorithm made a precise real-time extubation outcome prediction for patients at different time points.", 
    "abstract": "Immunogenic cell death (ICD) serves a critical role in regulating cell death adequate to activate an adaptive immune response, and it is associated with various inflammation-related diseases. However, the specific role of ICD-related genes in COVID-19 remains unclear. We acquired COVID-19-related information from the GEO database and a total of 14 ICD-related differentially expressed genes (DEGs) were identified. These ICD-related DEGs were closely associated with inflammation and immune activity. Afterward, CASP1, CD4, and EIF2AK3 among the 14 DEGs were selected as feature genes based on LASSO, Random Forest, and SVM-RFE algorithms, which had reliable diagnostic abilities. Moreover, functional enrichment analysis indicated that these feature genes may have a potential role in COVID-19 by being involved in the regulation of immune response and metabolism. Further CIBERSORT analysis demonstrated that the variations in the immune microenvironment of COVID-19 patients may be correlated with CASP1, CD4, and EIF2AK3. Additionally, 33 drugs targeting 3 feature genes had been identified, and the ceRNA network demonstrated a complicated regulative association based on these feature genes. Our work identified that CASP1, CD4, and EIF2AK3 were diagnostic genes of COVID-19 and correlated with immune activity. This study presents a reliable diagnostic signature and offers an overview to investigate the mechanism of COVID-19.", 
    "abstract": null, 
    "abstract": "Aesthetic facial surgeries historically rely on subjective analysis in determining success; this limits objective comparison of surgical outcomes.\nThis case study exemplifies the use of an artificial intelligence software on objectively analyzing facial rejuvenation techniques with the aim of reducing subjective bias.\nRetrospectively, all patients who underwent facial rejuvenation surgery with concomitant procedures from 2015 to 2017 were included (\nPostoperatively, Group A experienced a decrease in happiness by 0.84% and a decrease in anger by 6.87% (\nThis study provides the first proof of concept for the use of a machine learning software application to objectively assess various aesthetic surgical outcomes in facial rejuvenation. Due to limitations in patient heterogeneity, this study does not claim one technique's superiority but serves as a conceptual foundation for future investigation.", 
    "abstract": "Drug compliance is the act of taking medication on schedule or taking medication as prescribed and obeying other medical instructions. It is the most crucial aspect in the treatment of chronic diseases particularly for patients with multidrug-resistant tuberculosis (MDR-TB). Drug non-compliance is the main reason for causing drug resistance and poor treatment outcomes. Hence, developing a risk prediction model by using early obtainable prognostic determinants of non-compliance is vital in averting the existing, unacceptably high level of poor treatment outcomes and reducing drug resistance among MDR-TB patients.\nA retrospective follow-up study was conducted on a total of 517 MDR-TB patients in Northwest Ethiopia. A logistic regression-based machine learning algorithm was used to develop a risk score for the prediction of treatment non-compliance among MDR-TB patients in selected referral hospitals of Northwest Ethiopia. The data were incorporated in EpiData version 3.1 and exported to STATA version 16 and R version 4.0.5 software for analysis. A simplified risk prediction model was developed, and its performance was reported. It was also internally validated by using a bootstrapping method.\nEducational status, registration group (previously treated/new), treatment support, model of care, and khat use were significant prognostic features of treatment non-compliance. The model has a discriminatory power of area under curve (AUC)\u2009=\u20090.79 with a 95% CI of 0.74-0.85 and a calibration test of \nEducational status, registration group, treatment supporter, model of care, and khat use are important features that can predict treatment non-compliance of MDR-TB patients. The risk score developed has a satisfactory level of accuracy and good calibration. In addition, it is clinically interpretable and easy to use in clinical practice, because its features are easily ascertainable even at the initial stage of patient enrolment. Hence, it becomes important to reduce poor treatment outcomes and drug resistance.", 
    "abstract": "Early detection of breast cancer can be achieved through mutation detection in DNA sequences, which can be acquired through patient blood samples. Mutation detection can be performed using alignment and machine learning techniques. However, alignment techniques require reference sequences, and machine learning techniques still cannot predict index mutation and require supporting tools. Therefore, in this research, a Temporal Convolutional Network (TCN) model was proposed to detect the type and index mutation faster and without reference sequences and supporting tools. The architecture of the proposed TCN model is specifically designed for sequential labeling tasks on DNA sequence data. This allows for the detection of the mutation type of each nucleotide in the sequence, and if the nucleotide has a mutation, the index mutation can be obtained. The proposed model also uses 2-mers and 3-mers mapping techniques to improve detection performance. Based on the tests that have been carried out, the proposed TCN model can achieve the highest F1-score of 0.9443 for COSMIC dataset and 0.9629 for RSCM dataset, Additionally, the proposed TCN model can detect index mutation six times faster than BiLSTM model. Furthermore, the proposed model can detect type and index mutations based on the patient's DNA sequence, without the need for reference sequences or other additional tools.", 
    "abstract": "The morphology of the nuclei represents most of the clinical pathological information, and nuclei segmentation is a vital step in current automated histopathological image analysis. Supervised machine learning-based segmentation models have already achieved outstanding performance with sufficiently precise human annotations. Nevertheless, outlining such labels on numerous nuclei is extremely professional needing and time consuming. Automatic nuclei segmentation with minimal manual interventions is highly needed to promote the effectiveness of clinical pathological researches. Semi-supervised learning greatly reduces the dependence on labeled samples while ensuring sufficient accuracy. In this paper, we propose a Multi-Edge Feature Fusion Attention Network (MEFFA-Net) with three feature inputs including image, pseudo-mask and edge, which enhances its learning ability by considering multiple features. Only a few labeled nuclei boundaries are used to train annotations on the remaining mostly unlabeled data. The MEFFA-Net creates more precise boundary masks for nucleus segmentation based on pseudo-masks, which greatly reduces the dependence on manual labeling. The MEFFA-Block focuses on the nuclei outline and selects features conducive to segment, making full use of the multiple features in segmentation. Experimental results on public multi-organ databases including MoNuSeg, CPM-17 and CoNSeP show that the proposed model has the mean IoU segmentation evaluations of 0.706, 0.751, and 0.722, respectively. The model also achieves better results than some cutting-edge methods while the labeling work is reduced to 1/8 of common supervised strategies. Our method provides a more efficient and accurate basis for nuclei segmentations and further quantifications in pathological researches.", 
    "abstract": "Lyme disease is the most prevalent vector-borne disease in the United States, yet its host factors are poorly understood and diagnostic tests are limited. We evaluated patients in a large health system to uncover the role of cholesterol in the susceptibility, severity, and machine learning-based diagnosis of Lyme disease.\nA longitudinal health system cohort comprised 1,019,175 individuals with electronic health record data and 50,329 with linked genetic data. Associations of blood cholesterol level, a cholesterol genetic score comprising common genetic variants, and burden of rare loss-of-function (LoF) variants in cholesterol metabolism genes with Lyme disease were investigated. A portable machine learning model was constructed and tested to predict Lyme disease using routine lipid and clinical measurements.\nThere were 3,832 cases of Lyme disease. Increasing cholesterol was associated with greater risk of Lyme disease and hypercholesterolemia was more prevalent in Lyme disease cases than controls. Cholesterol genetic scores and rare LoF variants in CD36 and LDLR were associated with elevated Lyme disease risk. Serological profiling of cases revealed parallel trajectories of rising cholesterol and immunoglobulin levels over the disease course, including marked increases in individuals with LoF variants and high cholesterol genetic scores. The machine learning model predicted Lyme disease solely using routine lipid panel, blood count, and metabolic measurements.\nThese results demonstrate the value of large-scale genetic and clinical data to reveal host factors underlying infectious disease biology, risk, and prognosis, and the potential for their clinical translation to machine learning diagnostics that do not need specialized assays.", 
    "abstract": "The classification of limb movements can provide with control commands in non-invasive brain-computer interface. Previous studies on the classification of limb movements have focused on the classification of left/right limbs; however, the classification of different types of upper limb movements has often been ignored despite that it provides more active-evoked control commands in the brain-computer interface. Nevertheless, few machine learning method can be used as the state-of-the-art method in the multi-class classification of limb movements. This work focuses on the multi-class classification of upper limb movements and proposes the multi-class filter bank task-related component analysis (mFBTRCA) method, which consists of three steps: spatial filtering, similarity measuring and filter bank selection. The spatial filter, namely the task-related component analysis, is first used to remove noise from EEG signals. The canonical correlation measures the similarity of the spatial-filtered signals and is used for feature extraction. The correlation features are extracted from multiple low-frequency filter banks. The minimum-redundancy maximum-relevance selects the essential features from all the correlation features, and finally, the support vector machine is used to classify the selected features. The proposed method compared against previously used models is evaluated using two datasets. mFBTRCA achieved a classification accuracy of 0.4193 \u00b10.0780 (7 classes) and 0.4032 \u00b10.0714 (5 classes), respectively, which improves on the best accuracies achieved using the compared methods (0.3590 \u00b10.0645 and 0.3159 \u00b10.0736, respectively). The proposed method is expected to provide more control commands in the applications of non-invasive brain-computer interfaces.", 
    "abstract": "In recent years, machine learning methods have become increasingly popular prediction methods in psychology. At the same time, psychological researchers are typically not only interested in making predictions about the dependent variable, but also in learning which predictor variables are relevant, how they influence the dependent variable, and which predictors interact with each other. However, most machine learning methods are not directly interpretable. Interpretation techniques that support researchers in describing how the machine learning technique came to its prediction may be a means to this end. We present a variety of interpretation techniques and illustrate the opportunities they provide for interpreting the results of two widely used black box machine learning methods that serve as our examples: random forests and neural networks. At the same time, we illustrate potential pitfalls and risks of misinterpretation that may occur in certain data settings. We show in which way correlated predictors impact interpretations with regard to the relevance or shape of predictor effects and in which situations interaction effects may or may not be detected. We use simulated didactic examples throughout the article, as well as an empirical data set for illustrating an approach to objectify the interpretation of visualizations. We conclude that, when critically reflected, interpretable machine learning techniques may provide useful tools when describing complex psychological relationships. (PsycInfo Database Record (c) 2023 APA, all rights reserved).", 
    "abstract": "Non-obstructive azoospermia (NOA) is a common cause of male infertility, and no specific diagnostic indicators exist. In this study, we used human testis datasets GSE45885, GSE45887, and GSE108886 from GEO database as training datasets, and screened 6 signature genes (all lowly expressed in the NOA group) using Boruta algorithm and Lasso regression: C12orf54, TSSK6, OR2H1, FER1L5, C9orf153, XKR3. The diagnostic efficacy of the above genes was examined by constructing models with LightGBM algorithm: the AUC (Area Under Curve) of both ROC and Precision-Recall curves for internal validation was 1.0 (", 
    "abstract": "To develop an assistant tool based on machine learning for early frailty screening in patients receiving maintenance hemodialysis.\nThis is a single-center retrospective study. 141 participants' basic information, scale results and laboratory findings were collected and the FRAIL scale was used to assess frailty. Then participants were divided into the frailty group (n\u2009=\u200984) and control group (n\u2009=\u200957). After feature selection, data split and oversampling, ten commonly used binary machine learning methods were performed and a voting classifier was developed.\nThe grade results of Clinical Frailty Scale, age, serum magnesium, lactate dehydrogenase, comorbidity and fast blood glucose were considered to be the best feature set for early frailty screening. After abandoning models with overfitting or poor performance, the voting classifier based on Support Vector Machine, Adaptive Boosting and Naive Bayes achieved a good screening performance (sensitivity: 68.24%\u2009\u00b1\u20098.40%, specificity:72.50%\u2009\u00b1\u200911.81%, F1 score: 72.55%\u2009\u00b1\u20094.65%, AUC:78.38%\u2009\u00b1\u20096.94%).\nA simple and efficient early frailty screening assistant tool for patients receiving maintenance hemodialysis based on machine learning was developed. It can provide assistance on frailty, especially pre-frailty screening and decision-making tasks.", 
    "abstract": null, 
    "abstract": "Cardiac uptake on technetium-99m whole-body scintigraphy (WBS) is almost pathognomonic of transthyretin cardiac amyloidosis. The rare false positives are often related to light-chain cardiac amyloidosis. However, this scintigraphic feature remains largely unknown, leading to misdiagnosis despite characteristic images. A retrospective review of all WBSs in a hospital database to detect those with cardiac uptake may allow the identification of undiagnosed patients.\nThe authors sought to develop and validate a deep learning-based model that automatically detects significant cardiac uptake (\u2265Perugini grade 2) on WBS from large hospital databases in order to retrieve patients at risk of cardiac amyloidosis.\nThe model is based on a convolutional neural network with image-level labels. The performance evaluation was performed with C-statistics using a 5-fold cross-validation scheme stratified so that the proportion of positive and negative WBSs remained constant across folds and using an external validation data set.\nThe training data set consisted of 3,048 images: 281 positives (\u2265Perugini 2) and 2,767 negatives. The external validation data set consisted of 1,633 images: 102 positives and 1,531 negatives. The performance of the 5-fold cross-validation and external validation was as follows: 98.9% (\u00b1 1.0) and 96.1% for sensitivity, 99.5% (\u00b1 0.4) and 99.5% for specificity, and 0.999 (SD\u00a0=\u00a00.000) and 0.999 for the area under the curve of the receiver-operating characteristic curves. Sex, age\u00a0<90 years, body mass index, injection-acquisition delay, radionuclides, and the indication of WBS only slightly affected performances.\nThe authors' detection model is effective at identifying patients with cardiac uptake \u2265Perugini 2 on WBS and may help in the diagnosis of patients with cardiac amyloidosis.", 
    "abstract": "PittUDT, a recursive partitioning decision tree algorithm for predicting urine culture (UC) positivity based on macroscopic and microscopic urinalysis (UA) parameters, was developed in support of a broader system-wide diagnostic stewardship initiative to increase appropriateness of UC testing. Reflex algorithm training utilized results from 19,511 paired UA and UC cases (26.8% UC positive); the average patient age was 57.4\u2009years, and 70% of samples were from female patients. Receiver operating characteristic (ROC) analysis identified urine white blood cells (WBCs), leukocyte esterase, and bacteria as the best predictors of UC positivity, with areas under the ROC curve of 0.79, 0.78, and 0.77, respectively. Using the held-out test data set (9,773 cases; 26.3% UC positive), the PittUDT algorithm met the prespecified target of a negative predictive value above 90% and resulted in a 30 to 60% total negative proportion (true-negative plus false-negative predictions). These data show that a supervised rule-based machine learning algorithm trained on paired UA and UC data has adequate predictive ability for triaging urine specimens by identifying low-risk urine specimens, which are unlikely to grow pathogenic organisms, with a false-negative proportion under 5%. The decision tree approach also generates human-readable rules that can be easily implemented across multiple hospital sites and settings. Our work demonstrates how a data-driven approach can be used to optimize UA parameters for predicting UC positivity in a reflex protocol, with the intent of improving antimicrobial stewardship and UC utilization, a potential avenue for cost savings.", 
    "abstract": "Upstream open reading frames (uORFs) are potent ", 
    "abstract": "Teen mothers experience disadvantage across a wide range of outcomes. However, previous research is equivocal with respect to possible long-term mental health consequences of teen motherhood and has not adequately considered the possibility that effects on mental health may be heterogeneous. Drawing on data from the 1970 British Birth Cohort Study, this article applies a novel statistical machine-learning approach-Bayesian Additive Regression Trees-to estimate the effects of teen motherhood on mental health outcomes at ages 30, 34, and 42. We extend previous work by estimating not only sample-average effects but also individual-specific estimates. Our results show that sample-average mental health effects of teen motherhood are substantively small at all time points, apart from age 30 comparisons to women who first became mothers at age 25\u201230. Moreover, we find that these effects are largely homogeneous for all women in the sample-indicating that there are no subgroups in the data who experience important detrimental mental health consequences. We conclude that there are likely no mental health benefits to policy and interventions that aim to prevent teen motherhood.", 
    "abstract": null, 
    "abstract": "Protein transporters not only have essential functions in regulating the transport of endogenous substrates and remote communication between organs and organisms, but they also play a vital role in drug absorption, distribution, and excretion and are recognized as major determinants of drug safety and efficacy. Understanding transporter function is important for drug development and clarifying disease mechanisms. However, the experimental-based functional research on transporters has been challenged and hinged by the expensive cost of time and resources. With the increasing volume of relevant omics datasets and the rapid evolution of artificial intelligence (AI) techniques, next-generation AI is becoming increasingly prevalent in the functional and pharmaceutical research of transporters. Thus, a comprehensive discussion on the state-of-the-art application of AI in three cutting-edge directions was provided in this review, which included (a) transporter classification and function annotation, (b) structure discovery of membrane transporters, and (c) drug-transporter interaction prediction. This study provides a panoramic view of AI algorithms and tools applied to the field of transporters. It is expected to guide a better understanding and utilization of AI techniques for in-depth studies of transporter-centered functional and pharmaceutical research.", 
    "abstract": "Differential diagnosis of single-nodule pulmonary metastasis (SNPM) and second primary lung cancer (SPLC) in patients with colorectal cancer (CRC) prior to lung surgery is relatively complex. Radiomics is an emerging technique for image information analysis, while it has not yet been applied to construct a differential diagnostic model between SNPM and SPLC in patients with CRC. In the present study, we aimed to extract radiomics signatures from thin-section computed tomography (CT) images of the chest. These radiomics signatures were combined with clinical features to construct a composite differential diagnostic model.\nA total of 91 patients with CRC, including 66 patients with SNPM and 25 patients with SPLC, were enrolled in this study. Patients were randomly assigned to the training cohort (n\u2009\u2009=\u2009\u200963) and validation cohort (n\u2009\u2009=\u2009\u200928) at a ratio of 7 to 3. Moreover, 107 radiomics features were extracted from the chest thin-section CT images. The least absolute shrinkage and selection operator (LASSO) regression was used to filter these features, and clinical features were screened by univariate analysis. The screened radiomics and clinical features were combined to construct a multifactorial logistic regression composite model. The receiver operating characteristic (ROC) curves were adopted to evaluate the models, and the corresponding nomograms were created.\nA series of 6 radiomics characteristics was screened by LASSO. After univariate logistic regression analysis, the composite model finally included 4 radiomics features and 4 clinical features. In the training cohort, the area under the curve scores of ROC curves were 0.912 (95% confidence interval [CI]: 0.813-0.969), 0.884 (95% CI: 0.778-0.951), and 0.939 (95% CI: 0.848-0.984) for models derived from radiomics, clinical, and combined features, respectively. Similarly, these values were 0.756 (95% CI: 0.558-0.897), 0.888 (95% CI: 0.711-0.975), and 0.950 (95% CI: 0.795-0.997) in the validation cohort, respectively.\nWe constructed a model for differential diagnosis of SNPM and SPLC in patients with CRC using radiomics and clinical features. Moreover, our findings provided a new assessment tool for patients with CRC in the future.", 
    "abstract": "Triple-negative breast cancer (TNBC) is the most aggressive breast cancer subtype. Patients with TNBC are primarily treated with neoadjuvant chemotherapy (NAC). The response to NAC is prognostic, with reductions in overall survival and disease-free survival rates in those patients who do not achieve a pathological complete response (pCR). Based on this premise, we hypothesized that paired analysis of primary and residual TNBC tumors following NAC could identify unique biomarkers associated with post-NAC recurrence.\nWe investigated 24 samples from 12 non-LAR TNBC patients with paired pre- and post-NAC data, including four patients with recurrence shortly after surgery (<\u200924\u00a0months) and eight who remained recurrence-free (>\u200948\u00a0months). These tumors were collected from a prospective NAC breast cancer study (BEAUTY) conducted at the Mayo Clinic. Differential expression analysis of pre-NAC biopsies showed minimal gene expression differences between early recurrent and nonrecurrent TNBC tumors; however, post-NAC samples demonstrated significant alterations in expression patterns in response to intervention. Topological-level differences associated with early recurrence were implicated in 251 gene sets, and an independent assessment of microarray gene expression data from the 9 paired non-LAR samples available in the NAC I-SPY1 trial confirmed 56 gene sets. Within these 56 gene sets, 113 genes were observed to be differentially expressed in the I-SPY1 and BEAUTY post-NAC studies. An independent (n\u2009=\u2009392) breast cancer dataset with relapse-free survival (RFS) data was used to refine our gene list to a 17-gene signature. A threefold cross-validation analysis of the gene signature with the combined BEAUTY and I-SPY1 data yielded an average AUC of 0.88 for six machine-learning models. Due to the limited number of studies with pre- and post-NAC TNBC tumor data, further validation of the signature is needed.\nAnalysis of multiomics data from post-NAC TNBC chemoresistant tumors showed down regulation of mismatch repair and tubulin pathways. Additionally, we identified a 17-gene signature in TNBC associated with post-NAC recurrence enriched with down-regulated immune genes.", 
    "abstract": "Since the past decades, more lung cancer patients have been experiencing lasting benefits from immunotherapy. It is imperative to accurately and intelligently select appropriate patients for immunotherapy or predict the immunotherapy efficacy. In recent years, machine learning (ML)-based artificial intelligence (AI) was developed in the area of medical-industrial convergence. AI can help model and predict medical information. A growing number of studies have combined radiology, pathology, genomics, proteomics data in order to predict the expression levels of programmed death-ligand 1 (PD-L1), tumor mutation burden (TMB)\u00a0and tumor microenvironment (TME) in cancer patients or predict the likelihood of immunotherapy benefits and side effects. Finally, with the advancement of AI and ML, it is believed that \"digital biopsy\" can replace the traditional single assessment method to benefit more cancer patients and help clinical decision-making in the future. In this review, the applications of AI in PD-L1/TMB prediction, TME prediction and lung cancer immunotherapy are discussed.", 
    "abstract": "Cancer registries collect patient-specific information about cancer diseases. The collected information is verified and made available to clinical researchers, physicians, and patients. When processing information, cancer registries verify that the patient-specific records they collect are plausible. This means that the collected information about a particular patient makes medical sense.\nUnsupervised machine learning approaches can detect implausible electronic health records without human guidance. Therefore, this article investigates two unsupervised anomaly detection approaches, a pattern-based approach\u00a0(FindFPOF) and a compression-based approach\u00a0(autoencoder), to identify implausible electronic health records in cancer registries. Unlike most existing work that analyzes synthetic anomalies, we compare the performance of both approaches and a baseline (random selection of records) on a real-world dataset. The dataset contains\u00a021,104 electronic health records of patients with breast, colorectal, and prostate tumors. Each record consists of\u00a016 categorical variables describing the disease, the patient, and the diagnostic procedure. The samples identified by FindFPOF, the autoencoder, and a random selection-a total of\u00a0785 different records-are evaluated in a real-world scenario by medical domain experts.\nBoth anomaly detection methods are good at detecting implausible electronic health records. First, domain experts identified\u00a0[Formula: see text] of\u00a0300 randomly selected records as implausible. With FindFPOF and the autoencoder,\u00a0[Formula: see text] of the proposed\u00a0300 records in each sample were implausible. This corresponds to a precision of\u00a0[Formula: see text] for FindFPOF and the autoencoder. Second, for\u00a0300 randomly selected records that were labeled by domain experts, the sensitivity of the autoencoder was\u00a0[Formula: see text] and the sensitivity of FindFPOF was\u00a0[Formula: see text]. Both anomaly detection methods had a specificity of\u00a0[Formula: see text]. Third, FindFPOF and the autoencoder suggested samples with a different distribution of values than the overall dataset. For example, both anomaly detection methods suggested a higher proportion of colorectal records, the tumor localization with the highest percentage of implausible records in a randomly selected sample.\nUnsupervised anomaly detection can significantly reduce the manual effort of domain experts to find implausible electronic health records in cancer registries. In our experiments, the manual effort was reduced by a factor of approximately\u00a03.5 compared to evaluating a random sample.", 
    "abstract": "Diabetes mellitus (DM) affects the quality of life and leads to disability, high morbidity, and premature mortality. DM is a risk factor for cardiovascular, neurological, and renal diseases, and places a major burden on healthcare systems globally. Predicting the one-year mortality of patients with DM can considerably help clinicians tailor treatments to patients at risk. In this study, we aimed to show the feasibility of predicting the one-year mortality of DM patients based on administrative health data. We use clinical data for 472,950 patients that were admitted to hospitals across Kazakhstan between mid-2014 to December 2019 and were diagnosed with DM. The data was divided into four yearly-specific cohorts (2016-, 2017-, 2018-, and 2019-cohorts) to predict mortality within a specific year based on clinical and demographic information collected up to the end of the preceding year. We then develop a comprehensive machine learning platform to construct a predictive model of one-year mortality for each year-specific cohort. In particular, the study implements and compares the performance of nine classification rules for predicting the one-year mortality of DM patients. The results show that gradient-boosting ensemble learning methods perform better than other algorithms across all year-specific cohorts while achieving an area under the curve (AUC) between 0.78 and 0.80 on independent test sets. The feature importance analysis conducted by calculating SHAP (SHapley Additive exPlanations) values shows that age, duration of diabetes, hypertension, and sex are the top four most important features for predicting one-year mortality. In conclusion, the results show that it\u00a0is possible to use machine learning to build accurate predictive models of one-year mortality for DM patients based on administrative health data. In the future, integrating this information with laboratory data or patients' medical history could potentially boost the performance of the predictive models.", 
    "abstract": "Active machine learning is widely used in computational studies where repeated numerical simulations can be conducted on high performance computers without human intervention. But translation of these active learning methods to physical systems has proven more difficult and the accelerated pace of discoveries aided by these methods remains as yet unrealized. Through the presentation of a general active learning framework and its application to large-scale boundary layer wind tunnel experiments, we demonstrate that the active learning framework used so successfully in computational studies is directly applicable to the investigation of physical experimental systems and the corresponding improvements in the rate of discovery can be transformative. We specifically show that, for our wind tunnel experiments, we are able to achieve in approximately 300 experiments a learning objective that would be impossible using traditional methods.", 
    "abstract": "This study is a simple illustration of the benefit of averaging over cohorts, rather than developing a prediction model from a single cohort. We show that models trained on data from multiple cohorts can perform significantly better in new settings than models based on the same amount of training data but from just a single cohort. Although this concept seems simple and obvious, no current prediction model development guidelines recommend such an approach.", 
    "abstract": "The actual 5-year survival rates for Gynecological Endometrioid Adenocarcinoma with Squamous Differentiation (GE-ASqD) are rarely reported. The purpose of this study was to evaluate how histological subtypes affected long-term survivors of GE-ASqD (>\u20095\u00a0years). We conducted a retrospective analysis of patients diagnosed GE-ASqD from the Surveillance, Epidemiology, and End Results database (2004-2015). In order to conduct the studies, we employed the chi-square test, univariate cox regression, and multivariate cox proportional hazards model. A total of 1131 patients with GE-ASqD were included in the survival study from 2004 to 2015 after applying the inclusion and exclusion criteria and the sample randomly split into a training set and a test set at a ratio of 7:3. Five machine learning algorithms were trained based on nine clinical variables to predict the 5-year overall survival. The AUC of the training group for the LR, Decision Tree, forest, Gbdt, and gbm algorithms were 0.809, 0.336, 0.841, 0.823, and 0.856 respectively. The AUC of the testing group was 0.779, 0.738, 0.753, 0.767 and 0.734, respectively. The calibration curves confirmed good performance of the five machine learning algorithms. Finally, five algorithms were combined to create a machine learning model that forecasts the 5-year overall survival rate of patients with GE-ASqD.", 
    "abstract": null, 
    "abstract": "Dexterous tongue deformation underlies eating, drinking, and speaking. The orofacial sensorimotor cortex has been implicated in the control of coordinated tongue kinematics, but little is known about how the brain encodes-and ultimately drives-the tongue's 3D, soft-body deformation. Here we combine a biplanar x-ray video technology, multi-electrode cortical recordings, and machine-learning-based decoding to explore the cortical representation of lingual deformation. We trained long short-term memory (LSTM) neural networks to decode various aspects of intraoral tongue deformation from cortical activity during feeding in male Rhesus monkeys. We show that both lingual movements and complex lingual shapes across a range of feeding behaviors could be decoded with high accuracy, and that the distribution of deformation-related information across cortical regions was consistent with previous studies of the arm and hand.", 
    "abstract": "The aim of this study was to measure the ability of radiomics analysis to diagnose different stages of sialadenitis, compare the diagnostic accuracy of computed tomography (CT) and ultrasonography (US), and suggest radiomics features selected through 3 machine learning algorithms that would be helpful in discriminating between stages of sialadenitis with both imaging systems.\nWistar rats were treated to induce acute and chronic sialadenitis in the left and right submandibular glands, respectively. Contrast-enhanced CT and US of the glands were performed, followed by extirpation and histopathologic confirmation. Radiomics feature values of the glands were obtained from all images. Based on 3 feature selection methods, an optimal feature set was defined after a comparison of the receiver operating characteristic area under the curve (AUC) of each combination of 3 deep learning algorithms and 3 classification models.\nThe attribute features for the CT model were 2 gray-level run length matrices and 2 gray-level zone length matrices. In the US model, there were 2 gray-level co-occurrence matrices and 2 gray-level zone length matrices. The most accurate diagnostic models of CT and US yielded outstanding (AUC\u00a0=\u00a01.000) and excellent (AUC\u00a0=\u00a00.879) discrimination, respectively.\nThe radiomics diagnostic model using gray-level zone length matrices-based features conferred clinically outstanding discriminating ability among stages of sialadenitis using CT and excellent discrimination with US in almost all combinations of machine learning feature selections and classification models.", 
    "abstract": "Bronchial arterial chemoembolization (BACE) was deemed as an effective and safe approach for advanced standard treatment-ineligible/rejected lung cancer patients. However, the therapeutic outcome of BACE varies greatly and there is no reliable prognostic tool in clinical practice. This study aimed to investigate the effectiveness of radiomics features in predicting tumor recurrence after BACE treatment in lung cancer patients.\nA total of 116 patients with pathologically confirmed lung cancer who received BACE treatment were retrospectively recruited. All patients underwent contrast-enhanced CT within 2 weeks before BACE treatment and were followed up for more than 6\u00a0months. We conducted a machine learning-based characterization of each lesion on the preoperative contrast-enhanced CT images. In the training cohort, recurrence-related radiomics features were screened by least absolute shrinkage and selection operator (LASSO) regression. Three predictive radiomics signatures were built with linear discriminant analysis (LDA), support vector machine (SVM) and logistic regression (LR) algorithms, respectively. Univariate and multivariate LR analyses were performed to select the independent clinical predictors for recurrence. The radiomics signature with best predictive performance was integrated with the clinical predictors to form a combined model, which was visualized as a nomogram. The performance of the combined model was assessed by receiver operating characteristic curve (ROC), calibration curve, and decision curve analysis (DCA).\nNine recurrence-related radiomics features were screened out, and three radiomics signatures (Radscore\nThe radiomics and clinical predictors-based nomogram can predict tumor recurrence after BACE treatment effectively, which allowing oncologists to identify potential recurrence and enable better patient management and clinical decision-making.", 
    "abstract": "Enzymatic reactions are crucial to explore the mechanistic function of metabolites and proteins in cellular processes and to understand the etiology of diseases. The increasing number of interconnected metabolic reactions allows the development of in silico deep learning-based methods to discover new enzymatic reaction links between metabolites and proteins to further expand the landscape of existing metabolite-protein interactome. Computational approaches to predict the enzymatic reaction link by metabolite-protein interaction (MPI) prediction are still very limited. In this study, we developed a Variational Graph Autoencoders (VGAE)-based framework to predict MPI in genome-scale heterogeneous enzymatic reaction networks across ten organisms. By incorporating molecular features of metabolites and proteins as well as neighboring information in the MPI networks, our MPI-VGAE predictor achieved the best predictive performance compared to other machine learning methods. Moreover, when applying the MPI-VGAE framework to reconstruct hundreds of metabolic pathways, functional enzymatic reaction networks and a metabolite-metabolite interaction network, our method showed the most robust performance among all scenarios. To the best of our knowledge, this is the first MPI predictor by VGAE for enzymatic reaction link prediction. Furthermore, we implemented the MPI-VGAE framework to reconstruct the disease-specific MPI network based on the disrupted metabolites and proteins in Alzheimer's disease and colorectal cancer, respectively. A substantial number of novel enzymatic reaction links were identified. We further validated and explored the interactions of these enzymatic reactions using molecular docking. These results highlight the potential of the MPI-VGAE framework for the discovery of novel disease-related enzymatic reactions and facilitate the study of the disrupted metabolisms in diseases.", 
    "abstract": "Effective cellular signaling relies on precise spatial localization and dynamic interactions among proteins in specific subcellular compartments or niches, such as cell-to-cell contact sites and junctions. In plants, endogenous and pathogenic proteins gained the ability to target plasmodesmata, membrane-lined cytoplasmic connections, through evolution to regulate or exploit cellular signaling across cell wall boundaries. For example, the receptor-like membrane protein PLASMODESMATA-LOCATED PROTEIN 5 (PDLP5), a potent regulator of plasmodesmal permeability, generates feed-forward or feed-back signals important for plant immunity and root development. However, the molecular features that determine the plasmodesmal association of PDLP5 or other proteins remain largely unknown, and no protein motifs have been identified as plasmodesmal targeting signals. Here, we developed an approach combining custom-built machine-learning algorithms and targeted mutagenesis to examine PDLP5 in Arabidopsis thaliana and Nicotiana benthamiana. We report that PDLP5 and its closely related proteins carry unconventional targeting signals consisting of short stretches of amino acids. PDLP5 contains two divergent, tandemly arranged signals, either of which is sufficient for localization and biological function in regulating viral movement through plasmodesmata. Notably, plasmodesmal targeting signals exhibit little sequence conservation but are located similarly proximal to the membrane. These features appear to be a common theme in plasmodesmal targeting.", 
    "abstract": "Deep neural network (DNN) techniques, as an advanced machine learning framework, have allowed various image diagnoses in plants, which often achieve better prediction performance than human experts in each specific field. Notwithstanding, in plant biology, the application of deep neural networks is still mostly limited to rapid and effective phenotyping. Recent development of explainable CNN frameworks has allowed visualization of the features in the prediction by convolutional neural network (CNN), which potentially contributes to the understanding of physiological mechanisms in objective phenotypes. In this study, we propose an integration of explainable CNN and transcriptomic approach to make a physiological interpretation of a fruit internal disorder in persimmon, rapid over-softening. We constructed CNN models to accurately predict the fate to be rapid softening in persimmon cv. Soshu, only with photo images. The explainable CNNs, such as Grad-CAM and Guided Grad-CAM, visualized specific featured regions relevant to the prediction of rapid-softening, which would correspond to the premonitory symptoms in a fruit. Transcriptomic analyses to compare the featured regions of predicted rapid-softening and control fruits suggested that rapid softening is triggered by precocious ethylene signal-dependent cell wall modification, despite exhibiting no direct phenotypic changes. Further transcriptomic comparison between the featured and non-featured regions in predicted rapid-softening fruit suggested that premonitory symptoms reflected hypoxia and the related stress signals finally to induce ethylene signals. These results would provide a good example for the collaboration of image analysis and omics approaches in plant physiology, which uncovered a novel aspect of fruit premonitory reactions in the rapid softening fate.", 
    "abstract": "In this paper, the analysis of exposure reference levels is performed for the case of a half-wavelength dipole antenna positioned in the immediate vicinity of non-planar body parts. The incident power density (IPD) spatially averaged over the spherical and cylindrical surface is computed at the 6-90 GHz range, and subsequently placed in the context of the current international guidelines and standards for limiting exposure to electromagnetic (EM) fields which are defined considering planar computational tissue models. As numerical errors are ubiquitous at such high frequencies, the spatial resolution of EM models needs to be increased which in turn results in increased computational complexity and memory requirements. To alleviate this issue, we hybridise machine learning and traditional scientific computing approaches through differentiable programming paradigm. Findings demonstrate a strong positive effect the curvature of non-planar models has on the spatially averaged IPD with up to 15% larger values compared to the corresponding planar model in considered exposure scenarios.", 
    "abstract": "Personal dosemeters using thermoluminescence detectors can provide information about the irradiation event beyond the pure dose estimation, which is valuable for improving radiation protection measures. In the presented study, the glow curves of the novel TL-DOS dosemeters developed by the Materialpr\u00fcfungsamt NRW in cooperation with the TU Dortmund University are analysed using deep learning approaches to predict the irradiation date of a single-dose irradiation of 10\u00a0mGy within a monitoring interval of 41\u00a0d. In contrast of previous work, the glow curves are measured using the current routine read-out process by pre-heating the detectors before the read-out. The irradiation dates are predicted with an accuracy of 2-5\u00a0d by the deep learning algorithm. Furthermore, the importance of the input features is evaluated using Shapley values to increase the interpretability of the neural network.", 
    "abstract": "Social behavior is naturally occurring in vertebrate species, which holds a strong evolutionary component and is crucial for the normal development and survival of individuals throughout life. Behavioral neuroscience has seen different influential methods for social behavioral phenotyping. The ethological research approach has extensively investigated social behavior in natural habitats, while the comparative psychology approach was developed utilizing standardized and univariate social behavioral tests. The development of advanced and precise tracking tools, together with post-tracking analysis packages, has recently enabled a novel behavioral phenotyping method, that includes the strengths of both approaches. The implementation of such methods will be beneficial for fundamental social behavioral research but will also enable an increased understanding of the influences of many different factors that can influence social behavior, such as stress exposure. Furthermore, future research will increase the number of data modalities, such as sensory, physiological, and neuronal activity data, and will thereby significantly enhance our understanding of the biological basis of social behavior and guide intervention strategies for behavioral abnormalities in psychiatric disorders.", 
    "abstract": "Polycyclic aromatic hydrocarbons (PAHs) and toxic metals are widely spread pollutants of public health concern. The co-contamination of these chemicals in the environment is frequent, but relatively little is known about their combined toxicities. In this context, this study aimed to evaluate the influence of the co-exposure to PAHs and toxic metals on DNA damage in Brazilian lactating women and their infants using machine learning approaches. Data were collected from an observational, cross-sectional study with 96 lactating women and 96 infants living in two cities. The exposure to these pollutants was estimated by determining urinary levels of seven mono-hydroxylated PAH metabolites and the free form of three toxic metals. 8-Hydroxydeoxyguanosine (8-OHdG) levels in the urine were used as the oxidative stress biomarker and set as the outcome. Individual sociodemographic factors were also collected using questionnaires. Sixteen machine learning algorithms were trained using 10-fold cross-validation to investigate the associations of urinary OH-PAHs and metals with 8-OHdG levels. This approach was also compared with models attained by multiple linear regression. The results showed that the urinary concentration of OH-PAHs was highly correlated between the mothers and their infants. Multiple linear regression did not show a statistically significant association between the contaminants and urinary 8OHdG levels. Machine learning models indicated that all investigated variables did not present predictive performance on 8-OHdG concentrations. In conclusion, PAHs and toxic metals were not associated with 8-OHdG levels in Brazilian lactating women and their infants. These novelty and originality results were achieved even after applying sophisticated statistical models to capture non-linear relationships. However, these findings should be interpreted cautiously because the exposure to the studied contaminants was considerably low, which may not reflect other populations at risk.", 
    "abstract": "Directed evolution has become one of the most successful and powerful tools for protein engineering. However, the efforts required for designing, constructing, and screening a large library of variants can be laborious, time-consuming, and costly. With the recent advent of machine learning (ML) in the directed evolution of proteins, researchers can now evaluate variants in silico and guide a more efficient directed evolution campaign. Furthermore, recent advancements in laboratory automation have enabled the rapid execution of long, complex experiments for high-throughput data acquisition in both industrial and academic settings, thus providing the means to collect a large quantity of data required to develop ML models for protein engineering. In this perspective, we propose a closed-loop in\u00a0vitro continuous protein evolution framework that levarages the best of both worlds, ML and automation, and provide a brief overview of the recent developments in the field.", 
    "abstract": "Owing to the rapid development of big data technology, use of machine learning methods to identify soil pollution of potentially contaminated sites (PCS) at regional scales and in different industries has become a research hot spot. However, due to the difficulty in obtaining key indexes of site pollution sources and pathways, current methods have problems such as low accuracy of model predictions and insufficient scientific basis. In this study, we collected the environmental data of 199 PCS in 6 typical industries involving heavy metal and organic pollution. Then, 21 indexes based on basic information, potential for pollution from product and raw material, pollution control level, and migration capacity of soil pollutants were used to established the soil pollution identification index system. We fused the original indexes into the new feature subset with 11 indexes through the method of consolidation calculation. The new feature subset was then used to train machine learning models of random forest (RF), support vector machine (SVM), and multilayer perceptron (MLP), and tested to determine whether it improved the accuracy and precision of soil pollination identification models. The results of correlation analysis showed that the four new indexes created by feature fusion have the correlation with soil pollution is similar to the original indexes. The accuracies and precisions of three machine learning models trained on the new feature subset were 67.4%-\u00a072.9% and 72.0%-\u00a074.7%, which were 2.1%-\u00a02.5% and 0.3%-\u00a05.7% higher than these of the models trained on original indexes, respectively. When the PCS were divided into typical heavy metal and organic pollution sites according to the enterprise industries, the accuracy of the model trained on the two datasets for identifying soil heavy metal and organic pollution were significantly improve to approximately 80%. Owing to the imbalance in positive and negative samples in the prediction of soil organic pollution, the precisions of soil organic pollution identification models were 58%-\u00a072.5%, which were significantly lower than their accuracies. According to the factors analysis based on the model interpretability of SHAP, most of the indexes of basic information, potential for pollution from product and raw material, and pollution control level had different degrees of impact on soil pollution. However, the indexes of migration capacity of soil pollutants had the least effect in the classification task of soil pollution identification of PCS. Among the indexes, traces of soil pollution, industrial utilization years/start-up time, pollution control risk scores and enterprise scale having the greatest effects on soil pollution with the mean SHAP values of 0.17-0.36, which reflected their contribution rate on soil pollution and could help to optimize the current index scoring of the technical regulation for identifying site soil pollution. This study provides a new technical method to identify soil pollution based on big data and machine learning methods, in addition to providing a reference and scientific basis for environmental management and soil pollution control of PCS.", 
    "abstract": "Biologic pathways underlying the association between outdoor air pollution and breast cancer risk are poorly understood. Breast tissue composition may reflect cumulative exposure to breast cancer risk factors and has been associated with breast cancer risk among patients with benign breast disease. Herein, we evaluated whether fine particulate matter (PM\nMachine-learning algorithms were applied to digitized hematoxylin and eosin-stained biopsies of normal breast tissue to quantify the epithelium, stroma, adipose and total tissue area from 3,977 individuals aged 18-75\u00a0years from a primarily Midwestern United States population who donated breast tissue samples to the Susan G. Komen Tissue Bank (2009-2019). Annual levels of PM\nHigher residential PM\nOur findings are consistent with a possible role of PM", 
    "abstract": "Azo dyes are used in textiles and leather clothing. Human exposure can occur from wearing textiles containing azo dyes. Since the body's enzymes and microbiome can cleave azo dyes, potentially resulting in mutagenic or carcinogenic metabolites, there is also an indirect health concern on the parent compounds. While several hazardous azo dyes are banned, many more are still in use that have not been evaluated systematically for potential health concerns. This systematic evidence map (SEM) aims to compile and categorize the available toxicological evidence on the potential human health risks of a set of 30 market-relevant azo dyes.\nPeer-reviewed and gray literature was searched and over 20,000 studies were identified. These were filtered using Sciome Workbench for Interactive computer-Facilitated Text-mining (SWIFT) Review software with evidence stream tags (human, animal, in vitro) yielding 12,800 unique records. SWIFT Active (a machine-learning software) further facilitated title/abstract screening. DistillerSR software was used for additional title/abstract, full-text screening, and data extraction.\n187 studies were identified that met populations, exposures, comparators, and outcomes (PECO) criteria. From this pool, 54 human, 78 animal, and 61 genotoxicity studies were extracted into a literature inventory. Toxicological evidence was abundant for three azo dyes (also used as food additives) and sparse for five of the remaining 27 compounds. Complementary search in ECHA's REACH database for summaries of unpublished study reports revealed evidence for all 30 dyes. The question arose of how this information can be fed into an SEM process. Proper identification of prioritized dyes from various databases (including U.S. EPA's CompTox Chemicals Dashboard) turned out to be a challenge. Evidence compiled by this SEM project can be evaluated for subsequent use in problem formulation efforts to inform potential regulatory needs and prepare for a more efficient and targeted evaluation in the future for human health assessments.", 
    "abstract": "Over the past decades, avian influenza (AI) outbreaks have been reported across different parts of the globe, resulting in large-scale economic and livestock loss and, in some cases raising concerns about their zoonotic potential. The virulence and pathogenicity of H5Nx (e.g., H5N1, H5N2) AI strains for poultry could be inferred through various approaches, and it has been frequently performed by detecting certain pathogenicity markers in their haemagglutinin (HA) gene. The utilization of predictive modeling methods represents a possible approach to exploring this genotypic-phenotypic relationship for assisting experts in determining the pathogenicity of circulating AI viruses. Therefore, the main objective of this study was to evaluate the predictive performance of different machine learning (ML) techniques for in-silico prediction of pathogenicity of H5Nx viruses in poultry, using complete genetic sequences of the HA gene. We annotated 2137 H5Nx HA gene sequences based on the presence of the polybasic HA cleavage site (HACS) with 46.33% and 53.67% of sequences previously identified as highly pathogenic (HP) and low pathogenic (LP), respectively. We compared the performance of different ML classifiers (e.g., logistic regression (LR) with the lasso and ridge regularization, random forest (RF), K-nearest neighbor (KNN), Na\u00efve Bayes (NB), support vector machine (SVM), and convolutional neural network (CNN)) for pathogenicity classification of raw H5Nx nucleotide and protein sequences using a 10-fold cross-validation technique. We found that different ML techniques can be successfully used for the pathogenicity classification of H5 sequences with \u223c99% classification accuracy. Our results indicate that for pathogenicity classification of (1) aligned deoxyribonucleic acid (DNA) and protein sequences, with NB classifier had the lowest accuracies of 98.41% (+/-0.89) and 98.31% (+/-1.06), respectively; (2) aligned DNA and protein sequences, with LR (L1/L2), KNN, SVM (radial basis function (RBF)) and CNN classifiers had the highest accuracies of 99.20% (+/-0.54) and 99.20% (+/-0.38), respectively; (3) unaligned DNA and protein sequences, with CNN's achieved accuracies of 98.54% (+/-0.68) and 99.20% (+/-0.50), respectively. ML methods show potential for regular classification of H5Nx virus pathogenicity for poultry species, particularly when sequences containing regular markers were frequently present in the training dataset.", 
    "abstract": "The aim of this study was to investigate whether the alterations of topological properties can facilitate the diagnosis of generalized anxiety disorder (GAD). Twenty first-episode drug-naive Chinese individuals with GAD and twenty age-sex-education-matched healthy controls (HCs) were included in the primary training set, and the results of which were validated using nineteen drug-free patients with GAD and nineteen unmatched HCs. Two 3 T scanners were used to acquire T1, diffusion tensor, and resting-state functional images. Topological properties were altered in the functional cerebral networks among patients with GAD, but not in the structural networks. Using the nodal topological properties in the anti-correlated functional networks, machine learning models distinguished drug-naive GADs from their matched HCs independent of the type of kernels and the amount of features. Although the models built with drug-naive GADs failed to distinguish drug-free GADs from HCs, the features selected for those models could be used to build new models for distinguishing drug-free GADs from HCs. Our findings suggested that it is feasible to utilize the topological characteristics of brain network to facilitate the diagnosis of GAD. However, further research with decent sample sizes, multimodal features, and improved modeling methods are needed to build more robust models.", 
    "abstract": "Acute respiratory diseases are a leading cause of morbidity and mortality in children. Cough is a common symptom of acute respiratory diseases and the sound of cough can be indicative of the respiratory disease. However, cough sound assessment in routine clinical practice is limited to human perception and the skills of the clinician. Objective cough sound evaluation has the potential to aid clinicians in acute respiratory disease diagnosis. In this systematic review, we assess and summarize the predictive ability of machine learning algorithms in analyzing cough sounds of acute respiratory diseases in the pediatric population.\nOur systematic search of the Scopus, Medline, and Embase databases on 25 January 2023 identified six articles meeting the inclusion criteria. Quality assessment of the included studies was performed using the checklist for the assessment of medical artificial intelligence.\nOur analysis shows variability in the input to the machine learning algorithms, such as the use of various cough sound features and combining cough sound features with clinical features. The use of the machine learning algorithms also varies from conventional algorithms, such as logistic regression and support vector machine, to deep learning techniques, such as convolutional neural networks. The classification accuracy for the detection of bronchiolitis, croup, pertussis, and pneumonia across five articles is in the range of 82-96%. However, a significant drop is observed in the detection accuracy for bronchiolitis and pneumonia in the remaining article.\nThe number of articles is limited but, in general, the predictive ability of cough sound classification algorithms in childhood acute respiratory diseases shows promise.", 
    "abstract": "We adapted an existing, spaceflight-proven, robust \"electronic nose\" (E-Nose) that uses an array of electrical resistivity-based nanosensors mimicking aspects of mammalian olfaction to conduct on-site, rapid screening for COVID-19 infection by measuring the pattern of sensor responses to volatile organic compounds (VOCs) in exhaled human breath. We built and tested multiple copies of a hand-held prototype E-Nose sensor system, composed of 64 chemically sensitive nanomaterial sensing elements tailored to COVID-19 VOC detection; data acquisition electronics; a smart tablet with software (App) for sensor control, data acquisition and display; and a sampling fixture to capture exhaled breath samples and deliver them to the sensor array inside the E-Nose. The sensing elements detect the combination of VOCs typical in breath at parts-per-billion (ppb) levels, with repeatability of 0.02% and reproducibility of 1.2%; the measurement electronics in the E-Nose provide measurement accuracy and signal-to-noise ratios comparable to benchtop instrumentation. Preliminary clinical testing at Stanford Medicine with 63 participants, their COVID-19-positive or COVID-19-negative status determined by concomitant RT-PCR, discriminated between these two categories of human breath with a 79% correct identification rate using \"leave-one-out\" training-and-analysis methods. Analyzing the E-Nose response in conjunction with body temperature and other non-invasive symptom screening using advanced machine learning methods, with a much larger database of responses from a wider swath of the population, is expected to provide more accurate on-the-spot answers. Additional clinical testing, design refinement, and a mass manufacturing approach are the main steps toward deploying this technology to rapidly screen for active infection in clinics and hospitals, public and commercial venues, or at home.", 
    "abstract": "A significant challenge faced by atomistic simulations is the difficulty, and often impossibility, to sample the transitions between metastable states of the free-energy landscape associated with slow molecular processes. Importance-sampling schemes represent an appealing option to accelerate the underlying dynamics by smoothing out the relevant free-energy barriers, but require the definition of suitable reaction-coordinate (RC) models expressed in terms of compact low-dimensional sets of collective variables (CVs). While most computational studies of slow molecular processes have traditionally relied on educated guesses based on human intuition to reduce the dimensionality of the problem at hand, a variety of machine-learning (ML) algorithms have recently emerged as powerful alternatives to discover meaningful CVs capable of capturing the dynamics of the slowest degrees of freedom. Considering a simple paradigmatic situation in which the long-time dynamics is dominated by the transition between two known metastable states, we compare two variational data-driven ML methods based on Siamese neural networks aimed at discovering a meaningful RC model\u2500the slowest decorrelating CV of the molecular process, and the committor probability to first reach one of the two metastable states. One method is the state-free reversible variational approach for Markov processes networks (VAMPnets), or SRVs\u2500the other, inspired by the transition path theory framework, is the variational committor-based neural networks, or VCNs. The relationship and the ability of these methodologies to discover the relevant descriptors of the slow molecular process of interest are illustrated with a series of simple model systems. We also show that both strategies are amenable to importance-sampling schemes through an appropriate reweighting algorithm that approximates the kinetic properties of the transition.", 
    "abstract": "The advent of high-throughput technologies has produced an increase in the dimensionality of omics datasets, which limits the application of machine learning methods due to the great unbalance between the number of observations and features. In this scenario, dimensionality reduction is essential to extract the relevant information within these datasets and project it in a low-dimensional space, and probabilistic latent space models are becoming popular given their capability to capture the underlying structure of the data as well as the uncertainty in the information. This article aims to provide a general classification and dimensionality reduction method based on deep latent space models that tackles two of the main problems that arise in omics datasets: the presence of missing data and the limited number of observations against the number of features. We propose a semi-supervised Bayesian latent space model that infers a low-dimensional embedding driven by the target label: the Deep Bayesian Logistic Regression (DBLR) model. During inference, the model also learns a global vector of weights that allows it to make predictions given the low-dimensional embedding of the observations. Since this kind of dataset is prone to overfitting, we introduce an additional probabilistic regularization method based on the semi-supervised nature of the model. We compared the performance of the DBLR against several state-of-the-art methods for dimensionality reduction, both in synthetic and real datasets with different data types. The proposed model provides more informative low-dimensional representations, outperforms the baseline methods in classification, and can naturally handle missing entries.", 
    "abstract": "The morphology and controller design of robots is often a labor-intensive task performed by experienced and intuitive engineers. Automatic robot design using machine learning is attracting increasing attention in the hope that it will reduce the design workload and result in better-performing robots. Most robots are created by joining several rigid parts and then mounting actuators and their controllers. Many studies limit the possible types of rigid parts to a finite set to reduce the computational burden. However, this not only limits the search space, but also prohibits the use of powerful optimization techniques. To find a robot closer to the global optimal design, a method that explores a richer set of robots is desirable. In this article, we propose a novel method to efficiently search for various robot designs. The method combines three different optimization methods with different characteristics. We apply proximal policy optimization (PPO) or soft actor-critic (SAC) as the controller, the REINFORCE algorithm to determine the lengths and other numerical parameters of the rigid parts, and a newly proposed method to determine the number and layout of the rigid parts and joints. Experiments with physical simulations confirm that when this method is used to handle two types of tasks-walking and manipulation-it performs better than simple combinations of existing methods. The source code and videos of our experiments are available online (https://github.com/r-koike/eagent).", 
    "abstract": "Urban areas are associated with higher depression risks than rural areas. However, less is known about how different types of urban environments relate to depression risk. Here, we use satellite imagery and machine learning to quantify three-dimensional (3D) urban form (i.e., building density and height) over time. Combining satellite-derived urban form data and individual-level residential addresses, health, and socioeconomic registers, we conduct a case-control study (", 
    "abstract": "Preterm birth (PTB) is the leading cause of death in children under five, yet comprehensive studies are hindered by its multiple complex etiologies. Epidemiological associations between PTB and maternal characteristics have been previously described. This work used multiomic profiling and multivariate modeling to investigate the biological signatures of these characteristics. Maternal covariates were collected during pregnancy from 13,841 pregnant women across five sites. Plasma samples from 231 participants were analyzed to generate proteomic, metabolomic, and lipidomic datasets. Machine learning models showed robust performance for the prediction of PTB (AUROC = 0.70), time-to-delivery (", 
    "abstract": "Although many people suffer from sleep disorders, most are undiagnosed, leading to impairments in health. The existing polysomnography method is not easily accessible; it's costly, burdensome to patients, and requires specialized facilities and personnel. Here, we report an at-home portable system that includes wireless sleep sensors and wearable electronics with embedded machine learning. We also show its application for assessing sleep quality and detecting sleep apnea with multiple patients. Unlike the conventional system using numerous bulky sensors, the soft, all-integrated wearable platform offers natural sleep wherever the user prefers. In a clinical study, the face-mounted patches that detect brain, eye, and muscle signals show comparable performance with polysomnography. When comparing healthy controls to sleep apnea patients, the wearable system can detect obstructive sleep apnea with an accuracy of 88.5%. Furthermore, deep learning offers automated sleep scoring, demonstrating portability, and point-of-care usability. At-home wearable electronics could ensure a promising future supporting portable sleep monitoring and home healthcare.", 
    "abstract": "Heterogeneities in structure and polarization have been employed to enhance the energy storage properties of ferroelectric films. The presence of nonpolar phases, however, weakens the net polarization. Here, we achieve a slush-like polar state with fine domains of different ferroelectric polar phases by narrowing the large combinatorial space of likely candidates using machine learning methods. The formation of the slush-like polar state at the nanoscale in cation-doped BaTiO", 
    "abstract": "The problem of antibiotic resistance among pathogenic bacteria has reached a crisis level. The treatment options against infections caused by multiple drug-resistant bacteria are shrinking gradually. The current pace of the discovery of new antibacterial entities is lagging behind the rate of development of new resistance. Efflux pumps play a central role in making a bacterium resistant to multiple antibiotics due to their ability to expel a wide range of structurally diverse compounds. Besides providing an escape from antibacterial compounds, efflux pumps are also involved in bacterial stress response, virulence, biofilm formation, and altering host physiology. Efflux pumps are unique yet challenging targets for the discovery of novel efflux pump inhibitors (EPIs). EPIs could help rejuvenate our currently dried pipeline of antibacterial drug discovery. The current article highlights the recent developments in the field of efflux pumps, challenges faced during the development of EPIs and potential approaches for their development. Additionally, this review highlights the utility of resources such as natural products and machine learning to expand our EPIs arsenal using these latest technologies.", 
    "abstract": "Chemical toxicity evaluations for drugs, consumer products, and environmental chemicals have a critical impact on human health. Traditional animal models to evaluate chemical toxicity are expensive, time-consuming, and often fail to detect toxicants in humans. Computational toxicology is a promising alternative approach that utilizes machine learning (ML) and deep learning (DL) techniques to predict the toxicity potentials of chemicals. Although the applications of ML- and DL-based computational models in chemical toxicity predictions are attractive, many toxicity models are \"black boxes\" in nature and difficult to interpret by toxicologists, which hampers the chemical risk assessments using these models. The recent progress of interpretable ML (IML) in the computer science field meets this urgent need to unveil the underlying toxicity mechanisms and elucidate the domain knowledge of toxicity models. In this review, we focused on the applications of IML in computational toxicology, including toxicity feature data, model interpretation methods, use of knowledge base frameworks in IML development, and recent applications. The challenges and future directions of IML modeling in toxicology are also discussed. We hope this review can encourage efforts in developing interpretable models with new IML algorithms that can assist new chemical assessments by illustrating toxicity mechanisms in humans.", 
    "abstract": "Understanding and optimizing adolescent-specific engagement with behavior change interventions will open doors for providers to promote healthy changes in an age group that is simultaneously difficult to engage and especially important to affect. For digital interventions, there is untapped potential in combining the vastness of process-level data with the analytical power of artificial intelligence (AI) to understand not only how adolescents engage but also how to improve upon interventions with the goal of increasing engagement and, ultimately, efficacy. Rooted in the example of the INSPIRE narrative-centered digital health behavior change intervention (DHBCI) for adolescent risky behaviors around alcohol use, we propose a framework for harnessing AI to accomplish 4 goals that are pertinent to health care providers and software developers alike: measurement of adolescent engagement, modeling of adolescent engagement, optimization of current interventions, and generation of novel interventions. Operationalization of this framework with youths must be situated in the ethical use of this technology, and we have outlined the potential pitfalls of AI with particular attention to privacy concerns for adolescents. Given how recently AI advances have opened up these possibilities in this field, the opportunities for further investigation are plenty.", 
    "abstract": "Lack of quantifiable biomarkers is a major obstacle in diagnosing and treating depression. In adolescents, increasing suicidality during antidepressant treatment further complicates the problem.\nWe sought to evaluate digital biomarkers for the diagnosis and treatment response of depression in adolescents through a newly developed smartphone app.\nWe developed the Smart Healthcare System for Teens At Risk for Depression and Suicide app for Android-based smartphones. This app passively collected data reflecting the social and behavioral activities of adolescents, such as their smartphone usage time, physical movement distance, and the number of phone calls and text messages during the study period. Our study consisted of 24 adolescents (mean age 15.4 [SD 1.4] years, 17 girls) with major depressive disorder (MDD) diagnosed with Kiddie Schedule for Affective Disorders and Schizophrenia for School-Age Children-Present and Lifetime Version and 10 healthy controls (mean age 13.8 [SD 0.6] years, 5 girls). After 1 week's baseline data collection, adolescents with MDD were treated with escitalopram in an 8-week, open-label trial. Participants were monitored for 5 weeks, including the baseline data collection period. Their psychiatric status was measured every week. Depression severity was measured using the Children's Depression Rating Scale-Revised and Clinical Global Impressions-Severity. The Columbia Suicide Severity Rating Scale was administered in order to assess suicide severity. We applied the deep learning approach for the analysis of the data. Deep neural network was employed for diagnosis classification, and neural network with weighted fuzzy membership functions was used for feature selection.\nWe could predict the diagnosis of depression with training accuracy of 96.3% and 3-fold validation accuracy of 77%. Of the 24 adolescents with MDD, 10 responded to antidepressant treatments. We predicted the treatment response of adolescents with MDD with training accuracy of 94.2% and 3-fold validation accuracy of 76%. Adolescents with MDD tended to move longer distances and use smartphones for longer periods of time compared to controls. The deep learning analysis showed that smartphone usage time was the most important feature in distinguishing adolescents with MDD from controls. Prominent differences were not observed in the pattern of each feature between the treatment responders and nonresponders. The deep learning analysis revealed that the total length of calls received as the most important feature predicting antidepressant response in adolescents with MDD.\nOur smartphone app demonstrated preliminary evidence of predicting diagnosis and treatment response in depressed adolescents. This is the first study to predict the treatment response of adolescents with MDD by examining smartphone-based objective data with deep learning approaches.", 
    "abstract": "The purpose of this study was to create multivariate models predicting early referral-warranted retinopathy of prematurity (ROP) using non-contact handheld spectral-domain optical coherence tomography (OCT) and demographic data.\nBetween July 2015 and February 2018, infants \u22641500 grams birth weight or \u226430 weeks gestational age from 2 academic neonatal intensive care units were eligible for this study. Infants were excluded if they were too unstable to participate in ophthalmologic examination (2), had inadequate image quality (20), or received prior ROP treatment (2). Multivariate models were created using demographic variables and imaging findings to identify early referral-warranted ROP (referral-warranted ROP and/or pre-plus disease) by routine indirect ophthalmoscopy.\nA total of 167 imaging sessions of 71 infants (45% male infants, gestational age 28.2+/-2.8 weeks, and birth weight 995.6+/-292.0 grams) were included. Twelve of 71 infants (17%) developed early referral-warranted ROP. The area under the receiver operating characteristic curve (AUC) was 0.94 for the generalized linear mixed model (sensitivity = 95.5% and specificity = 80.7%) and 0.83 for the machine learning model (sensitivity = 91.7% and specificity = 77.8%). The strongest variables in both models were birth weight, image-based Vitreous Opacity Ratio (an estimate of opacity density), vessel elevation, and hyporeflective vessels. A model using only birth weight and gestational age yielded an AUC of 0.68 (sensitivity = 77.3% and specificity = 63.4%), and a model using only imaging biomarkers yielded 0.88 (sensitivity = 81.8% and specificity = 84.8%).\nA generalized linear mixed model containing handheld OCT biomarkers can identify early referral-warranted ROP. Machine learning produced a less optimal model.\nWith further validation, this work may lead to a better-tolerated ROP screening tool.", 
    "abstract": "Efficient, rapid, and non-destructive detection of pesticide residues in fruits and vegetables is essential for food safety. The visible/near infrared (VNIR) and short-wave infrared (SWIR) hyperspectral imaging (HSI) systems were used to detect different types of pesticide residues on the surface of Hami melon. Taking four pesticides commonly used in Hami melon as the object, the effectiveness of single-band spectral range and information fusion in the classification of different pesticides was compared. The results showed that the classification effect of pesticide residues was better by using the spectral range after information fusion. Then, a custom multi-branch one-dimensional convolutional neural network (1D-CNN) model with the attention mechanism was proposed and compared with the traditional machine learning classification model K-nearest neighbor (KNN) algorithm and random forest (RF). The traditional machine learning classification model accuracy of both models was over 80.00%. However, the classification results using the proposed 1D-CNN were more satisfactory. After the full spectrum data was fused, it was input into the 1D-CNN model, and its accuracy, precision, recall, and F1-score value were 94.00%, 94.06%, 94.00%, and 0.9396, respectively. This study showed that both VNIR and SWIR hyperspectral imaging combined with a classification model could non-destructively detect different pesticide residues on the surface of Hami melon. The classification result using the SWIR spectrum was better than that using the VNIR spectrum, and the classification result using the information fusion spectrum was better than that using SWIR. This study can provide a valuable reference for the non-destructive detection of pesticide residues on the surface of other large, thick-skinned fruits.", 
    "abstract": "Preoperative imaging assessment of surgical risk is very important for the prognosis of these children. To develop and validate a radiomics-based machine learning model based on the analysis of radiomics features to predict surgical risk in children with abdominal neuroblastoma (NB).\nA retrospective study was conducted from April 2019 to March 2021 among 74 children with abdominal NB. A total of 1874 radiomic features in MR images were extracted from each patient. Support vector machines (SVMs) were used to establish the model. Eighty percent of the data were used as the training set to optimize the model, and 20% of the data were used to validate its accuracy, sensitivity, specificity and area under the curve (AUC) to verify its effectiveness.\nAmong the 74 children with abdominal NB, 55 (65%) had surgical risk and 19 (35%) had no surgical risk. A t test and Lasso identified that 28 radiomic features were associated with surgical risk. After developing an SVM-based model using these features, predictions were made about whether children with abdominal NB had surgical risk. The model achieved an AUC of 0.94 (a sensitivity of 0.83 and a specificity of 0.80) with 0.890 accuracy in the training set and an AUC of 0.81 (a sensitivity of 0.73 and a specificity of 0.82) with 0.838 accuracy in the test set.\nRadiomics and machine learning can be used to predict the surgical risk in children with abdominal NB. The model based on 28 radiomic features established by SVM showed good diagnostic efficiency.", 
    "abstract": "Breast cancer causes the most cancer-related death in women and is the costliest cancer in the US regarding medical service and prescription drug expenses. Breast cancer screening is recommended by health authorities in the US, but current screening efforts are often compromised by high false positive rates. Liquid biopsy based on circulating tumor DNA (ctDNA) has emerged as a potential approach to screen for cancer. However, the detection of breast cancer, particularly in early stages, is challenging due to the low amount of ctDNA and heterogeneity of molecular subtypes.\nHere, we employed a multimodal approach, namely Screen for the Presence of Tumor by DNA Methylation and Size (SPOT-MAS), to simultaneously analyze multiple signatures of cell free DNA (cfDNA) in plasma samples of 239 nonmetastatic breast cancer patients and 278 healthy subjects.\nWe identified distinct profiles of genome-wide methylation changes (GWM), copy number alterations (CNA), and 4-nucleotide oligomer (4-mer) end motifs (EM) in cfDNA of breast cancer patients. We further used all three signatures to construct a multi-featured machine learning model and showed that the combination model outperformed base models built from individual features, achieving an AUC of 0.91 (95% CI: 0.87-0.95), a sensitivity of 65% at 96% specificity.\nOur findings showed that a multimodal liquid biopsy assay based on analysis of cfDNA methylation, CNA and EM could enhance the accuracy for the detection of early- stage breast cancer.", 
    "abstract": "To develop and interpret optimal predictive models to identify epidermal growth factor receptor (EGFR) mutation status and subtypes in patients with lung adenocarcinoma based on multicentric \nThe \nAmong the 76 radiomics candidates, light gradient boosting machine classifier (LGBM) combined with recursive feature elimination wrapped LGBM feature selection method achieved best performance in predicting EGFR mutation status (AUC reached 0.80, 0.61, 0.71 in the internal test cohort and two external test cohorts, respectively). And extreme gradient boosting classifier combined with support vector machine feature selection method achieved best performance in predicting EGFR subtypes (AUC reached 0.76, 0.63, 0.61 in the internal test cohort and two external test cohorts, respectively). The C-index of the Cox proportional hazard model achieved 0.863.\nThe integration of cross-combination method and the external validation from multi-center data achieved a good prediction and generalization performance in predicting EGFR mutation status and its subtypes. The combination of handcrafted radiomics features and clinical factors achieved good performance in predicting prognosis. With the urgent needs of multicentric ", 
    "abstract": "The purpose of this research was to develop a radiomics model that combines several clinical features for preoperative prediction of the pathological grade of bladder cancer (BCa) using non-enhanced computed tomography (NE-CT) scanning images.\nThe computed tomography (CT), clinical, and pathological data of 105 BCa patients attending our hospital between January 2017 and August 2022 were retrospectively evaluated. The study cohort comprised 44 low-grade BCa and 61 high-grade BCa patients. The subjects were randomly divided into training (\nThe selected clinical factors for the model included age and tumor size. LASSO regression analysis identified 15 features most linked to BCa grade, which were included in the machine learning model. The SVM analysis revealed that the highest AUC of the model was 0.842.\u00a0A nomogram combining the radiomics signature and selected clinical variables showed accurate prediction of the pathological grade of BCa preoperatively. The AUC of the training cohort was 0.919, whereas that of the validation cohort was 0.854. The clinical value of the combined radiomics nomogram was validated using calibration curve and DCA.\nMachine learning models combining CT semantic features and the selected clinical variables can accurately predict the pathological grade of BCa, offering a non-invasive and accurate approach for predicting the pathological grade of BCa preoperatively.", 
    "abstract": "To explore the use of different machine learning models in prediction of COVID-19 mortality in hospitalized patients.\nA total of 44,112 patients from six academic hospitals who were admitted for COVID-19 between March 2020 and August 2021 were included in this study. Variables were obtained from their electronic medical records. Random forest-recursive feature elimination was used to select key features. Decision tree, random forest, LightGBM, and XGBoost model were developed. Sensitivity, specificity, accuracy, F-1 score, and receiver operating characteristic\u00a0(ROC)-AUC were used to compare the prediction performance of different models.\nRandom forest-recursive feature elimination\u00a0selected following features to include in the prediction model: Age, sex, hypertension, malignancy, pneumonia, cardiac problem, cough, dyspnea, and respiratory system disease. XGBoost and LightGBM showed the best performance with an ROC-AUC of 0.83 [0.822-0.842] and 0.83 [0.816-0.837] and sensitivity of 0.77.\nXGBoost, LightGBM, and random forest have a relatively high predictive performance in prediction of mortality in COVID-19 patients and can be applied in hospital settings, however, future research are needed to externally confirm the validation of these models.", 
    "abstract": "The National Forestry Commission of Mexico continuously monitors forest structure within the country's continental territory by the implementation of the National Forest and Soils Inventory (INFyS). Due to the challenges involved in collecting data exclusively from field surveys, there are spatial information gaps for important forest attributes. This can produce bias or increase uncertainty when generating estimates required to support forest management decisions. Our objective is to predict the spatial distribution of tree height and tree density in all Mexican forests. We performed wall-to-wall spatial predictions of both attributes in 1-km grids, using ensemble machine learning across each forest type in Mexico. Predictor variables include remote sensing imagery and other geospatial data (e.g., mean precipitation, surface temperature, canopy cover). Training data is from the 2009 to 2014 cycle (", 
    "abstract": "The unexpected and rapid spread of the COVID-19 pandemic has amplified the acceptance of remote healthcare systems such as telemedicine. Telemedicine effectively provides remote communication, better treatment recommendation, and personalized treatment on demand. It has emerged as the possible future of medicine. From a privacy perspective, secure storage, preservation, and controlled access to health data with consent are the main challenges to the effective deployment of telemedicine. It is paramount to fully overcome these challenges to integrate the telemedicine system into healthcare. In this regard, emerging technologies such as blockchain and federated learning have enormous potential to strengthen the telemedicine system. These technologies help enhance the overall healthcare standard when applied in an integrated way. The primary aim of this study is to perform a systematic literature review of previous research on privacy-preserving methods deployed with blockchain and federated learning for telemedicine. This study provides an in-depth qualitative analysis of relevant studies based on the architecture, privacy mechanisms, and machine learning methods used for data storage, access, and analytics. The survey allows the integration of blockchain and federated learning technologies with suitable privacy techniques to design a secure, trustworthy, and accurate telemedicine model with a privacy guarantee.", 
    "abstract": "Quantitative descriptions of confidence intervals and uncertainties of the predictions of a model are needed in many applications in vision and machine learning. Mechanisms that enable this for deep neural network (DNN) models are slowly becoming available, and occasionally, being integrated within production systems. But the literature is sparse in terms of how to perform statistical tests with the uncertainties produced by these overparameterized models. For two models with a similar accuracy profile, is the former model's uncertainty behavior better in a statistically significant sense compared to the second model? For high resolution images, performing hypothesis tests to generate meaningful actionable information (say, at a user specified significance level ", 
    "abstract": null, 
    "abstract": "The pathogeneses of psoriasis and metabolic syndrome are closely related; however, the underlying biological mechanisms are yet to be clarified. A psoriasis training set was downloaded from the Gene Expression Omnibus database and analyzed to identify the differentially expressed genes (|logFC|>\u20091 and adjust P\u2009<\u20090.05). Differentially expressed genes for metabolic syndrome were obtained from the GeneCards, Online Mendelian Inheritance in Man, and DisGeNET databases, and crosstalk genes were obtained for multiple enrichment analysis after identifying the disease intersection. Characteristic crosstalk genes were screened using the least absolute shrinkage and selection operator regression model and random forest tree model, and the genes with area under the receiver operating characteristic curve\u2009>\u20090.7 were selected for validation by the two validation sets. Differential analyses of immune cell infiltration were performed on psoriasis lesion and control samples using the CIBERSORT and ImmuCellAI methods, and correlation analyses were performed between the screened signature crosstalk genes and immune cell infiltration. Significant crosstalk genes were analyzed based on the psoriasis area and severity index and on the responses to biological agents. We found five signature genes (NLRX1, KYNU, ABCC1, BTC, and SERPINB4) were screened based on two machine learning algorithms, and NLRX1 was validated. The infiltration of multiple immune cells in psoriatic lesions and non-lesions was associated with NLRX1 expression. NLRX1 was found to be associated with psoriasis severity and response rate after the use of biologics. NLRX1 could be a significant crosstalk gene for psoriasis and metabolic syndrome.", 
    "abstract": "Adolescent suicide continues to rise despite burgeoning research on interpersonal risk for suicide. This may reflect challenges in applying developmental psychopathology research into clinical settings. In response, the present study used a translational analytic plan to examine indices of social well-being most accurate and statistically fair for indexing adolescent suicide. Data from the National Comorbidity Survey Replication Adolescent Supplement were used. Adolescents aged 13-17 (N\u2009=\u20099,900) completed surveys on traumatic events, current relationships, and suicidal thoughts and attempts. Both frequentist (e.g., receiver operating characteristics) and Bayesian (e.g., Diagnostic Likelihood Ratios; DLRs) techniques provided insight into classification, calibration, and statistical fairness. Final algorithms were compared to a machine learning-informed algorithm. Overall, parental care and family cohesion best classified suicidal ideation, while these indices and school engagement best classified attempts. Multi-indicator algorithms suggested adolescents at high risk across these indices were approximately 3-times more likely to engage in ideation (DLR\u2009=\u20093.26) and 5-times more likely to engage in attempts (DLR\u2009=\u20094.53). Although equitable for attempts, models for ideation underperformed in non-White adolescents. Supplemental, machine learning-informed algorithms performed similarly, suggesting non-linear and interactive effects did not improve model performance. Future directions for interpersonal theories for suicide are discussed and clinical implications for suicide screening are demonstrated.", 
    "abstract": "Adult T-cell leukemia/lymphoma (ATLL) is pathogen-caused cancer that is progressed after the infection by human T-cell leukemia virus type 1. Four significant subtypes comprising acute, lymphoma, chronic, and smoldering have been identified for this cancer. However, there are no trustworthy prognostic biomarkers for these subtypes. We utilized a combination of two powerful network-based and machine-learning algorithms including differential co-expressed genes (DiffCoEx) and support vector machine-recursive feature elimination with cross-validation (SVM-RFECV) methods to categorize disparate ATLL subtypes from asymptomatic carriers (ACs). The results disclosed the significant involvement of CBX6, CNKSR1, and MAX in chronic, MYH10 and P2RY1 in acute, C22orf46 and HNRNPA0 in smoldering subtypes. These genes also can classify each ATLL subtype from AC carriers. The integration of the results of two powerful algorithms led to the identification of reliable gene classifiers and biomarkers for diverse ATLL subtypes.", 
    "abstract": "Literature about SARS-CoV-2 widely discusses the effects of variations that have spread in the past 3 years. Such information is dispersed in the texts of several research articles, hindering the possibility of practically integrating it with related datasets (e.g., millions of SARS-CoV-2 sequences available to the community). We aim to fill this gap, by mining literature abstracts to extract-for each variant/mutation-its related effects (in epidemiological, immunological, clinical, or viral kinetics terms) with labeled higher/lower levels in relation to the nonmutated virus.\nThe proposed framework comprises (i) the provisioning of abstracts from a COVID-19-related big data corpus (CORD-19) and (ii) the identification of mutation/variant effects in abstracts using a GPT2-based prediction model. The above techniques enable the prediction of mutations/variants with their effects and levels in 2 distinct scenarios: (i) the batch annotation of the most relevant CORD-19 abstracts and (ii) the on-demand annotation of any user-selected CORD-19 abstract through the CoVEffect web application (http://gmql.eu/coveffect), which assists expert users with semiautomated data labeling. On the interface, users can inspect the predictions and correct them; user inputs can then extend the training dataset used by the prediction model. Our prototype model was trained through a carefully designed process, using a minimal and highly diversified pool of samples.\nThe CoVEffect interface serves for the assisted annotation of abstracts, allowing the download of curated datasets for further use in data integration or analysis pipelines. The overall framework can be adapted to resolve similar unstructured-to-structured text translation tasks, which are typical of biomedical domains.", 
    "results": "The proposed framework comprises (i) the provisioning of abstracts from a COVID-19-related big data corpus (CORD-19) and (ii) the identification of mutation/variant effects in abstracts using a GPT2-based prediction model. The above techniques enable the prediction of mutations/variants with their effects and levels in 2 distinct scenarios: (i) the batch annotation of the most relevant CORD-19 abstracts and (ii) the on-demand annotation of any user-selected CORD-19 abstract through the CoVEffect web application (http://gmql.eu/coveffect), which assists expert users with semiautomated data labeling. On the interface, users can inspect the predictions and correct them; user inputs can then extend the training dataset used by the prediction model. Our prototype model was trained through a carefully designed process, using a minimal and highly diversified pool of samples.", 
    "abstract": "Understanding factors influencing microbial interactions, and designing methods to identify key taxa that are candidates for synthetic communities, or SynComs, are complex challenges for achieving microbiome-based agriculture. Here, we study how grafting and the choice of rootstock influences root-associated fungal communities in a grafted tomato system. We studied three tomato rootstocks (BHN589, RST-04-106, and Maxifort) grafted to a BHN589 scion and profiled the fungal communities in the endosphere and rhizosphere by sequencing the internal transcribed spacer (ITS2). The data provided evidence for a rootstock effect (explaining ~2% of the total captured variation, ", 
    "abstract": "Diabetic peripheral neuropathy (DPN) is a primary cause of diabetic foot, early detection of DPN is essential. This study aimed to construct a machine learning model for DPN diagnosis based on microcirculatory parameters, and identify the most predictive parameters for DPN.\nOur study involved 261 subjects, including 102 diabetics with neuropathy (DMN), 73 diabetics without neuropathy (DM), and 86 healthy controls (HC). DPN was confirmed by nerve conduction velocity and clinical sensory tests. Microvascular function was measured by postocclusion reactive hyperemia (PORH), local thermal hyperemia (LTH), and transcutaneous oxygen pressure (TcPO<inf>2</inf>). Other physiological information was also investigated. Logistic regression (LR) and other machine learning (ML) algorithms were used to develop the model for DPN diagnosis. Kruskal-Wallis Test (non-parametric) were performed for multiple comparisons. Several performance measures, such as accuracy, sensitivity and specificity, were used to access the efficacy of the developed model. All the features were ranked based on the importance score to find features with higher DPN predictions.\nThere was an overall decrease in microcirculatory parameters in response to PORH and LTH, as well as TcPO<inf>2</inf>, in DMN group compared to DM group and HC group. Random forest (RF) was found to be the best model, and achieved 84.6% accuracy along with 90.2% sensitivity and 76.7% specificity. RF_PF% of PORH was the main predictor of DPN. In addition, diabetic duration was also an important risk factor.\nPORH Test is a reliable screening tool for DPN, which can accurately distinguish DPN from diabetics using RF.", 
    "abstract": "Dual specificity protein kinase threonine/Tyrosine kinase (TTK) is one of the mitotic kinases. High levels of TTK are detected in several types of cancer. Hence, TTK inhibition is considered a promising therapeutic anti-cancer strategy. In this work, we used multiple docked poses of TTK inhibitors to augment training data for machine learning QSAR modeling. Ligand-Receptor Contacts Fingerprints and docking scoring values were used as descriptor variables. Escalating docking-scoring consensus levels were scanned against orthogonal machine learners, and the best learners (Random Forests and XGBoost) were coupled with genetic algorithm and Shapley additive explanations (SHAP) to determine critical descriptors for predicting anti-TTK bioactivity and for pharmacophore generation. Three successful pharmacophores were deduced and subsequently used for ", 
    "abstract": "Molecular simulations employing empirical force fields have provided valuable knowledge about the ice growth process in the past decade. The development of novel computational techniques allows us to study this process, which requires long simulations of relatively large systems, with ab\u00a0initio accuracy. In this work, we use a neural-network potential for water trained on the revised Perdew-Burke-Ernzerhof functional to describe the kinetics of the ice-water interface. We study both ice melting and growth processes. Our results for the ice growth rate are in reasonable agreement with previous experiments and simulations. We find that the kinetics of ice melting presents a different behavior (monotonic) than that of ice growth (non-monotonic). In particular, a maximum ice growth rate of 6.5 \u00c5/ns is found at 14\u00a0K of supercooling. The effect of the surface structure is explored by investigating the basal and primary and secondary prismatic facets. We use the Wilson-Frenkel relation to explain these results in terms of the mobility of molecules and the thermodynamic driving force. Moreover, we study the effect of pressure by complementing the standard isobar with simulations at a negative pressure (-1000\u00a0bar) and at a high pressure (2000\u00a0bar). We find that prismatic facets grow faster than the basal one and that pressure does not play an important role when the speed of the interface is considered as a function of the difference between the melting temperature and the actual one, i.e., to the degree of either supercooling or overheating.", 
    "abstract": "Biotic and abiotic interactions shape natural microbial communities. The mechanisms behind microbe-microbe interactions, particularly those protein based, are not well understood. We hypothesize that released proteins with antimicrobial activity are a powerful and highly specific toolset to shape and defend plant niches. We have studied Albugo candida, an obligate plant parasite from the protist Oomycota phylum, for its potential to modulate the growth of bacteria through release of antimicrobial proteins into the apoplast. Amplicon sequencing and network analysis of Albugo-infected and uninfected wild Arabidopsis thaliana samples revealed an abundance of negative correlations between Albugo and other phyllosphere microbes. Analysis of the apoplastic proteome of Albugo-colonized leaves combined with machine learning predictors enabled the selection of antimicrobial candidates for heterologous expression and study of their inhibitory function. We found for three candidate proteins selective antimicrobial activity against Gram-positive bacteria isolated from A.\u2009thaliana and demonstrate that these inhibited bacteria are precisely important for the stability of the community structure. We could ascribe the antibacterial activity of the candidates to intrinsically disordered regions and positively correlate it with their net charge. This is the first report of protist proteins with antimicrobial activity under apoplastic conditions that therefore are potential biocontrol tools for targeted manipulation of the microbiome.", 
    "abstract": "This study addresses the lack of systematic investigation into the prognostic value of hand-crafted radiomic features derived from diffusion tensor imaging (DTI) in isocitrate dehydrogenase (IDH) wild-type glioblastoma (GBM), as well as the limited understanding of the biological interpretation of individual DTI radiomic features and metrics.\nTo develop and validate a DTI-based radiomic model for predicting prognosis in patients with IDH wild-type GBM and reveal the biological underpinning of individual DTI radiomic features and metrics.\nThe DTI-based radiomic signature was an independent prognostic factor (p\u2009<\u20090.001). Incorporating the radiomic signature into a clinical model resulted in a radiomic-clinical nomogram that predicted survival better than either the radiomic model or clinical model alone, with a better calibration and classification accuracy. Four categories of pathways (synapse, proliferation, DNA damage response, and complex cellular functions) were significantly correlated with the DTI-based radiomic features and DTI metrics.\nThe prognostic radiomic features derived from DTI are driven by distinct pathways involved in synapse, proliferation, DNA damage response, and complex cellular functions of GBM.", 
    "abstract": "This study aimed to develop an algorithm to automatically segment the oral potentially malignant diseases (OPMDs) and oral cancers (OCs) of all oral subsites with various deep convolutional neural network applications. A total of 510 intraoral images of OPMDs and OCs were collected over 3 years (2006-2009). All images were confirmed both with patient records and histopathological reports. Following the labeling of the lesions the dataset was arbitrarily split, using random sampling in Python as the study dataset, validation dataset, and test dataset. Pixels were classified as the OPMDs and OCs with the OPMD/OC label and the rest as the background. U-Net architecture was used and the model with the best validation loss was chosen for the testing among the trained 500 epochs. Dice similarity coefficient (DSC) score was noted. The intra-observer ICC was found to be 0.994 while the inter-observer reliability was 0.989. The calculated DSC and validation accuracy across all clinical images were 0.697 and 0.805, respectively. Our algorithm did not maintain an excellent DSC due to multiple reasons for the detection of both OC and OPMDs in oral cavity sites. A better standardization for both 2D and 3D imaging (such as patient positioning) and a bigger dataset are required to improve the quality of such studies. This is the first study which aimed to segment OPMDs and OCs in all subsites of oral cavity which is crucial not only for the early diagnosis but also for higher survival rates.", 
    "abstract": "Little information is available about deep learning methods used in ultrasound images of salivary gland tumors. We aimed to compare the accuracy of the ultrasound-trained model to computed tomography or magnetic resonance imaging trained model.\nSix hundred and thirty-eight patients were included in this retrospective study. There were 558 benign and 80 malignant salivary gland tumors. A total of 500 images (250 benign and 250 malignant) were acquired in the training and validation set, then 62 images (31 benign and 31 malignant) in the test set. Both machine learning and deep learning were used in our model.\nThe test accuracy, sensitivity, and specificity of our final model were 93.5%, 100%, and 87%, respectively. There were no over fitting in our model as the validation accuracy was similar with the test accuracy.\nThe sensitivity and specificity were comparable with current MRI and CT images using artificial intelligence.", 
    "abstract": "Monkeypox virus (MPXV) outbreak is a serious public health concern that requires international attention. P37 of MPXV plays a pivotal role in DNA replication and acts as one of the promising targets for antiviral drug design. In this study, we intent to screen potential analogs of existing FDA approved drugs of MPXV against P37 using state-of-the-art machine learning and computational biophysical techniques. AlphaFold2 guided all-atoms molecular dynamics simulations optimized P37 structure is used for molecular docking and binding free energy calculations. Similar to members of Phospholipase-D family , the predicted P37 structure also adopts a \u03b2-\u03b1-\u03b2-\u03b1-\u03b2 sandwich fold, harbouring strongly conserved HxKxxxxD motif. The binding pocket comprises of Tyr48, Lys86, His115, Lys117, Ser130, Asn132, Trp280, Asn240, His325, Lys327 and Tyr346 forming strong hydrogen bonds and dense hydrophobic contacts with the screened analogs and is surrounded by positively charged patches. Loops connecting the two domains and C-terminal region exhibit high degree of flexibility. In some structural ensembles, the partial disorderness in the C-terminal region is presumed to be due to its low confidence score, acquired during structure prediction. Transition from loop to \u03b2-strands (244-254 aa) in P37-Cidofovir and its analog complexes advocates the need for further investigations. MD simulations support the accuracy of the molecular docking results, indicating the potential of analogs as potent binders of P37. Taken together, our results provide preferable understanding of molecular recognition and dynamics of ligand-bound states of P37, offering opportunities for development of new antivirals against MPXV. However, the need of ", 
    "abstract": "Automotive light detection and ranging (LiDAR) requires accurate and computationally efficient range estimation methods. At present, such efficiency is achieved at the cost of curtailing the dynamic range of a LiDAR receiver. In this Letter, we propose using decision tree ensemble machine learning models to overcome such a trade-off. Simple and yet powerful models are developed and proven capable of performing accurate measurements across a 45-dB dynamic range.", 
    "abstract": "Amyotrophic Lateral Sclerosis (ALS) is an incurable neurodegenerative condition. Despite significant advances in pre-clinical models that enhance understanding of disease pathobiology, translation of candidate drugs to effective human therapies has been disappointing. There is increasing recognition of the need for a precision medicine approach toward drug development, as many failures in translation can be attributed in part to disease heterogeneity in humans. PRECISION-ALS is an academic industry collaboration between clinicians, Computer Scientists, Information engineers, technologists, data scientists and industry partners that will address the key clinical, computational, data science and technology associated research questions to generate a sustainable precision medicine based approach toward new drug development. Using extant and prospectively collected population based clinical data across nine European sites, PRECISION-ALS provides a General Data Protection Regulation (GDPR) compliant framework that seamlessly collects, processes and analyses research-quality multimodal and multi-sourced clinical, patient and caregiver journey, digitally acquired data through remote monitoring, imaging, neuro-electric-signaling, genomic and biomarker datasets using machine learning and artificial intelligence. PRECISION-ALS represents a first-in-kind modular transferable pan-European ICT framework for ALS that can be easily adapted to other regions that face similar precision medicine related challenges in multimodal data collection and analysis.", 
    "abstract": "Therapy resistance in cancer is often driven by a subpopulation of cells that are temporarily arrested in a non-proliferative G0 state, which is difficult to capture and whose mutational drivers remain largely unknown.\nWe develop methodology to robustly identify this state from transcriptomic signals and characterise its prevalence and genomic constraints in solid primary tumours. We show that G0 arrest preferentially emerges in the context of more stable, less mutated genomes which maintain TP53 integrity and lack the hallmarks of DNA damage repair deficiency, while presenting increased APOBEC mutagenesis. We employ machine learning to uncover novel genomic dependencies of this process and validate the role of the centrosomal gene CEP89 as a modulator of proliferation and G0 arrest capacity. Lastly, we demonstrate that G0 arrest underlies unfavourable responses to various therapies exploiting cell cycle, kinase signalling and epigenetic mechanisms in single-cell data.\nWe propose a G0 arrest transcriptional signature that is linked with therapeutic resistance and can be used to further study and clinically track this state.", 
    "abstract": "Understanding the relative contributions of ecological and evolutionary processes to the structuring of ecological communities is needed to improve our ability to predict how communities may respond to future changes in an increasingly human-modified world. Metabarcoding methods make it possible to gather population genetic data for all species within a community, unlocking a new axis of data to potentially unveil the origins and maintenance of biodiversity at local scales. Here, we present a new eco-evolutionary simulation model for investigating community assembly dynamics using metabarcoding data. The model makes joint predictions of species abundance, genetic variation, trait distributions and phylogenetic relationships under a wide range of parameter settings (e.g. high speciation/low dispersal or vice versa) and across a range of community states, from pristine and unmodified to heavily disturbed. We first demonstrate that parameters governing metacommunity and local community processes leave detectable signatures in simulated biodiversity data axes. Next, using a simulation-based machine learning approach we show that neutral and non-neutral models are distinguishable and that reasonable estimates of several model parameters within the local community can be obtained using only community-scale genetic data, while phylogenetic information is required to estimate those describing metacommunity dynamics. Finally, we apply the model to soil microarthropod metabarcoding data from the Troodos mountains of Cyprus, where we find that communities in widespread forest habitats are structured by neutral processes, while high-elevation and isolated habitats act as an abiotic filter generating non-neutral community structure. We implement our model within the ibiogen R package, a package dedicated to the investigation of island, and more generally community-scale, biodiversity using community-scale genetic data.", 
    "abstract": "Genomic conditions can be associated with developmental delay, intellectual disability, autism spectrum disorder, and physical and mental health symptoms. They are individually rare and highly variable in presentation, which limits the use of standard clinical guidelines for diagnosis and treatment. A simple screening tool to identify young people with genomic conditions associated with neurodevelopmental disorders (ND-GCs) who could benefit from further support would be of considerable value. We used machine learning approaches to address this question.\nA total of 493 individuals were included: 389 with a ND-GC, mean age\u2009=\u20099.01, 66% male) and 104 siblings without known genomic conditions (controls, mean age\u2009=\u200910.23, 53% male). Primary carers completed assessments of behavioural, neurodevelopmental and psychiatric symptoms and physical health and development. Machine learning techniques (penalised logistic regression, random forests, support vector machines and artificial neural networks) were used to develop classifiers of ND-GC status and identified limited sets of variables that gave the best classification performance. Exploratory graph analysis was used to understand associations within the final variable set.\nAll machine learning methods identified variable sets giving high classification accuracy (AUROC between 0.883 and 0.915). We identified a subset of 30 variables best discriminating between individuals with ND-GCs and controls which formed 5 dimensions: conduct, separation anxiety, situational anxiety, communication and motor development.\nThis study used cross-sectional data from a cohort study which was imbalanced with respect to ND-GC status. Our model requires validation in independent datasets and with longitudinal follow-up data for validation before clinical application.\nIn this study, we developed models that identified a compact set of psychiatric and physical health measures that differentiate individuals with a ND-GC from controls and highlight higher-order structure within these measures. This work is a step towards developing a screening instrument to identify young people with ND-GCs who might benefit from further specialist assessment.", 
    "abstract": "Biomedicine, i.e. the application of basic sciences to medicine, has become the cornerstone for the study of etiopathogenesis and treatment of diseases. Biomedicine has enormously contributed to the progress of medicine and healthcare and has become the preferred approach to medical problems in the West. The developments in statistical inference and machine learning techniques have provided the foundation for personalised medicine where clinical management can be fully informed by biomedicine. The deployment of precision medicine may impact the autonomy and self-normativity of the patients. Understanding the relationship between biomedicine and medical practice can help navigate the benefits and challenges offered by precision medicine.\nConventional content analysis was applied to \"Le Normal and le Pathologique\" (Canguilhem G. The Normal and the Pathological. Princeton: Princeton University Press; 1991) and further investigated with respect to its relationship with techne and precision medicine using PubMed and Google Scholar and the Standford Encyclopedia of Philosophy to search for the following keywords singularly or in combination: \"Canguilhem\", \"techne\", \"episteme\", \"precision medicine\", \"machine learning AND medicine\".\nThe Hippocratic concept of techne accounts for many characteristics of medical knowledge and practice. The advances of biomedicine, experimental medicine and, more recently, machine learning offer, in contrast, the model of a medicine based purely on episteme. I argue that Canguilhem medical epistemology establishes a framework where episteme and data-driven medicine is compatible with the promotion of patient's autonomy and self-normativity.\nCanguilhem's medical epistemology orders the relationship of applied medicine with experimental sciences, ethics and social sciences. It provides guidance to define the scope of medicine and the boundaries of medicalization of healthy life. Finally, it sets an agenda for a safe implementation of machine learning in medicine.", 
    "abstract": "Heart failure (HF) is a major complication following ischemic heart disease (IHD) and it adversely affects the outcome. Early prediction of HF risk in patients with IHD is beneficial for timely intervention and for reducing disease burden.\nTwo cohorts, cases for patients first diagnosed with IHD and then with HF (N\u2009=\u200911,862) and control IHD patients without HF (N\u2009=\u200925,652), were established from the hospital discharge records in Sichuan, China during 2015-2019. Directed personal disease network (PDN) was constructed for each patient, and then these PDNs were merged to generate the baseline disease network (BDN) for the two cohorts, respectively, which identifies the health trajectories of patients and the complex progression patterns. The differences between the BDNs of the two cohort was represented as disease-specific network (DSN). Three novel network features were exacted from PDN and DSN to represent the similarity of disease patterns and specificity trends from IHD to HF. A stacking-based ensemble model DXLR was proposed to predict HF risk in IHD patients using the novel network features and basic demographic features (i.e., age and sex). The Shapley Addictive exPlanations method was applied to analyze the feature importance of the DXLR model.\nCompared with the six traditional machine learning models, our DXLR model exhibited the highest AUC (0.934\u2009\u00b1\u20090.004), accuracy (0.857\u2009\u00b1\u20090.007), precision (0.723\u2009\u00b1\u20090.014), recall (0.892\u2009\u00b1\u20090.012) and F\nOur proposed approach that combines network analytics and ensemble learning effectively predicts HF risk in patients with IHD. This highlights the potential value of network-based machine learning in disease risk prediction field using administrative data.", 
    "abstract": "Accurate in-hospital mortality prediction can reflect the prognosis of patients, help guide allocation of clinical resources, and help clinicians make the right care decisions. There are limitations to using traditional logistic regression models when assessing the model performance of comorbidity measures to predict in-hospital mortality. Meanwhile, the use of novel machine-learning methods is growing rapidly. In 2021, the Agency for Healthcare Research and Quality published new guidelines for using the Present-on-Admission (POA) indicator from the International Classification of Diseases, Tenth Revision, for coding comorbidities to predict in-hospital mortality from the Elixhauser's comorbidity measurement method. We compared the model performance of logistic regression, elastic net model, and artificial neural network (ANN) to predict in-hospital mortality from Elixhauser's measures under the updated POA guidelines. In this retrospective analysis, 1,810,106 adult Medicare inpatient admissions from six US states admitted after September 23, 2017, and discharged before April 11, 2019 were extracted from the Centers for Medicare and Medicaid Services data warehouse. The POA indicator was used to distinguish pre-existing comorbidities from complications that occurred during hospitalization. All models performed well (C-statistics >0.77). Elastic net method generated a parsimonious model, in which there were five fewer comorbidities selected to predict in-hospital mortality with similar predictive power compared to the logistic regression model. ANN had the highest C-statistics compared to the other two models (0.800 vs. 0.791 and 0.791). Elastic net model and AAN can be applied successfully to predict in-hospital mortality.", 
    "abstract": "Optical coherence tomography angiography (OCTA) has been found to identify changes in the retinal microvasculature of people with various cardiometabolic factors. Machine learning has previously been applied within ophthalmic imaging but has not yet been applied to these risk factors. The study aims to assess the feasibility of predicting the presence or absence of cardiovascular conditions and their associated risk factors using machine learning and OCTA.\nCross-sectional study. Demographic and co-morbidity data was collected for each participant undergoing 3\u2009\u00d7\u20093\u2009mm, 6\u2009\u00d7\u20096\u2009mm and 8\u2009\u00d7\u20098\u2009mm OCTA scanning using the Carl Zeiss CIRRUS HD-OCT model 5000. The data was then pre-processed and randomly split into training and testing datasets (75%/25% split) before being applied to two models (Convolutional Neural Network and MoblieNetV2). Once developed on the training dataset, their performance was assessed on the unseen test dataset.\nTwo hundred forty-seven participants were included. Both models performed best in predicting the presence of hyperlipidaemia in 3\u2009\u00d7\u20093\u2009mm scans with an AUC of 0.74 and 0.81, and accuracy of 0.79 for CNN and MobileNetV2 respectively. Modest performance was achieved in the identification of diabetes mellitus, hypertension and congestive heart failure in 3\u2009\u00d7\u20093\u2009mm scans (all with AUC and accuracy >0.5). There was no significant recognition for 6\u2009\u00d7\u20096 and 8\u2009\u00d7\u20098\u2009mm for any cardiometabolic risk factor.\nThis study demonstrates the strength of ML to identify the presence cardiometabolic factors, in particular hyperlipidaemia, in high-resolution 3\u2009\u00d7\u20093\u2009mm OCTA scans. Early detection of risk factors prior to a clinically significant event, will assist in preventing adverse outcomes for people.", 
    "abstract": "While a robust literature on the psychology of conspiracy theories has identified dozens of characteristics correlated with conspiracy theory beliefs, much less attention has been paid to understanding the generalized predisposition towards interpreting events and circumstances as the product of supposed conspiracies. Using a unique national survey of 2015 U.S. adults from October 2020, we investigate the relationship between this predisposition-conspiracy thinking-and 34 different psychological, political, and social correlates. Using conditional inference tree modeling-a machine learning-based approach designed to facilitate prediction using a flexible modeling methodology-we identify the characteristics that are most useful for orienting individuals along the conspiracy thinking continuum, including (but not limited to): anomie, Manicheanism, support for political violence, a tendency to share false information online, populism, narcissism, and psychopathy. Altogether, psychological characteristics are much more useful in predicting conspiracy thinking than are political and social characteristics, though even our robust set of correlates only partially accounts for variance in conspiracy thinking.", 
    "abstract": null, 
    "abstract": "Recent advances in optical underwater imaging technologies enable the acquisition of huge numbers of high-resolution seafloor images during scientific expeditions. While these images contain valuable information for non-invasive monitoring of megabenthic fauna, flora and the marine ecosystem, traditional labor-intensive manual approaches for analyzing them are neither feasible nor scalable. Therefore, machine learning has been proposed as a solution, but training the respective models still requires substantial manual annotation. Here, we present an automated image-based workflow for Megabenthic Fauna Detection with Faster R-CNN (FaunD-Fast). The workflow significantly reduces the required annotation effort by automating the detection of anomalous superpixels, which are regions in underwater images that have unusual properties relative to the background seafloor. The bounding box coordinates of the detected anomalous superpixels are proposed as a set of weak annotations, which are then assigned semantic morphotype labels and used to train a Faster R-CNN object detection model. We applied this workflow to example underwater images recorded during cruise SO268 to the German and Belgian contract areas for Manganese-nodule exploration, within the Clarion-Clipperton Zone (CCZ). A performance assessment of our FaunD-Fast model showed a mean average precision of 78.1% at an intersection-over-union threshold of 0.5, which is on a par with competing models that use costly-to-acquire annotations. In more detail, the analysis of the megafauna detection results revealed that ophiuroids and xenophyophores were among the most abundant morphotypes, accounting for 62% of all the detections within the surveyed area. Investigating the regional differences between the two contract areas further revealed that both megafaunal abundance and diversity was higher in the shallower German area, which might be explainable by the higher food availability in form of sinking organic material that decreases from east-to-west across the CCZ. Since these findings are consistent with studies based on conventional image-based methods, we conclude that our automated workflow significantly reduces the required human effort, while still providing accurate estimates of megafaunal abundance and their spatial distribution. The workflow is thus useful for a quick but objective generation of baseline information to enable monitoring of remote benthic ecosystems.", 
    "abstract": "Machine learning is transforming the field of histopathology. Especially in classification related tasks, there have been many successful applications of deep learning already. Yet, in tasks that rely on regression and many niche applications, the domain lacks cohesive procedures that are adapted to the learning processes of neural networks. In this work, we investigate cell damage in whole slide images of the epidermis. A common way for pathologists to annotate a score, characterizing the degree of damage for these samples, is the ratio between healthy and unhealthy nuclei. The annotation procedure of these scores, however, is expensive and prone to be noisy among pathologists. We propose a new measure of damage, that is the total area of damage, relative to the total area of the epidermis. In this work, we present results of regression and segmentation models, predicting both scores on a curated and public dataset. We have acquired the dataset in collaborative efforts with medical professionals. Our study resulted in a comprehensive evaluation of the proposed damage metrics in the epidermis, with recommendations, emphasizing practical relevance for real world applications.", 
    "abstract": "Electric vehicles (EVs) have been introduced as an alternative to gasoline and diesel cars to reduce greenhouse gas emissions, optimize fossil fuel use, and protect the environment. Predicting EV sales is momentous for stakeholders, including car manufacturers, policymakers, and fuel suppliers. The data used in the modeling process significantly affects the prediction model's quality. This research's primary dataset contains monthly sales and registrations of 357 new vehicles in the United States of America from 2014 to 2020. In addition to this data, several web crawlers were used to gather the required information. Vehicles sale were predicted using long short-term memory (LSTM) and Convolutional LSTM (ConvLSTM) models. To enhance LSTM performance, the hybrid model with a new structure called \"Hybrid LSTM with two-dimensional Attention and Residual network\" has been proposed. Also, all three models are built as Automated Machine Learning models to improve the modeling process. The proposed hybrid model performs better than the other models based on the same evaluation units, including Mean Absolute Percentage Error, Normalized Root Mean Square Error, R-square, slope, and intercept of fitted linear regressions. The proposed hybrid model has been able to predict the share of EVs with an acceptable Mean Absolute Error of 3.5%.", 
    "abstract": "Plant-atmosphere exchange fluxes of CO", 
    "abstract": "The accurate encoding of operation notes is essential for activity-based funding and workforce planning. The aim of this project was to evaluate the procedural coding accuracy of vitrectomy and to develop machine learning, natural language processing (NLP) models that may assist with this task.\nThis retrospective cohort study involved vitrectomy operation notes between a 21-month period at the Royal Adelaide Hospital. Coding of procedures were based on the Medicare Benefits Schedule (MBS)-the Australian equivalent to the Current Procedural Terminology (CPT\u00ae) codes used in the United States. Manual encoding was conducted for all procedures and reviewed by two vitreoretinal consultants. XGBoost, random forest and logistic regression models were developed for classification experiments. A cost-based analysis was subsequently conducted.\nThere were a total of 1724 procedures with individual codes performed within 617 vitrectomy operation notes totalling $1\u2009528\u2009086.60 after manual review. A total of 1147 (66.5%) codes were missed in the original coding that amounted to $736\u2009539.20 (48.2%). Our XGBoost model had the highest classification accuracy (94.6%) in the multi-label classification for the five most common procedures. The XGBoost model was the most successful model in identifying operation notes with two or more missing codes with an AUC of 0.87 (95% CI 0.80-0.92).\nMachine learning has been successful in the classification of vitrectomy operation note encoding. We recommend a combined human and machine learning approach to clinical coding as automation may facilitate more accurate reimbursement and enable surgeons to prioritise higher quality clinical care.", 
    "abstract": "To develop a breed assignment model, three main steps are generally followed: 1) The selection of breed informative SNP; 2) The training of a model, based on a reference population, that allows to classify animals to their breed of origin; and 3) The validation of the developed model on external animals i.e., that were not used in previous steps. However, there is no consensus in the literature about which methodology to follow for the first step, nor about the number of SNP to be selected. This can raise many questions when developing the model and lead to the use of sophisticated methodologies for selecting SNP (e.g., with iterative algorithms, partitions of SNP or combination of several methods). Therefore, it may be of interest to avoid the first step by the use of all the available SNP. For this purpose, we propose the use of a genomic relationship matrix (GRM), combined or not with a machine learning method, for breed assignment. We compared it with a previously developed model based on selected informative SNP. Four methodologies were investigated: 1) The PLS_NSC methodology: selection of SNP based on a partial least square-discriminant analysis (PLS-DA) and breed assignment by classification based on the nearest shrunken centroids (NSC) method; 2) Breed assignment based on the highest mean relatedness of an animal to the reference populations of each breed (referred to mean_GRM); 3) Breed assignment based on the highest SD of the relatedness of an animal to the reference populations of each breed (referred to SD_GRM) and 4) The GRM_SVM methodology: the use of means and SD of the relatedness defined in mean_GRM and SD_GRM methodologies combined with the linear support vector machine (SVM), a machine learning method used for classification. Regarding mean global accuracies, results showed that the use of mean_GRM or GRM_SVM was not significantly different (Bonferroni corrected P > 0.0083) than the model based on a reduced SNP panel (PLS_NSC). Moreover, the mean_GRM and GRM_SVM methodology were more efficient than PLS_NSC as it was faster to compute. Therefore, it is possible to bypass the selection of SNP and, by the use of a GRM, to develop an efficient breed assignment model. In routine, we recommend the use of GRM_SVM over mean_GRM as it gave a slightly increased global accuracy, which can help endangered breeds to be maintained. The script to execute the different methodologies can be accessed on: https : //github.com/hwilmot675/Breed_assignment.", 
    "abstract": "Developing new crop varieties with superior performance is highly important to ensure robust and sustainable global food security. The speed of variety development is limited by long field cycles and advanced generation selections in plant breeding programs. While methods to predict yield from genotype or phenotype data have been proposed, improved performance and integrated models are needed.\nWe propose a machine learning model that leverages both genotype and phenotype measurements by fusing genetic variants with multiple data sources collected by unmanned aerial systems. We use a deep multiple instance learning framework with an attention mechanism that sheds light on the importance given to each input during prediction, enhancing interpretability. Our model reaches 0.754\u2009\u00b1\u20090.024 Pearson correlation coefficient when predicting yield in similar environmental conditions; a 34.8% improvement over the genotype-only linear baseline (0.559\u2009\u00b1\u20090.050). We further predict yield on new lines in an unseen environment using only genotypes, obtaining a prediction accuracy of 0.386\u2009\u00b1\u20090.010, a 13.5% improvement over the linear baseline. Our multi-modal deep learning architecture efficiently accounts for plant health and environment, distilling the genetic contribution and providing excellent predictions. Yield prediction algorithms leveraging phenotypic observations during training therefore promise to improve breeding programs, ultimately speeding up delivery of improved varieties.\nAvailable at https://github.com/BorgwardtLab/PheGeMIL (code) and https://doi.org/doi:10.5061/dryad.kprr4xh5p (data).", 
    "abstract": "Revision hip arthroplasty has a less favorable outcome than primary total hip arthroplasty and an understanding of the timing of total hip arthroplasty failure may be helpful. The aim of this study is to develop a combined deep learning (DL) and machine learning (ML) approach to automatically detect hip prosthetic failure from conventional plain radiographs.\nTwo cohorts of patients (of 280 and 352 patients) were included in the study, for model development and validation, respectively. The analysis was based on one antero-posterior and one lateral radiographic view obtained from each patient during routine post-surgery follow-up. After pre-processing, three images were obtained: the original image, the acetabulum image and the stem image. These images were analyzed through convolutional neural networks aiming to predict prosthesis failure. Deep features of the three images were extracted for each model and two feature-based pipelines were developed: one utilizing only the features of the original image (original image pipeline) and the other concatenating the features of the three images (3-image pipeline). The obtained features were either used directly or reduced through principal component analysis. Both support vector machine (SVM) and random forest (RF) classifiers were considered for each pipeline.\nThe SVM applied to the 3-image pipeline provided the best performance, with an accuracy of 0.958\u00a0\u00b1\u00a00.006 in the internal validation and an F1-score of 0.874 in the external validation set. The explainability analysis, besides identifying the features of the complete original images as the major contributor, highlighted the role of the acetabulum and stem images on the prediction.\nThis study demonstrated the potentialities of the developed DL-ML procedure based on plain radiographs in the detection of the failure of the hip prosthesis.", 
    "abstract": "", 
    "abstract": "Progression of liver fibrosis to cirrhosis, a severe non-reversible process, is one of the most critical risk factors in developing hepatocellular carcinoma and liver failure. Detection of liver fibrosis at an early stage is therefore essential for better patient management. Ultrasound (US) imaging can provide a noninvasive alternative to biopsies. This study evaluates quantitative US texture features to improve early-stage versus advanced liver fibrosis detection. 157 B-mode US images of different liver lobes acquired from early and advanced fibrosis rat cases were used for analysis. 5-6 regions of interest were placed on each image. Twelve quantitative features that describe liver texture changes were extracted from the images, including first-order histogram, run length (RL), and gray level co-occurrence matrix (GLCM). The diagnostic performance of individual features was high with AUC ranging from 0.80 to 0.94. Logistic regression with leave-one-out cross-validation was used to evaluate the performance of the combined features. All features combined showed a slight improvement in performance with AUC = 0.95, sensitivity = 96.8%, and specificity = 93.7%. Quantitative US texture features characterize liver fibrosis changes with high accuracy and can differentiate early from advanced disease. Quantitative ultrasound, if validated in future clinical studies, can have a potential role in identifying fibrosis changes that are not easily detected by visual US image assessments.", 
    "abstract": "COVID-19 has threatened the existence of human life for more than the last 2 years. More than 460 million confirmed cases and 6 million deaths have been reported worldwide due to COVID-19. To measure the severity of the COVID-19, the mortality rate plays an important role. Understanding the nature of COVID-19 and forecasting the death cases of COVID-19 require more investigation of the real effect for different risk factors. In this work, various regression machine learning models are proposed to extract the relationship between different factors and the death rate of COVID-19. The optimal regression tree algorithm employed in this work estimates the impact of essential causal variables that significantly affect the mortality rates. We have generated a real-time forecast for the death case of COVID-19 using machine learning techniques. The analysis is evaluated with the well-known regression models XGBoost, Random Forest, and SVM on the data sets of the US, India, Italy, and three continents Asia, Europe, and North America. The results show that the models can be used to forecast the death cases for the near future in case of an epidemic like Novel Coronavirus.", 
    "abstract": "With the rising number of people using social networks after the pandemic of COVID-19, cybercriminals took the advantage of (i) the increased base of possible victims and (ii) the use of a trending topic as the pandemic COVID-19 to lure victims and attract their attention and put malicious content to infect the most possible number of people. Twitter platform forces an auto-shortening to any included URL within a 140-character message called \"tweet\" and this makes it easier for the attackers to include malicious URLs within Tweets. Here comes the need to adopt new approaches to resolve the problem or at least identify it to better understand it to find a suitable solution. One of the proven effective approaches is the adaption of machine learning (ML) concepts and applying different algorithms to detect, identify, and even block the propagation of malware. Hence, this study's main objectives were to collect tweets from Twitter that are related to the topic of COVID-19 and extract features from these tweets and import them as independent variables for the machine learning models to be developed later, so they would identify imported tweets as to be malicious or not.", 
    "abstract": "Mesh-based reconstruction of the cerebral cortex is a fundamental component in brain image analysis. Classical, iterative pipelines for cortical modeling are robust but often time-consuming, mostly due to expensive procedures that involve topology correction and spherical mapping. Recent attempts to address reconstruction with machine learning methods have accelerated some components in these pipelines, but these methods still require slow processing steps to enforce topological constraints that comply with known anatomical structure. In this work, we introduce a novel learning-based strategy, TopoFit, which rapidly fits a topologically-correct surface to the white-matter tissue boundary. We design a joint network, employing image and graph convolutions and an efficient symmetric distance loss, to learn to predict accurate deformations that map a template mesh to subject-specific anatomy. This technique encompasses the work of current mesh correction, fine-tuning, and inflation processes and, as a result, offers a 150\u00d7 faster solution to cortical surface reconstruction compared to traditional approaches. We demonstrate that TopoFit is 1.8\u00d7 more accurate than the current state-of-the-art deep-learning strategy, and it is robust to common failure modes, such as white-matter tissue hypointensities.", 
    "abstract": "Multi-center research networks often supported by centralized data centers are integral in generating high-quality evidence needed to address the gaps in emergency care. However, there are substantial costs to maintain high-functioning data centers. A novel distributed or federated data health networks (FDHN) approach has been used recently to overcome the shortcomings of centralized data approaches. A FDHN in emergency care is comprised of a series of decentralized, interconnected emergency departments (EDs) where each site's data is structured according to a common data model that allows data to be queried and/or analyzed without the data leaving the site's institutional firewall. To best leverage FDHNs for emergency care research networks, we propose a stepwise, 2-level development and deployment process-creating a lower resource requiring Level I FDHN capable of basic analyses, or a more resource-intense Level II FDHN capable of sophisticated analyses such as distributed machine learning. Importantly, existing electronic health records-based analytical tools can be leveraged without substantial cost implications for research networks to implement a Level 1 FDHN. Fewer regulatory barriers associated with FDHN have a potential for diverse, non-network EDs to contribute to research, foster faculty development, and improve patient outcomes in emergency care.", 
    "abstract": "Fairness in data-driven decision-making studies scenarios where individuals from certain population segments may be unfairly treated when being considered for loan or job applications, access to public resources, or other types of services. In location-based applications, decisions are based on individual whereabouts, which often correlate with sensitive attributes such as race, income, and education. While fairness has received significant attention recently, e.g., in machine learning, there is little focus on achieving fairness when dealing with location data. Due to their characteristics and specific type of processing algorithms, location data pose important fairness challenges. We introduce the concept of ", 
    "abstract": "Acclimation and adaptation of metabolism to a changing environment are key processes for plant survival and reproductive success. In the present study, 241 natural accessions of Arabidopsis (Arabidopsis thaliana) were grown under two different temperature regimes, 16 \u00b0C and 6 \u00b0C, and growth parameters were recorded, together with metabolite profiles, to investigate the natural genome \u00d7 environment effects on metabolome variation. The plasticity of metabolism, which was captured by metabolic distance measures, varied considerably between accessions. Both relative growth rates and metabolic distances were predictable by the underlying natural genetic variation of accessions. Applying machine learning methods, climatic variables of the original growth habitats were tested for their predictive power of natural metabolic variation among accessions. We found specifically habitat temperature during the first quarter of the year to be the best predictor of the plasticity of primary metabolism, indicating habitat temperature as the causal driver of evolutionary cold adaptation processes. Analyses of epigenome- and genome-wide associations revealed accession-specific differential DNA-methylation levels as potentially linked to the metabolome and identified FUMARASE2 as strongly associated with cold adaptation in Arabidopsis accessions. These findings were supported by calculations of the biochemical Jacobian matrix based on variance and covariance of metabolomics data, which revealed that growth under low temperatures most substantially affects the accession-specific plasticity of fumarate and sugar metabolism. Our findings indicate that the plasticity of metabolic regulation is predictable from the genome and epigenome and driven evolutionarily by Arabidopsis growth habitats.", 
    "abstract": "Artificial intelligence (AI) and machine learning (ML) have occupied the center stage in healthcare as research groups and institutions investigate their capabilities and risks. Dermatology is often cited as one of the medical specialties most ripe for disruption with AI technology due to the heavy incorporation of visual information into clinical decisions and treatments. Although the literature on AI in dermatology is rapidly growing, there has been a noticeable absence of mature AI solutions utilized by dermatology departments or patients. This commentary provides insight into the regulatory challenges facing AI solutions for the specialty of dermatology and the unique considerations that should be factored into AI development and deployment.", 
    "abstract": "Microblogging sites are important vehicles for the users to obtain information and shape public opinion thus they are arenas of continuous competition for popularity. Most popular topics are usually indicated on ranking lists. In this study, we investigate the public attention dynamics through the Hot Search List (HSL) of the Chinese microblog Sina Weibo, where trending hashtags are ranked based on a multi-dimensional search volume index. We characterize the rank dynamics by the time spent by hashtags on the list, the time of the day they appear there, the rank diversity, and by the ranking trajectories. We show how the circadian rhythm affects the popularity of hashtags, and observe categories of their rank trajectories by a machine learning clustering algorithm. By analyzing patterns of ranking dynamics using various measures, we identify anomalies that are likely to result from the platform provider's intervention into the ranking, including the anchoring of hashtags to certain ranks on the HSL. We propose a simple model of ranking that explains the mechanism of this anchoring effect. We found an over-representation of hashtags related to international politics at 3 out of 4 anchoring ranks on the HSL, indicating possible manipulations of public opinion.", 
    "abstract": "Ovarian cancer (OC) is one of the deadliest cancers affecting the female reproductive system. It may present little or no symptoms at the early stages and typically unspecific symptoms at later stages. High-grade serous ovarian cancer (HGSC) is the subtype responsible for most ovarian cancer deaths. However, very little is known about the metabolic course of this disease, particularly in its early stages. In this longitudinal study, we examined the temporal course of serum lipidome changes using a robust HGSC mouse model and machine learning data analysis. Early progression of HGSC was marked by increased levels of phosphatidylcholines and phosphatidylethanolamines. In contrast, later stages featured more diverse lipid alterations, including fatty acids and their derivatives, triglycerides, ceramides, hexosylceramides, sphingomyelins, lysophosphatidylcholines, and phosphatidylinositols. These alterations underscored unique perturbations in cell membrane stability, proliferation, and survival during cancer development and progression, offering potential targets for early detection and prognosis of human ovarian cancer.", 
    "abstract": "Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. In this article, we investigate this relationship and reveal that federated averaging (FedAvg) algorithms actually have a negative impact on self-attention in cases of data heterogeneity, which limits the capabilities of the transformer model in federated learning settings. To address this issue, we propose FedTP, a novel transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism that maintains personalized self-attention layers of each client locally, we develop a learn-to-personalize mechanism to further encourage the cooperation among clients and to increase the scalability and generalization of FedTP. Specifically, we achieve this by learning a hypernetwork on the server that outputs the personalized projection matrices of self-attention layers to generate clientwise queries, keys, and values. Furthermore, we present the generalization bound for FedTP with the learn-to-personalize mechanism. Extensive experiments verify that FedTP with the learn-to-personalize mechanism yields state-of-the-art performance in the non-IID scenarios. Our code is available online https://github.com/zhyczy/FedTP.", 
    "abstract": "With the extensive use of Machine Learning (ML) in the biomedical field, there was an increasing need for Explainable Artificial Intelligence (XAI) to improve transparency and reveal complex hidden relationships between variables for medical practitioners, while meeting regulatory requirements. Feature Selection (FS) is widely used as a part of a biomedical ML pipeline to significantly reduce the number of variables while preserving as much information as possible. However, the choice of FS methods affects the entire pipeline including the final prediction explanations, whereas very few works investigate the relationship between FS and model explanations. Through a systematic workflow performed on 145 datasets and an illustration on medical data, the present work demonstrated the promising complementarity of two metrics based on explanations (using ranking and influence changes) in addition to accuracy and retention rate to select the most appropriate FS/ML models. Measuring how much explanations differ with/without FS are particularly promising for FS methods recommendation. While reliefF generally performs the best on average, the optimal choice may vary for each dataset. Positioning FS methods in a tridimensional space, integrating explanations-based metrics, accuracy and retention rate, would allow the user to choose the priorities to be given on each of the dimensions. In biomedical applications, where each medical condition may have its own preferences, this framework will make it possible to offer the healthcare professional the appropriate FS technique, to select the variables that have an important explainable impact, even if this comes at the expense of a limited drop of accuracy.", 
    "abstract": "Systematic reviews and meta-analysis are the cornerstones of evidence-based decision making and priority setting. However, traditional systematic reviews are time and labour intensive, limiting their feasibility to comprehensively evaluate the latest evidence in research-intensive areas. Recent developments in automation, machine learning and systematic review technologies have enabled efficiency gains. Building upon these advances, we developed Systematic Online Living Evidence Summaries (SOLES) to accelerate evidence synthesis. In this approach, we integrate automated processes to continuously gather, synthesise and summarise all existing evidence from a research domain, and report the resulting current curated content as interrogatable databases via interactive web applications. SOLES can benefit various stakeholders by (i) providing a systematic overview of current evidence to identify knowledge gaps, (ii) providing an accelerated starting point for a more detailed systematic review, and (iii) facilitating collaboration and coordination in evidence synthesis.", 
    "abstract": "Two-dimensional (2D) material research is rapidly evolving to broaden the spectrum of emergent 2D systems. Here, we review recent advances in the theory, synthesis, characterization, device, and quantum physics of 2D materials and their heterostructures. First, we shed insight into modeling of defects and intercalants, focusing on their formation pathways and strategic functionalities. We also review machine learning for synthesis and sensing applications of 2D materials. In addition, we highlight important development in the synthesis, processing, and characterization of various 2D materials (e.g., MXnenes, magnetic compounds, epitaxial layers, low-symmetry crystals, etc.) and discuss oxidation and strain gradient engineering in 2D materials. Next, we discuss the optical and phonon properties of 2D materials controlled by material inhomogeneity and give examples of multidimensional imaging and biosensing equipped with machine learning analysis based on 2D platforms. We then provide updates on mix-dimensional heterostructures using 2D building blocks for next-generation logic/memory devices and the quantum anomalous Hall devices of high-quality magnetic topological insulators, followed by advances in small twist-angle homojunctions and their exciting quantum transport. Finally, we provide the perspectives and future work on several topics mentioned in this review.", 
