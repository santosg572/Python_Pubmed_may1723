    "abstract": "The centrality measures, like betweenness b and degree k in complex networks remain fundamental quantities helping to classify them. It is realized from Barthelemy's paper [Eur. Phys. J. B 38, 163 (2004)10.1140/epjb/e2004-00111-4] that the maximal b-k exponent for the scale-free (SF) networks is \u03b7_{max}=2, belonging to SF trees, based on which one concludes \u03b4\u2265\u03b3+1/2, where \u03b3 and \u03b4 are the scaling exponents for the distribution functions of the degree and the betweenness centralities, respectively. This conjecture was violated for some special models and systems. Here we present a systematic study on this problem for visibility graphs of correlated time series, and show evidence that this conjecture fails in some correlation strengths. We consider the visibility graph of three models: two-dimensional Bak-Tang-Weisenfeld (BTW) sandpile model, one-dimensional (1D) fractional Brownian motion (FBM), and 1D Levy walks, the two latter cases are controlled by the Hurst exponent H and the step index \u03b1, respectively. In particular, for the BTW model and FBM with H\u22720.5, \u03b7 is greater than 2, and also \u03b4<\u03b3+1/2 for the BTW model, while the Barthelemy's conjecture remains valid for the Levy process. We assert that the failure of the Barthelemy's conjecture is due to large fluctuations in the scaling b-k relation resulting in the violation of hyperscaling relation \u03b7=\u03b3-1/\u03b4-1 and emergent anomalous behavior for the BTW model and FBM. Universal distribution function of generalized degree is found for these models which have the same scaling behavior as the Barabasi-Albert network.", 
    "abstract": "Haros graphs have been recently introduced as a set of graphs bijectively related to real numbers in the unit interval. Here we consider the iterated dynamics of a graph operator R over the set of Haros graphs. This operator was previously defined in the realm of graph-theoretical characterization of low-dimensional nonlinear dynamics and has a renormalization group (RG) structure. We find that the dynamics of R over Haros graphs is complex and includes unstable periodic orbits of arbitrary period and nonmixing aperiodic orbits, overall portraiting a chaotic RG flow. We identify a single RG stable fixed point whose basin of attraction is associated with the set of rational numbers, and find periodic RG orbits that relate to (pure) quadratic irrationals and aperiodic RG orbits, related with (nonmixing) families of nonquadratic algebraic irrationals and transcendental numbers. Finally, we show that the graph entropy of Haros graphs is globally decreasing as the RG flows towards its stable fixed point, albeit in a strictly nonmonotonic way, and that such graph entropy remains constant inside the periodic RG orbit associated to a subset of irrationals, the so-called metallic ratios. We discuss the possible physical interpretation of such chaotic RG flow and put results regarding entropy gradients along RG flow in the context of c-theorems.", 
    "abstract": "A major current focus in the analysis of protein-protein interaction (PPI) data is how to identify essential proteins. As massive PPI data are available, this warrants the design of efficient computing methods for identifying essential proteins. Previous studies have achieved considerable performance. However, as a consequence of the features of high noise and structural complexity in PPIs, it is still a challenge to further upgrade the performance of the identification methods.\nThis paper proposes an identification method, named CTF, which identifies essential proteins based on edge features including h-quasi-cliques and uv-triangle graphs and the fusion of multiple-source information. We first design an edge-weight function, named EWCT, for computing the topological scores of proteins based on quasi-cliques and triangle graphs. Then, we generate an edge-weighted PPI network using EWCT and dynamic PPI data. Finally, we compute the essentiality of proteins by the fusion of topological scores and three scores of biological information.\nWe evaluated the performance of the CTF method by comparison with 16 other methods, such as MON, PeC, TEGS, and LBCC, the experiment results on three datasets of Saccharomyces cerevisiae show that CTF outperforms the state-of-the-art methods. Moreover, our method indicates that the fusion of other biological information is beneficial to improve the accuracy of identification.", 
    "abstract": "Carbon nanotubes (CNTs) are nanoscale tubes made of carbon atoms with unique mechanical, electrical, and thermal properties. They have a variety of promising applications in electronics, energy storage, and composite materials and are found as single-wall carbon nanotubes (SWCNTs) and double-wall carbon nanotubes (DWCNTs). Considering such alluring attributes of nanotubes, the motive of the presented flow model is to compare the thermal performance of magnetohydrodynamic (MHD) mono (SWCNTs)/Ethylene glycol) and hybrid (DWCNTs- SWCNTs/Ethylene glycol) nanofluids over a bidirectional stretching surface. The thermal efficiency of the proposed model is gauged while considering the effects of Cattaneo-Christov heat flux with prescribed heat flux (PHF) and prescribed surface temperature (PST). The flow is assisted by the anisotropic slip at the boundary of the surface. The system of partial differential equations (PDEs) is converted into a nonlinear ordinary differential system by the use of similarity transformations and handled using the bvp4c numerical technique. To depict the relationship between the profiles and the parameters, graphs, and tables are illustrated. The significant outcome revealed that the fluid temperature rises in the scenario of both PST and PHF cases. In addition, the heat transfer efficiency of the hybrid nanoliquid is far ahead of the nanofluid flow. The truthfulness of the envisioned model in the limiting scenario is also given.", 
    "abstract": "Despite the importance of patient engagement in health care decision-making in the care of patients with chronic diseases, there is limited information about it and the factors affecting it in Ethiopia and in the Public Hospitals of West Shoa in particular. Thus, this study was designed to assess the engagement of patients with selected chronic non-communicable diseases in health care decision-making and associated factors in public hospitals of West Shoa Zone, Oromia, Ethiopia.\nWe used an institution -based cross-sectional study design. We used systematic sampling for the selection of study participants from June 7-July 26, 2020. Standardized, pretested, and structured Patient Activation Measure was used to measure patient engagement in healthcare decision-making. We did descriptive analysis to determine the magnitude of patient engagement in health care decision-making. Multivariate logistic regression analysis was used to determine factors associated with patients' engagement in the health care decision-making process. Adjusted odds ratio with a 95% confidence interval was calculated to measure the strength of association. We declared statistical significance at p<0.05. we presented the results in tables and graphs.\n406 patients with chronic diseases took part in the study, yielding a response rate of 96.2%. Less than a fifth [19.5% (95% CI: 15.5, 23.6)] of participants in the study area had a high engagement in their health care decision-making. Educational level (college or above) [AOR = 5.2, 95% CI (1.76-15.46)], duration of diagnosis >5 years [AOR = 1.8, 95% CI (1.03-3.2)], health literacy [AOR = 1.15, 95% CI (1.06-1.24)], autonomy preference in decision making [AOR = 1.35, 95% CI (1.03-1.96)] were factors significantly associated with participants' engagement in health care decision making among patients with chronic diseases.\nA high number of respondents had a low engagement in their health care decision-making. Preference for autonomy in decision making, educational level, health literacy, duration of diagnosis with the disease were factors associated with patient engagement in health care decision making among patients with chronic diseases in the study area. Thus, patients should be empowered to be involved in decision making to increase their engagement in the care.", 
    "abstract": "Due to the homophily assumption in graph convolution networks (GCNs), a common consensus in the graph node classification task is that graph neural networks (GNNs) perform well on homophilic graphs but may fail on heterophilic graphs with many interclass edges. However, the previous interclass edges' perspective and related homo-ratio metrics cannot well explain the GNNs' performance under some heterophilic datasets, which implies that not all the interclass edges are harmful to GNNs. In this work, we propose a new metric based on the von Neumann entropy to reexamine the heterophily problem of GNNs and investigate the feature aggregation of interclass edges from an entire neighbor identifiable perspective. Moreover, we propose a simple yet effective Conv-Agnostic GNN framework (CAGNNs) to enhance the performance of most GNNs on the heterophily datasets by learning the neighbor effect for each node. Specifically, we first decouple the feature of each node into the discriminative feature for downstream tasks and the aggregation feature for graph convolution (GC). Then, we propose a shared mixer module to adaptively evaluate the neighbor effect of each node to incorporate the neighbor information. The proposed framework can be regarded as a plug-in component and is compatible with most GNNs. The experimental results over nine well-known benchmark datasets indicate that our framework can significantly improve performance, especially for the heterophily graphs. The average performance gain is 9.81%, 25.81%, and 20.61% compared with graph isomorphism network (GIN), graph attention network (GAT), and GCN, respectively. Extensive ablation studies and robustness analysis further verify the effectiveness, robustness, and interpretability of our framework. Code is available at https://github.com/JC-202/CAGNN.", 
    "abstract": "Urgent care (UC) clinicians frequently prescribe inappropriate antibiotics for upper respiratory illnesses. In a national survey, pediatric UC clinicians reported family expectations as a primary driver for prescribing inappropriate antibiotics. Communication strategies effectively reduce unnecessary antibiotics while increasing family satisfaction. We aimed to reduce inappropriate prescribing practices in otitis media with effusion (OME), acute otitis media (AOM), and pharyngitis in pediatric UC clinics by a relative 20% within 6 months using evidence-based communication strategies.\nWe recruited participants via e-mails, newsletters, and Webinars from pediatric and UC national societies. We defined antibiotic-prescribing appropriateness based on consensus guidelines. Family advisors and UC pediatricians developed script templates based on an evidence-based strategy. Participants submitted data electronically. We reported data using line graphs and shared deidentified data during monthly Webinars. We used \u03c72 tests to evaluate change in appropriateness at the beginning and end of the study period.\nThe 104 participants from 14 institutions submitted 1183 encounters for analysis in the intervention cycles. Using a strict definition of inappropriateness, overall inappropriate antibiotic prescriptions for all diagnoses trended downward from 26.4% to 16.6% (P = 0.13). Inappropriate prescriptions trended upward in OME from 30.8% to 46.7% (P = 0.34) with clinicians' increased use of \"watch and wait\" for this diagnosis. Inappropriate prescribing for AOM and pharyngitis improved from 38.6% to 26.5% (P = 0.03) and 14.5% to 8.8% (P = 0.44), respectively.\nUsing templates to standardize communication with caregivers, a national collaborative decreased inappropriate antibiotic prescriptions for AOM and had downward trend in inappropriate antibiotic prescriptions for pharyngitis. Clinicians increased the inappropriate use of \"watch and wait\" antibiotics for OME. Future studies should evaluate barriers to the appropriate use of delayed antibiotic prescriptions.", 
    "abstract": "Graph data models are an emerging approach to structure clinical and biomedical information. These models offer intriguing opportunities for novel approaches in healthcare, such as disease phenotyping, risk prediction, and personalized precision care. The combination of data and information in a graph model to create knowledge graphs has rapidly expanded in biomedical research, but the integration of real-world data from the electronic health record has been limited. To broadly apply knowledge graphs to EHR and other real-world data, a deeper understanding of how to represent these data in a standardized graph model is needed. We provide an overview of the state-of-the-art research for clinical and biomedical data integration and summarize the potential to accelerate healthcare and precision medicine research through insight generation from integrated knowledge graphs.", 
    "abstract": "Information regarding serum insulin concentration in dogs newly diagnosed with insulinoma and its association with clinical stage and survival time is lacking.\nExamine association between serum insulin concentration and survival and clinical disease stage in dogs with insulinoma.\nFifty-nine client-owned dogs with a diagnosis of insulinoma from 2 referral hospitals.\nRetrospective observational study. The \u03c7\nMedian serum insulin concentration was 33 mIU/L (range, 8-200 mIU/L) in dogs with World Health Organization (WHO) stage I disease and 45 mIU/L (range, 12-213 mIU/L) in dogs with WHO stage II and III disease. No difference was found in the proportion of dogs with increased insulin concentration with or without metastasis (P\u2009=\u2009.09). No association was identified between insulin concentration and survival (P\u2009=\u2009.63), and between dogs grouped by insulin concentration and survival (P\u2009=\u2009.51).\nSerum insulin concentrations were not different between dogs with or without metastasis at diagnosis. The degree of insulinemia does not provide further information regarding the stage of the disease and is not associated with survival time in dogs with insulinoma.", 
    "abstract": "Contact structure among livestock populations influences the transmission of infectious agents among them. Models simulating realistic contact networks therefore have important applications for generating insights relevant to livestock diseases. This systematic review identifies and compares such models, their applications, data sources and how their validity was assessed. From 52 publications, 37 models were identified comprising seven model frameworks. These included mathematical models (", 
    "abstract": "Healthcare facilitation, an implementation strategy designed to improve the uptake of effective clinical innovations in routine practice, has produced promising yet mixed results in randomized implementation trials and has not been fully researched across different contexts.\nUsing mechanism mapping, which applies directed acyclic graphs that decompose an effect of interest into hypothesized causal steps and mechanisms, we propose a more concrete description of how healthcare facilitation works to inform its further study as a meta-implementation strategy.\nUsing a modified Delphi consensus process, co-authors developed the mechanistic map based on a three-step process. First, they developed an initial logic model by collectively reviewing the literature and identifying the most relevant studies of healthcare facilitation components and mechanisms to date. Second, they applied the logic model to write vignettes describing how facilitation worked (or did not) based on recent empirical trials that were selected via consensus for inclusion and diversity in contextual settings (US, international sites). Finally, the mechanistic map was created based on the collective findings from the vignettes.\nTheory-based healthcare facilitation components informing the mechanistic map included staff engagement, role clarification, coalition-building through peer experiences and identifying champions, capacity-building through problem solving barriers, and organizational ownership of the implementation process. Across the vignettes, engagement of leaders and practitioners led to increased socialization of the facilitator's role in the organization. This in turn led to clarifying of roles and responsibilities among practitioners and identifying peer experiences led to increased coherence and sense-making of the value of adopting effective innovations. Increased trust develops across leadership and practitioners through expanded capacity in adoption of the effective innovation by identifying opportunities that mitigated barriers to practice change. Finally, these mechanisms led to eventual normalization and ownership of the effective innovation and healthcare facilitation process.\nMapping methodology provides a novel perspective of mechanisms of healthcare facilitation, notably how sensemaking, trust, and normalization contribute to quality improvement. This method may also enable more efficient and impactful hypothesis-testing and application of complex implementation strategies, with high relevance for lower-resourced settings, to inform effective innovation uptake.", 
    "abstract": "Finding drugs that can interact with a specific target to induce a desired therapeutic outcome is key deliverable in drug discovery for targeted treatment. Therefore, both identifying new drug-target links, as well as delineating the type of drug interaction, are important in drug repurposing studies.\nA computational drug repurposing approach was proposed to predict novel drug-target interactions (DTIs), as well as to predict the type of interaction induced. The methodology is based on mining a heterogeneous graph that integrates drug-drug and protein-protein similarity networks, together with verified drug-disease and protein-disease associations. In order to extract appropriate features, the three-layer heterogeneous graph was mapped to low dimensional vectors using node embedding principles. The DTI prediction problem was formulated as a multi-label, multi-class classification task, aiming to determine drug modes of action. DTIs were defined by concatenating pairs of drug and target vectors extracted from graph embedding, which were used as input to classification via gradient boosted trees, where a model is trained to predict the type of interaction. After validating the prediction ability of DT2Vec+, a comprehensive analysis of all unknown DTIs was conducted to predict the degree and type of interaction. Finally, the model was applied to propose potential approved drugs to target cancer-specific biomarkers.\nDT2Vec+ showed promising results in predicting type of DTI, which was achieved via integrating and mapping triplet drug-target-disease association graphs into low-dimensional dense vectors. To our knowledge, this is the first approach that addresses prediction between drugs and targets across six interaction types.", 
    "abstract": "A hybrid quantum-classical method for learning Boltzmann machines (BM) for a generative and discriminative task is presented. BM are undirected graphs with a network of visible and hidden nodes where the former is used as the reading site. In contrast, the latter is used to manipulate visible states' probability. In Generative BM, the samples of visible data imitate the probability distribution of a given data set. In contrast, the visible sites of discriminative BM are treated as Input/Output (I/O) reading sites where the conditional probability of output state is optimized for a given set of input states. The cost function for learning BM is defined as a weighted sum of Kullback-Leibler (KL) divergence and Negative conditional Log-likelihood (NCLL), adjusted using a hyper-parameter. Here, the KL Divergence is the cost for generative learning, and NCLL is the cost for discriminative learning. A Stochastic Newton-Raphson optimization scheme is presented. The gradients and the Hessians are approximated using direct samples of BM obtained through quantum annealing. Quantum annealers are hardware representing the physics of the Ising model that operates on low but finite temperatures. This temperature affects the probability distribution of the BM; however, its value is unknown. Previous efforts have focused on estimating this unknown temperature through regression of theoretical Boltzmann energies of sampled states with the probability of states sampled by the actual hardware. These approaches assume that the control parameter change does not affect the system temperature; however, this is usually untrue. Instead of using energies, the probability distribution of samples is employed to estimate the optimal parameter set, ensuring that the optimal set can be obtained from a single set of samples. The KL divergence and NCLL are optimized for the system temperature, and the result is used to rescale the control parameter set. The performance of this approach, as tested against the theoretically expected distributions, shows promising results for Boltzmann training on quantum annealers.", 
    "abstract": "Graphical modeling of multivariate functional data is becoming increasingly important in a wide variety of applications. The changes of graph structure can often be attributed to external variables, such as the diagnosis status or time, the latter of which gives rise to the problem of dynamic graphical modeling. Most existing methods focus on estimating the graph by aggregating samples, but largely ignore the subject-level heterogeneity due to the external variables. In this article, we introduce a conditional graphical model for multivariate random functions, where we treat the external variables as conditioning set, and allow the graph structure to vary with the external variables. Our method is built on two new linear operators, the conditional precision operator and the conditional partial correlation operator, which extend the precision matrix and the partial correlation matrix to both the conditional and functional settings. We show that their nonzero elements can be used to characterize the conditional graphs, and develop the corresponding estimators. We establish the uniform convergence of the proposed estimators and the consistency of the estimated graph, while allowing the graph size to grow with the sample size, and accommodating both completely and partially observed data. We demonstrate the efficacy of the method through both simulations and a study of brain functional connectivity network.", 
    "abstract": "This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as ", 
    "abstract": "Collecting complete network data is expensive, time-consuming, and often infeasible. Aggregated Relational Data (ARD), which ask respondents questions of the form \"How many people with trait X do you know?\" provide a low-cost option when collecting complete network data is not possible. Rather than asking about connections between each pair of individuals directly, ARD collect the number of contacts the respondent knows with a given trait. Despite widespread use and a growing literature on ARD methodology, there is still no systematic understanding of when and why ARD should accurately recover features of the unobserved network. This paper provides such a characterization by deriving conditions under which statistics about the unobserved network (or functions of these statistics like regression coefficients) can be consistently estimated using ARD. We first provide consistent estimates of network model parameters for three commonly used probabilistic models: the beta-model with node-specific unobserved effects, the stochastic block model with unobserved community structure, and latent geometric space models with unobserved latent locations. A key observation is that cross-group link probabilities for a collection of (possibly unobserved) groups identify the model parameters, meaning ARD are sufficient for parameter estimation. With these estimated parameters, it is possible to simulate graphs from the fitted distribution and analyze the distribution of network statistics. We can then characterize conditions under which the simulated networks based on ARD will allow for consistent estimation of the unobserved network statistics, such as eigenvector centrality, or response functions by or of the unobserved network, such as regression coefficients.", 
    "abstract": "A basic parameter in non-equilibrium thermodynamics is the production of entropy (S-entropy), which is a consequence of the irreversible processes of mass, charge, energy, and momentum transport in various systems. The product of S-entropy production and absolute temperature (T) is called the dissipation function and is a measure of energy dissipation in non-equilibrium processes.\nThis study aimed to estimate energy conversion in membrane transport processes of homogeneous non-electrolyte solutions. The stimulus version of the R, L, H, and P equations for the intensity of the entropy source achieved this purpose.\nThe transport parameters for aqueous glucose solutions through Nephrophan\u00ae and Ultra-Flo 145 dialyser\u00ae synthetic polymer biomembranes were experimentally determined. Kedem-Katchalsky-Peusner (KKP) formalism was used for binary solutions of non-electrolytes, with Peusner coefficients introduced.\nThe R, L, H, and P versions of the equations for the S-energy dissipation were derived for the membrane systems based on the linear non-equilibrium Onsager and Peusner network thermodynamics. Using the equations for the S-energy and the energy conversion efficiency factor, equations for F-energy and U-energy were derived. The S-energy, F-energy and U-energy were calculated as functions of osmotic pressure difference using the equations obtained and presented as suitable graphs.\nThe R, L, H, and P versions of the equations describing the dissipation function had the form of second-degree equations. Meanwhile, the S-energy characteristics had the form of second-degree curves located in the 1st and 2nd quadrants of the coordinate system. These findings indicate that the R, L, H, and P versions of S-energy, F-energy and U-energy are not equivalent for the Nephrophan\u00ae and Ultra-Flo 145 dialyser\u00ae membranes.", 
    "abstract": "Knowledge graphs as external information has become one of the mainstream directions of current recommendation systems. Various knowledge-graph-representation methods have been proposed to promote the development of knowledge graphs in related fields. Knowledge-graph-embedding methods can learn entity information and complex relationships between the entities in knowledge graphs. Furthermore, recently proposed graph neural networks can learn higher-order representations of entities and relationships in knowledge graphs. Therefore, the complete presentation in the knowledge graph enriches the item information and alleviates the cold start of the recommendation process and too-sparse data. However, the knowledge graph's entire entity and relation representation in personalized recommendation tasks will introduce unnecessary noise information for different users. To learn the entity-relationship presentation in the knowledge graph while effectively removing noise information, we innovatively propose a model named knowledge-enhanced hierarchical graph capsule network (KHGCN), which can extract node embeddings in graphs while learning the hierarchical structure of graphs. Our model eliminates noisy entities and relationship representations in the knowledge graph by the entity disentangling for the recommendation and introduces the attentive mechanism to strengthen the knowledge-graph aggregation. Our model learns the presentation of entity relationships by an original graph capsule network. The capsule neural networks represent the structured information between the entities more completely. We validate the proposed model on real-world datasets, and the validation results demonstrate the model's effectiveness.", 
    "abstract": "Temporal knowledge graphs (KGs) have recently attracted increasing attention. The temporal KG forecasting task, which plays a crucial role in such applications as event prediction, predicts future links based on historical facts. However, current studies pay scant attention to the following two aspects. First, the interpretability of current models is manifested in providing reasoning paths, which is an essential property of path-based models. However, the comparison of reasoning paths in these models is operated in a black-box fashion. Moreover, contemporary models utilize separate networks to evaluate paths at different hops. Although the network for each hop has the same architecture, each network achieves different parameters for better performance. Different parameters cause identical semantics to have different scores, so models cannot measure identical semantics at different hops equally. Inspired by the observation that reasoning based on multi-hop paths is akin to answering questions step by step, this paper designs an Interpretable Multi-Hop Reasoning (IMR) framework based on consistent basic models for temporal KG forecasting. IMR transforms reasoning based on path searching into stepwise question answering. In addition, IMR develops three indicators according to the characteristics of temporal KGs and reasoning paths: the question matching degree, answer completion level, and path confidence. IMR can uniformly integrate paths of different hops according to the same criteria; IMR can provide the reasoning paths similarly to other interpretable models and further explain the basis for path comparison. We instantiate the framework based on common embedding models such as TransE, RotatE, and ComplEx. While being more explainable, these instantiated models achieve state-of-the-art performance against previous models on four baseline datasets.", 
    "abstract": "Entropy generation in peristaltic transport of hybrid nanofluid possessing temperature-dependent thermal conductivity through a two-dimensional vertical channel is studied in this paper. The hybrid nanofluid consists of multi-walled carbon nanotubes mixed with zinc oxide suspended in engine oil. Flow is affected by a uniform external magnetic field, hence generating Lorentz force, Hall and heating effects. Given the vertical orientation of the channel, the analysis accounts for mixed convection. To study heat transfer in the current flow configuration, the model considers phenomena such as viscous dissipation, heat generation or absorption, and thermal radiation. The mathematical modeling process employs the lubrication approach and Galilean transformation for enhanced accuracy. The slip condition for the velocity and convective conditions for the temperature are considered at the boundaries. The study analyzes entropy generation using the Homotopy Analysis Method (HAM) and includes convergence curves for HAM solutions. Results are presented using graphs and bar charts. The analysis shows that higher Brinkman and thermal radiation parameters result in higher temperatures, while higher thermal conductivity parameters lead to reduced entropy generation and temperature profile. Additionally, higher Hall parameter values decrease entropy generation, while an increased Hartman number improves entropy generation.", 
    "abstract": "Universal Causality is a mathematical framework based on higher-order category theory, which generalizes previous approaches based on directed graphs and regular categories. We present a hierarchical framework called UCLA (Universal Causality Layered Architecture), where at the top-most level, causal interventions are modeled as a higher-order category over simplicial sets and objects. Simplicial sets are contravariant functors from the category of ordinal numbers \u0394 into sets, and whose morphisms are order-preserving injections and surjections over finite ordered sets. Non-random interventions on causal structures are modeled as face operators that map ", 
    "abstract": "Knowledge about the interactions between dietary and biomedical factors is scattered throughout uncountable research articles in an unstructured form (e.g., text, images, etc.) and requires automatic structuring so that it can be provided to medical professionals in a suitable format. Various biomedical knowledge graphs exist, however, they require further extension with relations between food and biomedical entities. In this study, we evaluate the performance of three state-of-the-art relation-mining pipelines (FooDis, FoodChem and ChemDis) which extract relations between food, chemical and disease entities from textual data. We perform two case studies, where relations were automatically extracted by the pipelines and validated by domain experts. The results show that the pipelines can extract relations with an average precision around 70%, making new discoveries available to domain experts with reduced human effort, since the domain experts should only evaluate the results, instead of finding, and reading all new scientific papers.", 
    "abstract": "This research analyzes the three-dimensional magneto hydrodynamic nanofluid flow through chemical reaction and thermal radiation above the dual stretching surface in the presence of an inclined magnetic field. Different rotational nanofluid and hybrid nanofluids with constant angular velocity [Formula: see text] for this comparative study are considered. The constitutive relations are used to gain the equations of motion, energy, and concentration. This flow governing extremely non-linear equations cannot be handled by an analytical solution. So, these equations are transformed into ordinary differential equalities by using the similarity transformation and then handled in MATLAB by applying the boundary values problem practice. The outcomes for the considered problem are accessed through tables and graphs for different parameters. A maximum heat transfer amount is observed in the absence of thermal radiation and when the inclined magnetic field and axis of rotation are parallel.", 
    "abstract": "Protecting medical privacy can create obstacles in the analysis and distribution of healthcare graphs and statistical inferences accompanying them. We pose a graph simulation model which generates networks using degree and property augmentation and provide a flexible R package that allows users to create graphs that preserve vertex attribute relationships and approximating the retention of topological properties observed in the original graph (e.g., community structure). We illustrate our proposed algorithm using a case study based on Zachary's karate network and a patient-sharing graph generated from Medicare claims data in 2019. In both cases, we find that community structure is preserved, and normalized root mean square error between cumulative distributions of the degrees across the generated and the original graphs is low (0.0508 and 0.0514 respectively).", 
    "abstract": "Identifying promising targets is a critical step in modern drug discovery, with causative genes of diseases that are an important source of successful targets. Previous studies have found that the pathogeneses of various diseases are closely related to the evolutionary events of organisms. Accordingly, evolutionary knowledge can facilitate the prediction of causative genes and further accelerate target identification. With the development of modern biotechnology, massive biomedical data have been accumulated, and knowledge graphs (KGs) have emerged as a powerful approach for integrating and utilizing vast amounts of data. In this study, we constructed an evolution-strengthened knowledge graph (ESKG) and validated applications of ESKG in the identification of causative genes. More importantly, we developed an ESKG-based machine learning model named GraphEvo, which can effectively predict the targetability and the druggability of genes. We further investigated the explainability of the ESKG in druggability prediction by dissecting the evolutionary hallmarks of successful targets. Our study highlights the importance of evolutionary knowledge in biomedical research and demonstrates the potential power of ESKG in promising target identification. The data set of ESKG and the code of GraphEvo can be downloaded from https://github.com/Zhankun-Xiong/GraphEvo.", 
    "abstract": "Posterior urethral valves (PUV) is a congenital disorder causing an obstruction of the lower urinary tract that affects approximately 1 in 4,000 male live births. PUV is considered a multifactorial disorder, meaning that both genetic and environmental factors are involved in its development. We investigated maternal risk factors for PUV.\nWe included 407 PUV patients and 814 controls matched on year of birth from the AGORA data- and biobank and three participating hospitals. Information on potential risk factors (family history of congenital anomalies of the kidney and urinary tract (CAKUT), season of conception, gravidity, subfertility, and conception using assisted reproductive techniques (ART), plus maternal age, body mass index, diabetes, hypertension, smoking, and use of alcohol and folic acid) was derived from maternal questionnaires. After multiple imputation, adjusted odds ratios (aORs) were estimated using conditional logistic regression corrected for minimally sufficient sets of confounders determined using directed acyclic graphs.\nA positive family history and low maternal age (<25 years) were associated with PUV development [aORs: 3.3 and 1.7 with 95% confidence intervals (95% CI) 1.4-7.7 and 1.0-2.8, respectively], whereas higher maternal age (>35 years) was associated with a lower risk (aOR: 0.7 95% CI: 0.4-1.0). Maternal preexisting hypertension seemed to increase PUV risk (aOR: 2.1 95% CI: 0.9-5.1), while gestational hypertension seemed to decrease this risk (aOR: 0.6 95% CI: 0.3-1.0). Concerning use of ART, the aORs for the different techniques were all above one, but with very wide 95% CIs including one. None of the other factors studied were associated with PUV development.\nOur study showed that family history of CAKUT, low maternal age, and potentially preexisting hypertension were associated with PUV development, whereas higher maternal age and gestational hypertension seemed to be associated with a lower risk. Maternal age and hypertension as well as the possible role of ART in the development of PUV require further research.", 
    "abstract": "To examine the reasonable duration of continuous electrocardiographic monitoring (CEM) to detect AF at acute ischemic stroke.\n811 consecutive patients admitted to Tsuruga Municipal Hospital by acute ischemic stroke between April 2013 and December 2021 were enrolled in this study. Excluding 78 patients, 733 patients were analyzed by cluster analysis with SurvCART algorithm, followed by Kaplan-Meier analysis.\nThe analysis provided step graphs for 8 subgroups. The duration of CEM to achieve the sensitivity of 0.8, 0.9, and 0.95 in each could be calculated. The duration of CEM to achieve the sensitivity of 0.8 are 18 days in female patients with heart failure (HF) (subgroup 1), 24 days in male patients with HF (subgroup 2), 22 days in patients without HF with arterial occlusion and pulse rate (PR) more than 91 (subgroup 3), 24 days in patients without HF with occlusion with PR less than 91 (subgroup 4), 18 days in patients without HF without occlusion with lacuna (subgroup 5), 26 days in patients without HF, occlusion, and lacuna, with arterial stenosis (subgroup 6), 15 days in patients without HF, occlusion, lacuna, and stenosis with BMI more than 21%(subgroup 7), and 44 days in patients without HF, occlusion, lacuna, stenosis and with BMI less than 21% (subgroup 8).\nDuration of CEM with the sensitivity of 0.8, 0.9, and 0.95 could be determined by presence of HF, female sex, arterial occlusion, PR more than 91/minute, presence of lacuna, presence of stenosis, and BMI more than 21%. (250).", 
    "abstract": "To accurately predict molecular properties, it is important to learn expressive molecular representations. Graph neural networks (GNNs) have made significant advances in this area, but they often face limitations like neighbors-explosion, under-reaching, oversmoothing, and oversquashing. Additionally, GNNs tend to have high computational costs due to their large number of parameters. These limitations emerge or increase when dealing with larger graphs or deeper GNN models. One potential solution is to simplify the molecular graph into a smaller, richer, and more informative one that is easier to train GNNs. Our proposed molecular graph coarsening framework called ", 
    "abstract": "Graph contrastive learning has been developed to learn discriminative node representations on homogeneous graphs. However, it is not clear how to augment the heterogeneous graphs without substantially altering the underlying semantics or how to design appropriate pretext tasks to fully capture the rich semantics preserved in heterogeneous information networks (HINs). Moreover, early investigations demonstrate that contrastive learning suffer from sampling bias, whereas conventional debiasing techniques (e.g., hard negative mining) are empirically shown to be inadequate for graph contrastive learning. How to mitigate the sampling bias on heterogeneous graphs is another important yet neglected problem. To address the aforementioned challenges, we propose a novel multi-view heterogeneous graph contrastive learning framework in this paper. We use metapaths, each of which depicts a complementary element of HINs, as the augmentation to generate multiple subgraphs (i.e., multi-views), and propose a novel pretext task to maximize the coherence between each pair of metapath-induced views. Furthermore, we employ a positive sampling strategy to explicitly select hard positives by jointly considering semantics and structures preserved on each metapath view to alleviate the sampling bias. Extensive experiments demonstrate MCL consistently outperforms state-of-the-art baselines on five real-world benchmark datasets and even its supervised counterparts in some settings.", 
    "abstract": "The Coronavirus disease 2019 (COVID-19) pandemic has strained many healthcare systems. Google Trends is a tool that provides information on online interest in selected keywords and topics over time. The purpose of this study is to describe the effect of the COVID-19 pandemic on online interest in elective shoulder pathology. Online search pattern data were obtained via Google Trends from November 2019 to November 2020 using the search terms 'orthopedic surgery' and 'shoulder pathology' search terms. Relative search volume index (SVI) graphs were generated from this data and the 7-day average of new COVID-19 cases in the United States. Orthopaedic surgery and shoulder pathology search trends decreased during March 2020 with a sudden rise in the 7-day average of new COVID-19 cases. After March 2020, orthopaedic surgery and shoulder pathology search terms approached pre-COVID-19 pandemic values despite continued increases in the 7-day average of new COVID-19 cases. (Journal of Surgical Orthopaedic Advances 32(1):014-016, 2023).", 
    "abstract": "Photometry approaches detect sensor-mediated changes in fluorescence as a proxy for rapid molecular changes within the brain. As a flexible technique with a relatively low cost to implement, photometry is rapidly being incorporated into neuroscience laboratories. Yet, although multiple data acquisition systems for photometry now exist, robust analytical pipelines for the resulting data remain limited. Here we present the Photometry Analysis Toolkit (PhAT)-a free open-source analysis pipeline that provides options for signal normalization, incorporation of multiple data streams to align photometry data with behavior and other events, calculation of event-related changes in fluorescence, and comparison of similarity across fluorescent traces. A graphical user interface (GUI) enables use of this software without prior coding knowledge. In addition to providing foundational analytical tools, PhAT is designed to readily incorporate community-driven development of new modules for more bespoke analyses, and enables data to be easily exported to enable subsequent statistical testing and/or code-based analyses. In addition, we provide recommendations regarding technical aspects of photometry experiments, including sensor selection and validation, reference signal considerations, and best practices for experimental design and data collection. We hope that the distribution of this software and protocols will lower the barrier to entry for new photometry users and improve the quality of collected data, increasing transparency and reproducibility in photometry analyses. \u00a9 2023 Wiley Periodicals LLC. Basic Protocol 1: Software and environment installation Alternate Protocol 1: Software and environment update Basic Protocol 2: GUI-driven fiber photometry analysis Support Protocol 1: Examining signal quality Support Protocol 2: Interacting with graphs Basic Protocol 3: Adding modules to PhAT Alternate Protocol 2: Creating functions for use in Jupyter Notebook.", 
    "abstract": "Carcinogenicity assessment of any compound is a laborious and expensive exercise with several associated ethical and practical concerns. While artificial intelligence (AI) offers promising solutions, unfortunately, it is contingent on several challenges concerning the inadequacy of available experimentally validated (non)carcinogen datasets and variabilities within bioassays, which contribute to the compromised model training. Existing AI solutions that leverage classical chemistry-driven descriptors do not provide adequate biological interpretability involved in imparting carcinogenicity. This highlights the urgency to devise alternative AI strategies. We propose multiple strategies, including implementing data-driven (integrated databases) and known carcinogen-characteristic-derived features to overcome these apparent shortcomings. In summary, these next-generation approaches will continue facilitating robust chemical carcinogenicity prediction, concomitant with deeper mechanistic insights.", 
    "abstract": "Recently, there has been tremendous interest in developing graph-based subspace clustering in high-dimensional data, which does not require a priori knowledge of the number of dimensions and subspaces. The general steps of such algorithms are dictionary representation and spectral clustering. Traditional methods use the dataset itself as a dictionary when performing dictionary representation. There are some limitations that the redundant information present in the dictionary and features may make the constructed graph structure unclear and require post-processing to obtain labels. To address these problems, we propose a novel subspace clustering model that first introduces feature selection to process the input data, randomly selects some samples to construct a dictionary to remove redundant information and learns the optimal bipartite graph with K-connected components under the constraint of the (normalized) Laplacian rank. Finally, the labels are obtained directly from the graphs. The experimental results on motion segmentation and face recognition datasets demonstrate the superior effectiveness and stability of our algorithm.", 
    "abstract": "Unit square visibility graphs (USV) are described by axis-parallel visibility between unit squares placed in the plane. If the squares are required to be placed on integer grid coordinates, then USV become unit square grid visibility graphs (USGV), an alternative characterisation of the well-known rectilinear graphs. We extend known combinatorial results for USGV and we show that, in the weak case (i.e., visibilities do not necessarily translate into edges of the represented combinatorial graph), the area minimisation variant of their recognition problem is ", 
    "abstract": "The aim of the study was to determine reference graphs of power spectral density functions of forearm physiological tremor and to compare their parameters in the male and female population of young athletes from various sports. One hundred fifty-nine (159) female (15.7 \u00b1 2.1 years, 59.8 \u00b1 8.1 kg, 169.1 \u00b1 7.5 cm) and 276 male (16.4 \u00b1 1.9 years 72.7 \u00b1 10.3 kg and 180.9 \u00b1 8.7 cm) youth athletes participated in the study. Forearm tremor was measured accelerometrically in a sitting position. Power spectrum density (PSD) function was calculated for each individual tremor waveform. Because of right skewness of power distribution, the PSD functions were subjected to logarithmic transformation. Average log-powers in low (2-4 Hz) and high (8-14 Hz) frequency ranges and mean frequencies in those ranges were analyzed. Tremor log-powers for male were greater than for female athletes (p < 0.001), while frequencies of spectrum maxima did not differ from each other. Frequencies of spectrum maxima correlated (p < 0.001) with age (r = 0.277 and 0.326 for males and females, respectively). The obtained reference functions may be utilized in order to quantify and assess tremor size and its changes evoked by stress and fatigue, which can be applied for selection and training monitoring in sports, but also in medicine for detection and diagnosis of pathologic tremor in young individuals.", 
    "abstract": "Over the last decade, random hyperbolic graphs have proved successful in providing geometric explanations for many key properties of real-world networks, including strong clustering, high navigability, and heterogeneous degree distributions. These properties are ubiquitous in systems as varied as the internet, transportation, brain or epidemic networks, which are thus unified under the hyperbolic network interpretation on a surface of constant negative curvature. Although a few studies have shown that hyperbolic models can generate community structures, another salient feature observed in real networks, we argue that the current models are overlooking the choice of the latent space dimensionality that is required to adequately represent clustered networked data. We show that there is an important qualitative difference between the lowest-dimensional model and its higher-dimensional counterparts with respect to how similarity between nodes restricts connection probabilities. Since more dimensions also increase the number of nearest neighbors for angular clusters representing communities, considering only one more dimension allows us to generate more realistic and diverse community structures.", 
    "abstract": "In soccer, quantitatively evaluating the performance of players and teams is essential to improve tactical coaching and players' decision-making abilities. To achieve this, some methods use predicted probabilities of shoot event occurrences to quantify player performances, but conventional shoot prediction models have not performed well and have failed to consider the reliability of the event probability. This paper proposes a novel method that effectively utilizes players' spatio-temporal relations and prediction uncertainty to predict shoot event occurrences with greater accuracy and robustness. Specifically, we represent players' relations as a complete bipartite graph, which effectively incorporates soccer domain knowledge, and capture latent features by applying a graph convolutional recurrent neural network (GCRNN) to the constructed graph. Our model utilizes a Bayesian neural network to predict the probability of shoot event occurrence, considering spatio-temporal relations between players and prediction uncertainty. In our experiments, we confirmed that the proposed method outperformed several other methods in terms of prediction performance, and we found that considering players' distances significantly affects the prediction accuracy.", 
    "abstract": "Andrological diseases have an important social and economic impact as they cause a serious impairment of the quality of life of the affected patient. Epidemiologically, the impact of these disorders is progressively increasing, as demonstrated by the ever-growing prevalence of male infertility. This evidence justifies the rapid development of research in andrology that the scientific community has undertaken in recent decades. This study aims to evaluate the productivity index of the main andrological topics studied and reported in the literature.\nThe total number of published articles was extracted from the Scopus database by entering the following keywords and mesh terms: \"Male Infertility\", \"Erectile Dysfunction\", \"Premature Ejaculation\", \"Male Hypogonadism\", \"Testicular Tumors\", \"Prostate Cancer\", \"Prostatic hyperplasia\", \"Prostate hyperplasia\", \"Prostatitis\", \"Prostate inflammation\", and \"Male Accessory Gland Infections\". Furthermore, a list of the top 50 researchers sorted by productivity was created for each topic. For male infertility, a further search was performed by combining the search term \"male infertility\" with the above-mentioned terms. Thus, a list of the top 30 authors in order of productivity was also extracted. The graphs were created using Excel.\nAs could be expected, we observed that prostate cancer and male infertility were the two most investigated topics, followed by benign prostatic hyperplasia and erectile dysfunction, whose prevalence is set to increase given the progressive aging of the population. Less investigated is the inflammation of the accessory sexual glands. In conclusion, this study provides a ranking of the main andrological topics investigated in the literature, also presenting the top list of the most productive authors for each one.", 
    "abstract": "The accurate prediction of drug-target binding affinity (DTA) is an essential step in drug discovery and drug repositioning. Although deep learning methods have been widely adopted for DTA prediction, the complexity of extracting drug and target protein features hampers the accuracy of these predictions. In this study, we propose a novel model for DTA prediction named MSGNN-DTA, which leverages a fused multi-scale topological feature approach based on graph neural networks (GNNs). To address the challenge of accurately extracting drug and target protein features, we introduce a gated skip-connection mechanism during the feature learning process to fuse multi-scale topological features, resulting in information-rich representations of drugs and proteins. Our approach constructs drug atom graphs, motif graphs, and weighted protein graphs to fully extract topological information and provide a comprehensive understanding of underlying molecular interactions from multiple perspectives. Experimental results on two benchmark datasets demonstrate that MSGNN-DTA outperforms the state-of-the-art models in all evaluation metrics, showcasing the effectiveness of the proposed approach. Moreover, the study conducts a case study based on already FDA-approved drugs in the DrugBank dataset to highlight the potential of the MSGNN-DTA framework in identifying drug candidates for specific targets, which could accelerate the process of virtual screening and drug repositioning.", 
    "abstract": "Many research questions concern treatment effects on outcomes that can recur several times in the same individual. For example, medical researchers are interested in treatment effects on hospitalizations in heart failure patients and sports injuries in athletes. Competing events, such as death, complicate causal inference in studies of recurrent events because once a competing event occurs, an individual cannot have more recurrent events. Several statistical estimands have been studied in recurrent event settings, with and without competing events. However, the causal interpretations of these estimands, and the conditions that are required to identify these estimands from observed data, have yet to be formalized. Here we use a formal framework for causal inference to formulate several causal estimands in recurrent event settings, with and without competing events. When competing events exist, we clarify when commonly used classical statistical estimands can be interpreted as causal quantities from the causal mediation literature, such as (controlled) direct effects and total effects. Furthermore, we show that recent results on interventionist mediation estimands allow us to define new causal estimands with recurrent and competing events that may be of particular clinical relevance in many subject matter settings. We use causal directed acyclic graphs and single world intervention graphs to illustrate how to reason about identification conditions for the various causal estimands based on subject matter knowledge. Furthermore, using results on counting processes, we show that our causal estimands and their identification conditions, which are articulated in discrete time, converge to classical continuous time counterparts in the limit of fine discretizations of time. We propose estimators and establish their consistency for the various identifying functionals. Finally, we use the proposed estimators to compute the effect of blood pressure lowering treatment on the recurrence of acute kidney injury using data from the Systolic Blood Pressure Intervention Trial.", 
    "abstract": "The Internet of Things (IoT) is crucial in developing next-generation high-speed railways (HSRs). HSR IoT enables intelligent diagnosis of trains using multi-sensor data, which is critical for maintaining high speeds and ensuring passenger safety. Graph neural network (GNN)-based methods have gained popularity in HSR IoT research due to the ability to represent the sensor network as intuitive graphs. However, labeling monitoring data in the HSR scenario takes time and effort. To address this challenge, we propose a semi-supervised graph-level representation learning approach called MIM-Graph, which uses mutual information maximization to learn from a large amount of unlabeled data. First, the multi-sensor data is converted into association graphs based on their spatial topology. The unsupervised encoder is trained using global-local mutual maximization. The teacher-student framework transfers knowledge from the unsupervised encoder learned to the supervised encoder, which is trained using a small amount of labeled data. As a result, the supervised encoder learns distinguishable representations for intelligent diagnosis of HSR. We evaluate the proposed method using CWRU dataset and data from HSR Bogie test platform, and the experimental results demonstrate the effectiveness and superiority of MIM-Graph.", 
    "abstract": "To develop an interpretable artificial intelligence algorithm to rule out normal large bowel endoscopic biopsies, saving pathologist resources and helping with early diagnosis.\nA graph neural network was developed incorporating pathologist domain knowledge to classify 6591 whole-slides images (WSIs) of endoscopic large bowel biopsies from 3291 patients (approximately 54% female, 46% male) as normal or abnormal (non-neoplastic and neoplastic) using clinically driven interpretable features. One UK National Health Service (NHS) site was used for model training and internal validation. External validation was conducted on data from two other NHS sites and one Portuguese site.\nModel training and internal validation were performed on 5054 WSIs of 2080 patients resulting in an area under the curve-receiver operating characteristic (AUC-ROC) of 0.98 (SD=0.004) and AUC-precision-recall (PR) of 0.98 (SD=0.003). The performance of the model, named Interpretable Gland-Graphs using a Neural Aggregator (IGUANA), was consistent in testing over 1537 WSIs of 1211 patients from three independent external datasets with mean AUC-ROC=0.97 (SD=0.007) and AUC-PR=0.97 (SD=0.005). At a high sensitivity threshold of 99%, the proposed model can reduce the number of normal slides to be reviewed by a pathologist by approximately 55%. IGUANA also provides an explainable output highlighting potential abnormalities in a WSI in the form of a heatmap as well as numerical values associating the model prediction with various histological features.\nThe model achieved consistently high accuracy showing its potential in optimising increasingly scarce pathologist resources. Explainable predictions can guide pathologists in their diagnostic decision-making and help boost their confidence in the algorithm, paving the way for its future clinical adoption.", 
    "abstract": "A pangenome represents many diverse genome sequences of the same species. In order to cope with small variations as well as structural variations, recent research focused on the development of graph based models of pangenomes. Mapping is the process of finding the original location of a DNA read in a reference sequence, typically a genome. Using a pangenome instead of a (linear) reference genome can e.g. reduce mapping bias, the tendency to incorrectly map sequences that differ from the reference genome. Mapping reads to a graph, however, is more complex and needs more resources than mapping to a reference genome. Reducing the complexity of the graph by encoding simple variations like SNPs in a simple way can accelerate read mapping and reduce the memory requirements at the same time.\nWe introduce graphs based on elastic-degenerate strings (ED strings, EDS) and the linearised form of these EDS graphs as a new representation for pangenomes. In this representation, small variations are encoded directly in the sequence. Structural variations are encoded in a graph structure. This reduces the size of the representation in comparison to sequence graphs. In the linearised form, mapping techniques that are known from ordinary strings can be applied with appropriate adjustments. Since most variations are expressed directly in the sequence, the mapping process rarely has to take edges of the EDS graph into account. We developed a prototypical software tool GED-MAP that uses this representation together with a minimizer index to map short reads to the pangenome. Our experiments show that the new method works on a whole human genome scale, taking structural variants properly into account. The advantage of GED-MAP, compared with other pangenomic short read mappers, is that the new representation allows for a simple indexing method. This makes GED-MAP fast and memory efficient.\nSources are available at: https://github.com/thomas-buechler-ulm/gedmap.\nSupplementary data are available at Bioinformatics online.", 
    "abstract": "Haplotype networks are graphs used to represent evolutionary relationships between a set of taxa and are characterized by intuitiveness in analyzing genealogical relationships of closely related genomes. We here propose a novel algorithm termed McAN that considers mutation spectrum history (mutations in ancestry haplotype should be contained in descendant haplotype), node size (corresponding to sample count for a given node) and sampling time when constructing haplotype network. We show that McAN is two orders of magnitude faster than state-of-the-art algorithms without losing accuracy, making it suitable for analysis of a large number of sequences. Based on our algorithm, we developed an online web server and offline tool for haplotype network construction, community lineage determination, and interactive network visualization. We demonstrate that McAN is highly suitable for analyzing and visualizing massive genomic data and is helpful to enhance the understanding of genome evolution. Availability: Source code is written in C/C++ and available at https://github.com/Theory-Lun/McAN and https://ngdc.cncb.ac.cn/biocode/tools/BT007301 under the MIT license. Web server is available at https://ngdc.cncb.ac.cn/bit/hapnet/. SARS-CoV-2 dataset are available at https://ngdc.cncb.ac.cn/ncov/. Contact: songshh@big.ac.cn (Song S), zhaowm@big.ac.cn (Zhao W), baoym@big.ac.cn (Bao Y), zhangzhang@big.ac.cn (Zhang Z), ybxue@big.ac.cn (Xue Y).", 
    "abstract": "Drug-drug interaction (DDI) may lead to adverse reactions in patients, thus it is important to extract such knowledge from biomedical texts. However, previously proposed approaches typically focus on capturing sentence-aspect information while ignoring valuable knowledge concerning the whole corpus. In this paper, we propose a Multi-aspect Graph-based DDI extraction model, named DDI-MuG.\nWe first employ a bio-specific pre-trained language model to obtain the token contextualized representations. Then we use two graphs to get syntactic information from input instance and word co-occurrence information within the entire corpus, respectively. Finally, we combine the representations of drug entities and verb tokens for the final classification.\nTo validate the effectiveness of the proposed model, we perform extensive experiments on two widely used DDI extraction dataset, DDIExtraction-2013 and TAC 2018. It is encouraging to see that our model outperforms all twelve state-of-the-art models.\nIn contrast to the majority of earlier models that rely on the black-box approach, our model enables visualization of crucial words and their interrelationships by utilizing edge information from two graphs. To the best of our knowledge, this is the first model that explores multi-aspect graphs to the DDI extraction task, and we hope it can establish a foundation for more robust multi-aspect works in the future.", 
    "abstract": "Embedding knowledge graphs into low-dimensional spaces is a popular method for applying approaches, such as link prediction or node classification, to these databases. This embedding process is very costly in terms of both computational time and space. Part of the reason for this is the optimisation of hyperparameters, which involves repeatedly sampling, by random, guided, or brute-force selection, from a large hyperparameter space and testing the resulting embeddings for their quality. However, not all hyperparameters in this search space will be equally important. In fact, with prior knowledge of the relative importance of the hyperparameters, some could be eliminated from the search altogether without significantly impacting the overall quality of the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to evaluate the effects of tuning different hyperparameters on the variance of embedding quality. This was achieved by performing thousands of embedding trials, each time measuring the quality of embeddings produced by different hyperparameter configurations. We regressed the embedding quality on those hyperparameter configurations, using this model to generate Sobol sensitivity indices for each of the hyperparameters. By evaluating the correlation between Sobol indices, we find substantial variability in the hyperparameter sensitivities between knowledge graphs with differing dataset characteristics as the probable cause of these inconsistencies. As an additional contribution of this work we identify several relations in the UMLS knowledge graph that may cause data leakage via inverse relations, and derive and present UMLS-43, a leakage-robust variant of that graph.\nThe online version contains supplementary material available at 10.1186/s40537-023-00732-5.", 
    "abstract": "Nonalcoholic fatty liver disease (NAFLD) is the most common chronic liver disease with a broad spectrum of histologic manifestations. The rapidly growing prevalence and the complex pathologic mechanisms of NAFLD pose great challenges for treatment development. Despite tremendous efforts devoted to drug development, there are no FDA-approved medicines yet. Here, we present NAFLDkb, a specialized knowledge base and platform for computer-aided drug design against NAFLD. With multiperspective information curated from diverse source materials and public databases, NAFLDkb presents the associations of drug-related entities as individual knowledge graphs. Practical drug discovery tools that facilitate the utilization and expansion of NAFLDkb have also been implemented in the web interface, including chemical structure search, drug-likeness screening, knowledge-based repositioning, and research article annotation. Moreover, case studies of a knowledge graph repositioning model and a generative neural network model are presented herein, where three repositioning drug candidates and 137 novel lead-like compounds were newly established as NAFLD pharmacotherapy options reusing data records and machine learning tools in NAFLDkb, suggesting its clinical reliability and great potential in identifying novel drug-disease associations of NAFLD and generating new insights to accelerate NAFLD drug development. NAFLDkb is freely accessible at https://www.biosino.org/nafldkb and will be updated periodically with the latest findings.", 
    "abstract": "This secondary exploratory analysis examined rural-urban differences in response to a web-based physical activity self-management intervention for chronic obstructive pulmonary disease (COPD).\nParticipants with COPD (N = 239 US Veterans) were randomized to either a multicomponent web-based intervention (goal setting, iterative feedback of daily step counts, motivational and educational information, and an online community forum) or waitlist-control for 4 months with a 12-month follow-up. General linear modeling estimated the impact of rural/urban status (using Rural-Urban Commuting Area [RUCA] codes) on (1) 4- and 12-month daily step-count change compared to waitlist-control, and (2) intervention engagement (weekly logons and participant feedback).\nRural (n = 108) and urban (n = 131) participants' mean age was 66.7\u00b18.8 years. Rural/urban status significantly moderated 4-month change in daily step counts between randomization groups (p = 0.041). Specifically, among urban participants, intervention participants improved by 1500 daily steps more than waitlist-control participants (p = 0.001). There was no difference among rural participants. In the intervention group, rural participants engaged less with the step-count graphs on the website than urban participants at 4 months (p = 0.019); this difference dissipated at 12 months. More frequent logons were associated with greater change in daily step counts (p = 0.004); this association was not moderated by rural/urban status.\nThe web-based intervention was effective for urban, but not rural, participants at 4 months. Rural participants were also less engaged at 4 months, which may explain differences in effectiveness. Technology-based interventions can help address urban-rural disparities in patients with COPD, but may also contribute to them unless resources are available to support engagement with the technology.", 
    "abstract": "Pangenome references address biases of reference genomes by storing a representative set of diverse haplotypes and their alignment, usually as a graph. Alternate alleles determined by variant callers can be used to construct pangenome graphs, but advances in long-read sequencing are leading to widely available, high-quality phased assemblies. Constructing a pangenome graph directly from assemblies, as opposed to variant calls, leverages the graph's ability to represent variation at different scales. Here we present the Minigraph-Cactus pangenome pipeline, which creates pangenomes directly from whole-genome alignments, and demonstrate its ability to scale to 90 human haplotypes from the Human Pangenome Reference Consortium. The method builds graphs containing all forms of genetic variation while allowing use of current mapping and genotyping tools. We measure the effect of the quality and completeness of reference genomes used for analysis within the pangenomes and show that using the CHM13 reference from the Telomere-to-Telomere Consortium improves the accuracy of our methods. We also demonstrate construction of a Drosophila melanogaster pangenome.", 
    "abstract": "Due to the boom of Internet of Things\u00a0(IoT) in recent years, various IoT devices are connected to the Internet and communicate with each other through network protocols such as the Constrained Application Protocol\u00a0(CoAP). These protocols are typically defined and described in specification documents, such as Request for Comments\u00a0(RFC), which are written in natural or semi-formal languages. Since developers largely follow the specification documents when implementing web protocols, they have become the de facto protocol specifications. Therefore, it must be ensured that the descriptions in them are consistent to avoid technological issues, incompatibility, security risks, or even legal concerns. In this work, we propose Neural RFC Knowledge Graph (NRFCKG), a neural network-generated knowledge graph based contradictions detection tool for IoT protocol specification documents. Our approach can automatically parse the specification documents and construct knowledge graphs from them through entity extraction, relation extraction, and rule extraction with large language models. It then conducts an intra-entity and inter-entity contradiction detection over the generated knowledge graph. We implement NRFCKG and apply it to the most extensively used messaging protocols in IoT, including the main RFC\u00a0(RFC7252) of CoAP, the specification document of MQTT, and the specification document of AMQP. Our evaluation shows that NRFCKG generalizes well to other specification documents and it manages to detect contradictions from these IoT protocol specification documents.", 
    "abstract": "The purposes of the study were to develop reference ranges and maturation patterns of fetal cardiac function parameters measured by speckle tracking echocardiography (STE) using multiple biometric variables at 17-24 weeks' gestation among Thai fetuses and to compare with other previous reports.\nThe 4-chamber view of the fetal heart in 79 healthy fetuses was suitably analyzed by STE to establish the best-fit regression model. The 95% reference intervals and Z-score equations of fetal cardiac function parameters were computed.\nThe fractional area change of both ventricles, left ventricular (LV) end-diastolic and end-systolic volumes, LV stroke volume, LV cardiac output (CO), and LV CO per kilogram were all increased according to gestational age (GA) and 5 fetal biometric measurements. However, the global longitudinal strain, basal-apical length fractional shortening (BAL-FS), BAL annular free wall and septal wall FS, BAL free wall and septal wall annular plane systolic excursions, 24-segment transverse width FS as well as LV ejection fraction were all independent of GA or other somatic characteristics. There were varying development patterns between fetal right and left ventricles of these cardiac function indices across the gestation period.\nOur study created z-score and corresponding centile calculators, 5th and 95th centile reference tables, and corresponding graphs and determined the normal evolution across gestation using multiple somatic growth and age variables between 17 and 24 gestational weeks. These nomograms serve as an essential prerequisite for quantitative evaluating fetal cardiac contractility and allow for precisely detecting early changes of the fetal heart function.", 
    "abstract": "Few-shot Knowledge Graph Completion (FKGC) has recently attracted significant research interest due to its ability to expand few-shot relation coverage in Knowledge Graphs. Prevailing FKGC approaches focus on exploiting the one-hop neighbor information of entities to enhance few-shot relation embedding. However, these methods select one-hop neighbors randomly and neglect the rich multi-aspect information of entities. Although some methods have attempted to leverage Long Short-Term Memory (LSTM) to learn few-shot relation embedding, they are sensitive to the input order. To address these limitations, we propose the Capsule Neural Tensor Networks with Multi-Aspect Information approach (short for InforMix-FKGC). InforMix-FKGC employs a one-hop neighbor selection strategy based on how valuable they are and encodes multi-aspect information of entities, including one-hop neighbors, attributes and literal description. Then, a capsule network is responsible for integrating the support set and deriving few-shot relation embedding. Moreover, a neural tensor network is used to match the query set with the support set. In this way, InforMix-FKGC can learn few-shot relation embedding more precisely so as to enhance the accuracy of FKGC. Extensive experiments on the NELL-One and Wiki-One datasets demonstrate that InforMix-FKGC significantly outperforms ten state-of-the-art methods in terms of Mean Reciprocal Rank and Hits@K.", 
    "abstract": "Intraoperative neurophysiological monitoring (IONM) was investigated as a complex intervention (CI) as defined by the United Kingdom Medical Research Council (MRC) in published studies to identify challenges and solutions in estimating IONM's effects on postoperative outcomes.\nA scoping review to April 2022 of the influence of setting on what was implemented as IONM and how it influenced postoperative outcomes was performed for studies that compared IONM to no IONM cohorts. IONM complexity was assessed with the iCAT_SR tool. Causal graphs were used to represent this complexity.\nIONM implementation depended on the surgical procedure, institution and/or surgeon. \"How\" IONM influenced neurologic outcomes was attributed to surgeon or institutional experience with the surgical procedure, surgeon or institutional experience with IONM, co-interventions in addition to IONM, models of IONM service delivery and individual characteristics of the IONM provider. Indirect effects of IONM mediated by extent of tumor resection, surgical approach, changes in operative procedure, shorter operative time, and duration of aneurysm clipping were also described. There were no quantitative estimates of the relative contribution of these indirect effects to total IONM effects on outcomes.\nIONM is a complex intervention whose evaluation is more challenging than that of a simple intervention. Its implementation and largely indirect effects depend on specific settings that are usefully represented in causal graphs.\nIONM evaluation as a complex intervention aided by causal graphs and multivariable analysis could provide a valuable framework for future study design and assessments of IONM effectiveness in different settings.", 
    "abstract": "Clinical electronic medical records (EMRs) contain important information on patients' anatomy, symptoms, examinations, diagnoses, and medications. Large-scale mining of rich medical information from EMRs will provide notable reference value for medical research. With the complexity of Chinese grammar and blurred boundaries of Chinese words, Chinese clinical named entity recognition (CNER) remains a notable challenge. Follow-up tasks such as medical entity structuring, medical entity standardization, medical entity relationship extraction, and medical knowledge graph construction largely depend on medical named entity recognition effects. A promising CNER result would provide reliable support for building domain knowledge graphs, knowledge bases, and knowledge retrieval systems. Furthermore, it would provide research ideas for scientists and medical decision-making references for doctors and even guide patients on disease and health management. Therefore, obtaining excellent CNER results is essential.\nWe aimed to propose a Chinese CNER method to learn semantics-enriched representations for comprehensively enhancing machines to understand deep semantic information of EMRs by using multisemantic features, which makes medical information more readable and understandable.\nFirst, we used Robustly Optimized Bidirectional Encoder Representation from Transformers Pretraining Approach Whole Word Masking (RoBERTa-wwm) with dynamic fusion and Chinese character features, including 5-stroke code, Zheng code, phonological code, and stroke code, extracted by 1-dimensional convolutional neural networks (CNNs) to obtain fine-grained semantic features of Chinese characters. Subsequently, we converted Chinese characters into square images to obtain Chinese character image features from another modality by using a 2-dimensional CNN. Finally, we input multisemantic features into Bidirectional Long Short-Term Memory with Conditional Random Fields to achieve Chinese CNER. The effectiveness of our model was compared with that of the baseline and existing research models, and the features involved in the model were ablated and analyzed to verify the model's effectiveness.\nWe collected 1379 Yidu-S4K EMRs containing 23,655 entities in 6 categories and 2007 self-annotated EMRs containing 118,643 entities in 7 categories. The experiments showed that our model outperformed the comparison experiments, with F\nOur proposed CNER method would mine the richer deep semantic information in EMRs by multisemantic embedding using RoBERTa-wwm and CNNs, enhancing the semantic recognition of characters at different granularity levels and improving the generalization capability of the method by achieving information complementarity among different semantic features, thus making the machine semantically understand EMRs and improving the CNER task accuracy.", 
    "abstract": "In modern biology, new knowledge is generated quickly, making it challenging for researchers to efficiently acquire and synthesise new information from the large volume of primary publications. To address this problem, computational approaches that generate machine-readable representations of scientific findings in the form of knowledge graphs have been developed. These representations can integrate different types of experimental data from multiple papers and biological knowledge bases in a unifying data model, providing a complementary method to manual review for interacting with published knowledge. The Gene Ontology Consortium (GOC) has created a semantic modelling framework that extends individual functional gene annotations to structured descriptions of causal networks representing biological processes (Gene Ontology Causal Activity Modelling, or GO-CAM). In this study, we explored whether the GO-CAM framework could represent knowledge of the causal relationships between environmental inputs, neural circuits and behavior in the model nematode C. elegans (C. elegans Neural Circuit Causal Activity Modelling (CeN-CAM)). We found that, given extensions to several relevant ontologies, a wide variety of author statements from the literature about the neural circuit basis of egg-laying and carbon dioxide (CO2) avoidance behaviors could be faithfully represented with CeN-CAM. Through this process, we were able to generate generic data models for several categories of experimental results. We also generated representations of multisensory integration and sensory adaptation, and we discuss how semantic modelling may be used to functionally annotate the C. elegans connectome. Thus, Gene Ontology-based semantic modelling has the potential to support various machine-readable representations of neurobiological knowledge.", 
    "abstract": "Several important topological indices studied in mathematical chemistry are expressed in the following way $ \\sum_{uv \\in E(G)} F(d_u, d_v) $, where $ F $ is a two variable function that satisfies the condition $ F(x, y) = F(y, x) $, $ uv $ denotes an edge of the graph $ G $ and $ d_u $ is the degree of the vertex $ u $. Among them, the variable inverse sum deg index $ IS\\!D_a $, with $ F(d_u, d_v) = 1/(d_u^a+d_v^a) $, was found to have several applications. In this paper, we solve some problems posed by Vuki\u010devi\u0107 ", 
    "abstract": "The rapid accumulation of electronic health records (EHRs) and the advancements in data analysis technology have laid the foundation for research and clinical decision-making in the healthcare community. Graph neural networks (GNNs), a deep learning model family for graph embedding representations, have been widely used in the field of smart healthcare. However, traditional GNNs rely on the basic assumption that the graph structure extracted from the complex interactions among the EHRs must be a real topology. Noisy connections or false topology in the graph structure leads to inefficient disease prediction. We devise a new model named PM-GSL to improve diabetes clinical assistant diagnosis based on patient multi-relational graph structure learning. Specifically, we first build a patient multi-relational graph based on patient demographics, diagnostic information, laboratory tests, and complex interactions between medicines in EHRs. Second, to fully consider the heterogeneity of the patient multi-relational graph, we consider the node characteristics and the higher-order semantics of nodes. Thus, three candidate graphs are generated in the PM-GSL model: original subgraph, overall feature graph, and higher-order semantic graph. Finally, we fuse the three candidate graphs into a new heterogeneous graph and jointly optimize the graph structure with GNNs in the disease prediction task. The experimental results indicate that PM-GSL outperforms other state-of-the-art models in diabetes clinical assistant diagnosis tasks.", 
    "abstract": "Humanity has always benefited from an intercapillary study in the quantification of natural occurrences in mathematics and other pure scientific fields. Graph theory was extremely helpful to other studies, particularly in the applied sciences. Specifically, in chemistry, graph theory made a significant contribution. For this, a transformation is required to create a graph representing a chemical network or structure, where the vertices of the graph represent the atoms in the chemical compound and the edges represent the bonds between the atoms. The quantity of edges that are incident to a vertex determines its valency (or degree) in a graph. The degree of uncertainty in a system is measured by the entropy of a probability. This idea is heavily grounded in statistical reasoning. It is primarily utilized for graphs that correspond to chemical structures. The development of some novel edge-weighted based entropies that correspond to valency-based topological indices is made possible by this research. Then these compositions are applied to clay mineral tetrahedral sheets. Since they have been in use for so long, corresponding indices are thought to be the most effective methods for quantifying chemical graphs. This article develops multiple edge degree-based entropies that correlate to the indices and determines how to modify them to assess the significance of each type.", 
    "abstract": "Silicate minerals make up the majority of the earth's crust and account for almost 92 percent of the total. Silicate sheets, often known as silicate networks, are characterised as definite connectivity parallel designs. A key idea in studying different generalised classes of graphs in terms of planarity is the face of the graph. It plays a significant role in the embedding of graphs as well. Face index is a recently created parameter that is based on the data from a graph's faces. The current draft is utilizing a newly established face index, to study different silicate networks. It consists of a generalized chain of silicate, silicate sheet, silicate network, carbon sheet, polyhedron generalized sheet, and also triangular honeycomb network. This study will help to understand the structural properties of chemical networks because the face index is more generalized than vertex degree based topological descriptors.", 
    "abstract": "The purpose of the study is to investigate the influence of sulfobutyl ether \u03b2-cyclodextrin (SBE", 
    "abstract": "Dissolved organic matter (DOM) is a complex mixture of thousands of natural molecules that undergo constant transformation in the environment, such as sunlight induced photochemical reactions. Despite molecular level resolution from ultrahigh resolution mass spectrometry (UHRMS), trends of mass peak intensities are currently the only way to follow photochemically induced molecular changes in DOM. Many real-world relationships and temporal processes can be intuitively modeled using graph data structures (networks). Graphs enhance the potential and value of AI applications by adding context and interconnections allowing the uncovering of hidden or unknown relationships in data sets. We use a temporal graph model and link prediction to identify transformations of DOM molecules in a photo-oxidation experiment. Our link prediction algorithm simultaneously considers educt removal and product formation for molecules linked by predefined transformation units (oxidation, decarboxylation, etc.). The transformations are further weighted by the extent of intensity change and clustered on the graph structure to identify groups of similar reactivity. The temporal graph is capable of identifying relevant molecules subject to similar reactions and enabling to study their time course. Our approach overcomes previous data evaluation limitations for mechanistic studies of DOM and leverages the potential of temporal graphs to study DOM reactivity by UHRMS.", 
    "abstract": "", 
    "abstract": "Large tree structures are ubiquitous and real-world relational datasets often have information associated with nodes (e.g., labels or other attributes) and edges (e.g., weights or distances) that need to be communicated to the viewers. Yet, scalable, easy to read tree layouts are difficult to achieve. We consider tree layouts to be readable if they meet some basic requirements: node labels should not overlap, edges should not cross, edge lengths should be preserved, and the output should be compact. There are many algorithms for drawing trees, although very few take node labels or edge lengths into account, and none optimizes all requirements above. With this in mind, we propose a new scalable method for readable tree layouts. The algorithm guarantees that the layout has no edge crossings and no label overlaps, and optimizing one of the remaining aspects: desired edge lengths and compactness. We evaluate the performance of the new algorithm by comparison with related earlier approaches using several real-world datasets, ranging from a few thousand nodes to hundreds of thousands of nodes. Tree layout algorithms can be used to visualize large general graphs, by extracting a hierarchy of progressively larger trees. We illustrate this functionality by presenting several map-like visualizations generated by the new tree layout algorithm.", 
    "abstract": "The convergence rate and applicability to directed graphs with interaction topologies are two important features for practical applications of distributed optimization algorithms. In this article, a new kind of fast distributed discrete-time algorithms is developed for solving convex optimization problems with closed convex set constraints over directed interaction networks. Under the gradient tracking framework, two distributed algorithms are, respectively, designed over balanced and unbalanced graphs, where momentum terms and two time-scales are involved. Furthermore, it is demonstrated that the designed distributed algorithms attain linear speedup convergence rates provided that the momentum coefficients and the step size are appropriately selected. Finally, numerical simulations verify the effectiveness and the global accelerated effect of the designed algorithms.", 
    "abstract": "Machine learning research concerning protein structure has seen a surge in popularity over the last years with promising advances for basic science and drug discovery. Working with macromolecular structure in a machine learning context requires an adequate numerical representation, and researchers have extensively studied representations such as graphs, discretized 3D grids, and distance maps. As part of CASP14, we explored a new and conceptually simple representation in a blind experiment: atoms as points in 3D, each with associated features. These features-initially just the basic element type of each atom-are updated through a series of neural network layers featuring rotation-equivariant convolutions. Starting from all atoms, we further aggregate information at the level of alpha carbons before making a prediction at the level of the entire protein structure. We find that this approach yields competitive results in protein model quality assessment despite its simplicity and despite the fact that it incorporates minimal prior information and is trained on relatively little data. Its performance and generality are particularly noteworthy in an era where highly complex, customized machine learning methods such as AlphaFold 2 have come to dominate protein structure prediction.", 
    "abstract": "Fiber-wireless integration has been widely studied as a key technology to support radio access networks in sixth-generation wireless communication, empowered by artificial intelligence. In this study, we propose and demonstrate a deep-learning-based end-to-end (E2E) multi-user communication framework for a fiber-mmWave (MMW) integrated system, where artificial neural networks (ANN) are trained and optimized as transmitters, ANN-based channel models (ACM), and receivers. By connecting the computation graphs of multiple transmitters and receivers, we jointly optimize the transmission of multiple users in the E2E framework to support multi-user access in one fiber-MMW channel. To ensure that the framework matches the fiber-MMW channel, we employ a two-step transfer learning technique to train the ACM. In a 46.2 Gbit/s 10-km fiber-MMW transmission experiment, compared with the single-carrier QAM, the E2E framework achieves over 3.5 dB receiver sensitivity gain in the single-user case and 1.5 dB gain in the three-user case under the 7% hard-decision forward error correction threshold.", 
    "abstract": "The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.", 
    "abstract": "The frameshifting RNA element (FSE) in coronaviruses (CoVs) regulates the programmed -1 ribosomal frameshift (-1 PRF) mechanism common to many viruses. The FSE is of particular interest as a promising drug candidate. Its associated pseudoknot or stem loop structure is thought to play a large role in frameshifting and thus viral protein production. To investigate the FSE structural evolution, we use our graph theory-based methods for representing RNA secondary structures in the RNA-As-Graphs (RAG) framework to calculate conformational landscapes of viral FSEs with increasing sequence lengths for representative 10 Alpha and 13 Beta-CoVs. By following length-dependent conformational changes, we show that FSE sequences encode many possible competing stems which in turn favor certain FSE topologies, including a variety of pseudoknots, stem loops, and junctions. We explain alternative competing stems and topological FSE changes by recurring patterns of mutations. At the same time, FSE topology robustness can be understood by shifted stems within different sequence contexts and base pair coevolution. We further propose that the topology changes reflected by length-dependent conformations contribute to tuning the frameshifting efficiency. Our work provides tools to analyze virus sequence/structure correlations, explains how sequence and FSE structure have evolved for CoVs, and provides insights into potential mutations for therapeutic applications against a broad spectrum of CoV FSEs by targeting key sequence/structural transitions.", 
    "abstract": "The pathophysiology of major depressive disorder (MDD) has been demonstrated to be highly associated with the dysfunctional integration of brain activity. Existing studies only fuse multi-connectivity information in a one-shot approach and ignore the temporal property of functional connectivity. A desired model should utilize the rich information in multiple connectivities to help improve the performance. In this study, we develop a multi-connectivity representation learning framework to integrate multi-connectivity topological representation from structural connectivity, functional connectivity and dynamic functional connectivities for automatic diagnosis of MDD. Briefly, structural graph, static functional graph and dynamic functional graphs are first computed from the diffusion magnetic resonance imaging (dMRI) and resting state functional magnetic resonance imaging (rsfMRI). Secondly, a novel Multi-Connectivity Representation Learning Network (MCRLN) approach is developed to integrate the multiple graphs with modules of structural-functional fusion and static-dynamic fusion. We innovatively design a Structural-Functional Fusion (SFF) module, which decouples graph convolution to capture modality-specific features and modality-shared features separately for an accurate brain region representation. To further integrate the static graphs and dynamic functional graphs, a novel Static-Dynamic Fusion (SDF) module is developed to pass the important connections from static graphs to dynamic graphs via attention values. Finally, the performance of the proposed approach is comprehensively examined with large cohorts of clinical data, which demonstrates its effectiveness in classifying MDD patients. The sound performance suggests the potential of the MCRLN approach for the clinical use in diagnosis. The code is available at https://github.com/LIST-KONG/MultiConnectivity-master.", 
    "abstract": "Temporal grounding is the task of locating a specific segment from an untrimmed video according to a query sentence. This task has achieved significant momentum in the computer vision community as it enables activity grounding beyond pre-defined activity classes by utilizing the semantic diversity of natural language descriptions. The semantic diversity is rooted in the principle of compositionality in linguistics, where novel semantics can be systematically described by combining known words in novel ways\u00a0(compositional generalization). However, existing temporal grounding datasets are not carefully designed to evaluate the compositional generalizability. To systematically benchmark the compositional generalizability of temporal grounding models, we introduce a new Compositional Temporal Grounding task and construct two new dataset splits, i.e., Charades-CG and ActivityNet-CG. We empirically find that they fail to generalize to queries with novel combinations of seen words. We argue that the inherent compositional structure\u00a0(i.e., composition constituents and their relationships) inside the videos and language is the crucial factor to achieve compositional generalization. Based on this insight, we propose a variational cross-graph reasoning framework that explicitly decomposes video and language into hierarchical semantic graphs, respectively, and learns fine-grained semantic correspondence between the two graphs. Meanwhile, we introduce a novel adaptive structured semantics learning approach to derive the structure-informed and domain-generalizable graph representations, which facilitate the fine-grained semantic correspondence reasoning between the two graphs. To further evaluate the understanding of the compositional structure, we also introduce a more challenging setting, where one of the components in the novel composition is unseen. This requires more sophisticated understanding of the compositional structure to infer the potential semantics of the unseen word based on the other learned composition constituents appearing in both the video and language context, and their relationships. Extensive experiments validate the superior compositional generalizability of our approach, demonstrating its ability to handle queries with novel combinations of seen words as well as novel words in the testing composition.", 
    "abstract": "We present a simple method to quantitatively capture the heterogeneity in the degree distribution of a network graph using a single parameter ", 
    "abstract": "Lifelong graph learning deals with the problem of continually adapting graph neural network (GNN) models to changes in evolving graphs. We address two critical challenges of lifelong graph learning in this work: dealing with new classes and tackling imbalanced class distributions. The combination of these two challenges is particularly relevant since newly emerging classes typically resemble only a tiny fraction of the data, adding to the already skewed class distribution. We make several contributions: First, we show that the amount of unlabeled data does not influence the results, which is an essential prerequisite for lifelong learning on a sequence of tasks. Second, we experiment with different label rates and show that our methods can perform well with only a tiny fraction of annotated nodes. Third, we propose the gDOC method to detect new classes under the constraint of having an imbalanced class distribution. The critical ingredient is a weighted binary cross-entropy loss function to account for the class imbalance. Moreover, we demonstrate combinations of gDOC with various base GNN models such as GraphSAGE, Simplified Graph Convolution, and Graph Attention Networks. Lastly, our k-neighborhood time difference measure provably normalizes the temporal changes across different graph datasets. With extensive experimentation, we find that the proposed gDOC method is consistently better than a naive adaption of DOC to graphs. Specifically, in experiments using the smallest history size, the out-of-distribution detection score of gDOC is 0.09 compared to 0.01 for DOC. Furthermore, gDOC achieves an Open-F1 score, a combined measure of in-distribution classification and out-of-distribution detection, of 0.33 compared to 0.25 of DOC (32% increase).", 
    "abstract": "Family planning (FP) is an important public health intervention that is proven to reduce unplanned pregnancies, unsafe abortions, and maternal mortality. Increasing investments in FP would ensure stability and better maternal health outcomes in Nigeria. However, evidence is needed to make a case for more domestic investment in family planning in Nigeria. We undertook a literature review to highlight the unmet needs for family planning and the situation of its funding landscape in Nigeria. A total of 30 documents were reviewed, including research papers, reports of national surveys, programme reports, and academic/research blogs. The search for documents was performed on Google Scholar and organizational websites using predetermined keywords. Data were objectively extracted using a uniform template. Descriptive analysis was performed for quantitative data, and qualitative data were summarized using narratives. Frequencies, proportions, line graphs and illustrative chart were used to present the quantitative data. Although total fertility rate declined over time from 6.0 children per woman in 1990 to 5.3 in 2018, the gap between wanted fertility and actual fertility increased from 0.2 in 1990 to 0.5 in 2018. This is because wanted fertility rate decreased from 5.8 children per woman in 1990 to 4.8 per woman in 2018. Similarly, modern contraceptive prevalence rate (mCPR) decreased by 0.6% from 2013 to 2018, and unmet need for family planning increased by 2.5% in the same period. Funding for family planning services in Nigeria comes from both external and internal sources in the form of cash or commodities. The nature of external assistance for family planning services depends on the preferences of funders, although there are some similarities across funders. Irrespective of the type of funder and the length of funding, donations/funds are renewed on annual basis. Procurement of commodities receives most attention for funding whereas, commodities distribution which is critical for service delivery receives poor attention.\nNigeria has made slow progress in achieving its family planning targets. The heavy reliance on external donors makes funding for family planning services to be unpredictable and imbalanced. Hence, the need for more domestic resource mobilization through government funding.", 
    "abstract": "The genus Amaranthus L. consists of 70-80 species distributed across temperate and tropical regions of the world. Nine species are dioecious and native to North America; two of which are agronomically important weeds of row crops. The genus has been described as taxonomically challenging and relationships among species including the dioecious ones are poorly understood. In this study, we investigated the phylogenetic relationships among the dioecious amaranths and sought to gain insights into plastid tree incongruence. A total of 19 Amaranthus species' complete plastomes were analyzed. Among these, seven dioecious Amaranthus plastomes were newly sequenced and assembled, an additional two were assembled from previously published short reads sequences and 10 other plastomes were obtained from a public repository (GenBank).\nComparative analysis of the dioecious Amaranthus species' plastomes revealed sizes ranged from 150,011 to 150,735\u00a0bp and consisted of 112 unique genes (78 protein-coding genes, 30 transfer RNAs and 4 ribosomal RNAs). Maximum likelihood trees, Bayesian inference trees and splits graphs support the monophyly of subgenera Acnida (7 dioecious species) and Amaranthus; however, the relationship of A. australis and A. cannabinus to the other dioecious species in Acnida could not be established, as it appears a chloroplast capture occurred from the lineage leading to the Acnida\u2009+\u2009Amaranthus clades. Our results also revealed intraplastome conflict at some tree branches that were in some cases alleviated with the use of whole chloroplast genome alignment, indicating non-coding regions contribute valuable phylogenetic signals toward shallow relationship resolution. Furthermore, we report a very low evolutionary distance between A. palmeri and A. watsonii, indicating that these two species are more genetically related than previously reported.\nOur study provides valuable plastome resources as well as a framework for further evolutionary analyses of the entire Amaranthus genus as more species are sequenced.", 
    "abstract": "Langmuir adsorption of gas molecules of NO, NO\nThe partial electron density states based on \"PDOS\" graphs have explained that the NO and NO", 
    "abstract": "By its very nature, the nursing profession involves a lot of stress. Working in this field includes interacting with individuals who are already under a great deal of stress. Workplace stress affects the quality of services provided and also causes staff burnout, departure, and absenteeism.\nThis study is to determine occupational stress and associated factors among nurses working at public hospitals, Addis Ababa, Ethiopia, 2022.\nAn institutional based cross sectional study was conducted among 422 nurses working at public hospitals from March 1 to April 1/2022. Simple random sampling technique was used to select public hospitals. The calculated sample size was allocated proportionally to each hospital based on the number of nurses. Finally, systematic sampling method was used to approach the study participants. The data was collected by using a self-administered structured questionnaire (Expanded Nursing Stress Scale). The collected data was entered by Epi-data version 3.1 and analyzed by SPSS version 23. Descriptive analysis such as frequency distribution and measure of central tendency and variability (mean and standard deviation) was computed to describe variables of the study. Binary logistic regression was used to assess associations between dependent and independent variables. The degree of associations was interpreted using odds ratio (OR) and 95% confidence interval (CI) and statically significance at value of \nThe study finding showed that 198 (47.8%) of nurses were occupationally stressful. Factors significantly associated with occupational stress among nurses were having children (no: AOR\u2009=\u20090.46, 95% CI: 0.22, 0.96) and work shift (rotating: AOR\u2009=\u20092.89, 95% CI: 1.87, 4.45).\nIn this study, job stress affected over half of the nurses. The presence of children and respondents' work shifts were personal characteristics that were significantly linked to job stress. Therefore based on this result the government policy makers, different stakeholders and hospitals need to collaborate to reduce nurses job related stress.", 
    "abstract": "Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering (QA) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as DBpedia and Wikidata. We present SciQA a scientific QA benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph (ORKG) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the ORKG. The questions cover a range of research fields and question types and are translated into corresponding SPARQL queries over the ORKG. Based on two preliminary evaluations, we show that the resulting SciQA benchmark represents a challenging task for next-generation QA systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data (QALD) Challenge.", 
    "abstract": "To describe the gender trends in ophthalmology primary practice areas using the American Board of Ophthalmology (ABO) Diplomates.\nA trend study plus a cross-sectional study of the ABO's database.\nThe de-identified records of all (n=12,844) ABO-certified ophthalmologists between 1992 - 2020 were obtained. The year of certification, gender, and self-reported primary practice for each ophthalmologist was recorded. Subspecialty was defined as the self-reported primary practice emphasis. Practice trends of the entire population and the subpopulation of subspecialists were explored based on gender then visualized using tables, graphs and analyzed using Chi Square/Fisher Exact testing.\nA total of 12,844 board-certified ophthalmologists were included. Nearly half (47%) reported a subspecialty as their primary practice area (n=6,042) of which the majority were male (65%, n=3,940). In the first decade, men outnumbered women reporting subspecialty practices by more than 2:1. Over time the number of women subspecialists increased while the number of men remained stable such that by 2020 women comprised almost half of new ABO diplomates reporting subspecialty practice. When all subspecialists were compared within gender, there was not a significant difference (p=0.15) between the percentage of male (46%) and female (48%) ophthalmologist reporting a subspecialty practice. However, a significantly greater proportion of women than men reported primary practice in pediatrics (20.1% vs. 7.9%, p <0.001) and glaucoma (21.8% vs. 16.0%, p < 0.0001). Alternatively, a significantly greater proportion of men reported primary practice in vitreoretinal surgery (47.2% vs. 22.0%, p < 0.0001). There was no significant difference between the proportion of men and women reporting cornea (p=0.15) or oculoplastics (p=0.31).\nThe number of women in ophthalmology subspecialty practice has increased steadily over the past 30 years. Men and women subspecialize at the same rate, but significant differences exist between the types of ophthalmology practiced by each gender.", 
    "abstract": "Our aim is to provide an overview of how neck pain is classified in the literature, define and group conservative interventions into 'nodes', and develop draft networks of interventions in preparation for a network meta-analysis.\nWe performed a scoping review. For feasibility reasons, we searched for randomised clinical trials (RCTs) via neck pain clinical practice guidelines (CPGs) published from 2014. We used standardised data extraction forms to extract data about classification of neck pain and interventions evaluated in the included RCTs. We calculated frequencies of neck pain classifications and grouped interventions into nodes based on the definitions used in Cochrane reviews. Draft network graphs comparing interventions were constructed using the online Shiny R application CINEMA.\nWe included 242 RCTs from seven CPGs, evaluating 28,581 patients. We found three different classification systems of which The Neck Pain Task Force classification was used most often. We defined and grouped all interventions into 19 discrete potential nodes.\nWe found a wide variation in neck pain classifications and conservative interventions. Grouping the interventions was challenging and needs further evaluation before conducting a final network meta-analysis.", 
    "abstract": "There are several ways to assess complexity, but no method has yet been developed for quantitatively calculating the \"loss of fractal complexity\" under pathological or physiological states. In this paper, we aimed to quantitatively evaluate fractal complexity loss using a novel approach and new variables developed from Detrended Fluctuation Analysis (DFA) log-log graphics. Three study groups were established to evaluate the new approach: one for normal sinus rhythm (NSR), one for congestive heart failure (CHF), and white noise signal (WNS). ECG recordings of the NSR and CHF groups were obtained from PhysioNET Database were used for analysis. For all groups Detrended Fluctuation Analysis scaling exponents (DFA\u03b11, DFA\u03b12) were determined. Scaling exponents were used to recreate the DFA log-log graph and lines. Then, the relative total logarithmic fluctuations for each sample were identified and new parameters were computed. To do this, we used a standard log-log plane to standardize the DFA log-log curves and calculated the differences between the standardized and expected areas. We quantified the total difference in standardized areas using parameters called dS1, dS2, and TdS. Our results showed that; compared to the NSR group, DFA\u03b11 was lower in both CHF and WNS groups. However, DFA\u03b12 was only reduced in the CHF group and not in the WNS group. Newly derived parameters: dS1, dS2, and TdS were significantly higher in the NSR group compared to CHF and WNS groups. The new parameters derived from the DFA log-log graphs are highly distinguishing for congestive heart failure and white noise signal. In addition, it may be concluded that a potential feature of our approach can be beneficial in classifying the severity of cardiac abnormalities.", 
    "abstract": "Traditional histological stains, such as hematoxylin-eosin (HE), special stains, and immunofluorescence (IF), have defined myriads of cellular phenotypes and tissue structures in a separate stained section. However, the precise connection of information conveyed by the various stains in the same section, which may be important for diagnosis, is absent. Here, we present a new staining modality-Flow chamber stain, which complies with the current staining workflow but possesses newly additional features non-seen in conventional stains, allowing for (1) quickly switching staining modes between destain and restain for multiplex staining in one single section from routinely histological preparation, (2) real-time inspecting and digitally capturing each specific stained phenotype, and (3) efficiently synthesizing graphs containing the tissue multiple-stained components at site-specific regions. Comparisons of its stains with those by the conventional staining fashions using the microscopic images of mouse tissues (lung, heart, liver, kidney, esophagus, and brain), involving stains of HE, Periodic acid-Schiff, Sirius red, and IF for Human IgG, and mouse CD45, hemoglobin, and CD31, showed no major discordance. Repetitive experiments testing on targeted areas of stained sections confirmed the method is reliable with accuracy and high reproducibility. Using the technique, the targets of IF were easily localized and seen structurally in HE- or special-stained sections, and the unknown or suspected components or structures in HE-stained sections were further determined in histological special stains or IF. By the technique, staining processing was videoed and made a backup for off-site pathologists, which facilitates tele-consultation or -education in current digital pathology. Mistakes, which might occur during the staining process, can be immediately found and amended accordingly. With the technique, a single section can provide much more information than the traditional stained counterpart. The staining mode bears great potential to become a common supplementary tool for traditional histopathology.", 
    "abstract": "With the rapid development of human intestinal microbiology and diverse microbiome-related studies and investigations, a large amount of data have been generated and accumulated. Meanwhile, different computational and bioinformatics models have been developed for pattern recognition and knowledge discovery using these data. Given the heterogeneity of these resources and models, we aimed to provide a landscape of the data resources, a comparison of the computational models and a summary of the translational informatics applied to microbiota data. We first review the existing databases, knowledge bases, knowledge graphs and standardizations of microbiome data. Then, the high-throughput sequencing techniques for the microbiome and the informatics tools for their analyses are compared. Finally, translational informatics for the microbiome, including biomarker discovery, personalized treatment and smart healthcare for complex diseases, are discussed.", 
    "abstract": "Graph convolutional networks (GCNs) have shown superior performance on graph classification tasks, and their structure can be considered as an encoder-decoder pair. However, most existing methods lack the comprehensive consideration of global and local in decoding, resulting in the loss of global information or ignoring some local information of large graphs. And the commonly used cross-entropy loss is essentially an encoder-decoder global loss, which cannot supervise the training states of the two local components (encoder and decoder). We propose a multichannel convolutional decoding network (MCCD) to solve the above-mentioned problems. MCCD first adopts a multichannel GCN encoder, which has better generalization than a single-channel GCN encoder since different channels can extract graph information from different perspectives. Then, we propose a novel decoder with a global-to-local learning pattern to decode graph information, and this decoder can better extract global and local information. We also introduce a balanced regularization loss to supervise the training states of the encoder and decoder so that they are sufficiently trained. Experiments on standard datasets demonstrate the effectiveness of our MCCD in terms of accuracy, runtime, and computational complexity.", 
    "abstract": "Heterogeneous knowledge graphs (KGs) have enabled the modeling of complex systems, from genetic interaction graphs and protein-protein interaction networks to networks representing drugs, diseases, proteins, and side effects. Analytical methods for KGs rely on quantifying similarities between entities, such as nodes, in the graph. However, such methods must consider the diversity of node and edge types contained within the KG via, for example, defined sequences of entity types known as meta paths. We present metapaths, the first R software package to implement meta paths and perform meta-path-based similarity search in heterogeneous KGs. The metapaths package offers various built-in similarity metrics for node pair comparison by querying KGs represented as either edge or adjacency lists, as well as auxiliary aggregation methods to measure set-level relationships. Indeed, evaluation of these methods on an open-source biomedical KG recovered meaningful drug and disease-associated relationships, including those in Alzheimer's disease. The metapaths framework facilitates the scalable and flexible modeling of network similarities in KGs with applications across KG learning.\nThe metapaths R package is available via GitHub at https://github.com/ayushnoori/metapaths and is released under MPL 2.0 (Zenodo DOI: 10.5281/zenodo.7047209). Package documentation and usage examples are available at https://www.ayushnoori.com/metapaths.\nSupplementary information is available at Bioinformatics online.", 
    "abstract": "The association between appendicular skeletal muscle mass (ASM) and cardiometabolic risk has been emphasized. We estimated reference values of the percentage of ASM (PASM) and investigated its association with metabolic syndrome (MS) in Korean adolescents.\nData from Korea National Health and Nutrition Examination Survey performed between 2009 and 2011 was used. Tables and graphs of reference PASM were generated using 1,522 subjects (807 boys) aged 10 to 18. The relationship between PASM and each component of MS in adolescents was further analyzed in 1,174 subjects (613 boys). Moreover, the pediatric simple metabolic syndrome score (PsiMS), homeostasis model assessment of insulin resistance (HOMA-IR), and the triglyceride glucose (TyG) index were analyzed. Multivariate linear and logistic regressions adjusting for age, sex, house income, and daily energy intake were performed.\nIn boys, PASM increased with age, but girls showed a different trend that declines with age. PsiMS, HOMA-IR, and TyG index showed inverse associations with PASM (PsiMS, \u03b2 -0.105, p-value <0.001; HOMA-IR, \u03b2 -0.104, p-value <0.001; TyG index, \u03b2 -0.013, p-value <0.001). PASM z-score was negatively associated with obesity (aOR 0.22, 95% CI 0.17-0.30), abdominal obesity (aOR 0.27, 95% CI 0.20-0.36), hypertension (aOR 0.65, 95% CI 0.52-0.80), and elevated triglycerides (aOR 0.67, 95% CI 0.56-0.79).\nThe probability of acquiring MS and insulin resistance decreased with higher PASM values. The reference range may offer clinicians information that aid the effective management of patients. It is urged that clinicians monitor the body composition using standard reference databases.", 
    "abstract": "The Working Group (WG), initiated by the International Continence Society (ICS) Standardisation Steering Committee and supported by the Society of Urodynamics, Female Pelvic Medicine and Urogenital Reconstruction, has revised the ICS Standard for pressure-flow studies of 1997.\nBased on the ICS standard for developing evidence-based standards, the WG developed this new ICS standard in the period from May 2020 to December 2022. A draft was posted on the ICS website in December 2022 to facilitate public discussion and the comments received have been incorporated into this final release.\nThe WG has recommended analysis principles for the diagnosis of voiding dysfunction for adult men and women without relevant neurological abnormalities. New standard terms and parameters for objective and continuous grading of urethral resistance (UR), bladder outflow obstruction (BOO) and detrusor voiding contraction (DVC) are introduced in this part 2 of the standard. The WG has summarized the theory and recommendations for the practice of pressure-flow study (PFS) for patients in part 1. A pressure-flow plot is recommended for the diagnosis of every patient, in addition to time-based graphs. Voided percentage and post void residual volume should always be included in PFS analysis and diagnosis. Only parameters that represent the ratio or subtraction of pressure and synchronous flow are recommended to quantify UR and only parameters that combine pressure and flow in a product or sum are recommended to quantify DVC. The ICS BOO index and the ICS detrusor contraction index are introduced in this part 2 as the standard. The WG has suggested clinical PFS dysfunction classes for male and female patients. A pressure-flow scatter graph including every patient's p\nPFS is the gold standard used to objectively assess voiding function. Quantifying the dysfunction and grading of abnormalities are standardized for adult males and females.", 
    "abstract": "Little is known about the practice of traditional medicinal plant use, especially during pregnancy in Ethiopia. Moreover, there has been no previous studies conducted on practices and related factors of medicinal plant use among pregnant women in Gojjam, northwest Ethiopia.\nA multicentered facility-based cross-sectional study was conducted from July 1 to 30 2021. A total of 423 pregnant mothers receiving antenatal care were included in this study. To recruit study participants, multistage sampling techniques were used. Data were collected using a semi-structured interviewer-administered questionnaire. SPSS version 20.0 statistical package was used for statistical analysis. Univariable and multivariable logistic regression analysis was performed to identify factors related to the medicinal plants' utilization status in pregnant mothers. The study results were presented in both descriptive statistics (percents, tables, graphs, mean, and dispersion measurements like standard deviation) and inferential statistics (odds ratio).\nThe magnitude of traditional medicinal plants' utilization during pregnancy was 47.7% (95%CI: 42.8-52.8%). Pregnant mothers residing in rural areas [Adjusted Odds Ratio (AOR) = 3.13; 95% Confidence Interval (CI):1.53, 6.41], who are illiterate (AOR = 2.99; 95%CI:1.097, 8.17), have illiterate husbands (AOR = 3.08; 95%CI:1.29, 7.33), married farmers (AOR = 4.92; 95%CI:1.87, 12.94), married merchants (AOR = 0.27; 95%CI:0.09, 0.78), have a divorced and widowed marital status (AOR = 3.93; 95%CI:1.25, 12.395), have low antenatal care visits (AOR = 4.76; 95%CI:1.93, 11.74), substance use history (AOR = 7.21; 95%CI:3.49, 14.9), and used medicinal plants in previous pregnancy (AOR = 4.06; 95%CI:2.03, 8.13) had statistically significant association with medicinal plant use during current pregnancy.\nThe present study revealed that a relatively large number of mothers used medicinal plants of various types during their current pregnancy. Area of residence, maternal educational status, husband's education level, husband's occupation status, marital status, number of antenatal care visits, use of medicinal plants in previous pregnancies, and substance use history were among the factors which were significantly associated with the use of traditional medicinal plants during the current pregnancy. Overall, the current finding provides scientific evidence useful for health sector leaders and healthcare professionals on the utilization of unprescribed medicinal plants during pregnancy and the factors associated with the utilization of the plants. Hence, they may consider creating awareness and providing advice on the careful use of unprescribed medicinal plants among pregnant mothers, especially those residing in rural areas, who are illiterate, who have divorced and widowed marital status, and who have a previous history of herbal and substance use. This is because using traditional medicines without prior discussion with a healthcare expert may harm pregnant mothers and their unborn child, as the safety of the utilized plants in the current study area is not scientifically proven. Prospective studies which need to confirm the safety of the plants used are recommended mainly in the present study area.", 
    "abstract": "To evaluate the utility of EQIP as a novel tool for determining the quality of patient information on YouTube regarding refractive eye surgery.\nThree searches were conducted on YouTube using \"PRK eye surgery\", \"LASIK eye surgery\", and \"SMILE eye surgery\". 110 relevant videos were evaluated using the Ensuring Quality Information for Patients (EQIP) criteria.\nThe average EQIP score was 15.1 (moderate quality). On average, physician-authored videos scored significantly higher on questions 17 (\nEQIP was useful in identifying specific strengths and deficits in online refractive surgery patient education resources that were not evident from other screening tools. The quality of information on YouTube videos on refractive surgeries is average. Physician-authored videos could be improved by clarifying risks and quality of life issues. Quality evaluation of medical information is important for comprehensive online surgical education.", 
    "abstract": "Knowledge graphs are an increasingly common data structure for representing biomedical information. These knowledge graphs can easily represent heterogeneous types of information, and many algorithms and tools exist for querying and analyzing graphs. Biomedical knowledge graphs have been used in a variety of applications, including drug repurposing, identification of drug targets, prediction of drug side effects, and clinical decision support. Typically, knowledge graphs are constructed by centralization and integration of data from multiple disparate sources. Here, we describe BioThings Explorer, an application that can query a virtual, federated knowledge graph derived from the aggregated information in a network of biomedical web services. BioThings Explorer leverages semantically precise annotations of the inputs and outputs for each resource, and automates the chaining of web service calls to execute multi-step graph queries. Because there is no large, centralized knowledge graph to maintain, BioThing Explorer is distributed as a lightweight application that dynamically retrieves information at query time. More information can be found at https://explorer.biothings.io, and code is available at https://github.com/biothings/biothings_explorer.", 
    "abstract": "This paper deals with the large-scale behaviour of dynamical optimal transport on ", 
    "abstract": "Comprehensive collections approaching millions of sequenced genomes have become central information sources in the life sciences. However, the rapid growth of these collections makes it effectively impossible to search these data using tools such as BLAST and its successors. Here, we present a technique called phylogenetic compression, which uses evolutionary history to guide compression and efficiently search large collections of microbial genomes using existing algorithms and data structures. We show that, when applied to modern diverse collections approaching millions of genomes, lossless phylogenetic compression improves the compression ratios of assemblies, de Bruijn graphs, and ", 
    "abstract": "Gene expression profiling has helped tremendously in the understanding of biological processes and diseases. However, interpreting processed data to gain insights into biological mechanisms remain challenging, especially to the non-bioinformaticians, as many of these data visualization and pathway analysis tools require extensive data formatting. To circumvent these challenges, we developed STAGEs (Static and Temporal Analysis of Gene Expression studies) that provides an interactive visualisation of omics analysis outputs. Users can directly upload data created from Excel spreadsheets and use STAGEs to render volcano plots, differentially expressed genes stacked bar charts, pathway enrichment analysis by Enrichr and Gene Set Enrichment Analysis (GSEA) against established pathway databases or customized gene sets, clustergrams and correlation matrices. Moreover, STAGEs takes care of Excel gene to date misconversions, ensuring that every gene is considered for pathway analysis. Output data tables and graphs can be exported, and users can easily customize individual graphs using widgets such as sliders, drop-down menus, text boxes and radio buttons. Collectively, STAGEs is an integrative platform for data analysis, data visualisation and pathway analysis, and is freely available at https://kuanrongchan-stages-stages-vpgh46.streamlitapp.com/ . In addition, developers can customise or modify the web tool locally based on our existing codes, which is publicly available at https://github.com/kuanrongchan/STAGES .", 
    "abstract": "As the main channel for people to obtain information and express their opinions, online media generate a huge amount of unstructured news documents every day and make it difficult for people to perceive major societal events and grasp the evolution of events. Previous studies on storyline generation are generally based on document clustering without considering event arguments and relations between events. Event-centric knowledge graph has been used to facilitate the construction of news documents to form structured event representation. Although some studies have attempted to construct timelines based on event-centric knowledge graphs, it is difficult for timelines to depict the complex structures of event evolution. In this paper, we try to represent news documents as an event-centric knowledge graph, and compress the whole knowledge graph into salient complex events in temporal order to generate storylines named narrative graph. We first collect news documents from news platforms, construct an event ontology, and build an event-centric knowledge graph with temporal relations. Graph neural network is used to detect events, while BERT fine-tuning is leveraged to identify temporal relations between events. Then, a novel generation framework of narrative graph with constraints of coherence and coverage is proposed. In addition, a case study is implemented to demonstrate how to utilize narrative graph to analyze real-world event. The experiment results show that our approach significantly outperforms the baseline approaches.", 
    "abstract": "Genome-wide genealogies compactly represent the evolutionary history of a set of genomes and inferring them from genetic data has the potential to facilitate a wide range of analyses. We introduce a method, ARG-Needle, for accurately inferring biobank-scale genealogies from sequencing or genotyping array data, as well as strategies to utilize genealogies to perform association and other complex trait analyses. We use these methods to build genome-wide genealogies using genotyping data for 337,464 UK Biobank individuals and test for association across seven complex traits. Genealogy-based association detects more rare and ultra-rare signals (N\u2009=\u2009134, frequency range 0.0007-0.1%) than genotype imputation using ~65,000 sequenced haplotypes (N\u2009=\u200964). In a subset of 138,039 exome sequencing samples, these associations strongly tag (average r\u2009=\u20090.72) underlying sequencing variants enriched (4.8\u00d7) for loss-of-function variation. These results demonstrate that inferred genome-wide genealogies may be leveraged in the analysis of complex traits, complementing approaches that require the availability of large, population-specific sequencing panels.", 
    "abstract": "The current article examines a nonlinear axisymmetric streaming flow obeying the Rivlin-Ericksen viscoelastic model and overloaded by suspended dust particles. The fluids are separated by an infinite vertical cylindrical interface. A uniform axial magnetic field as well as mass and heat transmission (MHT) act everywhere the cylindrical flows. For the sake of simplicity, the viscous potential theory (VPT) is adopted to ease the analysis. The study finds its significance in wastewater treatment, petroleum transport as well as various practical engineering applications. The methodology of the nonlinear approach is conditional primarily on utilizing the linear fundamental equations of motion along with the appropriate nonlinear applicable boundary conditions (BCs). A dimensionless procedure reveals a group of physical dimensionless numerals. The linear stability requirements are estimated by means of the Routh-Hurwitz statement. The application of Taylor's theory with the multiple time scales provides a Ginzburg-Landau equation, which regulates the nonlinear stability criterion. Therefore, the theoretical nonlinear stability standards are determined. A collection of graphs is drawn throughout the linear as well as the nonlinear approaches. In light of the Homotopy perturbation method (HPM), an estimated uniform solution to the surface displacement is anticipated. This solution is verified by means of a numerical approach. The influence of different natural factors on the stability configuration is addressed. When the density number of the suspended inner dust particles is less than the density number of the suspended outer dust particles, and vice versa, it is found that the structure is reflected to be stable. Furthermore, as the pure outer viscosity of the liquid increases, the stable range contracts, this means that this parameter has a destabilizing effect. Additionally, the magnetic field and the transfer of heat don't affect the nature of viscoelasticity.", 
    "abstract": "Attribute-based person search aims to find the target person from the gallery images based on the given query text. It often plays an important role in surveillance systems when visual information is not reliable, such as identifying a criminal from a few witnesses. Although recent works have made great progress, most of them neglect the attribute labeling problems that exist in the current datasets. Moreover, these problems also increase the risk of non-alignment between attribute texts and visual images, leading to large semantic gaps. To address these issues, in this paper, we propose Weak Semantic Embeddings (WSEs), which can modify the data distribution of the original attribute texts and thus improve the representability of attribute features. We also introduce feature graphs to learn more collaborative and calibrated information. Furthermore, the relationship modeled by our feature graphs between all semantic embeddings can reduce the semantic gap in text-to-image retrieval. Extensive evaluations on three challenging benchmarks - PETA, Market-1501 Attribute, and PA100K, demonstrate the effectiveness of the proposed WSEs, and our method outperforms existing state-of-the-art methods.", 
    "abstract": "Discretizing a nonlinear time series enables us to calculate its statistics fast and rigorously. Before the turn of the century, the approach using partitions was dominant. In the last two decades, discretization via permutations has been developed to a powerful methodology, while recurrence plots have recently begun to be recognized as a method of discretization. In the meantime, horizontal visibility graphs have also been proposed to discretize time series. In this review, we summarize these methods and compare them from the viewpoint of symbolic dynamics, which is the right framework to study the symbolic representation of nonlinear time series and the inverse process: the symbolic reconstruction of dynamical systems. As we will show, symbolic dynamics is currently a very active research field with interesting applications.", 
    "abstract": "The COVID-19 pandemic has caused unprecedented disruptions to urban systems worldwide, but the extent and nature of these disruptions are not yet fully understood when it comes to transportation. In this work, we aim to explore how social distancing policies have affected passenger demand in urban mass transportation systems with the goal of supporting informed decisions in policy planning. We propose an approach based on complex networks and clustering time series with similar behavior, investigating possible changes in similarity patterns during pandemics and how they reflect into a regional scale. The methods shown here proved useful in detecting that lines in central or peripheral regions present different dynamics, that bus lines have changed their behavior during pandemic so that similarity relations have changed significantly, and that when social distancing started, there was an abrupt shock in the properties of daily passenger time series, and the system did not return to its original behavior until the end of the evaluated period. The approach allows to track evolution of the community structure in different scenarios providing managers with tools to reinforce or destabilize similarities if needed.", 
    "abstract": "Reputation-based cooperation on social networks offers a causal mechanism between graph properties and social trust. Using a simple model, this paper demonstrates the underlying mechanism in a way that is accessible to scientists not specializing in networks or mathematics. The paper shows that when the size and degree of the network is fixed (i.e. all graphs have the same number of agents, who all have the same number of connections), it is the clustering coefficient that drives differences in how cooperative social networks are.", 
    "abstract": "Characterizing the structural dynamics of proteins with heterogeneous conformational landscapes is crucial to understanding complex biomolecular processes. To this end, dimensionality reduction algorithms are used to produce low-dimensional embeddings of the high-dimensional conformational phase space. However, identifying a compact and informative set of input features for the embedding remains an ongoing challenge. Here, we propose to harness the power of Residue Interaction Networks (RINs) and their centrality measures, established tools to provide a graph theoretical view on molecular structure. Specifically, we combine the closeness centrality, which captures global features of the protein conformation at residue-wise resolution, with EncoderMap, a hybrid neural-network autoencoder/multidimensional-scaling like dimensionality reduction algorithm. We find that the resulting low-dimensional embedding is a meaningful visualization of the residue interaction landscape that resolves structural details of the protein behavior while retaining global interpretability. This feature-based graph embedding of temporal protein graphs makes it possible to apply the general descriptive power of RIN formalisms to the analysis of protein simulations of complex processes such as protein folding and multidomain interactions requiring no protein-specific input. We demonstrate this on simulations of the fast folding protein Trp-Cage and the multidomain signaling protein FAT10. Due to its generality and modularity, the presented approach can easily be transferred to other protein systems.", 
    "abstract": "Data-driven medical care delivery must always respect patient privacy-a requirement that is not easily met. This issue has impeded improvements to healthcare software and has delayed the long-predicted prevalence of artificial intelligence in healthcare. Until now, it has been very difficult to share data between healthcare organizations, resulting in poor statistical models due to unrepresentative patient cohorts. Synthetic data, i.e., artificial but realistic electronic health records, could overcome the drought that is troubling the healthcare sector. Deep neural network architectures, in particular, have shown an incredible ability to learn from complex data sets and generate large amounts of unseen data points with the same statistical properties as the training data. Here, we present a generative neural network model that can create synthetic health records with realistic timelines. These clinical trajectories are generated on a per-patient basis and are represented as linear-sequence graphs of clinical events over time. We use a variational graph autoencoder (VGAE) to generate synthetic samples from real-world electronic health records. Our approach generates health records not seen in the training data. We show that these artificial patient trajectories are realistic and preserve patient privacy and can therefore support the safe sharing of data across organizations.", 
    "abstract": "This research is taking the first steps toward applying a 2D dragonfly wing skeleton in the design of an airplane wing using artificial intelligence. The work relates the 2D morphology of the structural network of dragonfly veins to a secondary graph that is topologically dual and geometrically perpendicular to the initial network. This secondary network is referred as the reciprocal diagram proposed by Maxwell that can represent the static equilibrium of forces in the initial graph. Surprisingly, the secondary graph shows a direct relationship between the thickness of the structural members of a dragonfly wing and their in-plane static equilibrium of forces that gives the location of the primary and secondary veins in the network. The initial and the reciprocal graph of the wing are used to train an integrated and comprehensive machine-learning model that can generate similar graphs with both primary and secondary veins for a given boundary geometry. The result shows that the proposed algorithm can generate similar vein networks for an arbitrary boundary geometry with no prior topological information or the primary veins' location. The structural performance of the dragonfly wing in nature also motivated the authors to test this research's real-world application for designing the cellular structures for the core of airplane wings as cantilever porous beams. The boundary geometry of various airplane wings is used as an input for the design proccedure. The internal structure is generated using the training model of the dragonfly veins and their reciprocal graphs. One application of this method is experimentally and numerically examined for designing the cellular core, 3D printed by fused deposition modeling, of the airfoil wing; the results suggest up to 25% improvements in the out-of-plane stiffness. The findings demonstrate that the proposed machine-learning-assisted approach can facilitate the generation of multiscale architectural patterns inspired by nature to form lightweight load-bearable elements with superior structural\u00a0properties.", 
    "abstract": "Incorporating knowledge graphs into recommendation systems has attracted wide attention in various fields recently. A Knowledge graph contains abundant information with multi-type relations among multi-type nodes. The heterogeneous structure reveals not only the connectivity but also the complementarity between the nodes within a KG, which helps to capture the signal of potential interest of the user. However, existing research works have limited abilities in dealing with the heterogeneous nature of knowledge graphs, resulting in suboptimal recommendation results. In this paper, we propose a new recommendation method based on iterative heterogeneous graph learning on knowledge graphs (HGKR). By treating a knowledge graph as a heterogeneous graph, HGKR achieves more fine-grained modeling of knowledge graphs for recommendation. Specifically, we incorporate the graph neural networks into the message passing and aggregating of entities within a knowledge graph both at the graph and the semantic level. Furthermore, we designed a knowledge-perceiving item filter based on an attention mechanism to capture the user's potential interest in their historical preferences for the enhancement of recommendation. Extensive experiments conducted on two datasets in the context of two recommendations reveal the excellence of our proposed method, which outperforms other benchmark models.", 
    "abstract": "The present study investigated the association of interactions between gene polymorphisms in metabolic 'caretaker' genes (Phase I: CYP1A1, CYP2E1; Phase II: GSTM1, GSTT1), the cell cycle regulatory gene, p53, along with its negative controller, MDM-2, and the environment variable (tobacco). A nonparametric model, multifactor dimensionality reduction (MDR), was applied to analyse these interactions.\nThis case-control study was carried out on 242 subjects. Genomic DNA was extracted from peripheral blood lymphocytes.11 gene variants with an exposure variable (tobacco use) were analysed using MDR to identify the best locus model for gene-gene and gene-environment interactions. Statistical significance was evaluated using a 1000-fold permutation test using MDR permutation testing software (version 1.0 beta 2). The value of p<0.05 was considered statistically significant.\nThe best three-locus model for gene-gene interaction included two of the p53 gene polymorphisms; rs17878362 (intron 3) and rs1042522 (exon 4) and rs6413432 in the Phase I gene, CYP2E1(DraI). The three-locus model to evaluate the gene-environment interaction included two intronic polymorphisms of the p53 gene, that is, rs17878362 (intron 3) and rs1625895 (intron 6), and rs4646903 in the Phase I gene CYP1A1*2C. The interaction graphs revealed independent main effects of the tobacco and p53 polymorphism, rs1042522 (exon 4), and a significant additive interaction effect between rs17878362 (intron 3) and rs1042522 (exon 4).\nThe nonparametric approach highlighted the potential role of tobacco use and variations in the p53 gene as significant contributors to oral cancer risk. The findings of the present study will help implement preventive strategies in both tobacco use and screening using a molecular pathology approach.", 
    "abstract": "Therapy resistance to single agents has led to the realization that combination therapies could become the cornerstone of cancer treatment. To operationalize the selection of effective and safe multitarget therapies, we propose to integrate chemical and preclinical therapeutic information with clinical efficacy and toxicity data, allowing a new perspective on the drug target landscape. To assess the feasibility of this approach, we evaluated the publicly available chemical, preclinical, and clinical therapeutic data, and we addressed some potential limitations while integrating the data. First, by mapping available structured data from the main biomedical resources, we noticed that there is only a 1.7% overlap between drugs in chemical, preclinical, or clinical databases. Especially, the limited amount of structured data in the clinical domain hinders linking drugs to clinical aspects such as efficacy and side effects. Second, to overcome the abovementioned knowledge gap between the chemical, preclinical, and clinical domain, we suggest information extraction from scientific literature and other unstructured resources through natural language processing models, where BioBERT and PubMedBERT are the current state-of-the-art approaches. Finally, we propose that knowledge graphs can be used to link structured data, scientific literature, and electronic health records, to come to meaningful interpretations. Together, we expect this richer knowledge will lower barriers toward clinical application of personalized combination therapies with high efficacy and limited adverse events.", 
    "abstract": "The engine tests aimed to produce comparable data for fuel consumption, exhaust emissions, and thermal efficiency. The computational fluid dynamics (CFD) program FLUENT was used to simulate the combustion parameters of a direct injection diesel engine. In-cylinder turbulence is controlled using the RNG k-model. The model's conclusions are validated when the projected p-curve is compared to the observed p-curve. The thermal efficiency of the 50E50B blend (50% ethanol, 50% biofuel) is higher than the other blends as well as diesel. Diesel has lower brake thermal efficiency among the other fuel blends used. The 10E90B mix (10% ethanol, 90% biofuel) has a lower brake-specific fuel consumption (BSFC) than other blends but is slightly higher than diesel. The temperature of the exhaust gas rises for all mixtures as the brake power is increased. CO emissions from 50E50B are lower than diesel at low loads but slightly greater at heavy loads. According to the emission graphs, the 50E50B blend produces less HC than diesel. NOx emission rises with increasing load in the exhaust parameter for all mixes. A 50E50B biofuel-ethanol combination achieves the highest brake thermal efficiency, 33.59%. The BSFC for diesel is 0.254 kg/kW-hr at maximum load, while the BSFC for the 10E90B mix is 0.269 kg/kW-hr, higher than diesel. In comparison to diesel, BSFC has increased by 5.90%.", 
    "abstract": "This paper introduces a novel and minimized sample preparation technique based on hollow fiber-protected liquid-phase microextraction that can be used in joint with gas chromatography-mass spectrometry (GC-MS) detection to extract three organochlorine pesticides-Endrin, Chlordane, and Dieldrin-from rice samples. To that end, a single-walled carbon nanotube (SWCNT) and a proper ionic liquid (IL) were ultrasonically dispersed and injected into the lumen of hollow fiber as the extraction phase for preconcentrating and extracting the target analytes from the rice samples. The effects of the type of nanoparticles, ILs, and desorption solvent on the efficiency of extracting the analytes were investigated based on the one-factor-at-a-time (OFAT) approach. In addition, other parameters influencing the extraction procedure were optimized using an experimental design that decreased the number of experiments, reagent consumption, and costs. Under optimized conditions, the limits of detection and quantification in determining mentioned pesticides varied between 0.019-0.029 and 0.064-0.098\u00a0ng mL", 
    "abstract": "The acronym COVID, which stands for coronavirus disease, has become one of the most infamous acronyms in the world since 2020. An analysis of acronyms in health and medical journals has previously found that acronyms have become more common in titles and abstracts over time (e.g., DNA and human immunodeficiency virus are the most common acronyms). However, the trends in acronyms related to COVID remain unclear. It is necessary to verify whether the dramatic rise in COVID-related research can be observed by visualizations. The purpose of this study was to display the acronym trends in comparison through the use of temporal graphs and to verify that the COVID acronym has a significant edge over the other 2 in terms of research dominance.\nAn analysis of the 30 most frequently used acronyms related to COVID in PubMed since 1950 was carried out using 4 graphs to conduct this bibliometric analysis, including line charts, temporal bar graphs (TBGs), temporal heatmaps (THM), and growth-share matrices (GSM). The absolute advantage coefficient (AAC) was used to measure the dominance strength for COVID acronym since 2020. COVID's AAC trend was expected to decline over time.\nThis study found that COVID, DNA, and human immunodeficiency virus have been the most frequently observed research acronyms since 2020, followed by computed tomography and World Health Organization; although there is no ideal method for displaying acronym trends over time, researchers can utilize the GSM to complement traditional line charts, TBGs, and THMs, as shown in this study; and COVID has a significant edge over the other 2 in terms of research dominance by ACC (\u22650.67), but COVID's AAC trend has declined (e.g., AACs 0.83, 0.80, and 0.69) since 2020.\nIt is recommended that the GSM complement traditional line charts, TBGs, and THMs in trend analysis, rather than being restricted to acronyms in future research. This research provides readers with the AAC to understand how research dominates its counterparts, which will be useful for future bibliometric analyses.", 
    "abstract": "Graphs are data structures that effectively represent relational data in the real world. Graph representation learning is a significant task since it could facilitate various downstream tasks, such as node classification, link prediction, etc. Graph representation learning aims to map graph entities to low-dimensional vectors while preserving graph structure and entity relationships. Over the decades, many models have been proposed for graph representation learning. This paper aims to show a comprehensive picture of graph representation learning models, including traditional and state-of-the-art models on various graphs in different geometric spaces. First, we begin with five types of graph embedding models: graph kernels, matrix factorization models, shallow models, deep-learning models, and non-Euclidean models. In addition, we also discuss graph transformer models and Gaussian embedding models. Second, we present practical applications of graph embedding models, from constructing graphs for specific domains to applying models to solve tasks. Finally, we discuss challenges for existing models and future research directions in detail. As a result, this paper provides a structured overview of the diversity of graph embedding models.", 
    "abstract": "This paper presents a procedure for classifying objects based on their compliance with information gathered using tactile sensors. Specifically, smart tactile sensors provide the raw moments of the tactile image when the object is squeezed and desqueezed. A set of simple parameters from moment-versus-time graphs are proposed as features, to build the input vector of a classifier. The extraction of these features was implemented in the field programmable gate array (FPGA) of a system on chip (SoC), while the classifier was implemented in its ARM core. Many different options were realized and analyzed, depending on their complexity and performance in terms of resource usage and accuracy of classification. A classification accuracy of over 94% was achieved for a set of 42 different classes. The proposed approach is intended for developing architectures with preprocessing on the embedded FPGA of smart tactile sensors, to obtain high performance in real-time complex robotic systems.", 
    "abstract": "Nosocomial bacterial and fungal infections are one of the main causes of high morbidity and mortality worldwide, owing to the high prevalence of multidrug-resistant microbial strains. Hence, the study aims to synthesize, characterize, and investigate the antifungal and antibacterial activity of silver nanoparticles (AgNPs) fabricated using ", 
    "abstract": "Overweight and obesity are public health problems that affects the workplace. This paper aims to analyse the effectiveness of workplace health promotion interventions in reducing Body Mass Index (BMI); Methods: Following PRISMA guidelines, a systematic review was conducted using PubMed, MEDLINE, and SCOPUS databases. The inverse variance statistical method was used for the meta-analysis with a random effects analysis model and standardised means. The results have been represented by Forest Plots and Funnel Plots graphs; Results: The multicomponent approach had the best results for reducing BMI (-0.14 [-0.24, -0.03], 95% CI; \nThe multicomponent approach could be an effective intervention to reduce obesity in the working population. However, workplace health promotion programs must be standardised to conduct quality analyses and highlight their importance to workers' well-being.", 
    "abstract": "Molecular representation learning is an essential component of many molecule-oriented tasks, such as molecular property prediction and molecule generation. In recent years, graph neural networks (GNNs) have shown great promise in this area, representing a molecule as a graph composed of nodes and edges. There are increasing studies showing that coarse-grained or multiview molecular graphs are important for molecular representation learning. Most of their models, however, are too complex and lack flexibility in learning different granular information for different tasks. Here, we proposed a flexible and simple graph transformation layer (i.e., LineEvo), a plug-and-use module for GNNs, which enables molecular representation learning from multiple perspectives. The LineEvo layer transforms fine-grained molecular graphs into coarse-grained ones based on the line graph transformation strategy. Especially, it treats the edges as nodes and generates the new connected edges, atom features, and atom positions. By stacking LineEvo layers, GNNs can learn multilevel information, from atom-level to triple-atoms level and coarser level. Experimental results show that the LineEvo layers can improve the performance of traditional GNNs on molecular property prediction benchmarks on average by 7%. Additionally, we show that the LineEvo layers can help GNNs have more expressive power than the Weisfeiler-Lehman graph isomorphism test.", 
    "abstract": "Medical dialog systems have the potential to assist e-medicine in improving access to healthcare services, improving patient treatment quality, and lowering medical expenses. In this research, we describe a knowledge-grounded conversation generation model that demonstrates how large-scale medical information in the form of knowledge graphs can aid in language comprehension and generation in medical dialog systems. Generic responses are often produced by existing generative dialog systems, resulting in monotonous and uninteresting conversations. To solve this problem, we combine various pre-trained language models with a medical knowledge base (UMLS) to generate clinically correct and human-like medical conversations using the recently released MedDialog-EN dataset. The medical-specific knowledge graph contains broadly 3 types of medical-related information, including disease, symptom and laboratory test. We perform reasoning over the retrieved knowledge graph by reading the triples in each graph using MedFact attention, which allows us to use semantic information from the graphs for better response generation. In order to preserve medical information, we employ a policy network, which effectively injects relevant entities associated with each dialog into the response. We also study how transfer learning can significantly improve the performance by utilizing a relatively small corpus, created by extending the recently released CovidDialog dataset, containing the dialogs for diseases that are symptoms of Covid-19. Empirical results on the MedDialog corpus and the extended CovidDialog dataset demonstrate that our proposed model significantly outperforms the state-of-the-art methods in terms of both automatic evaluation and human judgment.", 
    "abstract": "Hydropeaking is one of the major hydropower-related disturbances of natural processes in river systems. The artificial flow fluctuations that are caused by the on-demand production of electricity are known for their severe impacts on aquatic ecosystems. These particularly affect those species and life stages that are not able to adjust their habitat selection to rapid up- and downramping rates. To date, the stranding risk has both experimentally and numerically mainly been investigated with variable hydropeaking graphs over stable river bathymetries. There is a lack of knowledge on how single, discrete peaking events vary concerning their impact on the stranding risk when the river morphology changes in the long-term perspective. The present study precisely addresses this knowledge gap by investigating morphological changes on the reach scale over a period of 20\u00a0years and the related variability of the lateral ramping velocity as a proxy for stranding risk. Two alpine gravel bed rivers impacted by hydropeaking over decades were tested by applying a one-dimensional and two-dimensional unsteady modelling approach. Both the Bregenzerach River and the Inn River exhibit alternating gravel bars on the reach scale. The results of the morphological development, however, showed different developments in the period 1995-2015. The Bregenzerach River displayed continuous aggradation (uplift of river bed) over the various selected submonitoring periods. In contrast, the Inn River showed continuous incision (erosion of river bed). The stranding risk exhibited high variability on a single cross-sectional basis. However, on the reach scale, no significant changes in stranding risk were calculated for either river reach. In addition, the impacts of river incision on the substrate composition were investigated. Here, in line with preceding studies, the results show that the coarsening of substrate increases the stranding risk and that especially the d", 
    "abstract": "Energy is an important network indicator defined by the eigenvalues of an adjacency matrix that includes the neighbor information for each node. This article expands the definition of network energy to include higher-order information between nodes. We use resistance distances to characterize the distances between nodes and order complexes to extract higher-order information. Topological energy ( T E), defined by the resistance distance and order complex, reveals the characteristics of the network structure from multiple scales. In particular, calculations show that the topological energy can be used to distinguish graphs with the same spectrum well. In addition, topological energy is robust, and small random perturbations of edges do not significantly affect the T E values. Finally, we find that the energy curve of the real network is significantly different from that of the random graph, thus showing that T E can be used to distinguish the network structure well. This study shows that T E is an indicator that distinguishes the structure of a network and has some potential applications for real-world problems.", 
    "abstract": "Historical negative control data (HCD) have played an increasingly important role in interpreting the results of genotoxicity tests. In particular, Organisation for Economic Co-operation and Development (OECD) genetic toxicology Test Guidelines recommend comparing responses produced by exposure to test substances with the distribution of HCD as one of three criteria for evaluating and interpreting study results (referred to herein as \"Criterion C\"). Because of the potential for inconsistency in how HCD are acquired, maintained, described, and used to interpret genotoxicity testing results, a workgroup of the International Workshops for Genotoxicity Testing was convened to provide recommendations on this crucial topic. The Workgroup used example data sets from four in vivo tests, the Pig-a gene mutation assay, the erythrocyte-based micronucleus test, the transgenic rodent gene mutation assay, and the in vivo alkaline comet assay to illustrate how the quality of HCD can be evaluated. In addition, recommendations are offered on appropriate methods for evaluating HCD distributions. Recommendations of the Workgroup are: When concurrent negative control data fulfill study acceptability criteria, they represent the most important comparator for judging whether a particular test substance induced a genotoxic effect. HCD can provide useful context for interpreting study results, but this requires supporting evidence that i) HCD were generated appropriately, and ii) their quality has been assessed and deemed sufficiently high for this purpose. HCD should be visualized before any study comparisons take place; graph(s) that show the degree to which HCD are stable over time are particularly useful. Qualitative and semi-quantitative assessments of HCD should also be supplemented with quantitative evaluations. Key factors in the assessment of HCD include: i) the stability of HCD over time, and ii) the degree to which inter-study variation explains the total variability observed. When animal-to-animal variation is the predominant source of variability, the relationship between responses in the study and an HCD-derived interval or upper bounds value (i.e., OECD Criterion C) can be used with a strong degree of confidence in contextualizing a particular study's results. When inter-study variation is the major source of variability, comparisons between study data and the HCD bounds are less useful, and consequentially, less emphasis should be placed on using HCD to contextualize a particular study's results. The Workgroup findings add additional support for the use of HCD for data interpretation; but relative to most current OECD Test Guidelines, we recommend a more flexible application that takes into consideration HCD quality. The Workgroup considered only commonly used in vivo tests, but it anticipates that the same principles will apply to other genotoxicity tests, including many in vitro tests.", 
    "abstract": "Recent work [Mirth et al., J. Chem. Phys. 154, 114114 (2021)] has demonstrated that sublevelset persistent homology provides a compact representation of the complex features of an energy landscape in 3 N-dimensions. This includes information about all transition paths between local minima (connected by critical points of index \u22651) and allows for differentiation of energy landscapes that may appear similar when considering only the lowest energy pathways (as tracked by other representations, such as disconnectivity graphs, using index 1 critical points). Using the additive nature of the conformational potential energy landscape of n-alkanes, it became apparent that some topological features-such as the number of sublevelset persistence bars-could be proven. This work expands the notion of predictable energy landscape topology to any additive intramolecular energy function on a product space, including the number of sublevelset persistent bars as well as the birth and death times of these topological features. This amounts to a rigorous methodology to predict the relative energies of all topological features of the conformational energy landscape in 3N dimensions (without the need for dimensionality reduction). This approach is demonstrated for branched alkanes of varying complexity and connectivity patterns. More generally, this result explains how the sublevelset persistent homology of an additive energy landscape can be computed from the individual terms comprising that landscape.", 
    "abstract": "Perinatal depression (PND) describes depression experienced by parents during pregnancy or in the first year after a baby is born. The EQ-5D instrument (a generic measure of health status) is not often collected in perinatal research, however disease-specific measures, such as the Edinburgh Postnatal Depression Scale (EPDS) are widely used. Mapping can be used to estimate generic health utility index values from disease-specific measures like the EPDS.\nTo develop a mapping algorithm to estimate EQ-5D utility index values from the EPDS.\nPatient-level data from the BaBY PaNDA study (English observational cohort study) provided 1068 observations with paired EPDS and EQ-5D (3-level version; EQ-5D-3L) responses. We compared the performance of six alternative regression model types, each with four specifications of covariates (EPDS score and age: base, squared, and cubed). Model performance (ability to predict utility values) was assessed by ranking mean error, mean absolute error, and root mean square error. Algorithm performance in 3 external datasets was also evaluated.\nThere was moderate correlation between EPDS score and utility values (coefficient: \u2009-\u20090.42). The best performing model type was a two-part model, followed by ordinary least squared. Inclusion of squared and cubed covariates improved model performance. Based on graphs of observed and predicted utility values, the algorithm performed better when utility was above 0.6.\nThis direct mapping algorithm allows the estimation of health utility values from EPDS scores. The algorithm has good external validity but is likely to perform better in samples with higher health status.", 
    "abstract": "Clonogenic assays are routinely used to evaluate the response of cancer cells to external radiation fields, assess their radioresistance and radiosensitivity, estimate the performance of radiotherapy. However, classic clonogenic tests focus on the number of colonies forming on a substrate upon exposure to ionizing radiation, and disregard other important characteristics of cells such their ability to generate structures with a certain shape. The radioresistance and radiosensitivity of cancer cells may depend less on the number of cells in a colony and more on the way cells interact to form complex networks. In this study, we have examined whether the topology of 2D cancer-cell graphs is influenced by ionizing radiation. We subjected different cancer cell lines, i.e. H4 epithelial neuroglioma cells, H460 lung cancer cells, PC3 bone metastasis of grade IV of prostate cancer and T24 urinary bladder cancer cells, cultured on planar surfaces, to increasing photon radiation levels up to 6\u00a0Gy. Fluorescence images of samples were then processed to determine the topological parameters of the cell-graphs developing over time. We found that the larger the dose, the less uniform the distribution of cells on the substrate-evidenced by high values of small-world coefficient (cc), high values of clustering coefficient (cc), and small values of characteristic path length (cpl). For all considered cell lines, [Formula: see text] for doses higher or equal to 4\u00a0Gy, while the sensitivity to the dose varied for different cell lines: T24 cells seem more distinctly affected by the radiation, followed by the H4, H460 and PC3 cells. Results of the work reinforce the view that the characteristics of cancer cells and their response to radiotherapy can be determined by examining their collective behavior-encoded in a few topological parameters-as an alternative to classical clonogenic assays.", 
    "abstract": "Precipitation is one of the most significant components for the basin's hydrological cycle. Numerous features of a basin's water circulation may be affected by the chronological, geographical, and seasonal fluctuation of precipitation. It could be an important factor that influences hydrometeorological phenomena including floods and droughts. In this research, the innovative trend risk analysis (ITRA), innovative trend pivot analysis (ITPAM), and trend polygon star (TPS) methodologies of visualizing precipitation data are used to detect precipitation changes at six stations in Algeria's Wadi Ouahrane basin from 1972 to 2018. ITRA graphs show the direction of the precipitation trend (increasing-decreasing) and the trend risk class. Disparities in the polygons generated by the arithmetic mean and standard deviation ITPAM graphs demonstrate variations in precipitation seasonally and in the seasonal precipitation trends (increasing or decreasing) between sites. The TPS maps depict monthly variations in precipitation and highlight the autumn and spring transitions between the dry and wet seasons.", 
    "abstract": "Apical periodontitis is a prevalent oral inflammatory disease that has recently been linked to transcription factor EB (TFEB)-mediated autophagy. Regulator of G-protein signalling 10 (RGS10) is reported to be an effective regulator of the immune system and inflammation. This study aimed to investigate the involvement of RGS10 during the development of apical periodontitis through the TFEB-mediated autophagy signalling pathway.\nSixty BALB/c mice were randomly divided into four groups of 15 mice for the in vivo experiment. Rgs10 was locally overexpressed through eight injections of an adeno-associated virus vector. The model of apical periodontitis was established 21\u2009days following pulp exposure, and the mice were euthanized to obtain mandibles for analysis. Micro-computed tomography was employed to assess alveolar bone destruction, and the levels of Rgs10, TFEB-mediated autophagy signalling factors and inflammatory factors were measured using quantitative reverse transcription polymerase chain reactions, western blotting, enzyme-linked immunosorbent assays, immunofluorescence and immunohistochemistry. All experimental results were displayed as images or graphs. For the in vitro experiments, we employed small interfering RNA (siRNA) to silence Rgs10 expression in RAW 264.7 cells. The data were analysed via one-way anova or Mann-Whitney U test/Kruskal-Wallis test of variance, where p\u2009<\u2009.05 or U\u2009>\u20091.96 was considered statistically significant.\nLocal overexpression of Rgs10 reduced alveolar bone destruction within the apical periodontitis lesion and significantly decreased macrophage infiltration (p\u2009<\u2009.05). Meanwhile, the expression of TFEB-mediated autophagy signalling factors was upregulated, along with a decrease in inflammatory factor expression (p\u2009<\u2009.05). Lipopolysaccharide-stimulated RAW 264.7 cells exhibited decreased Rgs10 expression and TFEB-mediated autophagy signalling. siRNA-mediated silencing of Rgs10 further suppressed autophagy and concomitantly upregulated inflammatory factors (p\u2009<\u2009.05).\nCollectively, the findings revealed that RGS10 suppresses the inflammatory response and bone destruction through TFEB-mediated autophagy in apical periodontitis.", 
    "abstract": "This study evaluated the luminous behavior applied to materials used in intraocular surgeries.\nDiscs of the different products were delivered in 19.00\u2009mm \u00d7 3.00\u2009mm. Each sample was fixed on support keeping it perpendicular to the spectrophotometer beam. Later, their analyses were carried out in the air/PMMA ratio. The graphs of individual profiles of the measurements along the length were constructed according to each of the filters from the spectrophotometric analysis. In addition, descriptive statistics of transmittance and absorbance for each wavelength presented were correlated for each filter.\nIt is possible to observe that the minimum absorption measure was found in the Red Filter, especially in the blue and green light spectrum.\nUsing filters in PMMA materials appears to improve visual quality in corneal implants, especially the red filter, due to greater absorbance of light leading to fewer light scattering phenomena through corneal rings. However, further studies comparing the effects of different filters on Intracorneal rings should be carried out to elucidate this field of study.", 
    "abstract": "We construct the first infinite families of locally 2-arc transitive graphs with the property that the automorphism group has two orbits on vertices and is quasiprimitive on exactly one orbit, of twisted wreath type. This work contributes to Giudici, Li and Praeger's program for the classification of locally 2-arc transitive graphs by showing that the star normal quotient twisted wreath category also contains infinitely many graphs.", 
    "abstract": "Many papers offer methods for preparing a systematic literature review. These methods assume that the researchers have some experience in research, are proficient in English, and that the research objective is solely a literature review. This article presents a systematic method for preparing a literature review aimed at novice researchers who have four to twelve weeks to develop their work and do not have the guidance of a professor. Originality is associated with the objective of the literature review. The proposed method aims to elaborate the literature section of a technical article, while the other methods aim to elaborate a literature review article. The method's flexible structure allows for increasing the depth of the results according to the researcher's capacity. Another innovation of the presented method consists of a structure that allows the simultaneous consideration of international and national literature.\u2022This paper introduces a systematized method to guide novice researchers in preparing the literature review section of their research.\u2022The method has an easy-to-follow structure that does not require the novice researcher to follow up with a professor.\u2022The method allows adjusting the depth level of the international literature review through the number of articles subject to content analysis, exploring the international and national literature through a set of materials (graphs, forms, and figures) that facilitate and speed up the elaboration, synthesis, and presentation of the results.", 
    "abstract": "Recent studies in human brain connectomics with multimodal magnetic resonance imaging (MRI) data have widely reported abnormalities in brain structure, function and connectivity associated with schizophrenia (SZ). However, most previous discriminative studies of SZ patients were based on MRI features of brain regions, ignoring the complex relationships within brain networks.\nWe applied a graph convolutional network (GCN) to discriminating SZ patients using the features of brain region and connectivity derived from a combined multimodal MRI and connectomics analysis. Structural magnetic resonance imaging (sMRI) and resting-state functional magnetic resonance imaging (rs-fMRI) data were acquired from 140 SZ patients and 205 normal controls. Eighteen types of brain graphs were constructed for each subject using 3 types of node features, 3 types of edge features, and 2 brain atlases. We investigated the performance of 18 brain graphs and used the TopK pooling layers to highlight salient brain regions (nodes in the graph).\nThe GCN model, which used functional connectivity as edge features and multimodal features (sMRI + fMRI) of brain regions as node features, obtained the highest average accuracy of 95.8%, and outperformed other existing classification studies in SZ patients. In the explainability analysis, we reported that the top 10 salient brain regions, predominantly distributed in the prefrontal and occipital cortices, were mainly involved in the systems of emotion and visual processing.\nOur findings demonstrated that GCN with a combined multimodal MRI and connectomics analysis can effectively improve the classification of SZ at an individual level, indicating a promising direction for the diagnosis of SZ patients. The code is available at https://github.com/CXY-scut/GCN-SZ.git.", 
    "abstract": "", 
    "abstract": "Training with backpropagation (BP) in standard deep learning consists of two main steps: a forward pass that maps a data point to its prediction, and a backward pass that propagates the error of this prediction back through the network. This process is highly effective when the goal is to minimize a specific objective function. However, it does not allow training on networks with cyclic or backward connections. This is an obstacle to reaching brain-like capabilities, as the highly complex heterarchical structure of the neural connections in the neocortex are potentially fundamental for its effectiveness. In this paper, we show how predictive coding (PC), a theory of information processing in the cortex, can be used to perform inference and learning on arbitrary graph topologies. We experimentally show how this formulation, called ", 
    "abstract": "Rumor posts have received substantial attention with the rapid development of online and social media platforms. The automatic detection of rumor from posts has emerged as a major concern for the general public, the government, and social media platforms. Most existing methods focus on the linguistic and semantic aspects of posts content, while ignoring knowledge entities and concepts hidden within the article which facilitate rumor detection. To address these limitations, in this paper, we propose a novel end-to-end attention and graph-based neural network model (KAGN), which incorporates external knowledge from the knowledge graphs to detect rumor. Specifically, given the post's sparse and ambiguous semantics, we identify entity mentions in the post's content and link them to entities and concepts in the knowledge graphs, which serve as complementary semantic information for the post text. To effectively inject external knowledge into textual representations, we develop a knowledge-aware attention mechanism to fuse local knowledge. Additionally, we construct a graph consisting of posts texts, entities, and concepts, which is fed to graph convolutional networks to explore long-range knowledge through graph structure. Our proposed model can therefore detect rumor by combining semantic-level and knowledge-level representations of posts. Extensive experiments on four publicly available real-world datasets show that KAGN outperforms or is comparable to other state-of-the-art methods, and also validate the effectiveness of knowledge.", 
    "abstract": "The current investigation employs a numerical simulation to demonstrate the impact of hall current on unsteady free convective flow caused by hybrid-nanofluid over a revolving sphere approaching the stagnation point. The prominent characteristics of Lorentz force as a result of magnetic field coupling with hybrid nanofluid is also explored. The process of energy and mass transmission is inspected with nonlinear thermal radiations, non-uniform energy supply, dissipation and nonlinear chemical reaction. In current flow model, a unique class of nanofluid known as the hybrid nanofluid is being used, which contain ", 
    "abstract": "Coronavirus disease 2019 (COVID-19), is a pandemic disease that has appeared in recent years with different symptoms and manifestations. This disease has Co-infection with other infections and has aggravated the symptoms in patients. This study was conducted with the aim of reporting a case of Co-infection COVID-19 and influenza with psychotic symptoms. In this study, the manifestations of a case of co-infection with COVID-19 and influenza with psychotic symptoms were discussed in Shahroud city in Iran in 2022. Based on this, the patient's laboratory, pathological and therapeutic findings were investigated. The patient, a 16-year-old boy, had symptoms of fever, chills, cough, body pain, and headache with seizures and delirium. Reverse transcription-polymerase chain reaction tests for covid-19 and influenza were positive, but no abnormalities were observed in laboratory variables and graphs. The patient was treated for psychotic disorders for 40 days and then recovered.", 
    "abstract": "Enabled by wearable sensing, e.g., photoplethysmography (PPG) and electrocardiography (ECG), and machine learning techniques, study on cuffless blood pressure (BP) measurement with data-driven methods has become popular in recent years. However, causality has been overlooked in most of current studies. In this study, we aim to examine the feasibility of causal inference for cuffless BP estimation. We first attempt to detect wearable features that are causally related, rather than correlated, to BP changes by identifying causal graphs of interested variables with fast causal inference (FCI) algorithm. With identified causal features, we then employ time-lagged link to integrate the mechanism of causal inference into the BP estimated model. The proposed method was validated on 62 subjects with their continuous ECG, PPG and BP signals being collected. We found new causal features that can better track BP changes than pulse transit time (PTT). Further, the developed causal-based estimation model achieved an estimation error of mean absolute difference (MAD) being 5.10 mmHg and 2.85 mmHg for SBP and DBP, respectively, which outperformed traditional model without consideration of causality. To the best of our knowledge, this work is the first to study the causal inference for cuffless BP estimation, which can shed light on the mechanism, method and application of cuffless BP measurement.", 
    "abstract": "Causal feature selection is essential for estimating effects from observational data. Identifying confounders is a crucial step in this process. Traditionally, researchers employ content-matter expertise and literature review to identify confounders. Uncontrolled confounding from unidentified confounders threatens validity, conditioning on intermediate variables (mediators) weakens estimates, and conditioning on common effects (colliders) induces bias. Additionally, without special treatment, erroneous conditioning on variables combining roles introduces bias. However, the vast literature is growing exponentially, making it infeasible to assimilate this knowledge. To address these challenges, we introduce a novel knowledge graph (KG) application enabling causal feature selection by combining computable literature-derived knowledge with biomedical ontologies. We present a use case of our approach specifying a causal model for estimating the total causal effect of depression on the risk of developing Alzheimer's disease (AD) from observational data.\nWe extracted computable knowledge from a literature corpus using three machine reading systems and inferred missing knowledge using logical closure operations. Using a KG framework, we mapped the output to target terminologies and combined it with ontology-grounded resources. We translated epidemiological definitions of confounder, collider, and mediator into queries for searching the KG and summarized the roles played by the identified variables. We compared the results with output from a complementary method and published observational studies and examined a selection of confounding and combined role variables in-depth.\nOur search identified 128 confounders, including 58 phenotypes, 47 drugs, 35 genes, 23 collider, and 16 mediator phenotypes. However, only 31 of the 58 confounder phenotypes were found to behave exclusively as confounders, while the remaining 27 phenotypes played other roles. Obstructive sleep apnea emerged as a potential novel confounder for depression and AD. Anemia exemplified a variable playing combined roles.\nOur findings suggest combining machine reading and KG could augment human expertise for causal feature selection. However, the complexity of causal feature selection for depression with AD highlights the need for standardized field-specific databases of causal variables. Further work is needed to optimize KG search and transform the output for human consumption.", 
    "abstract": "The purpose of this study series, which involves a questionnaire survey and qualitative interviews, was to (a) evaluate patient-reported usefulness of continuous glucose monitor (CGM) hypoglycemia-informing features and (b) identify challenges in using these features (ie, CGM glucose numbers, trend arrows, trend graphs, and hypoglycemia alarms) during hypoglycemia in adults with type 1 diabetes (T1DM).\nA cross-sectional questionnaire survey study was conducted with adults who have T1DM and were using CGMs to assess the perceived usefulness of hypoglycemia-informing features. A semistructured interview study with T1DM CGM-using adults and inductive thematic analysis were subsequently performed to identify challenges in using CGM hypoglycemia-informing features to manage hypoglycemia.\nIn the survey study (N\u2009=\u2009252), the CGM glucose numbers, trend arrows, trend graphs, and hypoglycemia alarms were found to be very useful by 79%, 70%, 43%, and 64% of participants, respectively. Several challenges in using these features to manage hypoglycemia were identified in the qualitative study (N\u2009=\u200923): (1) hypoglycemia information not fully reliable,; (2) unpredictability of future blood glucose levels, (3) lack of awareness about how information can be used, and (4) disruptions associated with information.\nAlthough the majority of T1DM adults found their CGMs' hypoglycemia-informing features helpful, challenges in optimally using these features persisted. Targeted knowledge and behavioral interventions could improve CGM use to reduce hypoglycemia.", 
    "abstract": "Standardization for reporting medical outcomes enables clinical study comparisons and has a fundamental role in research reproducibility. In this context, we present mEYEstro, a free novel standalone application for automated standardized refractive surgery graphs. mEYEstro can be used for single and multiple group comparisons in corneal and intraocular refractive surgery patients. In less than 30\u00a0s and with minimal user manipulation, mEYEstro automatically creates the required journal standard graphs while simultaneously performing valid statistical analyses.\nThe software produces the following 11 standard graphs; Efficacy: 1. Cumulative uncorrected (UDVA) and corrected visual acuity (CDVA), 2. Difference between UDVA and CDVA, Safety: 3. Change in line of CDVA, Accuracy: 4. Spherical equivalent (SEQ) to intended target, 5. Attempted vs. achieved SEQ, 6. Defocus equivalent (DEQ) accuracy, 7. Refractive astigmatism accuracy, 8. Target-induced astigmatism vs. Surgically-induced astigmatism, 9. Correction index histogram, 10. Angle of error histogram, Stability: 11. SEQ stability over time. Percent proportions, means, standard deviations, Cohen's d effect sizes, and p-values are calculated and displayed on each graph. All graphs can be easily exported as high-resolution TIFF images for figures to use in scientific manuscripts and presentations.\nmEYEstro software enables clinicians, surgeons, and researchers, to easily and efficiently analyze refractive surgery outcomes using the standardized methodology required by several peer-reviewed ophthalmology journals.", 
    "abstract": "To evaluate the relationship between warm ischemia time (WIT) duration and renal function after robot-assisted partial nephrectomy (RAPN).\nThe CLOCK trial is a phase 3 randomized controlled trial comparing on- vs off-clamp RAPN. All patients underwent pre- and postoperative renal scintigraphy. Six-month absolute variation of eGFR (AV-GFR), rate of relative variation in eGFR over 25% (RV-GFR\u2009>\u200925), absolute variation of split renal function (SRF) at scintigraphy (AV-SRF). The relationships WIT/outcomes were assessed by correlation graphs and then modeled by uni- and multivariable regression.\n324 patients were included (206 on-clamp, 118 off-clamp RAPN). Correlation graphs showed a threshold on WIT equal to 10\u00a0min. The differences in outcome measures between cases with WIT\u2009<\u2009vs\u2009\u2265\u200910\u00a0min were: AV-GFR -\u00a03.7 vs -\u00a07.5\u00a0ml/min (p\u2009<\u20090.001); AV-SRF -\u00a01% vs -\u00a03.6% (p\u2009<\u20090.001); RV-GFR\u2009>\u200925 9.3% vs 17.8% (p\u2009=\u20090.008). Multivariable models found that AV-GFR was related to WIT\u2009\u2265\u200910\u00a0min (regression coefficient [RC] -\u00a00.52, p\u2009=\u20090.019), age (RC -\u00a00.35, p\u2009=\u20090.001) and baseline eGFR (RC -\u00a00.30, p\u2009<\u20090.001); RV-GFR\u2009>\u200925 to WIT\u2009\u2265\u200910\u00a0min (odds ratio [OR] 1.11, p\u2009=\u20090.007) and acute kidney injury defined as\u2009>\u200950% increase in serum creatinine (OR 19.7, p\u2009=\u20090.009); AV-SRF to WIT\u2009\u2265\u200910\u00a0min (RC -\u00a00.30, p\u2009=\u20090.018), baseline SRF (RC -\u00a00.76, p\u2009<\u20090.001) and RENAL score (RC -\u00a00.60. p\u2009=\u20090.028). The main limitation was that the CLOCK trial was designed on a different endpoint and therefore the present analysis could be underpowered.\nUp to 10\u00a0min WIT had no consequences on functional outcomes. Above the 10-min threshold, a statistically significant, but clinically negligible impact was found.", 
    "abstract": "For the advancement in fields of organic and perovskite solar cells, various techniques of structural alterations are being employed on previously reported chromophores. This way, molecules with all the properties desired for better performance of solar cells can be achieved. In this regard, theoretical modeling of chromophores has gained quite an interest due to its ability to save time, resources, and money. Herein, five new Y-shaped donor materials were theoretically engineered by adding electron-withdrawing acceptors on reported 2DP molecule. The results explored that, in comparison to 2DP, the produced molecules showed red shift in the absorption peaks, smaller bandgaps and binding energies, lower excitation potential, and greater dipole moment and were also highly reactive. When paired with PC\nPrecisely, a DFT and TD-DFT analysis on 2DP and all of the proposed molecules was conducted, using the functional MPW1PW91 at 6-31G (d,p) basis set to examine their optoelectronic aspects; additionally, the solvent state computations were studied with a TD-SCF simulation. For all these simulations, Gaussian 09 and GaussView 5.0 were employed. Moreover, the Origin 6.0 software, Multiwfn 3.8 software, and PyMOlyze 1.1 software were utilized for the visual depiction of the graphs of absorption, TDM, and DOS, respectively, of the studied molecules. A number of crucial aspects such as FMOs, bandgaps, light-harvesting efficiency, electrostatic potential, dipole moment, ionization potential, open-circuit voltage, fill factor, binding energy, interaction coefficient, chemical hardness-softness, and electrophilicity index were also investigated for the studied molecules.", 
    "abstract": "Language has been taken as a privileged window to investigate mental processes. More recently, descriptions of psychopathological symptoms have been analyzed with the help of natural language processing tools. An example is the study of speech organization using graph theoretical approaches that began around ten years ago. After its application in different areas, there is a need to characterize better what aspects can be associated with typical and atypical behavior throughout the lifespan, given variables related to aging, as well as biological and social contexts. The precise quantification of mental processes assessed through language may allow us to disentangle bio/social markers by looking at naturalistic protocols in different contexts. In the current review, we discuss ten years of studies in which word recurrence graphs were adopted to characterize the chain of thoughts expressed by individuals while producing discourse. Initially developed to understand formal thought disorder in the context of psychotic syndromes, this line of research has been expanded to understand atypical development in different stages of psychosis, differential diagnosis (such as dementia), as well as typical development of thought organization in school-age children/teenagers in naturalistic and school-based protocols. We comment on the effects of environmental factors, such as education and reading habits (in monolingual and bilingual contexts), in clinical and non-clinical populations at different developmental stages (from childhood to aging). Looking towards the future, there is an opportunity to employ word recurrence graphs to address complex questions that consider bio/social factors within a developmental perspective in typical and atypical contexts.", 
    "abstract": "Protein sequence comparison is a fundamental element in the bioinformatics toolkit. When sequences are annotated with features such as functional domains, transmembrane domains, low complexity regions or secondary structure elements, the resulting feature architectures allow better informed comparisons. However, many existing schemes for scoring architecture similarities cannot cope with features arising from multiple annotation sources. Those that do fall short in the resolution of overlapping and redundant feature annotations.\nHere, we introduce FAS, a scoring method that integrates features from multiple annotation sources in a directed acyclic architecture graph. Redundancies are resolved as part of the architecture comparison by finding the paths through the graphs that maximize the pair-wise architecture similarity. In a large-scale evaluation on more than 10\u00a0000 human-yeast ortholog pairs, architecture similarities assessed with FAS are consistently more plausible than those obtained using e-values to resolve overlaps or leaving overlaps unresolved. Three case studies demonstrate the utility of FAS on architecture comparison tasks: benchmarking of orthology assignment software, identification of functionally diverged orthologs, and diagnosing protein architecture changes stemming from faulty gene predictions. With the help of FAS, feature architecture comparisons can now be routinely integrated into these and many other applications.\nFAS is available as python package: https://pypi.org/project/greedyFAS/.", 
    "abstract": "Hydrogen bonds play a vital role in the stability and functioning of biomolecules. Suitable binary liquids are often used as prototypes for the study of biologically significant hydrogen bond studies and their intricate networks. Often, such systems show deviations in their physico-chemical properties from ideal conditions. As a continuation of our research interest in biologically important hydrogen-bonded systems, this paper reports the classical molecular dynamic studies on mixtures of aniline with 8 primary alcohols (C\nInteraction energies are calculated using B3LYP/6-311G++(d, p) density functional theory using Gaussian-09. The molecular dynamics simulations are carried out using GROMACS (V 2020.6) with the OPLS/AA force field and the simulation box is visualized using VMD. The NetworkX Python package is used for GTA calculation.", 
    "abstract": "Network approaches have successfully been used to help reveal complex mechanisms of diseases including Chronic Obstructive Pulmonary Disease (COPD). However despite recent advances, we remain limited in our ability to incorporate protein-protein interaction (PPI) network information with omics data for disease prediction. New deep learning methods including convolution Graph Neural Network (ConvGNN) has shown great potential for disease classification using transcriptomics data and known PPI networks from existing databases. In this study, we first reconstructed the COPD-associated PPI network through the AhGlasso (Augmented High-Dimensional Graphical Lasso Method) algorithm based on one independent transcriptomics dataset including COPD cases and controls. Then we extended the existing ConvGNN methods to successfully integrate COPD-associated PPI, proteomics, and transcriptomics data and developed a prediction model for COPD classification. This approach improves accuracy over several conventional classification methods and neural networks that do not incorporate network information. We also demonstrated that the updated COPD-associated network developed using AhGlasso further improves prediction accuracy. Although deep neural networks often achieve superior statistical power in classification compared to other methods, it can be very difficult to explain how the model, especially graph neural network(s), makes decisions on the given features and identifies the features that contribute the most to prediction generally and individually. To better explain how the spectral-based Graph Neural Network model(s) works, we applied one unified explainable machine learning method, SHapley Additive exPlanations (SHAP), and identified CXCL11, IL-2, CD48, KIR3DL2, TLR2, BMP10 and several other relevant COPD genes in subnetworks of the ConvGNN model for COPD prediction. Finally, Gene Ontology (GO) enrichment analysis identified glycosaminoglycan, heparin signaling, and carbohydrate derivative signaling pathways significantly enriched in the top important gene/proteins for COPD classifications.", 
    "abstract": "Many diabetic patients develop and progress to diabetic foot ulcers, which seriously affect health and quality of life and cause great economic and psychological stress, especially in elderly diabetic patients who often have various underlying diseases, and the consequences of their progression to diabetic foot ulcers are more serious and seriously affect elderly patients in surgery. Therefore, it is particularly important to analyze the influencing factors related to the progression of elderly diabetic patients to diabetic foot, and the column line graph prediction model is drawn based on regression analysis to derive the influencing factors of the progression of elderly diabetic patients to diabetic foot, and the total score derived from the combination of various influencing factors can visually calculate the probability of the progression of elderly diabetic patients to diabetic foot.\nThe influencing factors of progression deterioration to diabetic foot in elderly diabetic patients based on LASSO regression analysis and logistics regression analysis, and the column line graph prediction model was established by statistically significant risk factors.\nThe clinical data of elderly diabetic patients aged 60 years or older in the orthopedic ward and endocrine ward of the Third Hospital of Shanxi Medical University from 2015-01-01 to 2021-12-31 were retrospectively analyzed and divided into a modeling population (211) and an internal validation population (88) according to the random assignment principle. Firstly, LASSO regression analysis was performed based on the modeling population to screen out the independent influencing factors for progression to diabetic foot in elderly diabetic patients; Logistics univariate and multifactor regressions were performed by the screened influencing factors, and then column line graph prediction models for progression to diabetic foot in elderly diabetic patients were made by these influencing factors, using ROC (subject working characteristic curve) and AUC (their area under the curve), C-index validation, and calibration curve to initially evaluate the model discrimination and calibration. Model validation was performed by the internal validation set, and the ROC curve, C-index and calibration curve were used to further evaluate the column line graph model performance. Finally, using DCA (decision curve analysis), we observed whether the model could be used better in clinical settings.\n(1) LASSO (Least absolute shrinkage and selection operator) regression analysis yielded a more significant significance on risk factors for progression to diabetic foot in elderly diabetic patients, such as age, presence of peripheral neuropathy, history of smoking, duration of disease, serum lactate dehydrogenase, and high-density cholesterol; (2) Based on the influencing factors and existing theories, a column line graph prediction model for progression to diabetic foot in elderly diabetic patients was constructed. The working characteristic curves of subjects in the training group and their area under the curve (area under the curve = 0.840) were also analyzed simultaneously with the working characteristic curves of subjects in the external validation population and their area under the curve (area under the curve = 0.934), which finally showed that the model was effective in predicting column line graphs; (iii) the C-index in the modeled cohort was 0.840 (95%CI: 0.779-0.901) and the C-index in the validation cohort was 0.934 (95%CI: 0.887-0.981), indicating that the model had good predictive accuracy; the calibration curve fit was good; (iv) the results of the decision curve analysis showed that the model would have good results in clinical use; (v) it indicated that the established predictive model for predicting progression to diabetic foot in elderly diabetic patients had good test efficacy and helped clinically screen the possibility of progression to diabetic foot in elderly diabetic patients and give personalized interventions to different patients in time.", 
    "abstract": "The occurrence of extended spectrum beta-lactamase (ESBL) producing bacteria such as \nA total of 100 ESBL-EC isolates from humans 35/100 (35%), animals 56/100 (56%), and the environment 9/100 (9%) were tested for susceptibility to 11 antibiotics. This was done using the Kirby-Bauer disk diffusion method according to Clinical and Laboratory Standards Institute (CLSI) guidelines. Data were analyzed in STATA ver. 16 and graphs were drawn in Microsoft excel ver. 10.\nMost of the ESBL-EC isolates (98%) were resistant to more than two antibiotics. ESBL-EC isolates were most susceptible to meropenem (MEM) (88.0%), and imipenem (82.0%) followed by gentamicin (72%). ESBL-EC isolates from humans were most susceptible to meropenem (MEM) followed by imipenem (IPM)> gentamicin (CN)> ciprofloxacin (CIP). Animal samples were more susceptible to MEM, IPM, and CN but were highly resistant to cefotaxime (CTX)> cefepime (FEP)>other antibiotics. Multidrug resistance (MDR) was mostly reported among households keeping goats under intensive husbandry practices. Seven percent of the isolates exhibited carbapenem resistance while 22% showed aminoglycoside resistance. Similar resistance patterns among humans, animals, and environmental samples were also reported.\nOur study provides baseline information on non-hospital-based MDR caused by ESBL-EC using a One Health approach. ESBL-EC isolates were prevalent among apparently healthy community members, animals, and their environment. It is important to conduct more One Health approach studies to generate evidence on the drivers, resistance patterns, and transmission of ESBL-producing organisms at the human-animal-environmental interface.", 
    "abstract": "The isometry in crisp graph theory is a well-known fact. But, isometry under a fuzzy environment was developed recently and studied many facts. In a m-polar fuzzy graph, we have to think m components for each node and edge. Since, in our consideration, we consider m components for each nodes as well as edges, therefore we can not handle this type of situation using fuzzy model as their is a single components for this concept. Again, we can not apply bipolar or intuitionistic fuzzy graph model as each edges or nodes have just two components. Thus, these mPFG models give more efficient fuzziness results than other fuzzy model. Also, it is very interesting to develop and analyze such types of mPFGs with examples and related theorems. Considering all those things together, we have presented isometry under a m-polar fuzzy environment. In this paper, we have discussed the isometric m-polar fuzzy graph along with many exciting facts about it. Metric space properties have also been implemented on m-polar fuzzy isometric graph. We also have initiated a generalized fuzzy graph, namely antipodal m-polar fuzzy graphs, along with several issues. The degree of it is also presented along with edge regularity properties. We also give a relation between m-polar fuzzy antipodal graphs and their underlying crisp graphs. Its properties have also been discussed on m-polar fuzzy odd as well as even cycles, complete graphs, etc. Finally, a real-life application on a road network system in a m-polar fuzzy environment using the [Formula: see text]-distance concept is also presented.", 
    "abstract": "To compare perinatal outcomes between singleton live births following blastocyst-stage and cleavage-stage fresh embryo transfer using data from all United Kingdom (UK) licensed fertility clinics.\nCohort study.\nIn vitro fertilization (IVF)/intracytoplasmic sperm injection (ICSI) cycle-based data recorded in the UK Human Fertilisation and Embryology Authority (HFEA) anonymized dataset.\n60,926 IVF/ICSI cycles resulting in a singleton live birth after blastocyst-stage and cleavage-stage fresh embryo transfer between 2012 and 2018.\nBaseline characteristics between IVF/ICSI blastocyst and cleavage-stage transfer groups were compared using the Chi-squared test for categorical/dichotomised variables and the Mann-Whitney test for continuous variables. Statistical significance was set at <0.05. Association between perinatal outcomes and blastocyst transfer compared to cleavage-stage transfer was assessed using multinomial logistic regression, adjusting for confounders selected using directed acyclic graphs (DAGs) (95% confidence interval [CI], adjusted relative risk ratio [aRRRs]). A subgroup analysis included cycles in women undergoing their first IVF/ICSI cycle.\nGestational age at birth and birth weight.\nThe blastocyst group comprised 42,677 IVF/ICSI cycles and cleavage-stage group 18,249 cycles. There was likely little to no difference in the risk of preterm birth (PTB) (aRRR, 1.07; 95% CI, 1.00-1.15) and very preterm birth (VPTB) (aRRR, 1.05; 95% CI, 0.91-1.21) in singleton live births after fresh blastocyst and cleavage-stage transfer. Risk of low birth weight (LBW) (aRRR, 1.02; 95% CI, 0.95-1.09), very low birth weight (VLBW) (aRRR 0.96; 95% CI, 0.83-1.11), high birth weight (HBW) (aRRR, 0.97; 95% CI, 0.90-1.04) and very high birth weight (VHBW) (aRRR, 0.91; 95% CI, 0.77-1.08) were likely similar between the groups. Findings were consistent in the subgroup analysis.\nFresh blastocyst transfer does not appear to have a negative impact on gestational age at birth and birth weight in singleton live births compared to fresh cleavage-stage transfer.", 
    "abstract": "Driver distraction has been recognized for a long time as a significant road safety issue. It has been consistently reported that drivers spend considerable time engaged in activities that are secondary to the driving task. The temporary diversion of attention from safety-critical driving tasks has often been associated with various adverse driving outcomes, from minor driving errors to serious motor vehicle crashes. This study explores the role of the driving context on a driver's decision to engage in secondary activities non-critical to the driving task.\nThe study utilises the Naturalistic Engagement in Secondary Tasks (NEST) dataset, a complementary dataset derived from the SHRP2 naturalistic dataset, the most extensive naturalistic study to date. An initial exploratory analysis is conducted to identify patterns of secondary task engagements in relation to context variables. Maximum likelihood Chi-square tests were applied to test for differences in engagement between types of driver distraction for the selected contextual variables. Pearson residual graphs were employed as a supplementary method to visually depict the residuals that constitute the chi-square statistic.Lastly, a two-step cluster analysis was conducted to identify common execution scenarios among secondary tasks.\nThe exploratory analysis revealed interesting behavioral trends among drivers, with higher engagement rates in left curves compared to right curves, while driving uphill compared to driving downhill, in low-density traffic scenarios compared to high-density traffic scenarios, and during afternoon periods compared to morning periods. Significant differences in engagement were found among secondary tasks in relation to locality, speed, and roadway design. The clustering analysis showed no significant associations between driving scenarios of similar characteristics and the type of secondary activity executed.\nOverall, the findings confirm that the road traffic environment can influence how car drivers engage in distracted driving behavior.", 
    "abstract": "To analyze the association between birth weight and bone mineral density (BMD) in adolescence.\nA birth cohort study in S\u00e3o Lu\u00eds, Maranh\u00e3o, using data from two moments: at birth and at 18-19 years. Exposure was the birth weight in grams, continuously analyzed. The outcome was BMD, using the Z-score index (whole body) measured by double X-ray densitometry (Dexa). A theoretical model was constructed in acyclic graphs to identify the minimum set of adjustment variables - household income, the mother knowing how to read and write at the time of birth, prenatal care, tobacco use during pregnancy, and parity - to evaluate the association between birth weight and bone mineral density in adolescence. Multiple linear regression was used in Stata 14.0 software. A 5% significance level was adopted.\nFrom 2,112 adolescents, 8.2% had low birth weight and 2.8% had a low BMD for their age. The mean full-body Z-score was 0.19 (\u00b1 1.00). The highest birth weight was directly and linearly associated with BMD values in adolescence (Coef.: 0.10; 95%CI: 0.02-0.18), even after adjustment for the variables household income (Coef.: -0.33; 95%CI: -0.66-0.33) and the mother knowing how to read and write (Coef.: 0.23%; 95%CI: 0.03-0.43).\nAlthough after adjusting the variables the association attenuated, birth weight positively and linearly relates to BMD in adolescence.\nAnalisar a associa\u00e7\u00e3o entre o peso ao nascer e a densidade mineral \u00f3ssea (DMO) na adolesc\u00eancia.\nEstudo de coorte de nascimentos em S\u00e3o Lu\u00eds, Maranh\u00e3o, utilizando dados de dois momentos: ao nascimento e aos 18\u201319 anos. A exposi\u00e7\u00e3o foi o peso ao nascer em gramas, analisado de forma cont\u00ednua. O desfecho foi a DMO, utilizando o \u00edndice Z-escore (corpo inteiro) medido pela densitometria por dupla emiss\u00e3o de raios X (DEXA). Foi constru\u00eddo modelo te\u00f3rico em gr\u00e1ficos ac\u00edclicos direcionados para identificar o conjunto m\u00ednimo de vari\u00e1veis de ajuste \u2013 renda familiar, a m\u00e3e saber ler e escrever \u00e0 \u00e9poca do nascimento, realiza\u00e7\u00e3o de pr\u00e9-natal, tabagismo durante a gesta\u00e7\u00e3o e paridade \u2013 para avaliar a associa\u00e7\u00e3o entre o peso ao nascer e a densidade mineral \u00f3ssea na adolesc\u00eancia. Utilizou-se regress\u00e3o linear m\u00faltipla no \nDos 2.112 adolescentes, 8,2% apresentaram baixo peso ao nascer e 2,8% apresentaram DMO considerada baixa para a idade. O Z-escore m\u00e9dio de corpo inteiro foi de 0,19 (\u00b1 1,00). O maior peso ao nascer foi associado de forma linear e direta aos valores de DMO na adolesc\u00eancia (Coef.: 0,10; IC95% 0,02\u20130,18), mesmo ap\u00f3s ajuste para as vari\u00e1veis renda familiar (Coef.: -0,33; IC95% -0,66\u20130,33) e a m\u00e3e saber ler e escrever (Coef.: 0,23; IC95% 0,03\u20130,43).\nApesar de a associa\u00e7\u00e3o ter sido atenuada ap\u00f3s ajuste das vari\u00e1veis, o peso ao nascer est\u00e1 associado de forma positiva e linear \u00e0 DMO na adolesc\u00eancia.", 
    "abstract": "Childhood stunting is still a global public health challenge, including in Ethiopia. Over the past decade, in developing countries, stunting has been characterized by large rural and urban disparities. To design an effective intervention, it is necessary to understand the urban and rural disparities in stunting.\nTo assess the urban-rural disparities in stunting among Ethiopian children aged 6-59 months.\nThis study was done based on the data obtained from the 2019 mini-Ethiopian Demographic and Health Survey, conducted by the Central Statistical Agency of Ethiopia and ICF international. The result of descriptive statistics was reported using the mean with standard deviation, frequency, percentages, graphs, and tables. A multivariate decomposition analysis was used to decompose the urban-rural disparity in stunting into two components: one that is explained by residence differences in the level of the determinants (covariate effects), and the other component is explained by differences in the effect of the covariates on the outcome (coefficient effects). The results were robust to the different decomposition weighting schemes.\nThe prevalence of stunting among Ethiopian children aged 6-59 months was 37.8% (95% CI: 36.8%, 39.6%). The difference in stunting prevalence between urban and rural residences was high (rural prevalence was 41.5%, while in urban areas it was 25.5%). Endowment and coefficient factors explained the urban-rural disparity in stunting with magnitudes of 35.26% and 64.74%, respectively. Maternal educational status, sex, and age of children were the determinants of the urban-rural disparity in stunting.\nThere is a significant stunting disparity among urban and rural children in Ethiopia. A larger portion of the urban-rural stunting disparity was explained by coefficient effects (differences in behaviour). Maternal educational status, sex, and age of children were the determinants of the disparity. So, to narrow this disparity, emphasis should be given to both resource distribution and the appropriate utilization of available interventions, including improvement of maternal education and consideration of sex and age differences during child feeding practices.", 
    "abstract": "Different objects in the same scene are more or less related to each other, but only a limited number of these relationships are noteworthy. Inspired by Detection Transformer, which excels in object detection, we view scene graph generation as a set prediction problem. In this paper, we propose an end-to-end scene graph generation model Relation Transformer (RelTR), which has an encoder-decoder architecture. The encoder reasons about the visual feature context while the decoder infers a fixed-size set of triplets subject-predicate-object using different types of attention mechanisms with coupled subject and object queries. We design a set prediction loss performing the matching between the ground truth and predicted triplets for the end-to-end training. In contrast to most existing scene graph generation methods, RelTR is a one-stage method that predicts sparse scene graphs directly only using visual appearance without combining entities and labeling all possible predicates. Extensive experiments on the Visual Genome, Open Images V6, and VRD datasets demonstrate the superior performance and fast inference of our model.", 
    "abstract": "The Ising model on networks plays a fundamental role as a testing ground for understanding cooperative phenomena in complex systems. Here we solve the synchronous dynamics of the Ising model on random graphs with an arbitrary degree distribution in the high-connectivity limit. Depending on the distribution of the threshold noise that governs the microscopic dynamics, the model evolves to nonequilibrium stationary states. We obtain an exact dynamical equation for the distribution of local magnetizations, from which we find the critical line that separates the paramagnetic from the ferromagnetic phase. For random graphs with a negative binomial degree distribution, we demonstrate that the stationary critical behavior as well as the long-time critical dynamics of the first two moments of the local magnetizations depend on the distribution of the threshold noise. In particular, for an algebraic threshold noise, these critical properties are determined by the power-law tails of the distribution of thresholds. We further show that the relaxation time of the average magnetization inside each phase exhibits the standard mean-field critical scaling. The values of all critical exponents considered here are independent of the variance of the negative binomial degree distribution. Our work highlights the importance of certain details of the microscopic dynamics for the critical behavior of nonequilibrium spin systems.", 
    "abstract": "This work is dedicated to the topological analysis of complex transitional networks for dynamic state detection. Transitional networks are formed from time series data and they leverage graph theory tools to reveal information about the underlying dynamic system. However, traditional tools can fail to summarize the complex topology present in such graphs. In this work, we leverage persistent homology from topological data analysis to study the structure of these networks. We contrast dynamic state detection from time series using a coarse-grained state-space network (CGSSN) and topological data analysis (TDA) to two state of the art approaches: ordinal partition networks (OPNs) combined with TDA and the standard application of persistent homology to the time-delay embedding of the signal. We show that the CGSSN captures rich information about the dynamic state of the underlying dynamical system as evidenced by a significant improvement in dynamic state detection and noise robustness in comparison to OPNs. We also show that because the computational time of CGSSN is not linearly dependent on the signal's length, it is more computationally efficient than applying TDA to the time-delay embedding of the time series.", 
    "abstract": "In various machinery engines, the engine oil is utilized as a lubricant. Heat transportation rate and to saving the energy dissipated due to higher temperature are the basic goals of all thermal systems. Thus, current work is mainly focused to develop a model for the Marangoni flow of nanofluids (NFs) with viscous dissipation. The considered NFs are made of nanoparticles (NPs) i.e. [Formula: see text] and base fluid (BF) as Engine Oil (EO). Darcy Forchheimer (DF) law which leads to porous medium is implemented in the model to investigate the variations of NF velocity and temperature. The governing flow expressions are simplified through similarity variables. The obtained expressions are solved numerically via an effective technique known as the NDSolve algorithm. The consequences of pertinent variables on temperature, velocity and Nusselt number are designed through tables and graphs. The obtained results reveal that velocity rises for higher Marangoni number, Darcy Forchheimer (DF) parameter whereas it shows decaying behavior against nanoparticles volume fraction.", 
    "abstract": "Sequence-to-graph alignment is crucial for applications such as variant genotyping, read error correction, and genome assembly. We propose a novel seeding approach that relies on long inexact matches rather than short exact matches, and demonstrate that it yields a better time-accuracy trade-off in settings with up to a 25% mutation rate. We use sketches of a subset of graph nodes, which are more robust to indels, and store them in a ", 
    "abstract": "The sparse representation of graphs has shown great potential for accelerating the computation of graph applications (e.g., social networks and knowledge graphs) on traditional computing architectures (CPU, GPU, or TPU). But, the exploration of large-scale sparse graph computing on processing-in-memory (PIM) platforms (typically with memristive crossbars) is still in its infancy. To implement the computation or storage of large-scale or batch graphs on memristive crossbars, a natural assumption is that a large-scale crossbar is demanded, but with low utilization. Some recent works question this assumption; to avoid the waste of storage and computational resource, the fixed-size or progressively scheduled \"block partition\" schemes are proposed. However, these methods are coarse-grained or static and are not effectively sparsity-aware. This work proposes the dynamic sparsity-aware mapping scheme generating method that models the problem with a sequential decision-making model, and optimizes it by reinforcement learning (RL) algorithm (REINFORCE). Our generating model long short-term memory (LSTM), combined with the dynamic-fill scheme generates remarkable mapping performance on the small-scale graph/matrix data (complete mapping costs 43% area of the original matrix) and two large-scale matrix data (costing 22.5% area on qh882 and 17.1% area on qh1484). Our method may be extended to sparse graph computing on other PIM architectures, not limited to the memristive device-based platforms.", 
    "abstract": "Class I Major Histocompatibility Complex plays a critical role in the adaptive immune response by binding to peptides processed by Proteasome and Transporter associated with antigen processing complex and presenting them on the cell surface to cytotoxic T-cells. Understanding the process of peptide presentation and studying how presented peptides are distributed in the huge space of all potential epitopes could have a dramatic impact in the context of vaccine design, transplantation, autoimmunity, and cancer development.\nIn the present work we propose a graph-driven approach to investigate the landscape of both self (human) and viral (254 organisms) peptides presented on cell surface through class I Major Histocompatibility Complex considering specific HLAs. For each considered HLA (N\u00a0=\u00a089) we designed a network, namely Peptide Hamming Graph, where nodes are peptides predicted to be presented by a given HLA and an edge is set when the Hamming distance between two peptides is equal or smaller than 2 (i.e. the same amino acid occurs in at least 7 positions of the two sequences).\nThrough the analysis of Peptide Hamming Graphs we studied how predicted presented peptides are distributed in the whole configurational space for different HLAs, identifying sets of viral peptides that can constitute a potential target for the immune system. In particular we selected connected components of the graph made exclusively of viral peptides and sets of viral peptides with high node degree interacting exclusively with viral neighbours.\nThis work constitutes an innovative approach to study potential cytotoxic T-cell epitopes relying on a network approach, overcoming the classical paradigm based on the identification of potential epitopes only considering their features as single peptides. T-cell cross-reactivity plays a focal role for the efficacy of this strategy increasing the probability of recognition, and consequently a stronger immune response, of presented peptides far from self, sharing a common pattern in terms of sequence similarity.", 
    "abstract": "Node representation learning has attracted increasing attention due to its efficacy for various applications on graphs. However, fairness is a largely under-explored territory within the field, although it is shown that the use of graph structure in learning amplifies bias. To this end, this work theoretically explains the sources of bias in node representations obtained via graph neural networks (GNNs). It is revealed that both nodal features and graph structure lead to bias in the obtained representations. Building upon the analysis, fairness-aware data augmentation frameworks are developed to reduce the intrinsic bias. Our theoretical analysis and proposed schemes can be readily employed in understanding and mitigating bias for various GNN-based learning mechanisms. Extensive experiments on node classification and link prediction over multiple real networks are carried out, and it is shown that the proposed augmentation strategies can improve fairness while providing comparable utility to state-of-the-art methods.", 
    "abstract": "Patients and families need to be provided with trusted information more than ever with the abundance of online information. Several organizations aim to build databases that can be searched based on the needs of target groups. One such group is individuals with neurodevelopmental disorders (NDDs) and their families. NDDs affect up to 18% of the population and have major social and economic impacts. The current limitations in communicating information for individuals with NDDs include the absence of shared terminology and the lack of efficient labeling processes for web resources. Because of these limitations, health professionals, support groups, and families are unable to share, combine, and access resources.\nWe aimed to develop a natural language-based pipeline to label resources by leveraging standard and free-text vocabularies obtained through text analysis, and then represent those resources as a weighted knowledge graph.\nUsing a combination of experts and service/organization databases, we created a data set of web resources for NDDs. Text from these websites was scraped and collected into a corpus of textual data on NDDs. This corpus was used to construct a knowledge graph suitable for use by both experts and nonexperts. Named entity recognition, topic modeling, document classification, and location detection were used to extract knowledge from the corpus.\nWe developed a resource annotation pipeline using diverse natural language processing algorithms to annotate web resources and stored them in a structured knowledge graph. The graph contained 78,181 annotations obtained from the combination of standard terminologies and a free-text vocabulary obtained using topic modeling. An application of the constructed knowledge graph is a resource search interface using the ordered weighted averaging operator to rank resources based on a user query.\nWe developed an automated labeling pipeline for web resources on NDDs. This work showcases how artificial intelligence-based methods, such as natural language processing and knowledge graphs for information representation, can enhance knowledge extraction and mobilization, and could be used in other fields of medicine.", 
    "abstract": "Good evidence is available that socioeconomic status (SES) positively correlates with access to orthodontic treatment. There is much less literature, however, on whether socioeconomic inequities affect patients once they are in treatment. SES predicts of treatment outcomes across many health disciplines.\nTo determine whether a similar relationship exists with orthodontic treatment and identify, evaluate and summarise the available evidence.\nSystematic review with searches of multiple databases to identify studies of children and adolescents who underwent orthodontic treatment, in which parental SES was the variable of interest, and treatment duration, treatment outcome or adherence of patients to the treatment plan were the measured outcomes of interest. Quality appraisal used CASP checklists. Data were synthesised narratively and in tables and graphs.\nSeventeen studies were included in the final review. The high level of heterogeneity between studies made it hard to draw conclusions from the data as a whole. Many studies also had several quality issues. Some evidence suggested an association between low SES and discontinuation of orthodontic treatment, and between the receipt of state subsidised care and poor appointment attendance.\nNo strong associations can be concluded. There is a need for more high-quality studies, perhaps incorporating access and uptake variables, to capture how different socioeconomic groups interact with orthodontic care.", 
    "abstract": "The formation rate, magnitude, and duration of the antibody-mediated humoral immune response that develops against different viral proteins of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) are considered important in vaccine success. It is known that the response to vaccinations decreases due to immunosenescence in older adults. This study was aimed to investigate the levels of serum IgA response at 1st and 3rd month after vaccination of people over 60 years old who were immunized with CoronaVac and Pfizer-BioNTech. A total of 35 people living in the North Cyprus who have not previously had COVID-19 infection were included in the study. After the 2nd dose of vaccination, serum IgA levels were measured after the 1st and 3rd month with the anti-SARS-CoV-2 IgA (Euroimmun, Lubeck, Germany) kit. The statistical significance was determined as 0.05 in the whole study. SPSS and GraphPad Prism software were used for calculations, analyses and graphs. The possible effect of demographic variables on serum IgA level was compared between the vaccine groups and it was found that there was no statistically significant difference between them. For the IgA titer-positive individuals who had been vaccinated with the Pfizer-BioNTech vaccine, for both 1st and 3rd months were observed to be higher than CoronaVac vaccinated IgA titer-positive individuals. In individuals who received the CoronaVac vaccine, there was a statistically significant change in serum IgA levels between 1st and 3rd months, but there was no statistically significant change in the Pfizer-BioNTech vaccine administered group. When the Pfizer/BioNTech and CoronaVac vaccines were compared with each other in terms of serum IgA antibody titers, it was found that the mean serum IgA levels of the individuals in the Pfizer/BioNTech group were statistically higher at the 1st and 3rd months than the CoronaVac group. Serum IgA titers in both vaccine groups were statistically significantly decreased from 1st month to 3rd month. This study showed that the Pfizer/BioNTech vaccine induced higher SARS-CoV-2 specific serum IgA antibodies than the CoronaVac vaccine and remained seropositive for a longer time in individuals aged 60 years and older. It is believed that the serum IgA levels that were determined may not reflect the serum IgA levels. However, these findings support the studies in other literature, showing that the Pfizer-BioNTech mRNA vaccine induces higher SARS-CoV-2 specific serum IgA antibodies than the inactive CoronaVac vaccine and that it remains seropositive for a longer period of time. This study is important as it is the first study to compare the SARS-CoV-2 IgA antibody responses of individuals over 60 years of age in the Turkish Republic of Northern Cyprus in two different vaccine groups.", 
    "abstract": "Pangenome graphs can represent all variation between multiple genomes, but existing methods for constructing them are biased due to reference-guided approaches. In response, we have developed PanGenome Graph Builder (PGGB), a reference-free pipeline for constructing unbi-ased pangenome graphs. PGGB uses all-to-all whole-genome alignments and learned graph embeddings to build and iteratively refine a model in which we can identify variation, measure conservation, detect recombination events, and infer phylogenetic relationships.", 
    "abstract": "De novo assembly of next generation metagenomic reads is widely used to provide taxonomic and functional information of genomes in a microbial community. As strains are functionally specific, recovery of strain-resolved genomes is important but still a challenge. Unitigs and assembly graphs are mid-products generated during the assembly of reads into contigs, and they provide higher resolution for sequences connection information. In this study, we propose a new approach UGMAGrefiner (a unitig level assembly graph-based metagenome-assembled Genome refiner), which uses the connection and coverage information from unitig level assembly graphs to recruit unbinned unitigs to MAGs, adjust binning result, and infer unitigs shared by multiple MAGs. In two simulated datasets (Simdata and CAMI data) and one real dataset (GD02), it outperforms two state-of-the-art assembly graph-based binning refine tools in the refinement of MAGs' quality by stably increasing the completeness of genomes. UGMAGrefiner can identify genome specific clusters of genomes with below 99% average nucleotide identity for homologous sequences. For MAGs mixed with 99% similarity genome clusters, it could distinguish 8 out of 9 genomes in Simdata and 8 out of 12 genomes in CAMI data. In GD02 data, it could identify 16 new unitig clusters representing genome specific regions of mixed genomes and 4 unitig clusters representing new genomes from total 135 MAGs for further functional analysis. UGMAGrefiner provides an efficient way to obtain more complete MAGs and study genome specific functions. It will be useful to improve taxonomic and functional information of genomes after de novo assembly.", 
    "abstract": "Clustering is usually the first exploratory analysis step in empirical data. When the data set comprises graphs, the most common approaches focus on clustering its vertices. In this work, we are interested in grouping networks with similar connectivity structures together instead of grouping vertices of the graph. We could apply this approach to functional brain networks (FBNs) for identifying subgroups of people presenting similar functional connectivity, such as studying a mental disorder. The main problem is that real-world networks present natural fluctuations, which we should consider.\nIn this context, spectral density is an exciting feature because graphs generated by different models present distinct spectral densities, thus presenting different connectivity structures. We introduce two clustering methods: k-means for graphs of the same size and gCEM, a model-based approach for graphs of different sizes. We evaluated their performance in toy models. Finally, we applied them to FBNs of monkeys under anesthesia and a dataset of chemical compounds.\nWe show that our methods work well in both toy models and real-world data. They present good results for clustering graphs presenting different connectivity structures even when they present the same number of edges, vertices, and degree of centrality.\nWe recommend using k-means-based clustering for graphs when graphs present the same number of vertices and the gCEM method when graphs present a different number of vertices.", 
    "abstract": "The prevalence of diabetes mellitus-induced erectile dysfunction (DMED) has recently increased, which has prompted numerous DMED studies. Here, we conduct a bibliometric analysis of relevant literature in the field of DMED and to discuss the research hotspots and future development directions.\nThe Web of Science Core Collection database was searched for literature on DMED, and literature characterization including the number of articles, journals, countries/regions, institutions, authors, keywords, and other information was performed using VOS viewer and CiteSpace software. In addition, Pajek software was used for visual map adjustment, and GraphPad Prism was used to generate line graphs.\nA total of 804 articles concerning DMED were included in this study. \nGlobal research on DMED is expected to increase further. The investigation of the mechanism of DMED and the exploration of new therapeutic means and targets are the focus of future research.", 
    "abstract": "Goodman proved that the sum of the number of triangles in a graph on ", 
    "abstract": "Sickle cell disease (SCD) is an autosomal recessive genetic disorder affecting millions of people worldwide. A reversible and selective DNMT1 inhibitor, GSK3482364, has been known to decrease the overall methylation activity of DNMT1, resulting in the increase of HbF levels and percentage of HbF-expressing erythrocytes in an ", 
    "abstract": "Healthcare providers (HCPs) often encounter clinical trial results in the form of data displays in prescription drug promotions. Information conveyed in data displays vary in their presentation and complexity. This study describes characteristics of data displays in prescription drug advertising targeted to HCPs.\nThis study characterized the content of 140 data displays in 98 unique print advertisements from 2009 to present and identified in AdPharm, an online database of pharmaceutical advertisements. Two reviewers independently coded the advertisements for characteristics (\u03ba\u2009=\u20090.85) including complexity, format, and quality.\nAbout one-third (32%) of the advertisements contained multiple data displays (range 2 to 6) and 44% showed clinical data from oncology trials; other disease domains were mental and behavioral health (14%), rheumatology and autoimmune disorders (8%), endocrinology (7%), cardiology (6%), infectious disease (6%), pulmonology and allergy (4%), and others (<\u20092% each). About one-half (51%) of displays were classified as \"simple\" which included \"pseudographs\" and basic tables or charts. \"Complex\" displays appeared as survival curves, line graphs, or bar graphs with complex features. Most complex displays included a comparator drug (90%), plain language restatement of the key finding (93%) and disclosure statements (91%) with additional study details, although their placement varied. Complex displays were of high quality, according to our selected indicators; our analysis found no data distortion or errors.\nData displays in prescription drug advertising are often highly complex. Future research assessing understanding of data displays and the potentially beneficial effect of disclosures and other features is warranted.", 
    "abstract": "Most current studies on information diffusion in online social networks focus on the deterministic aspects of social networks. However, the behavioral parameters of online social networks are uncertain, unpredictable, and time-varying. Thus, deterministic graphs for modeling information diffusion in online social networks are too restrictive to solve most real network problems, such as influence maximization. Recently, stochastic graphs have been proposed as a graph model for social network applications where the weights associated with links in the stochastic graph are random variables. In this paper, we first propose a diffusion model based on a stochastic graph, in which influence probabilities associated with its links are unknown random variables. Then we develop an approach using the set of learning automata residing in the proposed diffusion model to estimate the influence probabilities by sampling from the links of the stochastic graph. Numerical simulations conducted on real and artificial stochastic networks demonstrate the effectiveness of the proposed stochastic diffusion model for influence maximization.", 
    "abstract": "Data simulation is fundamental for machine learning and causal inference, as it allows exploration of scenarios and assessment of methods in settings with full control of ground truth. Directed acyclic graphs (DAGs) are well established for encoding the dependence structure over a collection of variables in both inference and simulation settings. However, while modern machine learning is applied to data of an increasingly complex nature, DAG-based simulation frameworks are still confined to settings with relatively simple variable types and functional forms. We here present DagSim, a Python-based framework for DAG-based data simulation without any constraints on variable types or functional relations. A succinct YAML format for defining the simulation model structure promotes transparency, while separate user-provided functions for generating each variable based on its parents ensure simulation code modularization. We illustrate the capabilities of DagSim through use cases where metadata variables control shapes in an image and patterns in bio-sequences. DagSim is available as a Python package at PyPI. Source code and documentation are available at: https://github.com/uio-bmi/dagsim.", 
    "abstract": "The overall survival benefits of perioperative chemotherapy (PCT) and perioperative chemoradiotherapy (PCRT) for patients with locally advanced gastric cancer (GC) have not been fully explored. The aim of this study was to compare the benefits of PCT and PCRT in GC patients and determine the factors affecting survival rate using directed acyclic graphs (DAGs). The data of 1,442 patients with stage II-IV GC who received PCT or PCRT from 2000 to 2018 were retrieved from the Surveillance, Epidemiology, and End Results (SEER) database. First, the least absolute shrinkage and selection operator (LASSO) was used to identify possible influencing factors for overall survival. Second, the variables that were selected by LASSO were then used in univariate and Cox regression analyses. Third, corrective analyses for confounding factors were selected based on DAGs that show the possible association between advanced GC patients and outcomes and evaluate the prognosis. Patients who received PCRT had longer overall survival than those who received PCT treatment (P = 0.015). The median length of overall survival of the PCRT group was 36.5 (15.0 - 53.0) months longer than that of the PCT group (34.6 (16.0 - 48.0) months). PCRT is more likely to benefit patients who are aged \u2264 65, male, white, and have regional tumors (P<0.05). The multivariate Cox regression model showed that male sex, widowed status, signet ring cell carcinoma, and lung metastases were independent risk factors for a poor prognosis. According to DAG, age, race, and Lauren type may be confounding factors that affect the prognosis of advanced GC. Compared to PCT, PCRT has more survival benefits for patients with locally advanced GC, and ongoing investigations are needed to better determine the optimal treatment. Furthermore, DAGs are a useful tool for contending with confounding and selection biases to ensure the proper implementation of high-quality research.", 
    "abstract": "This article presents a solution to the leaderless formation control problem for first-order multiagent systems, which minimizes a global function composed of a sum of local strongly convex functions for each agent under weighted undirected graphs within a predefined time. The proposed distributed optimization process consists of two steps: 1) the controller initially leads each agent to the minimizer of its local function and 2) then guides all agents toward achieving leaderless formation and reaching the global function's minimizer. The proposed scheme requires fewer adjustable parameters than most existing methods in the literature without the need for auxiliary variables or time-variable gains. Additionally, one can consider highly nonlinear multivalued strongly convex cost functions, while the agents do not share the gradients and Hessians. Extensive simulations and comparisons with state-of-the-art algorithms demonstrate the effectiveness of our approach.", 
    "abstract": "Our understanding of population history in deep time has been assisted by fitting admixture graphs ('AGs') to data: models that specify the ordering of population splits and mixtures, which along with the amount of genetic drift on each lineage and the proportions of mixture, is the only information needed to predict the patterns of allele frequency correlation among populations. Not needing to specify population size changes, split times, or whether admixture events were sudden or drawn out simplifies the space of models that need to be searched. However, the space of possible AGs relating populations is vast and cannot be sampled fully, and thus most published studies have identified fitting AGs through a manual process driven by prior hypotheses, leaving the vast majority of alternative models unexplored. Here, we develop a method for systematically searching the space of all AGs that can incorporate non-genetic information in the form of topology constraints. We implement this ", 
    "abstract": "\"The mental map\" is a concept that has been used and defined in numerous ways. The cognitive map, and the concept map-also known as the \"heuristic\" or \"mind\" map-are the two distinct contextual meanings covered by the term mental map in the present article. In the mental map domain, the first major field of study is geography, spatial cognition, and neurophysiology and it aims to understand how the route taken by a subject (or a set of subjects) in space leads to memorization and internal representation(s). In general, the externalization of these representations takes the form of drawings, positioning in a graph, or oral/textual narratives, but it is primarily reflected as a behavior in space that can be recorded as tracking items. A second field of study, one which is geared more toward exploratory and combinatorial uses, is the concept (also heuristic or mind) map which consists in organizing notions, concepts, and information in the form of tree graphs or graphs that can be used to produce diagrams and flowcharts. The aim is projective, for clarification and discovery purposes or for data organization and visualization. To date, very few studies in the literature have examined the similar, overlapping and oppositional features in what is broadly referred to as \"representation(s) of space\" and \"space(s) of representation.\" How can we better apprehend the complex notion of \"mental map?\" The question of memorial transcription? Of \"symbolic projection?\" Can we identify meeting points between these two polarities and, if possible, a continuum? Through the notion of cognitive graph, recent advances in the understanding of brain mechanisms enable us to approach the distinctions between cognitive map and conceptual map as an articulated and continuous whole.", 
    "abstract": "The aim of this note is to revisit the connections between some stochastic games, namely Tug-of-War games, and a class of nonlocal PDEs on graphs. We consider a general formulation of Tug-of-War games which is shown to be related to many classical PDEs in the continuous setting. We transcribe these equations on graphs using ad hoc differential operators and we show that it covers several nonlocal PDEs on graphs such as [Formula: see text]-Laplacian, game p-Laplacian and the eikonal equation. This unifying mathematical framework allows us to easily design simple algorithms to solve several inverse problems in imaging and data science, with a particular focus on cultural heritage and medical imaging.", 
    "abstract": "We develop and test new machine learning strategies for accelerating molecular crystal structure ranking and crystal property prediction using tools from geometric deep learning on molecular graphs. Leveraging developments in graph-based learning and the availability of large molecular crystal data sets, we train models for density prediction and stability ranking which are accurate, fast to evaluate, and applicable to molecules of widely varying size and composition. Our density prediction model, MolXtalNet-D, achieves state-of-the-art performance, with lower than 2% mean absolute error on a large and diverse test data set. Our crystal ranking tool, MolXtalNet-S, correctly discriminates experimental samples from synthetically generated fakes and is further validated through analysis of the submissions to the Cambridge Structural Database Blind Tests 5 and 6. Our new tools are computationally cheap and flexible enough to be deployed within an existing crystal structure prediction pipeline both to reduce the search space and score/filter crystal structure candidates.", 
    "abstract": "In this paper, the author proposes a novel solution to evaluate the dependence between the maximum roll angle and the minimum vertical force at the wheel on the vehicle's height and speed. 4D graphs that fully and clearly describe the dependence between parameters are used to replace conventional 2D and 3D graphs. A complex dynamic model is established to describe the vehicle's oscillations when steering. Calculations and simulations are performed using Simulink\u00ae software with many specific cases. In all cases, the input values such as steering angle, speed, and distance from RA (roll axis) to CG (the center of gravity), all change flexibly. According to the paper's findings, the roll angle will rise once speed or height increases. In addition, the maximum roll angle increases significantly once both the velocity and the height increase. This causes the vertical force at the wheel to drop suddenly, and rollover may occur if this value reduces to zero. 4D graphs provide a more specific and intuitive assessment of vehicle rollovers.", 
    "abstract": "The increasing complexity of today's software requires the contribution of thousands of developers. This complex collaboration structure makes developers more likely to introduce defect-prone changes that lead to software faults. Determining when these defect-prone changes are introduced has proven challenging, and using traditional machine learning (ML) methods to make these determinations seems to have reached a plateau. In this work, we build contribution graphs consisting of developers and source files to capture the nuanced complexity of changes required to build software. By leveraging these contribution graphs, our research shows the potential of using graph-based ML to improve Just-In-Time (JIT) defect prediction. We hypothesize that features extracted from the contribution graphs may be better predictors of defect-prone changes than intrinsic features derived from software characteristics. We corroborate our hypothesis using graph-based ML for classifying edges that represent defect-prone changes. This new framing of the JIT defect prediction problem leads to remarkably better results. We test our approach on 14 open-source projects and show that our best model can predict whether or not a code change will lead to a defect with an F1 score as high as 77.55% and a Matthews correlation coefficient (MCC) as high as 53.16%. This represents a 152% higher F1 score and a 3% higher MCC over the state-of-the-art JIT defect prediction. We describe limitations, open challenges, and how this method can be used for operational JIT defect prediction.", 
    "abstract": "Cell science has made significant progress by focusing on understanding individual cellular processes through reductionist approaches. However, the sheer volume of knowledge collected presents challenges in integrating this information across different scales of space and time to comprehend cellular behaviors, as well as making the data and methods more accessible for the community to tackle complex biological questions. This perspective proposes the creation of next-generation virtual cells, which are dynamic 3D models that integrate information from diverse sources, including simulations, biophysical models, image-based models, and evidence-based knowledge graphs. These virtual cells would provide statistically accurate and holistic views of real cells, bridging the gap between theoretical concepts and experimental data, and facilitating productive new collaborations among researchers across related fields.", 
    "abstract": "Currently, the methods and means of human-machine interaction and visualization as its integral part are being increasingly developed. In various fields of scientific knowledge and technology, there is a need to find and select the most effective visualization models for various types of data, as well as to develop automation tools for the process of choosing the best visualization model for a specific case. There are many data visualization tools in various application fields, but at the same time, the main difficulty lies in presenting data of an interconnected (node-link) structure, i.e., networks. Typically, a lot of software means use graphs as the most straightforward and versatile models. To facilitate visual analysis, researchers are developing ways to arrange graph elements to make comparing, searching, and navigating data easier. However, in addition to graphs, there are many other visualization models that are less versatile but have the potential to expand the capabilities of the analyst and provide alternative solutions. In this work, we collected a variety of visualization models, which we call alternative models, to demonstrate how different concepts of information representation can be realized. We believe that adapting these models to improve the means of human-machine interaction will help analysts make significant progress in solving the problems researchers face when working with graphs.", 
    "abstract": "Speaker Recognition (SR) is a common task in AI-based sound analysis, involving structurally different methodologies such as Deep Learning or \"traditional\" Machine Learning (ML). In this paper, we compared and explored the two methodologies on the DEMoS dataset consisting of 8869 audio files of 58 speakers in different emotional states. A custom CNN is compared to several pre-trained nets using image inputs of spectrograms and Cepstral-temporal (MFCC) graphs. AML approach based on acoustic feature extraction, selection and multi-class classification by means of a Na\u00efve Bayes model is also considered. Results show how a custom, less deep CNN trained on grayscale spectrogram images obtain the most accurate results, 90.15% on grayscale spectrograms and 83.17% on colored MFCC. AlexNet provides comparable results, reaching 89.28% on spectrograms and 83.43% on MFCC.The Na\u00efve Bayes classifier provides a 87.09% accuracy and a 0.985 average AUC while being faster to train and more interpretable. Feature selection shows how F0, MFCC and voicing-related features are the most characterizing for this SR task. The high amount of training samples and the emotional content of the DEMoS dataset better reflect a real case scenario for speaker recognition, and account for the generalization power of the models.", 
    "abstract": "This paper reviews graph-theory-based methods that were recently developed in our group for post-processing molecular dynamics trajectories. We show that the use of algorithmic graph theory not only provides a direct and fast methodology to identify conformers sampled over time but also allows to follow the interconversions between the conformers through graphs of transitions in time. Examples of gas phase molecules and inhomogeneous aqueous solid interfaces are presented to demonstrate the power of topological 2D graphs and their versatility for post-processing molecular dynamics trajectories. An even more complex challenge is to predict 3D structures from topological 2D graphs. Our first attempts to tackle such a challenge are presented with the development of game theory and reinforcement learning methods for predicting the 3D structure of a gas-phase peptide.", 
    "abstract": "The exceptionally rapid development of highly flexible, reusable artificial intelligence (AI) models is likely to usher in newfound capabilities in medicine. We propose a new paradigm for medical AI, which we refer to as generalist medical AI (GMAI). GMAI models will be capable of carrying out a diverse set of tasks using very little or no task-specific labelled data. Built through self-supervision on large, diverse datasets, GMAI will flexibly interpret different combinations of medical modalities, including data from imaging, electronic health records, laboratory results, genomics, graphs or medical text. Models will in turn produce expressive outputs such as free-text explanations, spoken recommendations or image annotations that demonstrate advanced medical reasoning abilities. Here we identify a set of high-impact potential applications for GMAI and lay out specific technical capabilities and training datasets necessary to enable them. We expect that GMAI-enabled applications will challenge current strategies for regulating and validating AI devices for medicine and will shift practices associated with the collection of large medical datasets.", 
    "abstract": "Without appropriate and responsible waste management in place, the cursory storage of tailings and waste rocks on the surface can cause devastating damage to the planet's ecosystems. To proactively manage or abolish the damage, some techniques such as mine backfill have been already used repeatedly in mines. Microstructure and strength behavior of cementitious tailings-crushed rock backfill (CTCRB) with gold/tungsten tailings and rock contents (e.g., 10%, 20%, 30%, 40%, and 50%) were conducted in this study by using both UCS (unconfined compressive strength) tests (e.g., peak strengths, stress-strain curves, failure modes) and SEM micro-graphs. Key conclusions were shown that: when gradation and content of crushed rock was considered as 1-3\u00a0mm and 50% respectively, the UCS value of gold tailings based backfills was 1.02\u00a0MPa. In contrast, the UCS value of tungsten mine tailings based backfills was 1.36\u00a0MPa when the amount of crushed rock within the filling matrix became 10%. Tungsten tailings based backfills were more sensitive to crushed rock gradation than gold tailings based backfills. CTCRB's stress-strain curvatures were up-concave in the step of pore compaction. With the increase in the content and gradation of crushed rock, tungsten tailings based backfills showed swelling and crushing in complete destruction. Tailings' particle size, crushed rock content and gradation utterly affected the failure modes of CTCRB. Ettringite/CSH gel was found to be the leading hydration materials in the backfill matrix. The micro-cracks within CTCRB specimens were unfavorably correlated with its UCS data. To conclude, this study's main outcomes could give a significant guide for CTCRB's industrial uses.", 
    "abstract": "This article investigates the similar formation control problem for multirobot systems. Specifically, we propose an integrated relative localization and similar formation control scheme to navigate multirobot systems to a desired configuration, which is a similar transformation of a given template, based on interrobot and robot-landmark range measurements and odometry measurements of robots themselves. To achieve the exact relative localization, a persistent excitation (P.E.) signal is introduced in the controller which, however, perturbs the motion of each robot and affects the formation accuracy. To resolve the conflict, an autonomous system with its output regulated by a carefully designed function of range measurements is introduced to generate the persistent excitation. It is proved that the similar formation control problem can be solved by our proposed scheme with global asymptotic convergence for directed acyclic graphs (DAGs). Both numerical simulation and physical experiment are presented to verify and validate the effectiveness of our theoretical findings.", 
    "abstract": "Functional connectomes (FCs), represented by networks or graphs that summarize coactivation patterns between pairs of brain regions, have been related at a population level to age, sex, cognitive/behavioral scores, life experience, genetics, and disease/disorders. However, quantifying FC differences between individuals also provides a rich source of information with which to map to differences in those individuals' biology, experience, genetics or behavior. In this study, graph matching is used to create a novel inter-individual FC metric, called swap distance, that quantifies the distance between pairs of individuals' partial FCs, with a smaller swap distance indicating the individuals have more similar FC. We apply graph matching to align FCs between individuals from the the Human Connectome Project ", 
    "abstract": "General graph neural networks (GNNs) implement convolution operations on graphs based on polynomial spectral filters. Existing filters with high-order polynomial approximations can detect more structural information when reaching high-order neighborhoods but produce indistinguishable representations of nodes, which indicates their inefficiency of processing information in high-order neighborhoods, resulting in performance degradation. In this article, we theoretically identify the feasibility of avoiding this problem and attribute it to overfitting polynomial coefficients. To cope with it, the coefficients are restricted in two steps, dimensionality reduction of the coefficients' domain and sequential assignment of the forgetting factor. We transform the optimization of coefficients to the tuning of a hyperparameter and propose a flexible spectral-domain graph filter, which significantly reduces the memory demand and the adverse impacts on message transmission under large receptive fields. Utilizing our filter, the performance of GNNs is improved significantly in large receptive fields and the receptive fields of GNNs are multiplied as well. Meanwhile, the superiority of applying a high-order approximation is verified across various datasets, notably in strongly hyperbolic datasets. Codes are publicly available at: https://github.com/cengzeyuan/TNNLS-FFKSF.", 
    "abstract": "The early recovery of hip function after hip fracture surgery values more attention, especially for patients with delayed surgery of longer than 48 hours. We aim to evaluate the associations of in-hospital surgical waiting time with the functional outcomes (Harris Hip score, Parker Mobility Score and EuroQol 5 dimensions VAS score) in elderly patients who sustained hip fractures.\nData on sociodemographic and clinical factors were prospectively collected using a multicenter hip fracture registry system. Participants in the cohort underwent a 12-months follow-up investigation. After adjusting potential confounders identified by the directed acyclic graphs, the associations between surgical waiting time longer than 48 hours and functional outcomes were estimated by log-binomial regression and multivariable linear regression models with generalized estimating equations.\nOf 863 survival participants with available functional data at 12 months after surgery, an increased risk was obtained from receiving surgery after 48 hours and the poor functional outcomes (HHS<80: RR=1.56, 95%CI 1.00, 2.51; PMS<7: RR=1.49, 95%CI 1.13, 2.01; EQ-5D VAS<80: RR=1.97, 95%CI 1.57, 2.47). In-hospital waiting time>48 hours were time-invariantly associated with lower PMS during recovery (-0.44 units 95%CI -0.70, -0.18). In addition, delayed surgery was time-varying associated with HHS and EQ-5D VAS.\nThe associations between in-hospital waiting time and postoperative functional score suggest that delayed surgery can lead to poor functional outcomes, especially in patients waiting longer than 72 hours from injury. Delayed surgery mainly impacting hip function and mobility recovery with a slower speed in early recovery of the first three months. More attentions should be paid to mechanisms behind the associations between delayed surgery on general healthy status.", 
    "abstract": "With the exponential increase in the volume of biomedical literature, text mining tasks are becoming increasingly important in the medical domain. Named entities are the primary identification tasks in text mining, prerequisites and critical parts for building medical domain knowledge graphs, medical question and answer systems, medical text classification.\nThe study goal is to recognize biomedical entities effectively by fusing multi-feature embedding. Multiple features provide more comprehensive information so that better predictions can be obtained.\nFirstly, three different kinds of features are generated, including deep contextual word-level features, local char-level features, and part-of-speech features at the word representation layer. The word representation vectors are inputs into BiLSTM as features to obtain the dependency information. Finally, the CRF algorithm is used to learn the features of the state sequences to obtain the global optimal tagging sequences.\nThe experimental results showed that the model outperformed other state-of-the-art methods for all-around performance in six datasets among eight of four biomedical entity types.\nThe proposed method has a positive effect on the prediction results. It comprehensively considers the relevant factors of named entity recognition because the semantic information is enhanced by fusing multi-features embedding.", 
    "abstract": "Networks like those of healthcare infrastructure have been a primary target of cyberattacks for over a decade. From just a single cyberattack, a healthcare facility would expect to see millions of dollars in losses from legal fines, business interruption, and loss of revenue. As more medical devices become interconnected, more cyber vulnerabilities emerge, resulting in more potential exploitation that may disrupt patient care and give rise to catastrophic financial losses. In this paper, we propose a structural model of an aggregate loss distribution across multiple cyberattacks on a prototypical hospital network. Modeled as a mixed random graph, the hospital network consists of various patient-monitoring devices and medical imaging equipment as random nodes to account for the variable occupancy of patient rooms and availability of imaging equipment that are connected by bidirectional edges to fixed hospital and radiological information systems. Our framework accounts for the documented cyber vulnerabilities of a hospital's trusted internal network of its major medical assets. To our knowledge, there exist no other models of an aggregate loss distribution for cyber risk in this setting. We contextualize the problem in the probabilistic graph-theoretical framework using a percolation model and combinatorial techniques to compute the mean and variance of the loss distribution for a mixed random network with associated random costs that can be useful for healthcare administrators and cybersecurity professionals to improve cybersecurity management strategies. By characterizing this distribution, we allow for the further utility of pricing cyber\u00a0risk.", 
    "abstract": "Current literature has sparse recommendations that guide social networking practices in plastic surgery. To address this, we used natural language processing and sentiment analysis to investigate the differences in plastic surgery-related terms and hashtags on Twitter.\nOver 1 million tweets containing keywords #plasticsurgery, #cosmeticsurgery, and their non-hashtagged versions plastic surgery and cosmetic surgery were collected from the Twitter Gardenhose feed spanning from 2012 to 2016. We extracted the average happiness/positivity (h-avg) using hedonometrics and created word-shift graphs to determine influential words.\nThe most popular keywords were plastic and cosmetic surgery, comprising more than 90% of the sample. The positivity scores for plastic surgery, cosmetic surgery, #plasticsurgery, and #cosmeticsurgery were 5.72, 6.00, 6.17, and 6.18, respectively. Compared to plastic surgery, the term cosmetic surgery was more positive because it lacked antagonistic words, such as \"fake,\" \"ugly,\" \"bad,\" \"fails,\" and \"wrong.\" For similar reasons, #plasticsurgery and #cosmeticsurgery were more positively associated than their non-hashtagged counterparts.\nPlastic surgery-related hashtags are more positively associated than their non-hashtagged versions. The language associated with such hashtags suggests a different user profile than the public and, given their underutilization, remain viable channels for professionals to achieve their diverse social media goals.\nThis journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 .", 
    "abstract": "This paper studies few-shot molecular property prediction, which is a fundamental problem in cheminformatics and drug discovery. More recently, graph neural network based model has gradually become the theme of molecular property prediction. However, there is a natural deficiency for existing methods, that is, the scarcity of molecules with desired properties, which makes it hard to build an effective predictive model. In this paper, we propose a novel framework called Hierarchically Structured Learning on Relation Graphs (HSL-RG) for molecular property prediction, which explores the structural semantics of a molecule from both global-level and local-level granularities. Technically, we first leverage graph kernels to construct relation graphs to globally communicate molecular structural knowledge from neighboring molecules and then design self-supervised learning signals of structure optimization to locally learn transformation-invariant representations from molecules themselves. Moreover, we propose a task-adaptive meta-learning algorithm to provide meta knowledge customization for different tasks in few-shot scenarios. Experiments on multiple real-life benchmark datasets show that HSL-RG is superior to existing state-of-the-art approaches.", 
    "abstract": "Comprehensive two-dimensional gas chromatography coupled with mass spectrometry (GC\u2009\u00d7\u2009GC-MS) has great potential for analyses of complicated mixtures and sample matrices, due to its separation power and possible high resolution. The second component of the measurement results, the mass spectra, is reproducible. However, the reproducibility of two-dimensional chromatography is affected by many factors and makes the evaluation of long-term experiments or cross-laboratory collaborations complicated. This paper presents a new open-source data alignment tool to tackle the problem of retention time shifts - with 5 different algorithms implemented: BiPACE 2D, DISCO, MSort, PAM, and TNT-DA, along with Pearson's correlation and dot product as optional methods for mass spectra comparison. The implemented data alignment algorithms and their variations were tested on real samples to demonstrate the functionality of the presented tool. The suitability of each implemented algorithm for significantly/non-significantly shifted data was discussed on the basis of the results obtained. For the evaluation of the \"goodness\" of the alignment, Kolmogorov-Smirnov test values were calculated, and comparison graphs were generated. The DA_2DChrom is available online with its documentation, fully open-sourced, and the user can use the tool without the need of uploading their data to external third-party servers.", 
    "abstract": "Identification of ncRNA-protein interactions (ncRPIs) through wet experiments is still time-consuming and highly-costly. Although several computational approaches have been developed to predict ncRPIs using the structure and sequence information of ncRNAs and proteins, the prediction accuracy needs to be improved, and the results lack interpretability. In this work, we proposed a novel computational method (called ncRPI-LGAT) to predict the ncRNA-Protein Interactions by transforming the link prediction (", 
    "abstract": "Accurate earthquake location and magnitude estimation play critical roles in seismology. Recent deep learning frameworks have produced encouraging results on various seismological tasks (e.g., earthquake detection, phase picking, seismic classification, and earthquake early warning). Many existing machine learning earthquake location methods utilize waveform information from a single station. However, multiple stations contain more complete information for earthquake source characterization. Inspired by recent successes in applying graph neural networks (GNNs) in graph-structured data, we develop a Spatiotemporal Graph Neural Network (STGNN) for estimating earthquake locations and magnitudes. Our graph neural network leverages geographical and waveform information from multiple stations to construct graphs automatically and dynamically by adaptive message passing based on graphs' edges. Using a recent graph neural network and a fully convolutional neural network as baselines, we apply STGNN to earthquakes recorded by the Southern California Seismic Network from 2000 to 2019 and earthquakes collected in Oklahoma from 2014 to 2015. STGNN yields more accurate earthquake locations than those obtained by the baseline models and performs comparably in terms of depth and magnitude prediction, though the ability to predict depth and magnitude remains weak for all tested models. Our work demonstrates the potential of using GNNs and multiple stations for better automatic estimation of earthquake epicenters.", 
    "abstract": "Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integrative role and serve as a center for other periphery nodes to exchange information. We evaluated the proposed CP-ViT on multiple public datasets, including medical image datasets (INbreast) and natural image datasets. Interestingly, by incorporating the BNN-derived principle (CP structure) into the redesign of ViT, our CP-ViT outperforms other state-of-the-art ANNs. In general, our work advances the state of the art in three aspects: 1) This work provides novel insights for brain-inspired AI: we can utilize the principles found in BNNs to guide and improve our ANN architecture design; 2) We show that there exist sweet spots of CP graphs that lead to CP-ViTs with significantly improved performance; and 3) The core nodes in CP-ViT correspond to task-related meaningful and important image patches, which can significantly enhance the interpretability of the trained deep model.", 
    "abstract": "In this paper, the bipartite synchronization of signed Lur'e network is studied under intermittent control, where the communication relationship of these adjacent nodes in the network can be either cooperative or competitive. Assuming that the network is structurally balanced, bipartite synchronization can be reached with some conditions and coordinate transform criterion. Then, Based on Lyapunov stability theory, some important norms are established. Ultimately, the simulation results can illustrate validness of theoretical analysis.", 
    "abstract": "Walking is the most affected motor function in children with cerebral palsy (CP). Orthopaedic surgery is regularly used to improve ambulation in children with CP. Selective Percutaneous Myofascial Lengthening (SPML) is considered the state-of-the art technique for surgical lengthening of spastic/contracted muscles in CP. The purpose of this study was to investigate the effect of combined SPML surgery and postoperative functional physiotherapy on gait function and characteristics of children with spastic cerebral palsy (CP).\nTwenty-six children with spastic CP, aged 5-7 years, Gross Motor Function Classification System (GMFCS) levels II (n\u00a0=\u00a06), III (n\u00a0=\u00a012) and IV (n\u00a0=\u00a08) participated in a quasi-experimental one-group pretest-posttest study with a 9-month follow-up. The Global Motion Graph Deviation Index (MGDI) (including MGDI sub-indices of each joint in each plane of motion) and spatiotemporal parameters of a three-dimensional kinematic gait analysis were used to assess the gait function and characteristics, respectively.\nNine months following SPML and functional physiotherapy, statistically significant improvements (p\u00a0<\u00a00.05) were noted in the Global MGDI, the MGDIs of sagittal plane knee and ankle motion analysis graphs, and the four most common spatiotemporal measures of gait: walking velocity, stride length, step length, and cadence.\nChildren with spastic CP seem to gain better overall gait function following SPML procedure and functional physiotherapy, by achieving higher walking velocity, longer stride length and step length, and faster cadence. Further studies with control group and longer follow-up three-dimensional gait analyses are warranted to validate these positive results.", 
    "abstract": "In this study, we proposed a novel method called the graph capsule convolutional network (GCCN) to predict the progression from mild cognitive impairment to dementia and identify its pathogenesis. First, we proposed a novel risk gene discovery component to indirectly target genes with higher interactions with others. These risk genes and brain regions were collected as nodes to construct heterogeneous pathogenic information association graphs. Second, the graph capsules were established by projecting heterogeneous pathogenic information into a set of disentangled latent components. The orientation and length of capsules are representations of the format and intensity of pathogenic information. Third, graph capsule convolution network was used to model the information flows among pathogenic factors and elaborates the convergence of primary capsules to advanced capsules. The advanced capsule is a concept that organizes pathogenic information based on its consistency, and the synergistic effects of advanced capsules directed the development of the disease. Finally, discriminative pathogenic information flows were captured by a straightforward built-in interpretation mechanism, i.e., the dynamic routing mechanism, and applied to the identification of pathogenesis. GCCN has been experimentally shown to be significantly advanced on public datasets. Further experiments have shown that the pathogenic factors identified by GCCN are evidential and closely related to progressive mild cognitive impairment.", 
    "abstract": "Multiview clustering (MVC) aims to exploit heterogeneous information from different sources and was extensively investigated in the past decade. However, far less attention has been paid to handling large-scale multiview data. In this brief, we fill this gap and propose a fast multiview clustering by an optimal graph mining model to handle large-scale data. We mine a consistent clustering structure from landmark-based graphs of different views, from which the optimal graph based on the one-hot encoding of cluster labels is recovered. Our model is parameter-free, so intractable hyperparameter tuning is avoided. An efficient algorithm of linear complexity to the number of samples is developed to solve the optimization problems. Extensive experiments on real-world datasets of various scales demonstrate the superiority of our proposal.", 
    "abstract": "Graph neural networks (GNNs) have become effective learning techniques for many downstream network mining tasks including node and graph classification, link prediction, and network reconstruction. However, most GNN methods have been developed for homogeneous networks with only a single type of node and edge. In this work we present muxGNN, a multiplex graph neural network for heterogeneous graphs. To model heterogeneity, we represent graphs as multiplex networks consisting of a set of relation layer graphs and a coupling graph that links node instantiations across multiple relations. We parameterize relation-specific representations of nodes and design a novel coupling attention mechanism that models the importance of multi-relational contexts for different types of nodes and edges in heterogeneous graphs. We further develop two complementary coupling structures: node invariant coupling suitable for node- and graph-level tasks, and node equivariant coupling suitable for link-level tasks. Extensive experiments conducted on six real-world datasets for link prediction in both transductive and inductive contexts and graph classification demonstrate the superior performance of muxGNN over state-of-the-art heterogeneous GNNs. In addition, we show that muxGNN's coupling attention discovers interpretable connections between different relations in heterogeneous networks.", 
    "abstract": "Although previous graph-based multi-view clustering (MVC) algorithms have gained significant progress, most of them are still faced with three limitations. First, they often suffer from high computational complexity, which restricts their applications in large-scale scenarios. Second, they usually perform graph learning either at the single-view level or at the view-consensus level, but often neglect the possibility of the joint learning of single-view and consensus graphs. Third, many of them rely on the k -means for discretization of the spectral embeddings, which lack the ability to directly learn the graph with discrete cluster structure. In light of this, this article presents an efficient MVC approach via unified and discrete bipartite graph learning (UDBGL). Specifically, the anchor-based subspace learning is incorporated to learn the view-specific bipartite graphs from multiple views, upon which the bipartite graph fusion is leveraged to learn a view-consensus bipartite graph with adaptive weight learning. Furthermore, the Laplacian rank constraint is imposed to ensure that the fused bipartite graph has discrete cluster structures (with a specific number of connected components). By simultaneously formulating the view-specific bipartite graph learning, the view-consensus bipartite graph learning, and the discrete cluster structure learning into a unified objective function, an efficient minimization algorithm is then designed to tackle this optimization problem and directly achieve a discrete clustering solution without requiring additional partitioning, which notably has linear time complexity in data size. Experiments on a variety of multi-view datasets demonstrate the robustness and efficiency of our UDBGL approach. The code is available at https://github.com/huangdonghere/UDBGL.", 
    "abstract": "Nonfactoid question answering (QA) is one of the most extensive yet challenging applications and research areas in natural language processing (NLP). Existing methods fall short of handling the long-distance and complex semantic relations between the question and the document sentences. In this work, we propose a novel query-focused summarization method, namely a graph-enhanced multihop query-focused summarizer (GMQS), to tackle the nonfactoid QA problem. Specifically, we leverage graph-enhanced reasoning techniques to elaborate the multihop inference process in nonfactoid QA. Three types of graphs with different semantic relations, namely semantic relevance, topical coherence, and coreference linking, are constructed for explicitly capturing the question-document and sentence-sentence interrelationships. Relational graph attention network (RGAT) is then developed to aggregate the multirelational information accordingly. In addition, the proposed method can be adapted to both extractive and abstractive applications as well as be mutually enhanced by joint learning. Experimental results show that the proposed method consistently outperforms both existing extractive and abstractive methods on two nonfactoid QA datasets, WikiHow and PubMedQA, and possesses the capability of performing explainable multihop reasoning.", 
    "abstract": "Graph convolution networks (GCNs) have been widely used and achieved fruitful progress in the skeleton-based action recognition task. In GCNs, node interaction modeling dominates the context aggregation and, therefore, is crucial for a graph-based convolution kernel to extract representative features. In this article, we introduce a closer look at a powerful graph convolution formulation to capture rich movement patterns from these skeleton-based graphs. Specifically, we propose a novel heterogeneous graph convolution (HetGCN) that can be considered as the middle ground between the extremes of (2 + 1)-D and 3-D graph convolution. The core observation of HetGCN is that multiple information flows are jointly intertwined in a 3-D convolution kernel, including spatial, temporal, and spatial-temporal cues. Since spatial and temporal information flows characterize different cues for action recognition, HetGCN first dynamically analyzes pairwise interactions between each node and its cross-space-time neighbors and then encourages heterogeneous context aggregation among them. Considering the HetGCN as a generic convolution formulation, we further develop it into two specific instantiations (i.e., intra-scale and inter-scale HetGCN) that significantly facilitate cross-space-time and cross-scale learning on skeleton graphs. By integrating these modules, we propose a strong human action recognition system that outperforms state-of-the-art methods with the accuracy of 93.1% on NTU-60 cross-subject (X-Sub) benchmark, 88.9% on NTU-120 X-Sub benchmark, and 38.4% on kinetics skeleton.", 
    "abstract": "Existing dynamic weighted graph visualization approaches rely on users' mental comparison to perceive temporal evolution of dynamic weighted graphs, hindering users from effectively analyzing changes across multiple timeslices. We propose DiffSeer, a novel approach for dynamic weighted graph visualization by explicitly visualizing the differences of graph structures (e.g., edge weight differences) between adjacent timeslices. Specifically, we present a novel nested matrix design that overviews the graph structure differences over a time period as well as shows graph structure details in the timeslices of user interest. By collectively considering the overall temporal evolution and structure details in each timeslice, an optimization-based node reordering strategy is developed to group nodes with similar evolution patterns and highlight interesting graph structure details in each timeslice. We conducted two case studies on real-world graph datasets and in-depth interviews with 12 target users to evaluate DiffSeer. The results demonstrate its effectiveness in visualizing dynamic weighted graphs.", 
    "abstract": "Graph matching (GM) has been a long-standing combinatorial problem due to its NP-hard nature. Recently (deep) learning-based approaches have shown their superiority over the traditional solvers while the methods are almost based on supervised learning which can be expensive or even impractical. We develop a unified unsupervised framework from matching two graphs to multiple graphs, without correspondence ground truth for training. Specifically, a Siamese-style unsupervised learning framework is devised and trained by minimizing the discrepancy of a second-order classic solver and a first-order (differentiable) Sinkhorn net as two branches for matching prediction. The two branches share the same CNN backbone for visual graph matching. Our framework further allows unsupervised learning with graphs from a mixture of modes which is ubiquitous in reality. Specifically, we develop and unify the graduated assignment (GA) strategy for matching two-graph, multi-graph, and graphs from a mixture of modes, whereby two-way constraint and clustering confidence (for mixture case) are modulated by two separate annealing parameters, respectively. Moreover, for partial and outlier matching, an adaptive reweighting technique is developed to suppress the overmatching issue. Experimental results on real-world benchmarks including natural image matching show our unsupervised method performs comparatively and even better against two-graph based supervised approaches.", 
    "abstract": "Learning graphs represented by M-matrices via an l", 
    "abstract": "Representation learning based on dynamic graphs has received a lot of attention in recent years due to its wide range of application scenarios. Although many discrete or continuous dynamic graph representation learning methods have been proposed, many of them ignore the role of edge types. Through the observation of dynamic graphs in the real world, it is found that the types of various edges are very different in nature. They are roughly divided into two categories according to the frequency of occurrence: evolutive edges that appear infrequently and interactive edges that appear frequently. For both types of edges, we propose a coupling-process model (DyCPM) to capture the dynamic mechanisms of them. The model not only generates low-dimensional embedding vectors of nodes, but also aggregates the structural information and temporal information of two kinds of edges. In particular, we design a neural network parameterized discrete process to depict the change law of the topology of evolutive edges and a neural network parameterized temporal point process (TPP) to characterize the temporal dynamic rule of interactive edges. More importantly, we propose a coupling mechanism to transfer the information of the two processes through a shared embedding matrix and finally generate an embedding matrix that aggregates the topology information and temporal information of the two kinds of edges for the dynamic link prediction task. We evaluate our model and several baselines on real datasets. The experimental results show that our model can better aggregate the topology information and temporal information of the two kinds of edges according to their properties and outperforms several state-of-the-art baselines in the performance of dynamic link prediction.", 
    "abstract": "The exchange of information is a crucial factor in achieving consensus among agents. However, in real-world scenarios, nonideal information sharing is prevalent due to complex environmental conditions. Consider the information distortions (data) and stochastic information flow (media) during state transmission both caused by physical constraints, a novel model of transmission-constrained consensus over random networks is proposed in this work. The transmission constraints are represented by heterogeneous functions that reflect the impact of environmental interference in multiagent systems or social networks. A directed random graph is applied to model the stochastic information flow where every edge is connected probabilistically. Using stochastic stability theory and the martingale convergence theorem, it is demonstrated that the agent states will converge to a consensus value with probability 1, despite information distortions and randomness in information flow. Numerical simulations are presented to validate the effectiveness of the proposed model.", 
    "abstract": "Asymmetric kernels naturally exist in real life, e.g., for conditional probability and directed graphs. However, most of the existing kernel-based learning methods require kernels to be symmetric, which prevents the use of asymmetric kernels. This paper addresses the asymmetric kernel-based learning in the framework of the least squares support vector machine named AsK-LS, resulting in the first classification method that can utilize asymmetric kernels directly. We will show that AsK-LS can learn with asymmetric features, namely source and target features, while the kernel trick remains applicable, i.e., the source and target features exist but are not necessarily known. Besides, the computational burden of AsK-LS is as cheap as dealing with symmetric kernels. Experimental results on various tasks, including Corel, PASCAL VOC, Satellite, directed graphs, and UCI database, all show that in the case asymmetric information is crucial, the proposed AsK-LS can learn with asymmetric kernels and performs much better than the existing kernel methods that rely on symmetrization to accommodate asymmetric kernels.", 
    "abstract": "In recent years, artificial intelligence has played an important role on accelerating the whole process of drug discovery. Various of molecular representation schemes of different modals (e.g. textual sequence or graph) are developed. By digitally encoding them, different chemical information can be learned through corresponding network structures. Molecular graphs and Simplified Molecular Input Line Entry System (SMILES) are popular means for molecular representation learning in current. Previous works have done attempts by combining both of them to solve the problem of specific information loss in single-modal representation on various tasks. To further fusing such multi-modal imformation, the correspondence between learned chemical feature from different representation should be considered. To realize this, we propose a novel framework of molecular joint representation learning via Multi-Modal information of SMILES and molecular Graphs, called MMSG. We improve the self-attention mechanism by introducing bond-level graph representation as attention bias in Transformer to reinforce feature correspondence between multi-modal information. We further propose a Bidirectional Message Communication Graph Neural Network (BMC GNN) to strengthen the information flow aggregated from graphs for further combination. Numerous experiments on public property prediction datasets have demonstrated the effectiveness of our model.", 
    "abstract": "Multiview clustering algorithms have attracted intensive attention and achieved superior performance in various fields recently. Despite the great success of multiview clustering methods in realistic applications, we observe that most of them are difficult to apply to large-scale datasets due to their cubic complexity. Moreover, they usually use a two-stage scheme to obtain the discrete clustering labels, which inevitably causes a suboptimal solution. In light of this, an efficient and effective one-step multiview clustering (E ", 
    "abstract": "Deep neural networks (DNNs) have been widely used for mesh processing in recent years. However, current DNNs can not process arbitrary meshes efficiently. On the one hand, most DNNs expect 2-manifold, watertight meshes, but many meshes, whether manually designed or automatically generated, may have gaps, non-manifold geometry, or other defects. On the other hand, the irregular structure of meshes also brings challenges to building hierarchical structures and aggregating local geometric information, which is critical to conduct DNNs. In this paper, we present DGNet, an efficient, effective and generic deep neural mesh processing network based on dual graph pyramids; it can handle arbitrary meshes. Firstly, we construct dual graph pyramids for meshes to guide feature propagation between hierarchical levels for both downsampling and upsampling. Secondly, we propose a novel convolution to aggregate local features on the proposed hierarchical graphs. By utilizing both geodesic neighbors and Euclidean neighbors, the network enables feature aggregation both within local surface patches and between isolated mesh components. Experimental results demonstrate that DGNet can be applied to both shape analysis and large-scale scene understanding. Furthermore, it achieves superior performance on various benchmarks, including ShapeNetCore, HumanBody, ScanNet and Matterport3D. Code and models will be available at https://github.com/li-xl/DGNet.", 
    "abstract": "Despite significant advances in graph representation learning, little attention has been paid to the more practical continual learning scenario in which new categories of nodes (e.g., new research areas in citation networks, or new types of products in co-purchasing networks) and their associated edges are continuously emerging, causing catastrophic forgetting on previous categories. Existing methods either ignore the rich topological information or sacrifice plasticity for stability. To this end, we present Hierarchical Prototype Networks (HPNs) which extract different levels of abstract knowledge in the form of prototypes to represent the continuously expanded graphs. Specifically, we first leverage a set of Atomic Feature Extractors (AFEs) to encode both the elemental attribute information and the topological structure of the target node. Next, we develop HPNs to adaptively select relevant AFEs and represent each node with three levels of prototypes. In this way, whenever a new category of nodes is given, only the relevant AFEs and prototypes at each level will be activated and refined, while others remain uninterrupted to maintain the performance over existing nodes. Theoretically, we first demonstrate that the memory consumption of HPNs is bounded regardless of how many tasks are encountered. Then, we prove that under mild constraints, learning new tasks will not alter the prototypes matched to previous data, thereby eliminating the forgetting problem. The theoretical results are supported by experiments on five datasets, showing that HPNs not only outperform state-of-the-art baseline techniques but also consume relatively less memory. Code and datasets are available at https://github.com/QueuQ/HPNs.", 
    "abstract": "Multivariate time series forecasting plays an increasingly critical role in various applications, such as power management, smart cities, finance, and healthcare. Recent advances in temporal graph neural networks (GNNs) have shown promising results in multivariate time series forecasting due to their ability to characterize high-dimensional nonlinear correlations and temporal patterns. However, the vulnerability of deep neural networks (DNNs) constitutes serious concerns about using these models to make decisions in real-world applications. Currently, how to defend multivariate forecasting models, especially temporal GNNs, is overlooked. The existing adversarial defense studies are mostly in static and single-instance classification domains, which cannot apply to forecasting due to the generalization challenge and the contradiction issue. To bridge this gap, we propose an adversarial danger identification method for temporally dynamic graphs to effectively protect GNN-based forecasting models. Our method consists of three steps: 1) a hybrid GNN-based classifier to identify dangerous times; 2) approximate linear error propagation to identify the dangerous variates based on the high-dimensional linearity of DNNs; and 3) a scatter filter controlled by the two identification processes to reform time series with reduced feature erasure. Our experiments, including four adversarial attack methods and four state-of-the-art forecasting models, demonstrate the effectiveness of the proposed method in defending forecasting models against adversarial attacks.", 
    "abstract": "Cancer survival prediction requires exploiting related multimodal information (e.g., pathological, clinical and genomic features, etc.) and it is even more challenging in clinical practices due to the incompleteness of patient's multimodal data. Furthermore, existing methods lack sufficient intra- and inter-modal interactions, and suffer from significant performance degradation caused by missing modalities. This manuscript proposes a novel hybrid graph convolutional network, entitled HGCN, which is equipped with an online masked autoencoder paradigm for robust multimodal cancer survival prediction. Particularly, we pioneer modeling the patient's multimodal data into flexible and interpretable multimodal graphs with modality-specific preprocessing. HGCN integrates the advantages of graph convolutional networks (GCNs) and a hypergraph convolutional network (HCN) through node message passing and a hyperedge mixing mechanism to facilitate intra-modal and inter-modal interactions between multimodal graphs. With HGCN, the potential for multimodal data to create more reliable predictions of patient's survival risk is dramatically increased compared to prior methods. Most importantly, to compensate for missing patient modalities in clinical scenarios, we incorporated an online masked autoencoder paradigm into HGCN, which can effectively capture intrinsic dependence between modalities and seamlessly generate missing hyperedges for model inference. Extensive experiments and analysis on six cancer cohorts from TCGA show that our method significantly outperforms the state-of-the-arts in both complete and missing modal settings. Our codes are made available at https://github.com/lin-lcx/HGCN.", 
    "abstract": "Neighborhood reconstruction methods have been widely applied to feature engineering. Existing reconstruction-based discriminant analysis methods normally project high-dimensional data into a low-dimensional space while preserving the reconstruction relationships among samples. However, there are three limitations: 1) the reconstruction coefficients are learned based on the collaborative representation of all sample pairs, which requires the training time to be the cube of the number of samples; 2) these coefficients are learned in the original space, ignoring the interference of the noise and redundant features; and 3) there is a reconstruction relationship between heterogeneous samples; this will enlarge the similarity of heterogeneous samples in the subspace. In this article, we propose a fast and adaptive discriminant neighborhood projection model to tackle the above drawbacks. First, the local manifold structure is captured by bipartite graphs in which each sample is reconstructed by anchor points derived from the same class as that sample; this can avoid the reconstruction between heterogeneous samples. Second, the number of anchor points is far less than the number of samples; this strategy can reduce the time complexity substantially. Third, anchor points and reconstruction coefficients of bipartite graphs are updated adaptively in the process of dimensionality reduction, which can enhance the quality of bipartite graphs and extract discriminative features simultaneously. An iterative algorithm is designed to solve this model. Extensive results on toy data and benchmark datasets show the effectiveness and superiority of our model.", 
    "abstract": "Brain signal-based emotion recognition has recently attracted considerable attention since it has powerful potential to be applied in human-computer interaction. To realize the emotional interaction of intelligent systems with humans, researchers have made efforts to decode human emotions from brain imaging data. The majority of current efforts use emotion similarities (e.g., emotion graphs) or brain region similarities (e.g., brain networks) to learn emotion and brain representations. However, the relationships between emotions and brain regions are not explicitly incorporated into the representation learning process. As a result, the learned representations may not be informative enough to benefit specific tasks, e.g., emotion decoding. In this work, we propose a novel idea of graph-enhanced emotion neural decoding, which takes advantage of a bipartite graph structure to integrate the relationships between emotions and brain regions into the neural decoding process, thus helping learn better representations. Theoretical analyses conclude that the suggested emotion-brain bipartite graph inherits and generalizes the conventional emotion graphs and brain networks. Comprehensive experiments on visually evoked emotion datasets demonstrate the effectiveness and superiority of our approach.", 
    "abstract": "In this paper, we have compared a new type of similarity transformation derived systematically by using Lie point symmetries with the existing similarity transformations for unsteady fluid flow and heat transfer in the boundary layer in the presence of radiation. It is observed that the existing transformations map the steady and marginally accelerating flows only, while the Lie similarity transformations provide solutions for all types of accelerating flows and are independent of unsteadiness in the fluid. The previous transformations are valid for a specific time interval which depends on a range of unsteadiness parameter, however the Lie similarity transformations provide valid solutions at any given time. This implies that the Lie similarity transformations yield solutions for previously unexplored ranges of unsteadiness in the fluid. Boundary layer flow physics for both types of transformations is discussed by employing the Homotopy analysis method. We show that for accelerating fluids, in the developing region, the boundary layer thickness first increases and than starts to decrease with increase in unsteadiness for fully developed flow. Detailed comparison of velocity and temperature profiles in the boundary layer is made using the tables and graphs which show that with Lie similarity transformations the region of study of the considered flow extends significantly for the unsteadiness parameter. The effect of the Prandtl number and radiation parameter on temperature distribution is also compared for both types of similarity transformations. The Lie symmetry similarity transformations are shown to explain the unsteady laminar boundary layer flow and heat transfer to an extent where the existing similarity transformations do not work.", 
    "abstract": "In recent years, the high-resolution manometry (HRM) technique has been increasingly used to study esophageal and colonic pressurization and has become a standard routine for discovering mobility disorders. In addition to evolving guidelines for the interpretation of HRM like Chicago standard, some complexities, such as the dependency of normative reference values on the recording device and other external variables, still remain for medical professions. In this study, a decision support framework is developed to aid the diagnosis of esophageal mobility disorders based on HRM data. To abstract HRM data, Spearman correlation is employed to model the spatio-temporal dependencies of pressure values of HRM components and convolutional graph neural networks are then utilized to embed relation graphs to the features vector. In the decision-making stage, a novel Expert per Class Fuzzy Classifier (EPC-FC) is presented that employs the ensemble structure and contains expertized sub-classifiers for recognizing a specific disorder. Training sub-classifiers using the negative correlation learning method makes the EPC-FC highly generalizable. Meanwhile, separating the sub-classifiers of each class gives flexibility and interpretability to the structure. The suggested framework is evaluated on a dataset of 67 patients in 5 different classes recorded in Shariati Hospital. The average accuracy of 78.03% for a single swallow and 92.54% for subject-level is achieved for distinguishing mobility disorders. Moreover, compared with the other studies, the presented framework has an outstanding performance considering that it imposes no limits on the type of classes or HRM data. On the other hand, the EPC-FC outperforms other comparative classifiers such as SVM and AdaBoost not only in HRM diagnosis but also on other benchmark classification problems.", 
    "abstract": "", 
    "abstract": "Drug repurposing or repositioning (DR) refers to finding new therapeutic applications for existing drugs. Current computational DR methods face data representation and negative data sampling challenges. Although retrospective studies attempt to operate various representations, it is a crucial step for an accurate prediction to aggregate these features and bring the associations between drugs and diseases into a unified latent space. In addition, the number of unknown associations between drugs and diseases, which is considered negative data, is much higher than the number of known associations, or positive data, leading to an imbalanced dataset. In this regard, we propose the DrugRep-KG method, which applies a knowledge graph embedding approach for representing drugs and diseases, to address these challenges. Despite the typical DR methods that consider all unknown drug-disease associations as negative data, we select a subset of unknown associations, provided the disease occurs because of an adverse reaction to a drug. DrugRep-KG has been evaluated based on different settings and achieves an AUC-ROC (area under the receiver operating characteristic curve) of 90.83% and an AUC-PR (area under the precision-recall curve) of 90.10%, which are higher than in previous works. Besides, we checked the performance of our framework in finding potential drugs for coronavirus infection and skin-related diseases: contact dermatitis and atopic eczema. DrugRep-KG predicted beclomethasone for contact dermatitis, and fluorometholone, clocortolone, fluocinonide, and beclomethasone for atopic eczema, all of which have previously been proven to be effective in other studies. Fluorometholone for contact dermatitis is a novel suggestion by DrugRep-KG that should be validated experimentally. DrugRep-KG also predicted the associations between COVID-19 and potential treatments suggested by DrugBank, in addition to new drug candidates provided with experimental evidence. The data and code underlying this article are available at https://github.com/CBRC-lab/DrugRep-KG.", 
    "abstract": "Bipartite graphs model the relationships between two disjoint sets of entities in several applications and are naturally drawn as 2-layer graph drawings. In such drawings, the two sets of entities (vertices) are placed on two parallel lines (layers), and their relationships (edges) are represented by segments connecting vertices. Methods for constructing 2-layer drawings often try to minimize the number of edge crossings. We use vertex splitting to reduce the number of crossings, by replacing selected vertices on one layer by two (or more) copies and suitably distributing their incident edges among these copies. We study several optimization problems related to vertex splitting, either minimizing the number of crossings or removing all crossings with fewest splits. While we prove that some variants are ${\\mathsf {NP}}$NP-complete, we obtain polynomial-time algorithms for others. We run our algorithms on a benchmark set of bipartite graphs representing the relationships between human anatomical structures and cell types.", 
    "abstract": "Identifying the subtypes of low-grade glioma (LGG) can help prevent brain tumor progression and patient death. However, the complicated non-linear relationship and high dimensionality of 3D brain MRI limit the performance of machine learning methods. Therefore, it is important to develop a classification method that can overcome these limitations. This study proposes a self-attention similarity-guided graph convolutional network (SASG-GCN) that uses the constructed graphs to complete multi-classification (tumor-free (TF), WG, and TMG). In the pipeline of SASG-GCN, we use a convolutional deep belief network and a self-attention similarity-based method to construct the vertices and edges of the constructed graphs at 3D MRI level, respectively. The multi-classification experiment is performed in a two-layer GCN model. SASG-GCN is trained and evaluated on 402 3D MRI images which are produced from the TCGA-LGG dataset. Empirical tests demonstrate that SASGGCN accurately classifies the subtypes of LGG. The accuracy of SASG-GCN achieves 93.62%, outperforming several other state-of-the-art classification methods. In-depth discussion and analysis reveal that the self-attention similarity-guided strategy improves the performance of SASG-GCN. The visualization revealed differences between different gliomas.", 
    "abstract": "Despite being vaccine-preventable, tick-borne encephalitis (TBE) continues to cause considerable morbidity in Germany. Limited insight into potentially debilitating consequences of TBE may partially underly low (~\u200920%) TBE vaccine uptake. We aimed to systematically assess TBE sequelae and other consequences.\nRoutinely notified TBE patients from 2018 to 2020 from Southern Germany were invited to telephone interviews acutely and again after 18\u00a0months. Duration of acute symptoms was prospectively assessed. Recovery was defined as score 0 on the modified RANKIN scale. Determinants of time to recovery were analysed with cox regression, adjusted for covariates identified using directed acyclic graphs, yielding hazard ratios (HR) and 95% confidence intervals (CI).\nOf 558 cases, 523 (93.7%) completed follow-up. Full recovery was reported by 67.3% (children: 94.9%, adults: 63.8%). Sequelae included fatigue (17.0%), weakness (13.4%), concentration deficit (13.0%), and impaired balance (12.0%). Compared with 18-39-year-olds, recovery rates were 44% lower in\u2009\u2265\u200950-year-olds (HR: 0.56, 95%CI 0.42-0.75) and 79% higher in children (HR: 1.79, 95%CI 1.25-2.56). The recovery rate was 64% lower after severe TBE (compared to mild; HR: 0.36, 95%CI 0.25-0.52) and 22% lower with comorbidities (HR: 0.78, 95%CI 0.62-0.99). Substantial health-care use was reported (90.1% hospitalisation, 39.8% rehabilitation). Of employed cases, 88.4% required sick leave; 10.3% planned/reported premature retirement due to sequelae.\nHalf the adult and 5% of paediatric patients reported persisting sequelae after 18\u00a0months. Improved prevention could alleviate both individual (morbidity) and societal TBE burden (health-care costs, productivity losses). Insights into sequelae can help guide at-risk populations towards tick-avoidant strategies and encourage TBE vaccination.", 
    "abstract": "Unsupervised hashing methods have attracted widespread attention with the explosive growth of large-scale data, which can greatly reduce storage and computation by learning compact binary codes. Existing unsupervised hashing methods attempt to exploit the valuable information from samples, which fails to take the local geometric structure of unlabeled samples into consideration. Moreover, hashing based on auto-encoders aims to minimize the reconstruction loss between the input data and binary codes, which ignores the potential consistency and complementarity of multiple sources data. To address the above issues, we propose a hashing algorithm based on auto-encoders for multiview binary clustering, which dynamically learns affinity graphs with low-rank constraints and adopts collaboratively learning between auto-encoders and affinity graphs to learn a unified binary code, called graph-collaborated auto-encoder (GCAE) hashing for multiview binary clustering. Specifically, we propose a multiview affinity graphs' learning model with low-rank constraint, which can mine the underlying geometric information from multiview data. Then, we design an encoder-decoder paradigm to collaborate the multiple affinity graphs, which can learn a unified binary code effectively. Notably, we impose the decorrelation and code balance constraints on binary codes to reduce the quantization errors. Finally, we use an alternating iterative optimization scheme to obtain the multiview clustering results. Extensive experimental results on five public datasets are provided to reveal the effectiveness of the algorithm and its superior performance over other state-of-the-art alternatives.", 
    "abstract": "Finding the causal structure from a set of variables given observational data is a crucial task in many scientific areas. Most algorithms focus on discovering the global causal graph but few efforts have been made toward the local causal structure (LCS), which is of wide practical significance and easier to obtain. LCS learning faces the challenges of neighborhood determination and edge orientation. Available LCS algorithms build on conditional independence (CI) tests, they suffer the poor accuracy due to noises, various data generation mechanisms, and small-size samples of real-world applications, where CI tests do not work. In addition, they can only find the Markov equivalence class, leaving some edges undirected. In this article, we propose a GradieNt-based LCS learning approach (GraN-LCS) to determine neighbors and orient edges simultaneously in a gradient-descent way, and, thus, to explore LCS more accurately. GraN-LCS formulates the causal graph search as minimizing an acyclicity regularized score function, which can be optimized by efficient gradient-based solvers. GraN-LCS constructs a multilayer perceptron (MLP) to simultaneously fit all other variables with respect to a target variable and defines an acyclicity-constrained local recovery loss to promote the exploration of local graphs and to find out direct causes and effects of the target variable. To improve the efficacy, it applies preliminary neighborhood selection (PNS) to sketch the raw causal structure and further incorporates an l", 
    "abstract": "Transformers are more and more popular in computer vision, which treat an image as a sequence of patches and learn robust global features from the sequence. However, pure transformers are not entirely suitable for vehicle re-identification because vehicle re-identification requires both robust global features and discriminative local features. For that, a graph interactive transformer (GiT) is proposed in this paper. In the macro view, a list of GiT blocks are stacked to build a vehicle re-identification model, in where graphs are to extract discriminative local features within patches and transformers are to extract robust global features among patches. In the micro view, graphs and transformers are in an interactive status, bringing effective cooperation between local and global features. Specifically, one current graph is embedded after the former level's graph and transformer, while the current transform is embedded after the current graph and the former level's transformer. In addition to the interaction between graphs and transforms, the graph is a newly-designed local correction graph, which learns discriminative local features within a patch by exploring nodes' relationships. Extensive experiments on three large-scale vehicle re-identification datasets demonstrate that our GiT method is superior to state-of-the-art vehicle re-identification approaches.", 
    "abstract": "Visual reasoning between visual images and natural language remains a long-standing challenge in computer vision. Conventional deep supervision methods target at finding answers to the questions relying on the datasets containing only a limited amount of images with textual ground-truth descriptions. Facing learning with limited labels, it is natural to expect to constitute a larger scale dataset consisting of several million visual data annotated with texts, but this approach is extremely time-intensive and laborious. Knowledge-based works usually treat knowledge graphs (KGs) as static flattened tables for searching the answer, but fail to take advantage of the dynamic update of KGs. To overcome these deficiencies, we propose a Webly supervised knowledge-embedded model for the task of visual reasoning. On the one hand, vitalized by the overwhelming successful Webly supervised learning, we make much use readily available images from the Web with their weakly annotated texts for an effective representation. On the other hand, we design a knowledge-embedded model, including the dynamically updated interaction mechanism between semantic representation models and KGs. Experimental results on two benchmark datasets demonstrate that our proposed model significantly achieves the most outstanding performance compared with other state-of-the-art approaches for the task of visual reasoning.", 
    "abstract": "Clustering aims to make data points in the same group have higher similarity or make data points in different groups have lower similarity. Therefore, we propose three novel fast clustering models motivated by maximizing within-class similarity, which can obtain more instinct clustering structure of data. Different from traditional clustering methods, we divide all n samples into m classes by the pseudo label propagation algorithm first, and then m classes are merged to c classes ( ) by the proposed three co-clustering models, where c is the real number of categories. On the one hand, dividing all samples into more subclasses first can preserve more local information. On the other hand, proposed three co-clustering models are motivated by the thought of maximizing the sum of within-class similarity, which can utilize the dual information between rows and columns. Besides, the proposed pseudo label propagation algorithm can be a new method to construct anchor graphs with linear time complexity. A series of experiments are conducted on both synthetic and real-world datasets and the experimental results show the superior performance of three models. It is worth noting that for the proposed models, FMAWS2 is the generalization of FMAWS1 and FMAWS3 is the generalization of other two.", 
    "abstract": "Node-link diagrams are widely used to visualize graphs. Most graph layout algorithms only use graph topology for aesthetic goals (e.g., minimize node occlusions and edge crossings) or use node attributes for exploration goals (e.g., preserve visible communities). Existing hybrid methods that bind the two perspectives still suffer from various generation restrictions (e.g., limited input types and required manual adjustments and prior knowledge of graphs) and the imbalance between aesthetic and exploration goals. In this paper, we propose a flexible embedding-based graph exploration pipeline to enjoy the best of both graph topology and node attributes. First, we leverage embedding algorithms for attributed graphs to encode the two perspectives into latent space. Then, we present an embedding-driven graph layout algorithm, GEGraph, which can achieve aesthetic layouts with better community preservation to support an easy interpretation of the graph structure. Next, graph explorations are extended based on the generated graph layout and insights extracted from the embedding vectors. Illustrated with examples, we build a layout-preserving aggregation method with Focus+Context interaction and a related nodes searching approach with multiple proximity strategies. Finally, we conduct quantitative and qualitative evaluations, a user study, and two case studies to validate our approach.", 
    "abstract": "The importance of microbe-drug associations (MDA) prediction is evidenced in research. Since traditional wet-lab experiments are both time-consuming and costly, computational methods are widely adopted. However, existing research has yet to consider the cold-start scenarios that commonly seen in real-world clinical research and practices where data of confirmed microbe-drug associations are highly sparse. Therefore, we aim to contribute by developing two novel computational approaches, the GNAEMDA (Graph Normalized Auto-Encoder to predict Microbe-Drug Associations), and a variational extension of the GNAEMDA (called VGNAEMDA), to provide effective and efficient solutions for well-annotated cases and cold-start scenarios. Multi-modal attribute graphs are constructed by collecting multiple features of microbes and drugs, and then input into a graph normalized convolutional network, where a l", 
    "abstract": "In this paper, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student's t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.", 
    "abstract": "Neuropsychological studies suggest that co-operative activities among different brain functional areas drive high-level cognitive processes. To learn the brain activities within and among different functional areas of the brain, we propose local-global-graph network (LGGNet), a novel neurologically inspired graph neural network (GNN), to learn local-global-graph (LGG) representations of electroencephalography (EEG) for brain-computer interface (BCI). The input layer of LGGNet comprises a series of temporal convolutions with multiscale 1-D convolutional kernels and kernel-level attentive fusion. It captures temporal dynamics of EEG which then serves as input to the proposed local-and global-graph-filtering layers. Using a defined neurophysiologically meaningful set of local and global graphs, LGGNet models the complex relations within and among functional areas of the brain. Under the robust nested cross-validation settings, the proposed method is evaluated on three publicly available datasets for four types of cognitive classification tasks, namely the attention, fatigue, emotion, and preference classification tasks. LGGNet is compared with state-of-the-art (SOTA) methods, such as DeepConvNet, EEGNet, R2G-STNN, TSception, regularized graph neural network (RGNN), attention-based multiscale convolutional neural network-dynamical graph convolutional network (AMCNN-DGCN), hierarchical recurrent neural network (HRNN), and GraphNet. The results show that LGGNet outperforms these methods, and the improvements are statistically significant ( ) in most cases. The results show that bringing neuroscience prior knowledge into neural network design yields an improvement of classification performance. The source code can be found at https://github.com/yi-ding-cs/LGG.", 
    "abstract": "An interesting problem is to study how gene co-expression varies in two different populations, associated with healthy and unhealthy individuals, respectively. To this aim, two important aspects should be taken into account: (i) in some cases, pairs/groups of genes show collaborative attitudes, emerging in the study of disorders and diseases; (ii) information coming from each single individual may be crucial to capture specific details, at the basis of complex cellular mechanisms; therefore, it is important avoiding to miss potentially powerful information, associated with the single samples.\nHere, a novel approach is proposed, such that two different input populations are considered, and represented by two datasets of edge-labeled graphs. Each graph is associated to an individual, and the edge label is the co-expression value between the two genes associated to the nodes. Discriminative patterns among graphs belonging to different sample sets are searched for, based on a statistical notion of 'relevance' able to take into account important local similarities, and also collaborative effects, involving the co-expression among multiple genes. Four different gene expression datasets have been analyzed by the proposed approach, each associated to a different disease. An extensive set of experiments show that the extracted patterns significantly characterize important differences between healthy and unhealthy samples, both in the cooperation and in the biological functionality of the involved genes/proteins. Moreover, the provided analysis confirms some results already presented in the literature on genes with a central role for the considered diseases, still allowing to identify novel and useful insights on this aspect.\nThe algorithm has been implemented using the Java programming language. The data underlying this article and the code are available at https://github.com/CriSe92/DiscriminativeSubgraphDiscovery.", 
    "abstract": "In this article, we present a novel hierarchical bidirected graph convolution network (HiBi-GCN) for large-scale 3-D point cloud place recognition. Unlike place recognition methods based on 2-D images, those based on 3-D point cloud data are typically robust to substantial changes in real-world environments. However, these methods have difficulty in defining convolution for point cloud data to extract informative features. To solve this problem, we propose a new hierarchical kernel defined as a hierarchical graph structure through unsupervised clustering from the data. In particular, we pool hierarchical graphs from the fine to coarse direction using pooling edges and fuse the pooled graphs from the coarse to fine direction using fusing edges. The proposed method can, thus, learn representative features hierarchically and probabilistically; moreover, it can extract discriminative and informative global descriptors for place recognition. Experimental results demonstrate that the proposed hierarchical graph structure is more suitable for point clouds to represent real-world 3-D scenes.", 
    "abstract": "Online learning with expert advice is widely used in various machine learning tasks. It considers the problem where a learner chooses one from a set of experts to take advice and make a decision. In many learning problems, experts may be related, henceforth the learner can observe the losses associated with a subset of experts that are related to the chosen one. In this context, the relationship among experts can be captured by a feedback graph, which can be used to assist the learner's decision-making. However, in practice, the nominal feedback graph often entails uncertainties, which renders it impossible to reveal the actual relationship among experts. To cope with this challenge, the present work studies various cases of potential uncertainties and develops novel online learning algorithms to deal with uncertainties while making use of the uncertain feedback graph. The proposed algorithms are proved to enjoy sublinear regret under mild conditions. Experiments on real datasets are presented to demonstrate the effectiveness of the novel algorithms.", 
    "abstract": "Modified Rankin Scale (mRS) scores are used to measure functional outcomes after stroke. Researchers create horizontal stacked bar graphs (nicknamed \"Grotta bars\") to illustrate distributional differences in scores between groups. In well-conducted randomized controlled trials, Grotta bars have a causal interpretation. However, the common practice of exclusively presenting unadjusted Grotta bars in observational studies can be misleading in the presence of confounding. We demonstrated this problem and a possible solution using an empirical comparison of 3-month mRS scores among stroke/TIA patients discharged home versus elsewhere after hospitalization.\nUsing data from the Berlin-based B-SPATIAL registry, we estimated the probability of being discharged home conditional on prespecified measured confounding factors and generated stabilized inverse probability of treatment (IPT) weights for each patient. We visualized mRS distributions by group with Grotta bars for the IPT-weighted population in which measured confounding was removed. We then used ordinal logistic regression to quantify unadjusted and adjusted associations between being discharged home and the 3-month mRS score.\nOf 3184 eligible patients, 2537 (79.7%) were discharged home. In the unadjusted analyses, those discharged home had considerably lower mRS compared with patients discharged elsewhere (common odds ratio, cOR\u2009=\u20090.13, 95% CI: 0.11-0.15). After removing measured confounding, we obtained substantially different mRS distributions, visually apparent in the adjusted Grotta bars. No statistically significant association was found after confounding adjustment (cOR\u2009=\u20090.82, 95% CI: 0.60-1.12).\nThe practice of presenting only unadjusted stacked bar graphs for mRS scores together with adjusted effect estimates in observational studies can be misleading. IPT weighting can be implemented to create Grotta bars that account for measured confounding, which are more consistent with the presentation of adjusted results in observational studies.", 
    "abstract": "The abundance of biomedical knowledge gained from biological experiments and clinical practices is an invaluable resource for biomedicine. The emerging biomedical knowledge graphs (BKGs) provide an efficient and effective way to manage the abundant knowledge in biomedical and life science. In this study, we created a comprehensive BKG called the integrative Biomedical Knowledge Hub (iBKH) by harmonizing and integrating information from diverse biomedical resources. To make iBKH easily accessible for biomedical research, we developed a web-based, user-friendly graphical portal that allows fast and interactive knowledge retrieval. Additionally, we also implemented an efficient and scalable graph learning pipeline for discovering novel biomedical knowledge in iBKH. As a proof of concept, we performed our iBKH-based method for computational in-silico drug repurposing for Alzheimer's disease. The iBKH is publicly available.", 
    "abstract": "In this exceptional COVID-19 crisis, telemedicine had arisen as a substitute technique for medicines. Even more unequivocally, pediatric children were at high risk to outside homes. The spread of COVID-19 has suddenly ascended. Because of lockdown conventions and isolation protocols, kids were confined to live inside their homes. Non-emergency youngsters ought to be managed remotely through the telepediatric health. An establishment of Neural Series Transmission Keys (NSTKs) has been created and security had been planned on the intraoral data. Oral cavity is a kind of dental disease occurring in children. It is for the most cases caused due to drawn out bacterial invasions. Bacterial attacks are more because of sticky chocolates, desserts, sugar, and so forth. Homeopathy medicines are the best prescribed to fix such dental diseases in this current unprecedented COVID-19. Since, it needs no dental medical procedure for the non-invasive kids, which is the reason that the homeopathy medicines are most appropriate in this COVID-19 lockdown stages. The doctor can gather symptoms of the kids from their parents through online interfaces. Some of the normal homeopathy drugs are: Kreasotum, Mercurius, Mezereum, etc. Moreover, in this pandemic situation online telepediatric homeopathy medicines were better alternatives to investigate from home disengages. Additionally it bears no voyaging consumptions and costs. Secure online transmission of clinical pediatric information has been the most challenging issue in COVID-19 telepediatric oral wellbeing. Data mystery factor is protected with tendency in this proposed cryptographic technique. Neural Series Transmission Keys (NSTKs) were established based on neural network based hamming codes. It has been diffused inside the intraoral pediatric data. The proposed key was so particularly amazing that it gives assorted blend after each bit of evolving. Beginning seeds were kept at the dentists and the patients, in order to go against external attacks inside the public channel, especially during this hyper digitized COVID-19 times. Standard graphs were drawn with accuracy using the proposed cryptographic method. The absolute cryptographic time in this strategy was 2.88 ms which was significantly important. By applying Chi Square test, we have noted ", 
    "abstract": "The stability analysis of a rocking rigid rod is investigated in this paper using a\u00a0time-delayed square position and velocity. The time delay is an additional safety against the nonlinearly vibrating system under consideration. Because time-delayed technologies have lately been the core of several investigations, the subject of this inquiry is extremely relevant. The Homotopy perturbation method (HPM) is modified to produce a more precise approximate outcome. Therefore, the novelty of the exciting paper arises from the coupling of the time delay and its correlation with the modified HPM. A comparison\u00a0with the fourth-order Runge-Kutta (RK4) technique is employed to evaluate the precision between the analytical as well as the numerical solutions. The study allows for a comprehensive examination of the recognition of the outcome of the realistic approximation analytical methodology. For different amounts of the physical frequency and time delay factors, the time histories of the found solutions are depicted in various plots. These graphs are discussed in the context of the shown curves according to the relevant parameter values. The organized\u00a0nonlinear prototype approach is examined by the multiple-time scale method up to the first approximation. The obtained results have periodic behavior and a stable manner. The current study makes it possible to carefully examine the findings arrived at by employing the analytical technique of practicable estimation. Additionally, the time delay performs as extra protection as opposed to the system potential for nonlinear oscillation.", 
    "abstract": "This study aims to elucidate the electrotaxis response of alveolar epithelial cells (AECs) in direct-current electric fields (EFs), explore the impact of EFs on the cell fate of AECs, and lay the foundation for future exploitation of EFs for the treatment of acute lung injury.\nAECs were extracted from rat lung tissues using magnetic-activated cell sorting. To elucidate the electrotaxis responses of AECs, different voltages of EFs (0, 50, 100, and 200\u00a0mV/mm) were applied to two types of AECs, respectively. Cell migrations were recorded and trajectories were pooled to better demonstrate cellular activities through graphs. Cell directionality was calculated as the cosine value of the angle formed by the EF vector and cell migration. To further demonstrate the impact of EFs on the pulmonary tissue, the human bronchial epithelial cells transformed with Ad12-SV40 2B (BEAS-2B cells) were obtained and experimented under the same conditions as AECs. To determine the influence on cell fate, cells underwent electric stimulation were collected to perform Western blot analysis.\nThe successful separation and culturing of AECs were confirmed through immunofluorescence staining. Compared with the control, AECs in EFs demonstrated a significant directionality in a voltage-dependent way. In general, type \u2160 alveolar epithelial cells migrated faster than type \u2161 alveolar epithelial cells, and under EFs, these two types of cells exhibited different response threshold. For type \u2161 alveolar epithelial cells, only EFs at 200\u00a0mV/mm resulted a significant difference to the velocity, whereas for, EFs at both 100 mV/mm and 200\u00a0mV/mm gave rise to a significant difference. Western blotting suggested that EFs led to an increased expression of a AKT and myeloid leukemia 1 and a decreased expression of Bcl-2-associated X protein and Bcl-2-like protein 11.\nEFs could guide and accelerate the directional migration of AECs and exert antiapoptotic effects, which indicated that EFs are important biophysical signals in the re-epithelialization of alveolar epithelium in lung injury.", 
    "abstract": "Automatic generation of medical reports can provide diagnostic assistance to doctors and reduce their workload. To improve the quality of the generated medical reports, injecting auxiliary information through knowledge graphs or templates into the model is widely adopted in previous methods. However, they suffer from two problems: 1) The injected external information is limited in amount and difficult to adequately meet the information needs of medical report generation in content. 2) The injected external information increases the complexity of model and is hard to be reasonably integrated into the generation process of medical reports. Therefore, we propose an Information Calibrated Transformer (ICT) to address the above issues. First, we design a Precursor-information Enhancement Module (PEM), which can effectively extract numerous inter-intra report features from the datasets as the auxiliary information without external injection. And the auxiliary information can be dynamically updated with the training process. Secondly, a combination mode, which consists of PEM and our proposed Information Calibration Attention Module (ICA), is designed and embedded into ICT. In this method, the auxiliary information extracted from PEM is flexibly injected into ICT and the increment of model parameters is small. The comprehensive evaluations validate that the ICT is not only superior to previous methods in the X-Ray datasets, IU-X-Ray and MIMIC-CXR, but also successfully be extended to a CT COVID-19 dataset COV-CTR.", 
    "abstract": "Inspired by the impressive success of contrastive learning (CL), a variety of graph augmentation strategies have been employed to learn node representations in a self-supervised manner. Existing methods construct the contrastive samples by adding perturbations to the graph structure or node attributes. Although impressive results are achieved, it is rather blind to the wealth of prior information assumed: with the increase of the perturbation degree applied on the original graph: 1) the similarity between the original graph and the generated augmented graph gradually decreases and 2) the discrimination between all nodes within each augmented view gradually increases. In this article, we argue that both such prior information can be incorporated (differently) into the CL paradigm following our general ranking framework. In particular, we first interpret CL as a special case of learning to rank (L2R), which inspires us to leverage the ranking order among positive augmented views. Meanwhile, we introduce a self-ranking paradigm to ensure that the discriminative information among different nodes can be maintained and also be less altered to the perturbations of different degrees. Experiment results on various benchmark datasets verify the effectiveness of our algorithm compared with the supervised and unsupervised models.", 
    "abstract": "Script event prediction aims to infer subsequent events given an incomplete script. It requires a deep understanding of events, and can provide support for a variety of tasks. Existing models rarely consider the relational knowledge between events, they regard scripts as sequences or graphs, which cannot capture the relational information between events and the semantic information of script sequences jointly. To address this issue, we propose a new script form, relational event chain, that combines event chains and relational graphs. We also introduce a new model, relational-transformer, to learn embeddings based on this new script form. In particular, we first extract the relationship between events from an event knowledge graph to formalize scripts as relational event chains, then use the relational-transformer to calculate the likelihood of different candidate events, where the model learns event embeddings that encode both semantic and relational knowledge by combining transformers and graph neural networks (GNNs). Experimental results on both one-step inference and multistep inference tasks show that our model can outperform existing baselines, indicating the validity of encoding relational knowledge into event embeddings. The influence of using different model structures and different types of relational knowledge is analyzed as well.", 
    "abstract": "Heterogeneous graphs with multiple types of nodes and link relationships are ubiquitous in many real-world applications. Heterogeneous graph neural networks (HGNNs) as an efficient technique have shown superior capacity of dealing with heterogeneous graphs. Existing HGNNs usually define multiple meta-paths in a heterogeneous graph to capture the composite relations and guide neighbor selection. However, these models only consider the simple relationships (i.e., concatenation or linear superposition) between different meta-paths, ignoring more general or complex relationships. In this article, we propose a novel unsupervised framework termed Heterogeneous Graph neural network with bidirectional encoding representation (HGBER) to learn comprehensive node representations. Specifically, the contrastive forward encoding is firstly performed to extract node representations on a set of meta-specific graphs corresponding to meta-paths. We then introduce the reversed encoding for the degradation process from the final node representations to each single meta-specific node representations. Moreover, to learn structure-preserving node representations, we further utilize a self-training module to discover the optimal node distribution through iterative optimization. Extensive experiments on five open public datasets show that the proposed HGBER model outperforms the state-of-the-art HGNNs baselines by 0.8%-8.4% in terms of accuracy on most datasets in various downstream tasks.", 
    "abstract": "Graph neural networks (GNNs) have been playing important roles in various graph-related tasks. However, most existing GNNs are based on the assumption of homophily, so they cannot be directly generalized to heterophily settings where connected nodes may have different features and class labels. Moreover, real-world graphs often arise from highly entangled latent factors, but the existing GNNs tend to ignore this and simply denote the heterogeneous relations between nodes as binary-valued homogeneous edges. In this article, we propose a novel relation-based frequency adaptive GNN (RFA-GNN) to handle both heterophily and heterogeneity in a unified framework. RFA-GNN first decomposes an input graph into multiple relation graphs, each representing a latent relation. More importantly, we provide detailed theoretical analysis from the perspective of spectral signal processing. Based on this, we propose a relation-based frequency adaptive mechanism that adaptively picks up signals of different frequencies in each corresponding relation space in the message-passing process. Extensive experiments on synthetic and real-world datasets show qualitatively and quantitatively that RFA-GNN yields truly encouraging results for both the heterophily and heterogeneity settings. Codes are publicly available at: https://github.com/LirongWu/RFA-GNN.", 
    "abstract": "Complex networks play a fundamental role in understanding phenomena from the collective behavior of spins, neural networks, and power grids to the spread of diseases. Topological phenomena in such networks have recently been exploited to preserve the response of systems in the presence of disorder. We propose and demonstrate topological structurally disordered systems with a modal structure that enhances nonlinear phenomena in the topological channels by inhibiting the ultrafast leakage of energy from edge modes to bulk modes. We present the construction of the graph and show that its dynamics enhances the topologically protected photon pair generation rate by an order of magnitude. Disordered nonlinear topological graphs will enable advanced quantum interconnects, efficient nonlinear sources, and light-based information processing for artificial intelligence.", 
    "abstract": "Lymph node metastasis (LNM) is critical for treatment decision-making for cancer patients, but it is difficult to diagnose accurately before surgery. Machine learning can learn nontrivial knowledge from multi-modal data to support accurate diagnosis. In this paper, we proposed a Multi-modal Heterogeneous Graph Forest (MHGF) approach to extract the deep representations of LNM from multi-modal data. Specifically, we first extracted the deep image features from CT images to represent the pathological anatomic extent of the primary tumor (pathological T stage) using a ResNet-Trans network. And then, a heterogeneous graph with six vertices and seven bi-directional relations was defined by medical experts to describe the possible relations between the clinical and image features. After that, we proposed a graph forest approach to construct the sub-graphs by removing each vertex in the complete graph iteratively. Finally, we used graph neural networks to learn the representations of each sub-graph in the forest to predict LNM and averaged all the prediction results as final results. We conducted experiments on 681 patients' multi-modal data. The proposed MHGF achieves the best performances with a 0.806 AUC value and 0.513 AP value compared with state-of-art machine learning and deep learning methods. The results indicate that the graph method can explore the relations between different types of features to learn effective deep representations for LNM prediction. Moreover, we found that the deep image features about the pathological anatomic extent of the primary tumor are useful for LNM prediction. And the graph forest approach can further improve the generalization ability and stability of the LNM prediction model.", 
    "abstract": "In biochemistry, graph structures have been widely used for modeling compounds, proteins, functional interactions, etc. A common task that divides these graphs into different categories, known as graph classification, highly relies on the quality of the representations of graphs. With the advance in graph neural networks, message-passing-based methods are adopted to iteratively aggregate neighborhood information for better graph representations. These methods, though powerful, still suffer from some shortcomings. The first challenge is that pooling-based methods in graph neural networks may sometimes ignore the part-whole hierarchies naturally existing in graph structures. These part-whole relationships are usually valuable for many molecular function prediction tasks. The second challenge is that most existing methods do not take the heterogeneity embedded in graph representations into consideration. Disentangling the heterogeneity will increase the performance and interpretability of models. This paper proposes a graph capsule network for graph classification tasks with disentangled feature representations learned automatically by well-designed algorithms. This method is capable of, on the one hand, decomposing heterogeneous representations to more fine-grained elements, whilst on the other hand, capturing part-whole relationships using capsules. Extensive experiments performed on several public-available biochemistry datasets demonstrated the effectiveness of the proposed method, compared with nine state-of-the-art graph learning methods.", 
    "abstract": "Many neural networks for graphs are based on the graph convolution (GC) operator, proposed more than a decade ago. Since then, many alternative definitions have been proposed, which tend to add complexity (and nonlinearity) to the model. Recently, however, a simplified GC operator, dubbed simple graph convolution (SGC), which aims to remove nonlinearities was proposed. Motivated by the good results reached by this simpler model, in this article we propose, analyze, and compare simple graph convolution operators of increasing complexity that rely on linear transformations or controlled nonlinearities, and that can be implemented in single-layer graph convolutional networks (GCNs). Their computational expressiveness is characterized as well. We show that the predictive performance of the proposed GC operators is competitive with the ones of other widely adopted models on the considered node classification benchmark datasets.", 
    "abstract": "Hybrid visualizations combine different metaphors into a single network layout, in order to help humans in finding the \"right way\" of displaying the different portions of the network, especially when it is globally sparse and locally dense. We investigate hybrid visualizations in two complementary directions: (i) On the one hand, we evaluate the effectiveness of different hybrid visualization models through a comparative user study; (ii) On the other hand, we estimate the usefulness of an interactive visualization that integrates all the considered hybrid models together. The results of our study provide some hints about the usefulness of the different hybrid visualizations for specific tasks of analysis and indicates that integrating different hybrid models into a single visualization may offer a valuable tool of analysis.", 
    "abstract": "Instrumental variable (IV) is a powerful approach to inferring the causal effect of a treatment on an outcome of interest from observational data even when there exist latent confounders between the treatment and the outcome. However, existing IV methods require that an IV is selected and justified with domain knowledge. An invalid IV may lead to biased estimates. Hence, discovering a valid IV is critical to the applications of IV methods. In this article, we study and design a data-driven algorithm to discover valid IVs from data under mild assumptions. We develop the theory based on partial ancestral graphs (PAGs) to support the search for a set of candidate ancestral IVs (AIVs), and for each possible AIV, the identification of its conditioning set. Based on the theory, we propose a data-driven algorithm to discover a pair of IVs from data. The experiments on synthetic and real-world datasets show that the developed IV discovery algorithm estimates accurate estimates of causal effects in comparison with the state-of-the-art IV-based causal effect estimators.", 
    "abstract": "Reducing violence to others in community-based patients with schizophrenia has important implications for public health. Increasing medication adherence is often used to reduce the risk of violence, yet little is known about the association between medication nonadherence and violence to others in this population.\nTo examine the association between medication nonadherence and violence to others among community-based patients with schizophrenia.\nThis large, naturalistic, prospective cohort study was performed in western China from May 1, 2006, to December 31, 2018. The data set was from the integrated management information platform for severe mental disorders. As of December 31, 2018, 292 667 patients with schizophrenia were registered in the platform. During follow-up, patients could enter or leave the cohort at any time. Maximum follow-up was 12.8 years, with a mean (SD) of 4.2 (2.3) years. Data analysis was conducted from July 1, 2021, to September 30, 2022.\nMedication nonadherence.\nViolence to others throughout the follow-up period was the outcome, including minor nuisances, violating the Law of the People's Republic of China on Penalties for Administration of Public Security (APS law), and violating criminal law. Information about these behaviors was provided by the public security department. Directed acyclic graphs were used to identify and control confounders. Propensity score matching and generalized linear mixed-effects models were used for analysis.\nThe final study sample included 207 569 patients with schizophrenia. The mean (SD) age was 51.3 (14.5) years, and 107 271 (51.7%) were women; 27 698 (13.3%) perpetrated violence to others, including 22 312 of 142 394 with medication nonadherence (15.7%) and 5386 of 65 175 with adherence (8.3%). In 112 710 propensity score-matched cases, risks of minor nuisances (odds ratio [OR], 1.82 [95% CI, 1.75-1.90]; P\u2009<\u2009.001), violating APS law (OR, 1.91 [95% CI, 1.78-2.05]; P\u2009<\u2009.001), and violating criminal law (OR, 1.50 [95% CI, 1.33-1.71]; P\u2009<\u2009.001) were higher in patients with nonadherence. However, the risk did not increase with higher medication nonadherence. There were differences in risk of violating APS law between urban and rural areas.\nMedication nonadherence was associated with a higher risk of violence to others among community-based patients with schizophrenia, but the risk did not increase as medication nonadherence increased.", 
    "abstract": "Since the occurrence of the COVID-19 pandemic, many governments around the world have instituted several economic policy responses to swathe the real sectors of their economies from the ramifications of the pandemic. However, most economies still remain vulnerable to the pandemic. In this paper, we evaluate and quantify the potential short-run impact of the COVID-19 pandemic on economic activities in eighteen (18) developing countries using monthly time series data on Industrial Production Index and Composite Index of Economic Activity from January 2010 to December 2020. In addition, we employ a state-space model (a Bayesian structural time series model) to estimate the absolute and relative effects of the COVID-19 pandemic on economic activities in those countries. The results of our Bayesian posterior estimate show that, in relative terms, economic activities of six countries have significantly reduced during the occurrence of the COVID-19 pandemic, usually between -4.4% and -16%. Our Bayesian posterior distribution graphs show that the significant negative impacts of the COVID-19 pandemic on economic activities of most of the countries are rather short-lived. This finding suggests that the real sectors of those countries have seen a recovery after being adversely affected by the COVID-19 pandemic. We recommend a continuation of the policy tools introduced by the central banks and the international organizations with a key focus on sectors of that economy that involves significant human interactions such as the hospitality and tourism as well as the aviation industry which was hugely hit by the pandemic.", 
    "abstract": "The Worldwide spread of the Omicron lineage variants has now been confirmed. It is crucial to understand the process of cellular life and to discover new drugs need to identify the important proteins in a protein interaction network (PPIN). PPINs are often represented by graphs in bioinformatics, which describe cell processes. There are some proteins that have significant influences on these tissues, and which play a crucial role in regulating them. The discovery of new drugs is aided by the study of significant proteins. These significant proteins can be found by reducing the graph and using graph analysis. Studies examining protein interactions in the Omicron lineage (B.1.1.529) and its variants (BA.5, BA.4, BA.3, BA.2, BA.1.1, BA.1) are not yet available. Studying Omicron has been intended to find a significant protein. 68 nodes represent 68 proteins and 52 edges represent the relationship among the protein in the network. A few centrality measures are computed namely page rank centrality (PRC), degree centrality (DC), closeness centrality (CC), and betweenness centrality (BC) together with node degree and Local clustering coefficient (LCC). We also discover 18 network clusters using Markov clustering. 8 significant proteins (candidate gene of Omicron lineage variants) were detected among the 68 proteins, including AHSG, KCNK1, KCNQ1, MAPT, NR1H4, PSMC2, PTPN11 and, UBE21 which scored the highest among the Omicron proteins. It is found that in the variant of Omicron protein-protein interaction networks, the MAPT protein's impact is the most significant.", 
    "abstract": "Characterizing the topology of gene regulatory networks (GRNs) is a fundamental problem in systems biology. The advent of single cell technologies has made it possible to construct GRNs at finer resolutions than bulk and microarray datasets. However, cellular heterogeneity and sparsity of the single cell datasets render void the application of regular Gaussian assumptions for constructing GRNs. Additionally, most GRN reconstruction approaches estimate a single network for the entire data. This could cause potential loss of information when single cell datasets are generated from multiple treatment conditions/disease states.\nTo better characterize single cell GRNs under different but related conditions, we propose the joint estimation of multiple networks using multiple signed graph learning (scMSGL). The proposed method is based on recently developed graph signal processing (GSP) based graph learning, where GRNs and gene expressions are modeled as signed graphs and graph signals, respectively. scMSGL learns multiple GRNs by optimizing the total variation of gene expressions with respect to GRNs while ensuring that the learned GRNs are similar to each other through regularization with respect to a learned signed consensus graph. We further kernelize scMSGL with the kernel selected to suit the structure of single cell data.\nscMSGL is shown to have superior performance over existing state of the art methods in GRN recovery on simulated datasets. Furthermore, scMSGL successfully identifies well-established regulators in a mouse embryonic stem cell differentiation study and a cancer clinical study of medulloblastoma.", 
    "abstract": "The resting-state functional magnetic resonance imaging (rs-fMRI) faithfully reflects the brain activities and thus provides a promising tool for autism spectrum disorder (ASD) classification. Up to now, graph convolutional networks (GCNs) have been successfully applied in rs-fMRI based ASD classification. However, most of these methods were developed based on functional connectivities (FCs) that only reflect low-level correlation between brain regions, without integrating both high-level discriminative knowledge and phenotypic information into classification. Besides, they suffered from the overfitting problem caused by insufficient training samples. To this end, we propose a novel contrastive multi-view composite GCN (CMV-CGCN) for ASD classification using both FCs and HOFCs. Specifically, a pair of graphs are constructed based on the FC and HOFC features of the subjects, respectively, and they share the phenotypic information in the graph edges. A novel contrastive multi-view learning method is proposed based on the consistent representation of both views. A contribution learning mechanism is further incorporated, encouraging the FC and HOFC features of different subjects to have various contribution in the contrastive multi-view learning. The proposed CMV-CGCN is evaluated on 613 subjects (including 286 ASD patients and 327 NCs) from the Autism Brain Imaging Data Exchange (ABIDE). We demonstrate the performance of the method for ASD classification, which yields an accuracy of 75.20% and an area under the curve (AUC) of 0.7338. Experimental results show that our proposed method outperforms state-of-the-art methods on the ABIDE database.", 
    "abstract": "A computational graph in a deep neural network (DNN) denotes a specific data flow diagram (DFD) composed of many tensors and operators. Existing toolkits for visualizing computational graphs are not applicable when the structure is highly complicated and large-scale (e.g., BERT\u00a0[1]). To address this problem, we propose leveraging a suite of visual simplification techniques, including a cycle-removing method, a module-based edge-pruning algorithm, and an isomorphic subgraph stacking strategy. We design and implement an interactive visualization system that is suitable for computational graphs with up to 10 thousand elements. Experimental results and usage scenarios demonstrate that our tool reduces 60% elements on average and hence enhances the performance for recognizing and diagnosing DNN models. Our contributions are integrated into an open-source DNN visualization toolkit, namely, MindInsight\u00a0[2].", 
    "abstract": "In real applications, it is often that the collected multiview data contain missing views. Most existing incomplete multiview clustering (IMVC) methods cannot fully utilize the underlying information of missing data or sufficiently explore the consistent and complementary characteristics. In this article, we propose a novel Low-rAnk Tensor regularized viEws Recovery (LATER) method for IMVC, which jointly reconstructs and utilizes the missing views and learns multilevel graphs for comprehensive similarity discovery in a unified model. The missing views are recovered from a common latent representation, and the recovered views conversely improve the learning of shared patterns. Based on the shared subspace representations and recovered complete multiview data, the multilevel graphs are learned by self-representation to fully exploit the consistent and complementary information among views. Besides, a tensor nuclear norm regularizer is introduced to pursue the global low-rank property and explore the interview correlations. An alternating direction minimization algorithm is presented to optimize the proposed model. Moreover, a new initialization method is proposed to promote the effectiveness of our method for latent representation learning and missing data recovery. Extensive experiments demonstrate that our method outperforms the state-of-the-art approaches.", 
    "abstract": "Recently, online education has become popular. Many e-learning platforms have been launched with various intelligent services aimed at improving the learning efficiency and effectiveness of learners. Graphs are used to describe the pairwise relations between entities, and the node embedding technique is the foundation of many intelligent services, which have received increasing attention from researchers. However, the graph in the intelligent education scenario has three noteworthy properties, namely, heterogeneity, evolution, and lopsidedness, which makes it challenging to implement ecumenical node embedding methods on it. In this article, an autobalanced multitask node embedding model is proposed, named MNE, and applied to the interaction graph, settling a few actual tasks in intelligent education. More specifically, MNE builds two purpose-built self-supervised node embedding learning tasks for heterogeneous evolutive graphs. Edge-specific reconstruction tasks are built according to the semantic information and properties of the heterogeneous edges, and an evolutive weight regression task is designed, aiding the model to perceive the evolution of learners' implicit cognitive states. Then, both aleatoric and epistemic uncertainty quantification techniques are introduced, achieving both task-and node-level weight estimation and instructing subtask autobalancing. Experimental results on real-world datasets indicate that the proposed model outperforms the state-of-the-art graph embedding methods on two assessment tasks and demonstrates the validity of the proposed multitask framework and subtask balancing mechanism. Our implementations are available at https://github.com/ccnu-mathits/MNE4HEN.", 
    "abstract": "In de novo genome assembly using short Illumina reads, the accurate determination of node and arc multiplicities in a de Bruijn graph has a large impact on the quality and contiguity of the assembly. The multiplicity estimates of nodes and arcs guide the cleaning of the de Bruijn graph by identifying spurious nodes and arcs that correspond to sequencing errors. Additionally, they can be used to guide repeat resolution. Here, we model the entire de Bruijn graph and the accompanying read coverage information with a single Conditional Random Field (CRF) model. We show that approximate inference using Loopy Belief Propagation (LBP) on our model improves multiplicity assignment accuracy within feasible runtimes. The order in which messages are passed has a large influence on the speed of LBP convergence. Little theoretical guarantees exist and the conditions for convergence are not easily checked as our CRF model contains higher-order interactions. Therefore, we also present an empirical evaluation of several message passing schemes that may guide future users of LBP on CRFs with higher-order interactions in their choice of message passing scheme.", 
    "abstract": "Since the representative capacity of graph-based clustering methods is usually limited by the graph constructed on the original features, it is attractive to find whether graph neural networks (GNNs), a strong extension of neural networks to graphs, can be applied to augment the capacity of graph-based clustering methods. The core problems mainly come from two aspects. On the one hand, the graph is unavailable in the most general clustering scenes so that how to construct graph on the non-graph data and the quality of graph is usually the most important part. On the other hand, given n samples, the graph-based clustering methods usually consume at least O(n", 
    "abstract": "Inspired by our observation that numerous objects of remote sensing imageries are extremely consistent in geometric characteristics (e.g., object sizes/angles/layouts), in this work, we propose a novel Progressive Context-dependent Inference (PCI) method to make full use of large-scope contextual cues for better localizing objects in remote sensing imagery. Especially, to represent candidate objects and their geometric distributions, we build all of them into candidate object graphs, and subsequently perform inference learning by diffusing contextual object information. To make the inference more credible, we progressively accumulate these historical learning experiences on both label prediction and location regression processes into the next stage of network evolution, where topology structures and attributes of candidate object graphs would be dynamically updated. The graph update and ground object detection are jointly encapsulated as a closed-looping learning process. Hereby the problem of multi-object localization is converted into a progressive construction of dynamic graphs. Extensive experiments on three public datasets demonstrate the superiority of our proposed method over other state-of-the-art methods for ground object detection in remote sensing imagery.", 
    "abstract": "While graph neural networks (GNNs) are popular in the deep learning community, they suffer from several challenges including over-smoothing, over-squashing, and gradient vanishing. Recently, a series of models have attempted to relieve these issues by first augmenting the node features and then imposing node-wise functions based on multilayer perceptron (MLP), which are widely referred to as graph-augmented MLP (GA-MLP) models. However, while GA-MLP models enjoy deeper architectures for better accuracy, their efficiency largely deteriorates. Moreover, popular acceleration techniques such as stochastic-version or data-parallelism cannot be effectively applied due to the dependency among samples (i.e., nodes) in graphs. To address these issues, in this article, instead of data parallelism, we propose a parallel graph deep learning Alternating Direction Method of Multipliers (pdADMM-G) framework to achieve model parallelism: parameters in each layer of GA-MLP models can be updated in parallel. The extended pdADMM-G-Q algorithm reduces communication costs by introducing the quantization technique. Theoretical convergence to a (quantized) stationary point of the pdADMM-G algorithm and the pdADMM-G-Q algorithm is provided with a sublinear convergence rate o(1/k) , where k is the number of iterations. Extensive experiments demonstrate the convergence of two proposed algorithms. Moreover, they lead to a more massive speedup and better performance than all state-of-the-art comparison methods on nine benchmark datasets. Last but not least, the proposed pdADMM-G-Q algorithm reduces communication overheads by up to 45% without loss of performance. Our code is available at https://github.com/xianggebenben/pdADMM-G.", 
    "abstract": "Given an input image, scene graph generation (SGG) aims to generate comprehensive visual relationships between objects in the form of graphs. Recently, more attention to the design of complex networks and complicated strategies has been paid to the long tail issue caused by the imbalanced class distribution. However, most existing methods adopt the concatenated features of two objects in real space as the final relation representation for a given triplet. We mainly argue that such a simple concatenation may neglect the importance of complex interactions between objects, which results in the diversity of visual relations. In addition, the representation learning in real space is also inadequate to express this property. To alleviate these issues, we seamlessly incorporate Hermitian inner product into existing models to facilitate the generation of scene graphs by learning Relation Embedding in Complex space (CoRE). More specifically, we first introduce the concept of complex-valued representations for entities and then formulate the relation triplets with Hermitian inner product in complex space. Finally, we investigate the effect of utilizing only real component or both of Hermitian inner product on inferring more reasonable interaction between objects for scene graphs. Comprehensive experiments on two widely used benchmark datasets, Visual Genome (VG) and Open Image, demonstrate our effectiveness, superiority, and generalization on various metrics for biased or unbiased inference.", 
    "abstract": "The ability to capture joint connections in complicated motion is essential for skeleton-based action recognition. However, earlier approaches may not be able to fully explore this connection in either the spatial or temporal dimension due to fixed or single-level topological structures and insufficient temporal modeling. In this paper, we propose a novel multilevel spatial-temporal excited graph network (ML-STGNet) to address the above problems. In the spatial configuration, we decouple the learning of the human skeleton into general and individual graphs by designing a multilevel graph convolution (ML-GCN) network and a spatial data-driven excitation (SDE) module, respectively. ML-GCN leverages joint-level, part-level, and body-level graphs to comprehensively model the hierarchical relations of a human body. Based on this, SDE is further introduced to handle the diverse joint relations of different samples in a data-dependent way. This decoupling approach not only increases the flexibility of the model for graph construction but also enables the generality to adapt to various data samples. In the temporal configuration, we apply the concept of temporal difference to the human skeleton and design an efficient temporal motion excitation (TME) module to highlight the motion-sensitive features. Furthermore, a simplified multiscale temporal convolution (MS-TCN) network is introduced to enrich the expression ability of temporal features. Extensive experiments on the four popular datasets NTU-RGB+D, NTU-RGB+D 120, Kinetics Skeleton 400, and Toyota Smarthome demonstrate that ML-STGNet gains considerable improvements over the existing state of the art.", 
    "abstract": "Graph Neural Networks (GNNs) have established themselves as state-of-the-art for many machine learning applications such as the analysis of social and medical networks. Several among these datasets contain privacy-sensitive data. Machine learning with differential privacy is a promising technique to allow deriving insight from sensitive data while offering formal guarantees of privacy protection. However, the differentially private training of GNNs has so far remained under-explored due to the challenges presented by the intrinsic structural connectivity of graphs. In this work, we introduce a framework for differential private graph-level classification. Our method is applicable to graph deep learning on multi-graph datasets and relies on differentially private stochastic gradient descent (DP-SGD). We show results on a variety of datasets and evaluate the impact of different GNN architectures and training hyperparameters on model performance for differentially private graph classification, as well as the scalability of the method on a large medical dataset. Our experiments show that DP-SGD can be applied to graph classification tasks with reasonable utility losses. Furthermore, we apply explainability techniques to assess whether similar representations are learned in the private and non-private settings. Our results can also function as robust baselines for future work in this area.", 
    "abstract": "Among the blended components of a photoactive layer in organic photovoltaic (OPV) cells, the acceptor is of high importance. This importance is attributed to its increased ability to withdraw electrons toward itself for their effective transport toward the respective electrode. In this research work, seven new non-fullerene acceptors were designed for their possible utilization in the OPVs. These molecules were designed through side-chain engineering of the PTBTP-4F molecule, with its fused pyrrole ring-based donor core and different strongly electron-withdrawing acceptors. To elucidate their effectiveness, the band gaps, absorption characteristics, chemical reactivity indices, and photovoltaic parameters of all of the architecture molecules were compared with the reference. Through various computational software, transition density matrices, graphs of absorption, and density of states were also plotted for these molecules. From some chemical reactivity indices and electron mobility values, it was proposed that our newly designed molecules could be better electron-transporting materials than the reference. Among all, TP1, due to its most stabilized frontier molecular orbitals, lowest band gap and excitation energies, highest absorption maxima in both the solvent and gas medium, least hardness, highest ionization potential, superior electron affinity, lowest electron reorganization energy, as well as highest rate constant of charge hopping, seemed to be the best molecule in terms of its electron-withdrawing abilities in the photoactive layer blend. In addition, in terms of all of the photovoltaic parameters, TP4-TP7 was perceived to be better suited in comparison to TPR. Thus, all our suggested molecules could act as superior acceptors to TPR.", 
    "abstract": "We consider the Casson hybrid nanofluid (HN) (ZnO + Ag/Casson fluid) that flows steadily along a two-directional stretchable sheet under the influence of an applied changing magnetic flux and is electrically conducting. The basic Casson and Cattaneo-Christov double diffusion (CCDD) formulations are used for the simulation of the problem. This is the first study on the analysis of the Casson hybrid nanofluid by using the CCDD model. The use of these models generalize basic Fick's and Fourier's laws. The current produced due to the magnetic parameter is taken into consideration by using the generalized Oham law. The problem is formulated and then transformed to a coupled set of ordinary differential equations. The simplified set of equations is solved using the homotopy analysis method. The obtained results are presented through tables and graphs for various state variables. A comparative survey in all the graphs is presented for the nanofluid (ZnO/Casson fluid) with the HN (ZnO + Ag/Casson fluid). These graphs depict the effect of various pertinent parameters, like ", 
    "abstract": "A cut sparsifier is a reweighted subgraph that maintains the weights of the cuts of the original graph up to a multiplicative factor of ", 
    "abstract": "We consider two variants of ", 
    "abstract": "Ecologists often rely on randomized control trials (RCTs) to quantify causal relationships in nature. Many of our foundational insights of ecological phenomena can be traced back to well-designed experiments, and RCTs continue to provide valuable insights today. Although RCTs are often regarded as the \"gold standard\" for causal inference, it is important to recognize that they too rely on a set of causal assumptions that must be justified and met by the researcher to draw valid causal conclusions. We use key ecological examples to show how biases such as confounding, overcontrol, and collider bias can occur in experimental setups. In tandem, we highlight how such biases can be removed through the application of the structural causal model (SCM) framework. The SCM framework visualizes the causal structure of a system or process under study using directed acyclic graphs (DAGs) and subsequently applies a set of graphical rules to remove bias from both observational and experimental data. We show how DAGs can be applied across ecological experimental studies to ensure proper study design and statistical analysis, leading to more accurate causal estimates drawn from experimental data. Although causal conclusions drawn from RCTs are often taken at face value, ecologists are increasingly becoming aware that experimental approaches must be carefully designed and analyzed to avoid potential biases. By applying DAGs as a visual and conceptual tool, experimental ecologists can increasingly meet the causal assumptions required for valid causal inference.", 
    "abstract": "Electronic personal health record (e-PHR) system enables individuals to access their health information and manage it themselves. It helps patient engagement management of health information that is accessed and shared with their healthcare providers using the platform. This improves individual healthcare through the exchange of health information between patients and healthcare providers. However, less is known about e-PHRs among healthcare professionals.\nTherefore, this study aimed to assess Health professionals' Knowledge and attitude and its associated factors toward e-PHR at the teaching hospital in northwest Ethiopia.\nAn institution-based cross-sectional study design was used to determine healthcare professionals' knowledge and attitude and their associated factors toward e-PHR systems in teaching hospitals of Amhara regional state, Ethiopia, from 20 July to 20 August 2022. Pretested structured self-administered questionnaires were used to collect the data. Descriptive statistic was computed based on sociodemographic and other variables presented in the form of table graphs and texts. Bivariable and multivariable logistic analyses were performed with an adjusted odds ratio (AOR) and 95% CI to identify predictor variables.\nOf the total study participants, 57% were males and nearly half of the respondents had a bachelor's degree. Out of 402 participants, ~65.7% [61-70%] and 55.5% [50-60%] had good knowledge and favorable attitude toward e-PHR systems, respectively. Having a social media account 4.3 [AOR = 4.3, 95% CI (2.3-7.9)], having a smartphone 4.4 [AOR = 4.4, 95% CI (2.2-8.6)], digital literacy 8.8 [(AOR = 8.8, 95% CI (4.6-15.9)], being male 2.7 [AOR = 2.7, 95% CI (1.4-5.0)], and perceived usefulness 4.5 [(AOR = 4.5, 95% CI (2.5-8.5)] were positively associated with knowledge toward e-PHR systems. Similarly, having a personal computer 1.9 [AOR = 1.9, 95% CI (1.1-3.5)], computer training 3.9 [AOR = 3.9, 95% CI (1.8-8.3)], computer skill 19.8 [AOR = 19.8, 95% CI (10.7-36.9)], and Internet access 6.0 [AOR = 6.0, 95% CI (3.0-12.0)] were predictors for attitude toward e-PHR systems.\nThe findings from the study showed that healthcare professionals have good knowledge and a favorable attitude toward e-PHRs. Providing comprehensive basic computer training to improve healthcare professionals' expectation on the usefulness of e-PHR systems has a paramount contribution to the advancement of their knowledge and attitude toward successfully implementing e-PHRs.", 
    "abstract": "Accurate prediction of the drug-target affinity (DTA) ", 
    "abstract": "Eight well-known herbals in Zhejiang Province, Zhebawei, are commonly used as traditional Chinese herbal medicines owing to their rich active ingredients. However, the unavoidable use of pesticides during agricultural production has led to pesticide residue problems in these herbs. In this study, a simple, rapid, and accurate method was established to determine 22 triazole pesticide residues in Zhebawei. An improved QuEChERS method was used for sample pretreatment, and \n\u5efa\u7acb\u4e86\u201c\u6d59\u516b\u5473\u201d\u4e2d\u8349\u836f\u4e2d22\u79cd\u4e09\u5511\u7c7b\u6740\u83cc\u5242\u519c\u836f\u6b8b\u7559\u7684\u7b80\u4fbf\u3001\u5feb\u901f\u548c\u7cbe\u51c6\u5206\u6790\u65b9\u6cd5\u3002\u4ee5\u767d\u672f\u4e3a\u4ee3\u8868\u6027\u57fa\u8d28,\u5bf9QuEChERS\u524d\u5904\u7406\u65b9\u6cd5\u8fdb\u884c\u6539\u826f,\u6837\u54c1\u7ecf\u4e59\u8148\u8403\u53d6\u540e,\u9009\u53d610 mg\u7fa7\u57fa\u5316\u591a\u58c1\u78b3\u7eb3\u7c73\u7ba1\u548c20 mg C18\u7ec4\u5408\u4f5c\u4e3a\u51c0\u5316\u5438\u9644\u5242,\u5e76\u91c7\u7528\u6db2\u76f8\u8272\u8c31-\u4e32\u8054\u8d28\u8c31\u8fdb\u884c\u5206\u6790;\u5bf9\u5efa\u7acb\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8003\u5bdf,\u7ce0\u83cc\u5511\u3001\u6c1f\u73af\u5511\u548c\u4e59\u73af\u5511\u7684\u7ebf\u6027\u8303\u56f4\u4e3a2.5~200 \u03bcg/L,\u5176\u4f59\u519c\u836f\u7684\u7ebf\u6027\u8303\u56f4\u4e3a1~200 \u03bcg/L,\u76f8\u5173\u7cfb\u6570(", 
    "abstract": "We are at a time of considerable growth in transcriptomics studies and subsequent ", 
    "abstract": "A pathogen with high virulence potential in some host species, Perkinsus marinus remains a challenge for the ecological integrity of marine ecosystems and the health of bivalve molluscs. This study investigates the occurrence of P. marinus in Crassostrea sp. in estuaries of the Potengi River and the Guara\u00edras lagoon in Rio Grande do Norte, Brazil. A total of 203 oyster samples that tested positive for Perkinsus sp. in Ray's fluid thioglycollate medium (RFTM) were subjected to species-specific quantitiative PCR, where 61 animals (30.05\u00a0%) presented amplification graphs with a melting temperature of 80.1\u00a0\u00b1\u00a00.6\u00a0\u00b0C matching the positive control. This was the first record of P. marinus in oysters in these estuaries using qPCR as a diagnostic tool.", 
    "abstract": "Obsessive-compulsive disorder (OCD) is a distressing disorder characterized by the presence of intrusive thoughts, images or urges (obsessions) and/or behavioral efforts to reduce the anxiety (compulsions). OCD lifetime prevalence varies between 1% and 3% in the general population and there are no reliable markers that support the diagnosis. In order to fill this gap, Computational Psychiatry employs multiple types of quantitative analyses to improve the understanding, diagnosis, prediction, and treatment of mental illnesses including OCD. One of these computational tools is speech graphs analysis. A graph represents a network of nodes connected by edges: in non-semantic speech graphs, nodes correspond to words and edges correspond to the directed link between consecutive words. Using non-semantic speech graphs, we compared free speech samples from OCD patients and healthy controls (HC), to test whether speech graphs analysis can grasp structural differences in speech between these groups. To this end, 39 OCD patients and 37 HC were interviewed and recorded during six types of speech reports: yesterday, dream, old memory, positive image, negative image and neutral image. Also, the Obsessive-Compulsive Inventory-Revised (OCI-R) and the Yale Brown Obsessive-Compulsive Scale (Y-BOCS) were used to assess symptom severity. The graph-theoretical structural analysis of dream reports showed that OCD patients have significantly smaller lexical diversity, lower speech connectedness and a higher recurrence of words in comparison with HC. The other five report types failed to show differences between the groups, adding to the notion that dream reports are especially informative of speech structure in different psychiatric states. Further investigation is necessary to completely assess the potential of this tool in OCD.", 
    "abstract": "The tracking and documentation of procedures in gastrointestinal endoscopy including therapeutic interventions is an essential but challenging process. The University of Alberta has developed a smartphone app to help facilitate this task. This study evaluated the functionality, usefulness, and user satisfaction of this app.\nFour Gastroenterology (GI) residents and two therapeutic endoscopy fellows participated in the study. The trainees submitted all their data into the app from the procedures in which they participated hands-on for one year, data was collected and analyzed on the app and the website associated with it.\nTrainees were able to register the procedures immediately after each procedure without difficulty, this data was available to be reviewed at anytime in the app and associated website. Furthermore, the data collected was able to be transformed into tables and graphs on the app website. The total number of procedures and therapeutic interventions performed were easily accessed in the app and website at anytime. The app facilitated the calculation of the cecal intubation rate in colonoscopy and the cannulation rate in ERCP for the therapeutic endoscopy trainee. Trainees reported excellent experience with the app capabilities.\nA novel smartphone app was useful in collecting meaningful data submitted by gastrointestinal endoscopy trainees, furthermore, through an associated website, it was capable to create graphs and tables to show and facilitate the calculation of meaningful data such as key performance indicators.", 
    "abstract": "Maternal mortality remains a major health problem in Ethiopia. To generate contextual evidence on the burden and distribution of existing causes and contributing factors for programmatic and individual-level decision-making, the Maternal Death Surveillance and Response System was introduced in 2013. This assessment describes the Ethiopian health system's readiness to avail evidence for decision-making through the MDSR system.\nA cross-sectional study designed using the WHO framework for evaluating surveillance systems was used. By employing a multistage sampling, 631 health facilities and 539 health posts were included. ODK collect data entry software was used to collect data from September 2019 to April 2020. Findings are presented in text descriptions, graphs, maps, and tables.\nFour hundred (77.1%) health facilities (332 (74.6%) health centers and 68 (91.9%) hospitals) and 264 (71.5%) health posts reported implementing the MDSR system. Of the implementing health facilities, 349 (87.3%) had a death review committee, and only 42 (12.4%) were functional. About 89.4% of health centers and 79.4% of hospitals had sub-optimal maternal death identification and notification readiness. Only 23 (6.96%) and 18 (26.47%) MDSR-implementing health centers and hospitals had optimal readiness to investigate and review maternal deaths, respectively. Moreover, only 39 (14.0%) health posts had locally translated case definitions and 28 (10.6%) had verbal autopsy format to investigate maternal deaths. Six (1.5%) facility officers and 24 (9.1%) health extension workers were engaged in data analysis and evidence generation at least once during 2019/20. Regional variation is observed in system implementation.\nSub-optimal MDSR system implementation is recorded. Revitalizing the system by addressing all system components is critical. Having a national-level roadmap for MDSR system implementation and mobilizing all available resources and stakeholders to facilitate this is vital. Establishing a system for routine data quality monitoring and assurance by integrating with the existing PHEM structure, having a system for routine capacity building, advocacy, and monitoring and evaluating the availability and functionality of MDSR committees at health facilities are all critical. Digitalization, designing a system to fit emerging regions' health service delivery, and availing required resources for the system is key.", 
    "abstract": "The ", 
    "abstract": "Effective connectivity (EC), the causal influence that functional activity in a source brain location exerts over functional activity in a target brain location, has the potential to provide different information about brain network dynamics than functional connectivity (FC), which quantifies activity synchrony between locations. However, head-to-head comparisons between EC and FC from either task-based or resting-state functional MRI (fMRI) data are rare, especially in terms of how they associate with salient aspects of brain health.\nIn this study, 100 cognitively-healthy participants in the Bogalusa Heart Study aged 54.2 \u00b1 4.3years completed Stroop task-based fMRI, resting-state fMRI. EC and FC among 24 regions of interest (ROIs) previously identified as involved in Stroop task execution (EC-task and FC-task) and among 33 default mode network ROIs (EC-rest and FC-rest) were calculated from task-based and resting-state fMRI using deep stacking networks and Pearson correlation. The EC and FC measures were thresholded to generate directed and undirected graphs, from which standard graph metrics were calculated. Linear regression models related graph metrics to demographic, cardiometabolic risk factors, and cognitive function measures.\nWomen and whites (compared to men and African Americans) had better EC-task metrics, and better EC-task metrics associated with lower blood pressure, white matter hyperintensity volume, and higher vocabulary score (maximum value of \nIn a diverse, cognitively healthy, middle-aged community sample, EC and FC based graph metrics from task-based fMRI data, and EC based graph metrics from resting-state fMRI data, were associated with recognized indicators of brain health in differing ways. Future studies of brain health should consider taking both task-based and resting-state fMRI scans and measuring both EC and FC analyses to get a more complete picture of functional networks relevant to brain health.", 
    "abstract": "The ability to quantify structural changes of the endoplasmic reticulum (ER) is crucial for understanding the structure and function of this organelle. However, the rapid movement and complex topology of ER networks make this challenging. Here, we construct a state-of-the-art semantic segmentation method that we call ERnet for the automatic classification of sheet and tubular ER domains inside individual cells. Data are skeletonized and represented by connectivity graphs, enabling precise and efficient quantification of network connectivity. ERnet generates metrics on topology and integrity of ER structures and quantifies structural change in response to genetic or metabolic manipulation. We validate ERnet using data obtained by various ER-imaging methods from different cell types as well as ground truth images of synthetic ER structures. ERnet can be deployed in an automatic high-throughput and unbiased fashion and identifies subtle changes in ER phenotypes that may inform on disease progression and response to therapy.", 
    "abstract": "Predicting protein-protein interactions from sequences is an important goal of computational biology. Various sources of information can be used to this end. Starting from the sequences of two interacting protein families, one can use phylogeny or residue coevolution to infer which paralogs are specific interaction partners within each species. We show that these two signals can be combined to improve the performance of the inference of interaction partners among paralogs. For this, we first align the sequence-similarity graphs of the two families through simulated annealing, yielding a robust partial pairing. We next use this partial pairing to seed a coevolution-based iterative pairing algorithm. This combined method improves performance over either separate method. The improvement obtained is striking in the difficult cases where the average number of paralogs per species is large or where the total number of sequences is modest.", 
    "abstract": "[Formula: see text]-adenosine-methyltransferase (METTL3) is the catalytic domain of the 'writer' proteins which is involved in the post modifications of [Formula: see text]-methyladinosine ([Formula: see text]). Though its activities are essential in many biological processes, it has been implicated in several types of cancer. Thus, drug developers and researchers are relentlessly in search of small molecule inhibitors that can ameliorate the oncogenic activities of METTL3. Currently, STM2457 is a potent, highly selective inhibitor of METTL3 but is yet to be approved.\nIn this study, we employed structure-based virtual screening through consensus docking by using AutoDock Vina in PyRx interface and Glide virtual screening workflow of Schrodinger Glide. Thermodynamics via MM-PBSA calculations was further used to rank the compounds based on their total free binding energies. All atom molecular dynamics simulations were performed using AMBER 18 package. FF14SB force fields and Antechamber were used to parameterize the protein and compounds respectively. Post analysis of generated trajectories was analyzed with CPPTRAJ and PTRAJ modules incorporated in the AMBER package while Discovery studio and UCSF Chimera were used for visualization, and origin data tool used to plot all graphs.\nThree compounds with total free binding energies higher than STM2457 were selected for extended molecular dynamics simulations. The compounds, SANCDB0370, SANCDB0867, and SANCDB1033, exhibited stability and deeper penetration into the hydrophobic core of the protein. They engaged in relatively stronger intermolecular interactions involving hydrogen bonds with resultant increase in stability, reduced flexibility, and decrease in the surface area of the protein available for solvent interactions suggesting an induced folding of the catalytic domain. Furthermore, in silico pharmacokinetics and physicochemical analysis of the compounds revealed good properties suggesting these compounds could serve as promising MEETL3 entry inhibitors upon modifications and optimizations as presented by natural compounds. Further biochemical testing and experimentations would aid in the discovery of effective inhibitors against the berserk activities of METTL3.", 
    "abstract": "In the computation of molecular dynamic magnetizabilities and magnetic dipole moments, three different reference points are required: (i) origin of coordinate system, (ii) origin of vector potential ", 
    "abstract": "A recent breakthrough in differential network (DN) analysis of microbiome data has been realized with the advent of next-generation sequencing technologies. The DN analysis disentangles the microbial co-abundance among taxa by comparing the network properties between two or more graphs under different biological conditions. However, the existing methods to the DN analysis for microbiome data do not adjust for other clinical differences between subjects. We propose a Statistical Approach via Pseudo-value Information and Estimation for Differential Network Analysis (SOHPIE-DNA) that incorporates additional covariates such as continuous age and categorical BMI. SOHPIE-DNA is a regression technique adopting jackknife pseudo-values that can be implemented readily for the analysis. We demonstrate through simulations that SOHPIE-DNA consistently reaches higher recall and F1-score, while maintaining similar precision and accuracy to existing methods (NetCoMi and MDiNE). Lastly, we apply SOHPIE-DNA on two real datasets from the American Gut Project and the Diet Exchange Study to showcase the utility. The analysis of the Diet Exchange Study is to showcase that SOHPIE-DNA can also be used to incorporate the temporal change of connectivity of taxa with the inclusion of additional covariates. As a result, our method has found taxa that are related to the prevention of intestinal inflammation and severity of fatigue in advanced metastatic cancer patients.", 
    "abstract": "We are now in the era of millimeter-scale electron microscopy (EM) volumes collected at nanometer resolution (Shapson-Coe et al., 2021; Consortium et al., 2021). Dense reconstruction of cellular compartments in these EM volumes has been enabled by recent advances in Machine Learning (ML) (Lee et al., 2017; Wu et al., 2021; Lu et al., 2021; Macrina et al., 2021). Automated segmentation methods can now yield exceptionally accurate reconstructions of cells, but despite this accuracy, laborious post-hoc proofreading is still required to generate large connectomes free of merge and split errors. The elaborate 3-D meshes of neurons produced by these segmentations contain detailed morphological information, from the diameter, shape, and branching patterns of axons and dendrites, down to the fine-scale structure of dendritic spines. However, extracting information about these features can require substantial effort to piece together existing tools into custom workflows. Building on existing open-source software for mesh manipulation, here we present \"NEURD\", a software package that decomposes each meshed neuron into a compact and extensively-annotated graph representation. With these feature-rich graphs, we implement workflows for state of the art automated post-hoc proofreading of merge errors, cell classification, spine detection, axon-dendritic proximities, and other features that can enable many downstream analyses of neural morphology and connectivity. NEURD can make these new massive and complex datasets more accessible to neuroscience researchers focused on a variety of scientific questions.", 
    "abstract": "SARS-CoV-2 is a novel coronavirus that replicates itself via interacting with the host proteins. As a result, identifying virus and host protein-protein interactions could help researchers better understand the virus disease transmission behavior and identify possible COVID-19 drugs. The International Committee on Virus Taxonomy has determined that nCoV is genetically 89% compared to the SARS-CoV epidemic in 2003. This paper focuses on assessing the host-pathogen protein interaction affinity of the coronavirus family, having 44 different variants. In light of these considerations, a GO-semantic scoring function is provided based on Gene Ontology (GO) graphs for determining the binding affinity of any two proteins at the organism level. Based on the availability of the GO annotation of the proteins, 11 viral variants, ", 
    "abstract": "Analog mixed-signal (AMS) verification is one of the essential tasks in the development process of modern systems-on-chip (SoC). Most parts of the AMS verification flow are already automated, except for stimuli generation, which has been performed manually. It is thus challenging and time-consuming. Hence, automation is a necessity. To generate stimuli, subcircuits or subblocks of a given analog circuit module should be identified/classified. However, there currently needs to be a reliable industrial tool that can automatically identify/classify analog sub-circuits (eventually in the frame of a circuit design process) or automatically classify a given analog circuit at hand. Besides verification, several other processes would profit enormously from the availability of a robust and reliable automated classification model for analog circuit modules (which may belong to different levels). This paper presents how to use a Graph Convolutional Network (GCN) model and proposes a novel data augmentation strategy to automatically classify analog circuits of a given level. Eventually, it can be upscaled or integrated within a more complex functional module (for a structure recognition of complex analog circuits), targeting the identification of subcircuits within a more complex analog circuit module. An integrated novel data augmentation technique is particularly crucial due to the harsh reality of the availability of generally only a relatively limited dataset of analog circuits' schematics (i.e., sample architectures) in practical settings. Through a comprehensive ontology, we first introduce a graph representation framework of the circuits' schematics, which consists of converting the circuit's related netlists into graphs. Then, we use a robust classifier consisting of a GCN processor to determine the label corresponding to the given input analog circuit's schematics. Furthermore, the classification performance is improved and robust by involving a novel data augmentation technique. The classification accuracy was enhanced from 48.2% to 76.6% using feature matrix augmentation, and from 72% to 92% using Dataset Augmentation by Flipping. A 100% accuracy was achieved after applying either multi-Stage augmentation or Hyperphysical Augmentation. Overall, extensive tests of the concept were developed to demonstrate high accuracy for the analog circuit's classification endeavor. This is solid support for a future up-scaling towards an automated analog circuits' structure detection, which is one of the prerequisites not only for the stimuli generation in the frame of analog mixed-signal verification but also for other critical endeavors related to the engineering of AMS circuits.", 
    "abstract": "COVID-19 is a new multi-organ disease causing considerable worldwide morbidity and mortality. While many recognized pathophysiological mechanisms are involved, their exact causal relationships remain opaque. Better understanding is needed for predicting their progression, targeting therapeutic approaches, and improving patient outcomes. While many mathematical causal models describe COVID-19 epidemiology, none have described its pathophysiology.\nIn early 2020, we began developing such causal models. The SARS-CoV-2 virus's rapid and extensive spread made this particularly difficult: no large patient datasets were publicly available; the medical literature was flooded with sometimes conflicting pre-review reports; and clinicians in many countries had little time for academic consultations. We used Bayesian network (BN) models, which provide powerful calculation tools and directed acyclic graphs (DAGs) as comprehensible causal maps. Hence, they can incorporate both expert opinion and numerical data, and produce explainable, updatable results. To obtain the DAGs, we used extensive expert elicitation (exploiting Australia's exceptionally low COVID-19 burden) in structured online sessions. Groups of clinical and other specialists were enlisted to filter, interpret and discuss the literature and develop a current consensus. We encouraged inclusion of theoretically salient latent (unobservable) variables, likely mechanisms by extrapolation from other diseases, and documented supporting literature while noting controversies. Our method was iterative and incremental: systematically refining and validating the group output using one-on-one follow-up meetings with original and new experts. 35 experts contributed 126 hours face-to-face, and could review our products.\nWe present two key models, for the initial infection of the respiratory tract and the possible progression to complications, as causal DAGs and BNs with corresponding verbal descriptions, dictionaries and sources. These are the first published causal models of COVID-19 pathophysiology.\nOur method demonstrates an improved procedure for developing BNs via expert elicitation, which other teams can implement to model emergent complex phenomena. Our results have three anticipated applications: (i)\u00a0freely disseminating updatable expert knowledge; (ii)\u00a0guiding design and analysis of observational and clinical studies; (iii)\u00a0developing and validating automated tools for causal reasoning and decision support. We are developing such tools for the initial diagnosis, resource management, and prognosis of COVID-19, parameterized using the ISARIC and LEOSS databases.", 
    "abstract": "Human aromatase enzyme is a microsomal cytochrome P450 and catalyzes aromatization of androgens into estrogens during steroidogenesis. For breast cancer therapy, third-generation aromatase inhibitors (AIs) have proven to be effective; however patients acquire resistance to current AIs. Thus there is a need to predict aromatase-related proteins to develop efficacious AIs. A machine learning method was established to identify aromatase-related proteins using a five-fold cross validation technique. In this study, different SVM approach-based models were built using the following approaches like amino acid, dipeptide composition, hybrid and evolutionary profiles in the form of position-specific scoring matrix (PSSM); with maximum accuracy of 87.42%, 84.05%, 85.12%, and 92.02% respectively. Based on the primary sequence, the developed method is highly accurate to predict the aromatase-related proteins. Prediction scores graphs were developed using the known dataset to check the performance of the method. Based on the approach described above, a webserver for predicting aromatase-related proteins from primary sequence data was developed and implemented at https://bioinfo.imtech.res.in/servers/muthu/aromatase/home.html. We hope that the developed method will be useful for aromatase protein related research.", 
    "abstract": "In this article, a novel technique to evaluate and compare the neighborhood degree molecular descriptors of two variations of the carbon nanosheet C5C7(a,b) is presented. The conjugated molecules follow the graph spectral theory, in terms of bonding, non-bonding and antibonding Ruckel molecular orbitals. They are demonstrated to be immediately determinable from their topological characteristics. The effort of chemical and pharmaceutical researchers is significantly increased by the need to conduct numerous chemical experiments to ascertain the chemical characteristics of such a wide variety of novel chemicals. In order to generate novel cellular imaging techniques and to accomplish the regulation of certain cellular mechanisms, scientists have utilized the attributes of nanosheets such as their flexibility and simplicity of modification, out of which carbon nanosheets stand out for their remarkable strength, chemical stability, and electrical conductivity. With efficient tools like polynomials and functions that can forecast compound features, mathematical chemistry has a lot to offer. One such approach is the M-polynomial, a fundamental polynomial that can generate a significant number of degree-based topological indices. Among them, the neighborhood M-polynomial is useful in retrieving neighborhood degree sum-based topological indices that can help in carrying out physical, chemical, and biological experiments. This paper formulates the unique M-polynomial approach which is used to derive and compare a variety of neighborhood degree-based molecular descriptors and the corresponding entropy measures of two variations of pent-heptagonal carbon nanosheets. Furthermore, a regression analysis on these descriptors has also been carried out which can further help in the prediction of various properties of the molecule.", 
    "abstract": "Legionellosis is a respiratory disease related to environmental health. There have been manifold studies of pipe materials, risk installations and legionellosis without considering the type of transferred water. The objective of this study was to determine the potential development of the causative agent ", 
    "abstract": "", 
    "abstract": "The participation of males in joint spousal decisions is urgently needed in achieving the fundamental indicators of reproductive health. The low involvement of males in family planning (FP) decision-making is a major determining factor in low FP usage in Malawi and Tanzania. Despite this, there are inconsistent findings regarding the extent of male involvement and the determinants that aid male participation in FP decisions in these two countries. The objective of this study was to assess the prevalence of male involvement in FP decisions and its associated determinants within the household context in Malawi and Tanzania. We used data from the 2015-2016 Malawi and Tanzania Demographic and Health Surveys (DHSs) to examine the prevalence and the determinants inhibiting male involvement in FP decisions. The total sample size of 7478 from Malawi and 3514 males from Tanzania aged 15-54 years was employed in the analysis by STATA version 17. Descriptive (graphs, tables and means), bi-variate (chi-square) and logistic regression analyses (unadjusted (U) and adjusted odds ratio (AOR)) were performed to identify the determinants associated with male involvement in FP decisions. The mean age of respondents in Malawi was 32 years (\u00b18 SD) and in Tanzania, 36 years (\u00b16 SD), with the prevalence of male involvement in FP decisions being 53.0% in Malawi and 26.6% in Tanzania. Being aged 35-44 years [AOR = 1.81; 95% CI: 1.59-2.05] and 45-54 years [AOR = 1.43; 95% CI: 1.22-1.67], educated (secondary/higher) [AOR = 1.62; 95% CI: 1.31-1.99], having access to media information [AOR = 1.35; 95% CI: 1.21-1.51] and having a female head of household [AOR = 1.79; 95% CI: 1.70-1.90] were determinant factors of male involvement in FP decisions in Malawi. Primary education [AOR = 1.94; 95% CI: 1.39-2.72], having a middle wealth index ranking [AOR = 1.46; 95% CI: 1.17-1.81], being married [AOR = 1.62; 95% CI: 1.38-1.90] and working [AOR = 2.86; 95% CI: 2.10-3.88] were higher predictors of male involvement in FP decisions in Tanzania. Increasing the role of males in FP decisions and involvement in FP utilization may improve uptake and continuity of FP usage. Therefore, the findings from this cross-sectional study will support redesigning the ineffective strategic FP programs that accommodate socio-demographic determinants that may increase the likelihood of male involvement in FP decisions, especially in the grassroots settings in Malawi and Tanzania.", 
    "abstract": "We introduce the Random Walk Approximation (RWA), a new method to approximate the stationary solution of master equations describing stochastic processes taking place on graphs. Our approximation can be used for all processes governed by non-linear master equations without long-range interactions and with a conserved number of entities, which are typical in biological systems, such as gene regulatory or chemical reaction networks, where no exact solution exists. For linear systems, the RWA becomes the exact result obtained from the maximum entropy principle. The RWA allows having a simple analytical, even though approximated, form of the solution, which is global and easier to deal with than the standard System Size Expansion (SSE). Here, we give some theoretically sufficient conditions for the validity of the RWA and estimate the order of error calculated by the approximation with respect to the number of particles. We compare RWA with SSE for two examples, a toy model and the more realistic dual phosphorylation cycle, governed by the same underlying process. Both approximations are compared with the exact integration of the master equation, showing for the RWA good performances of the same order or better than the SSE, even in regions where sufficient conditions are not met.", 
    "abstract": "The integration of microarray technologies and machine learning methods has become popular in predicting the pathological condition of diseases and discovering risk genes. Traditional microarray analysis considers pathways as a simple gene set, treating all genes in the pathway identically while ignoring the pathway network's structure information. This study proposed an entropy-based directed random walk (e-DRW) method to infer pathway activities. Two enhancements from the conventional DRW were conducted, which are (1) to increase the coverage of human pathway information by constructing two inputting networks for pathway activity inference, and (2) to enhance the gene-weighting method in DRW by incorporating correlation coefficient values and ", 
    "abstract": "In the area of evidence-based medicine, the IPDfromKM-Shiny method is an innovative method of survival analysis, midway between artificial intelligence and advanced statistics. Its main characteristic is that an original software investigates the Kaplan-Meier graphs of trials so that individual-patient data are reconstructed. These reconstructed patients represent a new form of original clinical material. The typical objective of investigations based on this method is to analyze the available evidence, especially in oncology, to perform indirect comparisons, and determine the place in therapy of individual agents. This review examined the most recent applications of the IPDfromKM-Shiny method, in which a new web-based software-published in 2021-was used. Reported here are 14 analyses, mostly focused on oncological treatments. Indirect comparisons were based on overall survival or progression free survival. Each of these analyses provided original information to compare treatments with one another and select the most appropriate depending on patient characteristics. These analyses can also be useful to assess equivalence from a regulatory viewpoint. All investigations stressed the importance of heterogeneity to better interpret the evidence generated by IPDfromKM-Shiny investigations. In conclusion, these investigations showed that the reconstruction of individual patient data through this online tool is a promising new method for analyzing trials based on survival endpoints. This new approach deserves further investigation, particularly in the area of indirect comparisons.", 
    "abstract": "Molecular property prediction is an important direction in computer-aided drug design. In this paper, to fully explore the information from SMILE stings and graph data of molecules, we combined the SALSTM and GAT methods in order to mine the feature information of molecules from sequences and graphs. The embedding atoms are obtained through SALSTM, firstly using SMILES strings, and they are combined with graph node features and fed into the GAT to extract the global molecular representation. At the same time, data augmentation is added to enlarge the training dataset and improve the performance of the model. Finally, to enhance the interpretability of the model, the attention layers of both models are fused together to highlight the key atoms. Comparison with other graph-based and sequence-based methods, for multiple datasets, shows that our method can achieve high prediction accuracy with good generalizability.", 
    "abstract": "In this paper, the newly developed Fractal-Fractional derivative with power law kernel is used to analyse the dynamics of chaotic system based on a circuit design. The problem is modelled in terms of classical order nonlinear, coupled ordinary differential equations which is then generalized through Fractal-Fractional derivative with power law kernel. Furthermore, several theoretical analyses such as model equilibria, existence, uniqueness, and Ulam stability of the system have been calculated. The highly non-linear fractal-fractional order system is then analyzed through a numerical technique using the MATLAB software. The graphical solutions are portrayed in two dimensional graphs and three dimensional phase portraits and explained in detail in the discussion section while some concluding remarks have been drawn from the current study. It is worth noting that fractal-fractional differential operators can fastly converge the dynamics of chaotic system to its static equilibrium by adjusting the fractal and fractional parameters.", 
    "abstract": "This study discusses the flow of hybrid nanofluid over an infinite disk in a Darcy-Forchheimer permeable medium with variable thermal conductivity and viscosity. The objective of the current theoretical investigation is to identify the thermal energy characteristics of the nanomaterial flow resulting from thermo-solutal Marangoni convection on a disc surface. By including the impacts of activation energy, heat source, thermophoretic particle deposition and microorganisms the proposed mathematical model becomes more novel. The Cattaneo-Christov mass and heat flux law is taken into account when examining the features of mass and heat transmission rather than the traditional Fourier and Fick heat and mass flux law. MoS", 
    "abstract": "To solve recurring problems in drug discovery, matched molecular pair (MMP) analysis is used to understand relationships between chemical structure and function. For the MMP analysis of large data sets (>10,000 compounds), available tools lack flexible search and visualization functionality and require computational expertise. Here, we present Matcher, an open-source application for MMP analysis, with novel search algorithms and fully automated querying-to-visualization that requires no programming expertise. Matcher enables unprecedented control over the search and clustering of MMP transformations based on both variable fragment and constant environment structure, which is critical for disentangling relevant and irrelevant data to a given problem. Users can exert such control through a built-in chemical sketcher and with a few mouse clicks can navigate between resulting MMP transformations, statistics, property distribution graphs, and structures with raw experimental data, for confident and accelerated decision making. Matcher can be used with any collection of structure/property data; here, we demonstrate usage with a public ChEMBL data set of about 20,000 small molecules with CYP3A4 and/or hERG inhibition data. Users can reproduce all examples demonstrated herein via unique links within Matcher's interface-a functionality that anyone can use to preserve and share their own analyses. Matcher and all its dependencies are open-source, can be used for free, and are available with containerized deployment from code at https://github.com/Merck/Matcher. Matcher makes large structure/property data sets more transparent than ever before and accelerates the data-driven solution of common problems in drug discovery.", 
    "abstract": "Darwinian evolution is driven by random mutations, genetic recombination (gene shuffling) and selection that favors genotypes with high fitness. For systems where each genotype can be represented as a bitstring of length L, an overview of possible evolutionary trajectories is provided by the L-cube graph with nodes labeled by genotypes and edges directed toward the genotype with higher fitness. Peaks (sinks in the graphs) are important since a population can get stranded at a suboptimal peak. The fitness landscape is defined by the fitness values of all genotypes in the system. Some notion of curvature is necessary for a more complete analysis of the landscapes, including the effect of recombination. The shape approach uses triangulations (shapes) induced by fitness landscapes. The main topic for this work is the interplay between peak patterns and shapes. Because of constraints on the shapes for [Formula: see text] imposed by peaks, there are in total 25 possible combinations of peak patterns and shapes. Similar constraints exist for higher L. Specifically, we show that the constraints induced by the staircase triangulation can be formulated as a condition of universal positive epistasis, an order relation on the fitness effects of arbitrary sets of mutations that respects the inclusion relation between the corresponding genetic backgrounds. We apply the concept to a large protein fitness landscape for an immunoglobulin-binding protein expressed in Streptococcal bacteria.", 
    "abstract": "In this paper, a model for Cr (VI) removal and optimization was made using a novel aerogel material, chitosan-resole CS/R aerogel, where a freeze-drying and final thermal treatment was employed to fabricate the aerogel. This processing ensures a network structure and stability for the CS, despite the non-uniform ice growth promoted by this process. Morphological analysis indicated a successful aerogel elaboration process., FTIR spectroscopy corroborated the aerogel precursor's identity and ascertained chemical bonding after adsorption. Owing to the variability of formulations, the adsorption capacity was modeled and optimized using computational techniques. The response surface methodology (RSM), based on the Box-Behnken design using three levels, was used to calculate the best control parameters for the CS/R aerogel: the concentration at %vol (50-90%), the initial concentration of Cr (VI) (25-100 mg/L), and adsorption time (0.3-4 h). Analysis of variance (ANOVA) and 3D graphs reveal that the CS/R aerogel concentration and adsorption time are the main parameters that influence the initial concentration of CS/R aerogel metal-ion uptake. The developed model successfully describes the process with a correlation coefficient of R", 
    "abstract": "The objective of this study is to identify and analyze the most relevant scientific work being undertaken in HR analytics. Additionally, it is to understand the evolution of the conceptual, intellectual, and social structure of this topic in a way that allows the expansion of empirical and conceptual knowledge. Bibliometric analysis was performed using Bibliometrix and Biblioshiny software packages on academic articles indexed on the Scopus and Web of Science (WoS) databases. Search criteria were applied, initially resulting in a total of 331 articles in the period 2008-2022. Finally, after applying exclusion criteria, a total of 218 articles of interest were obtained. The results of this research present the relevant notable topics in HR analytics, providing a quantitative analysis that gives an overview of HR analytics featuring tables, graphs, and maps, as well as identifying the main performance indicators for the production of articles and their citations. The scientific literature on HR analytics is a novel, adaptive area that provides the option to transform traditional HR practices. Through the use of technology, HR analytics can improve HR strategies and organisational performance, as well as people's experiences.", 
    "abstract": "Transcriptomics-based analysis of key transcriptional molecules in the pathogenesis of trigeminal neuropathic pain was conducted to screen key molecules in the pathogenesis of trigeminal neuralgia.\nRat trigeminal nerve pathological pain model, namely chronic constriction injury of distal infraorbital nerve (IoN-CCI), was constructed and animal behaviors postsurgery were observed and analyzed. Trigeminal ganglia were collected for RNA-seq transcriptomics analysis. StringTie was used to annotate and quantify genome expression. DESeq2 was applied to compare between groups with P value less than 0.05 and fold change greater than 2 times and less than 0.5 times to screen differential genes, and display them with volcano graphs and cluster graphs. ClusterProfiler software was used to perform GO function enrichment analysis of differential genes.\nOn the fifth postoperative day (POD5), the rat's face-grooming behavior increased to a peak; on the seventh postoperative day (POD7), the von-frey value dropped to the lowest value, indicating that the mechanical pain threshold of rats was significantly decreased. RNA-seq analysis of IoN-CCI rat ganglia found that the significantly up-regulated signaling pathways included B cell receptor signaling pathway, cell adhesion, complement and coagulation cascade pathways; significantly down-regulated pathways were related to systemic lupus erythematosus. Multiple genes among Cacna1s, Cox8b, My1, Ckm, Mylpf, Myoz1, Tnnc2 were involved in mediating the occurrence of trigeminal neuralgia.\nB cell receptor signaling pathway, cell adhesion, complement and coagulation cascade pathways, neuroimmune pathways are closely related to the occurrence of trigeminal neuralgia. The interaction of multiple genes among Cacna1s, Cox8b, My11, Ckm, Mylpf, Myoz1, Tnnc2 leads to the occurrence of trigeminal neuralgia.", 
    "abstract": "Open-shell benzenoid polycyclic hydrocarbons (BPHs) are promising materials for future quantum applications. However, the search for and realization of open-shell BPHs with desired properties is a challenging task due to the gigantic chemical space of BPHs, requiring new strategies for both theoretical understanding and experimental advancement. In this work, by building a structure database of BPHs through graphical enumeration, performing data-driven analysis, and combining tight-binding and mean-field Hubbard calculations, we discovered that the number of the internal vertices of the BPH graphs is closely correlated to their open-shell characters. We further established a simple rule, the triangle counting rule, to predict the magnetic ground states of BPHs. These findings not only provide a database of open-shell BPHs, but also extend the well-known Lieb's theorem and Ovchinnikov's rule and provide a straightforward method for designing open-shell carbon nanostructures. These insights may aid in the exploration of emerging quantum phases and the development of magnetic carbon materials for technology applications.", 
    "abstract": "Acupuncture, a traditional Chinese medicine therapy, is an effective migraine treatment, especially in improving pain. In recent years, many acupuncture brain imaging studies have found significant changes in brain function following acupuncture treatment of migraine, providing a new perspective to elucidate the mechanism of action of acupuncture.\nTo analyse and summarize the effects of acupuncture on the modulation of specific patterns of brain region activity changes in migraine patients, thus providing a mechanism for treating migraine by acupuncture.\nChinese and English articles published up to May 2022 were searched in three English databases (PubMed, Embase and Cochrane) and four Chinese databases (China national knowledge infrastructure, CNKI; Chinese Biomedical Literature database, CBM; the Chongqing VIP database, VIP; and the Wanfang database, WF). A neuroimaging meta-analysis on ALFF, ReHo was performed on the included studies using Seed-based d Mapping with Permutation of Subject Images (SDM-PSI) software. Subgroup analyses were used to compare differences in brain regions between acupuncture and other groups. Meta-regression was used to explore the effect of demographic information and migraine alterations on brain imaging outcomes. Linear models were drawn using MATLAB 2018a, and visual graphs for quality evaluation were produced using R and RStudio software.\nA total of 7 studies comprising 236 patients in the treatment group and 173 in the control group were included in the meta-analysis. The results suggest that acupuncture treatment helps to improve pain symptoms in patients with migraine. The left angular gyrus is hyperactivation, and the left superior frontal gyrus and the right superior frontal gyrus are hypoactivated. The migraine group showed hyperactivation in the corpus callosum compared to healthy controls.\nAcupuncture can significantly regulate changes in brain regions in migraine patients. However, due to the experimental design of neuroimaging standards are not uniform, the results also have some bias. Therefore, to better understand the potential mechanism of acupuncture on migraine, a large sample, multicenter controlled trial is needed for further study. In addition, the application of machine learning methods in neuroimaging studies could help predict the efficacy of acupuncture and screen migraine patients suitable for acupuncture treatment.", 
    "abstract": "To aid in the prevention of reaction explosions, chemical engineers and scientists must analyze the Arrhenius kinetics and activation energies of chemical reactions involving binary chemical mixtures. Nanofluids with an Arrhenius kinetic are crucial for a broad variety of uses in the industrial sector, involving the manufacture of chemicals, thermoelectric sciences, biomedical devices, polymer extrusion, and the enhancement of thermal systems via technology. The goal of this study is to determine how the presence of thermal radiation influences heat and mass transfer during free convective unsteady stagnation point flow across extending/shrinking vertical Riga plate in the presence of a binary chemical reaction where the activation energy of the reaction is known in advance. For the purpose of obtaining numerical solutions to the mathematical model of the present issue the Runge-Kutta (RK-IV) with shooting technique in Mathematica was used. Heat and mass transfer processes, as well as interrupted flow phenomena, are characterized and explained by diagrams in the suggested suction variables along boundary surface in the stagnation point flow approaching a permeable stretching/shrinking Riga Plate. Graphs illustrated the effects of many other factors on temperature, velocity, concentration, Sherwood and Nusselt number as well as skin friction in detail. Velocity profile increased with ", 
    "abstract": "We report on the analysis of pure gadolinium oxide (Gd", 
    "abstract": "Wang tiles enable efficient pattern compression while avoiding the periodicity in tile distribution via programmable matching rules. However, most research in Wang tilings has considered tiling the infinite plane. Motivated by emerging applications in materials engineering, we consider the bounded version of the tiling problem and offer four integer programming formulations to construct valid or nearly-valid Wang tilings: a\u00a0decision, maximum-rectangular tiling, maximum cover, and maximum adjacency constraint satisfaction formulations. To facilitate a\u00a0finer control over the resulting tilings, we extend these programs with tile-based, color-based, packing, and variable-sized periodic constraints. Furthermore, we introduce an efficient heuristic algorithm for the maximum-cover variant based on the shortest path search in directed acyclic graphs and derive simple modifications to provide a\u00a01/2 approximation guarantee for arbitrary tile sets, and a\u00a02/3 guarantee for tile sets with cyclic transducers. Finally, we benchmark the performance of the integer programming formulations and of the heuristic algorithms showing that the heuristics provide very competitive outputs in a\u00a0fraction of time. As a by-product, we reveal errors in two well-known aperiodic tile sets: the Knuth tile set contains a\u00a0tile unusable in two-way infinite tilings, and the Lagae corner tile set is not aperiodic.", 
    "abstract": "This study aimed to develop a validated and reliable food frequency questionnaire (FFQ) that assess dietary intake related with dental health in children.\nChildren, two-to-nine-years old, who consulted to a paediatric dental clinic for any reason, were recruited to complete the FFQ and 24-h recall, inquired oral hygiene habits, performed oral examinations, recorded dmft(s)/DMFT(S) index, and taken anthropometric measurements. The statistical methods used for validation were Wilcoxon signed rank test, Spearman ranked correlations, weighted kappa statistic and Bland-Altman graphs were drawn. Besides, intraclass and spearman correlation coefficients calculated for the reliability.\nA total of 120 children participated in to the first stage of the study while 70 participants completed the 4-month period. The Spearman correlation coefficient and weighted kappa values confirmed that the FFQ had moderate validation against the food records for lactose, calcium and phosphorus. Dietary fat, fibre, lactose, calcium, potassium, fluoride, magnesium, phosphorus and zinc intakes were negatively and statistically significantly correlated with DMFT and DMFS according to both FFQ and 24-h (p\u00a0<\u00a00.05, for each). Furthermore, a positive correlation between DMFT/S and dietary carbohydrate, starch, polysaccharide and sucrose intakes was obtained.\nThese results provide the preliminary evidence for the moderated reliability and validity of the FFQ; the higher DMFT and DMFS scores might be linked to lower dietary intakes of fat, fibre, lactose, calcium, potassium, fluorine, magnesium, phosphorus and zinc; and probably higher dietary intakes of carbohydrate, starch, polysaccharide and sucrose in children.", 
    "abstract": "With the development of graph neural networks, how to handle large-scale graph data has become an increasingly important topic. Currently, most graph neural network models which can be extended to large-scale graphs are based on random sampling methods. However, the sampling process in these models is detached from the forward propagation of neural networks. Moreover, quite a few works design sampling based on statistical estimation methods for graph convolutional networks and the weights of message passing in GCNs nodes are fixed, making these sampling methods not scalable to message passing networks with variable weights, such as graph attention networks. Noting the end-to-end learning capability of neural networks, we propose a learnable sampling method. It solves the problem that random sampling operations cannot calculate gradients and samples nodes with an unfixed probability. In this way, the sampling process is dynamically combined with the forward propagation process of the features, allowing for better training of the networks. And it can be generalized to all message passing models. In addition, we apply the learnable sampling method to GNNs and propose two models. Our method can be flexibly combined with different graph neural network models and achieves excellent accuracy on benchmark datasets with large graphs. Meanwhile, loss function converges to smaller values at a faster rate during training than past methods.", 
    "abstract": "On 21st March 2020, the first COVID-19 case was detected in Uganda and a COVID-19 pandemic declared. On the same date, a nationwide lockdown was instituted in response to the pandemic. Subsequently, more cases were detected amongst the returning international travelers as the disease continued to spread across the country. On May 14th, 2020, a cholera epidemic was confirmed in Moroto district at a time when the district had registered several COVID-19 cases and was in lockdown. This study aimed to describe the cholera epidemic and response activities during the COVID-19 pandemic as well as the hurdles and opportunities for cholera control encountered during the response.\nIn a cross-sectional study design, we reviewed Moroto district's weekly epidemiological records on cholera and COVID-19 from April to July 2020. We obtained additional information through a review of the outbreak investigation and control reports. Data were analyzed and presented in frequencies, proportions, attack rates, case fatality rates, graphs, and maps.\nAs of June 28th, 2020, 458 cases presenting with severe diarrhea and/or vomiting were line listed in Moroto district. The most affected age group was 15-30 years, 30.1% (138/458). The females, 59.0% [270/458], were the majority. The Case Fatality Rate (CFR) was 0.4% (2/458). Whereas home use of contaminated water following the vandalization of the only clean water source in Natapar Kocuc village, Moroto district, could have elicited the epidemic, implementing COVID-19 preventive and control measures presented some hurdles and opportunities for cholera control. The significant hurdles were observing the COVID-19 control measures such as social distancing, wearing of masks, and limited time in the community due to the need to observe curfew rules starting at 6.00 pm. The opportunities from COVID-19 measures complementary to cholera control measures included frequent hand washing, travel restrictions within the district & surrounding areas, and closure of markets.\nCOVID-19 preventive and control measures such as social distancing, wearing of masks, and curfew rules may be a hurdle to cholera control whereas frequent hand washing, travel restrictions within the district & surrounding areas, and closure of markets may present opportunities for cholera control. Other settings experiencing concurrent cholera and COVID-19 outbreaks can borrow lessons from this study.", 
    "abstract": "Human walking reflects the state of human health. Numerous medical studies have been conducted to analyze walking patterns and to diagnose disease progression. However, this process requires expensive equipment and considerable time and manpower. Smartwatches are equipped with gyro sensors to detect human movements and graph-walking patterns. To measure the abnormality in walking using this graph, we developed a smartwatch gait coordination index (SGCI) and examined its usefulness. The phase coordination index was applied to analyze arm movements. Based on previous studies, the phase coordination index formula was applied to graphs obtained from arm movements, showing that arm and leg movements during walking are correlated with each other. To prove this, a smartwatch was worn on the arms and legs of 8 healthy adults and the difference in arm movements was measured. The SGCI values with abnormal walking patterns were compared with the SGCI values obtained during normal walking. In the first experiment, the measured leg SGCI in normal walking averaged 9.002\u2009\u00b1\u20093.872 and the arm SGCI averaged 9.847\u2009\u00b1\u20096.115. The movements of both arms and legs showed stable sinusoidal waves. In fact, as a result of performing a paired t test of both exercise phases measured by the strike point using the maximum and minimum values, it was confirmed that the 2 exercises were not statistically different, as it yielded a P value of 0.469 (significance level \u03b1 = 0.05). The arm SGCI measured after applying the 3\u2009kg weight impairment on 1 leg was 22.167\u2009\u00b1\u20094.705. It was confirmed that the leg SGCI and 3\u2009kg weight arm SGCI were statistically significant, as it yielded a P value of 0.001 (significance level \u03b1 = 0.05). The SCGI can be automatically and continuously measured with the gyro sensor of the smartwatch and can be used as an indirect indicator of human walking conditions.", 
    "abstract": "After the United States Congress passed the Water Pollution Control Act of 1948, biologists played an increasingly significant role in scientific studies of water pollution. Biologists interacted with other experts, notably engineers, who managed the public agencies devoted to water pollution control. Although biologists were at first marginalized within these agencies, the situation began to change by the early 1960s. Biological data became an integral part of water pollution control. While changing societal values, stimulated by an emerging ecological awareness, may explain broader shifts in expert opinion during the 1960s, this article explores how graphs changed experts' perceptions of water pollution. Experts communicated with each other via reports, journal articles, and conference speeches. Those sources reveal that biologists began experimenting with new graphical methods to simplify the complex ecological data they collected from the field. Biologists, I argue, followed the engineers' lead by developing graphical methods that were concise and quantitative. Their need to collaborate with engineers forced them to communicate, negotiate, and overcome conflicts and misunderstandings. By meeting engineers' expectations and promoting the value of their data through images as much as words, biologists asserted their authority within water pollution control by the early 1960s.", 
    "abstract": "Counting Polynomial is the mathematical function that was initially introduced for application in chemistry in 1936 by G. Polya. Partitioning of graphs can be seen in the coefficients of these mathematical functions, which also reveal the frequency with which these partitions happen. We developed a novel and efficient method for constructing the necessary counting polynomials for a zigzag-edge coronoid formed by the fusion of a Starphene graph and a Kekulenes graph. The study's methods expand our knowledge, and its findings potentially provide insight on the topology of these chemical structures.", 
    "abstract": "Directed acyclic graphs (DAGs) are used in epidemiological research to communicate causal assumptions and guide the selection of covariate adjustment sets when estimating causal effects. For any given DAG, a set of graphical rules can be applied to identify minimally sufficient adjustment sets that can be used to adjust for bias due to confounding when estimating the causal effect of an exposure on an outcome. The daggle app is a web-based application that aims to assist in the learning and teaching of adjustment set identification using DAGs.\nThe application offers two modes: tutorial and random. The tutorial mode presents a guided introduction to how common causal structures can be presented using DAGs and how graphical rules can be used to identify minimally sufficient adjustment sets for causal estimation. The random mode tests this understanding by presenting the user with a randomly generated DAG-a daggle. To solve the daggle, users must correctly identify a valid minimally sufficient adjustment set.\nThe daggle app is implemented as an R shiny application using the golem framework. The application builds upon existing R libraries including pcalg to generate reproducible random DAGs, dagitty to identify all valid minimal adjustment sets and ggdag to visualize DAGs.\nThe daggle app can be accessed online at [http://cbdrh.shinyapps.io/daggle]. The source code is available on GitHub [https://github.com/CBDRH/daggle] and is released under a Creative Commons CC BY-NC-SA 4.0 licence.", 
    "abstract": "Great environmental changes may affect the survival capability of a variety of organisms. Testudinidae is the most diverse family of terrestrial chelonians within the whole order (Testudines). Interestingly, however, the number of extinct species overcome the extant ones. In order to understand better how the diversification process of this family occurred, this work used the PyRate software, which estimates both the preservation and diversification processes in a continuous time interval. For such, the software used a list of fossil occurrences obtained from the Paleobiology Database whereas the extant species list was obtained from Catalogue of Life. This way, the software was able to infer the probability of the ancestral clade having resulted in these species during its evolutionary history. The analyses generated graphs containing the diversification, extinction and speciation curves and their respective associated 95% credibility intervals. A great rise in the extinction rate was observed starting 6 million years ago. This rise is believed to be related to the drop of atmospheric CO", 
    "abstract": "On evolutionary game theory (EGT), two intervention policies: vaccination and self-awareness, are considered to account for how human attitude impacts disease spreading. Although these interventions can impose, their implementation may depend on the various immunity systems such as shield immunity, innate immunity, waning immunity, natural immunity, and artificial immunity. This framework provides an epidemic SEIRVA (susceptible-exposed-infected-removed-vaccinated-aware) model and two EGT dynamics to analyze the interplay between the immunity system and social learning interventions. The prospect of exploring the individual's strategy and social dilemma for removing a disease could assist design an effective vaccine program and self-awareness policy. Also, we evaluated the indicator of social efficiency deficit (SED) for a social dual-dilemma to measure the presence of a dilemma situation. Extensive theoretical analysis displays that stability includes the reproduction number, conditions for positivity and uniqueness, and the strength number analyzed in the equilibria, including fundamental properties validated by numerical simulation of the discretization method that appraises a variety of graphs at adjusting parameters. We present extensive numerical studies investigating the affect of controlling parameters, individual vulnerability, optimal policies, and individual costs. It turns out that, even with the affordable vaccine, individuals may have very different behaviors; self-awareness strategy plays a vital role in controlling diseases.", 
    "abstract": "Lecanemab is a humanized immunoglobulin G1 (IgG1) monoclonal antibody that preferentially targets soluble aggregated A\u03b2 species (protofibrils) with activity at amyloid plaques. Amyloid-related imaging abnormalities (ARIA) profiles appear to differ for various anti-amyloid antibodies. Here, we present ARIA data from a large phase 2 lecanemab trial (Study 201) in early Alzheimer's disease.\nStudy 201 trial was double-blind, placebo-controlled (core) with an open-label extension (OLE). Observed ARIA events were summarized and modeled via Kaplan-Meier graphs. An exposure response model was developed.\nIn the phase 2 core and OLE, there was a low incidence of ARIA-E (<10%), with <3% symptomatic cases. ARIA-E was generally asymptomatic, mild-to-moderate in severity, and occurred early (<3 months). ARIA-E was correlated with maximum lecanemab serum concentration and incidence was higher in apolipoprotein E4 (ApoE4) homozygous carriers. ARIA-H and ARIA-E occurred with similar frequency in core and OLE.\nLecanemab can be administered without titration with modest incidence of ARIA.", 
    "abstract": "In present times modern electronic devices often come across thermal difficulties as an outcome of excessive heat production or reduction in surface area for heat exclusion. The current study is aimed to inspect the role of iron (III) oxide in heat transfer enhancement over the rotating disk in an axisymmetric flow. Water is utilized as base fluid conveying nano-particle over the revolving axisymmetric flow mechanism. Additionally, the computational fluid dynamics (CFD) approach is taken into consideration to design and compute the present problem. For our convenience, two-dimensional axisymmetric flow configurations are considered to illustrate the different flow profiles. For radial, axial, and tangential velocity profiles, the magnitude of the velocity, streamlines, and surface graphs are evaluated with the similarity solution in the computational fluid dynamics module. The solution of dimensionless equations and the outcomes of direct simulations in the CFD module show a comparable solution of the velocity profile. It is observed that with an increment in nanoparticle volumetric concentration the radial velocity decline where a tangential motion of flow enhances. Streamlines stretch around the circular surface with the passage of time. The high magnetization force [Formula: see text] resist the free motion of the nanofluid around the rotating disk. Such research has never been done, to the best of the researchers' knowledge. The outcomes of this numerical analysis could be used for the design, control, and optimization of numerous thermal engineering systems, as described above, due to the intricate physics of nanofluid under the influences of magnetic field and the inclusion of complex geometry. Ferrofluids are metallic nanoparticle colloidal solutions. These kinds of fluids do not exist in nature. Depending on their purpose, ferrofluids are produced using a variety of processes. One of the most essential characteristics of ferrofluids is that they operate in a zero-gravity environment. Ferrofluids have a wide range of uses in engineering and medicine. Ferrofluids have several uses, including heat control loudspeakers and frictionless sealing. In the sphere of medicine, however, ferrofluid is employed in the treatment of cancer via magneto hyperthermia.", 
    "abstract": "Any form of coherent discourse depends on saying different things about the same entities at different times. Such recurrent references to the same entity need to predictably happen within certain temporal windows. We hypothesized that a failure of control over reference in speakers with schizophrenia (Sz) would become manifest through dynamic temporal measures.\nConversational speech with a mean of 909.2 words (SD: 178.4) from 20 Chilean Spanish speakers with chronic Sz, 20 speakers at clinical high risk (CHR), and 20 controls were collected. Using directed speech graphs with referential noun phrases (NPs) as nodes, we studied deviances in the topology and temporal distribution of such NPs and of the entities they denote over narrative time.\nThe Sz group had a larger density of NPs (number of NPs divided by total words) relative to both controls and CHR. This related to topological measures of distance between recurrent entities, which revealed that the Sz group produced more recurrences, as well as greater topological distances between them, relative to controls. A logistic regression using five topological measures showed that Sz and controls can be distinguished with 84.2% accuracy.\nThis pattern indicates a widening of the temporal window in which entities are maintained in discourse and co-referenced in it. It substantiates and extends earlier evidence for deficits in the cognitive control over linguistic reference in psychotic discourse and informs both neurocognitive models of language in Sz and machine learning-based linguistic classifiers of psychotic speech.", 
    "abstract": "Self-medication with antibiotics has become an important factor driving antibiotic resistance and it masks the signs and symptoms of the underlying disease and hence complicates the problem, increasing drug resistance and delaying diagnosis. This study aimed to assess the extent of self-medication practice with antibiotics and its associated factors among adult patients attending outpatient departments (OPD) at selected public Hospitals, in Addis Ababa, Ethiopia.\nFacility-based cross-sectional study was employed. A systematic random sampling technique was used to include the study participants. Self-administered with structured questionnaires were applied among patients who visited outpatient departments at selected public Hospitals, in Addis Ababa. Data were entered into Epi-data version 4.6 and analyzed using SPSS version 26. Descriptive statistics such as frequencies and percentages were used for the present categorical data. The data are presented in pie charts, tables, and bar graphs. Furthermore, bivariable and multivariable binary logistic regression analyses were used to identify significant associations. Statistical significance was declared at p value\u2009<\u20090.05.\nOut of 421 respondents interviewed, 403 (95.7%) participants completed questionnaires. Among the respondents, 71% had generally practiced self-medication. Among these, 48.3% had self-medication with antibiotics, while 51.7% had used other drugs. The most commonly cited indication for self-medication practice was abdominal pain 44.9%, followed by Sore throat 21% commonly used antibiotics are amoxicillin (57%), ciprofloxacin (13%), amoxicillin/clavulanic (10%), erythromycin (8%), cotrimoxazole (7%), and doxycycline (5%).\nSelf-medication with antibiotics was common among the study participants. The prevalence of general self-medication was 71%, whereas that of self-medication with antibiotics was 48.3%. In general, the potentially dangerous effects of SMP seem to be underestimated by patients with OPD patients.", 
    "abstract": "When tackling complex public health challenges such as childhood obesity, interventions focused on immediate causes, such as poor diet and physical inactivity, have had limited success, largely because upstream root causes remain unresolved. A priority is to develop new modelling frameworks to infer the causal structure of complex chronic disease networks, allowing disease \"on-ramps\" to be identified and targeted.\nThe system surrounding childhood obesity was modelled as a Bayesian network, using data from The Longitudinal Study of Australian Children. The existence and directions of the dependencies between factors represent possible causal pathways for childhood obesity and were encoded in directed acyclic graphs (DAGs). The posterior distribution of the DAGs was estimated using the Partition Markov chain Monte Carlo.\nWe have implemented structure learning for each dataset\u00a0at a single time point. For each wave and cohort, socio-economic status was central to the DAGs, implying that socio-economic status drives the system regarding childhood obesity. Furthermore, the causal pathway socio-economic status and/or parental high school levels \u2192 parental body mass index (BMI) \u2192 child's BMI existed in over 99.99% of posterior DAG samples across all waves and cohorts. For children under the age of 8 years, the most influential proximate causal factors explaining child BMI were birth weight and parents' BMI. After age 8 years, free time activity became an important driver of obesity, while the upstream factors influencing free time activity for boys compared with girls were different.\nChildhood obesity is largely a function of socio-economic status, which is manifest through numerous downstream factors. Parental high school levels entangle with socio-economic status, and hence, are on-ramp to childhood obesity. The strong and independent causal relationship between birth weight and childhood BMI suggests a biological link. Our study implies that interventions that improve the socio-economic status, including through increasing high school completion rates, may be effective in reducing childhood obesity prevalence.", 
    "abstract": "Graph layout algorithms used in network visualization represent the first and the most widely used tool to unveil the inner structure and the behavior of complex networks. Current network visualization software relies on the force-directed layout (FDL) algorithm, whose high computational complexity makes the visualization of large real networks computationally prohibitive and traps large graphs into high energy configurations, resulting in hard-to-interpret \"hairball\" layouts. Here we use Graph Neural Networks (GNN) to accelerate FDL, showing that deep learning can address both limitations of FDL: it offers a 10 to 100 fold improvement in speed while also yielding layouts which are more informative. We analytically derive the speedup offered by GNN, relating it to the number of outliers in the eigenspectrum of the adjacency matrix, predicting that GNNs are particularly effective for networks with communities and local regularities. Finally, we use GNN to generate a three-dimensional layout of the Internet, and introduce additional measures to assess the layout quality and its interpretability, exploring the algorithm's ability to separate communities and the link-length distribution. The novel use of deep neural networks can help accelerate other network-based optimization problems as well, with applications from reaction-diffusion systems to epidemics.", 
    "abstract": "Schizophrenia has been associated with dysfunction in information integration/segregation dynamics. One of the neural networks whose role has been most investigated in schizophrenia is the default mode network (DMN). In this study, we have explored the possible alteration of integration and segregation dynamics in individuals diagnosed with schizophrenia with respect to healthy controls, based on the study of the topological properties of the graphs derived from the functional connectivity between the nodes of the DMN in the resting state. Our results indicate that the patients show a diminution of the modularity of the DMN and a higher global efficiency, in sparse graphs. Our data emphasise the interest in studying temporal changes in network measures and are compatible with the hypothesis of randomization of functional networks in schizophrenia.", 
    "abstract": "In this analysis, the generalized Fourier and Fick's law for Second-grade fluid flow at a slendering vertical Riga sheet is examined along with thermophoresis and Brownian motion effects. Boundary layer approximations in terms of PDE's (Partial Differential Equations) are used to build the mathematical model. An appropriate transformation has been developed by using the Lie symmetry method. PDE's (Partial Differential Equations) are transformed into ODE's (Ordinary Differential Equations) by implementing the suitable transformation. A numerical method called bvp4c is used to explain the dimensionless system (ODE's). Graphs and tables are used to interpret the impact of the significant physical parameters. The curves of temperature function declined due to enchanting the values of the thermophoresis Parameter. The temperature is produced at a low level due to enchanting the values of thermophoresis because this force transports burn at a low 10\u00a0\u03bcm diameter so the temperature becomes lessor. Increments of thermophoresis parameter which enhanced the values of concentration Function. As the concentration boundary layer increased which declined the mass transfer due increment in thermophoresis. The curves of temperature function are increasing due to enhancing the values of the Brownian parameter because addition in the Brownian motion, improved the movement of particles ultimately increasing the kinematic energy of fluid which improved the heat transfer phenomena. Increments of Brownian parameter which declined the values of concentration function. Physically, the kinematic energy improved which declined the mass transfer rate near the surface.", 
    "abstract": "The study of ", 
    "abstract": "Myasthenia gravis (MG) is an autoimmune disease with acquired neuromuscular junction transmission disorders. In the last two decades, various pathogenesis, application of immunosuppressive agents, and targeted immunotherapy have been significant events. However, extracting the most critical information from complex events is very difficult to guide clinical work. Therefore, we used bibliometrics to summarize and look forward.\nScience Citation Index Expanded (SCI-E) from the Web of Science Core Collection (WoSCC) database was identified as a source of material for obtaining MG-related articles. Scimago Graphica, CiteSpace, VOSviewer, and bibliometrix were utilized for bibliometric analysis. Knowledge network graphs were constructed and visualized; countries, institutions, authors, journals, references, and keywords were evaluated. In addition, GraphPad Prism and Microsoft Excel 365 were applied for statistical analysis.\nAs of October 25, 2022, 9,970 original MG-related articles were used for the bibliometric analysis; the cumulative number of citations to these articles was 236,987, with an H-index of 201. The United States ranked first in terms of the number of publications (2,877) and H-index (134). Oxford has the highest H-index (67), and Udice French Research University has the highest number of publications (319). The author with the highest average number of citations (66.19), publications (151), and H-index (53) was Vincent A. 28 articles have remained in an explosive period of citations. The final screening yielded predictive keywords related to clinical trials and COVID-19.\nWe conducted a bibliometric analysis of 9,970 original MG-related articles published between 1966 and 2022. Ultimately, we found that future MG research hotspots include two major parts: (1) studies directly related to MG disease itself: clinical trials of various targeted biological agents; the relationship between biomarkers and therapeutic decisions, pathogenesis and outcome events, ultimately serving individualized management or precision therapy; (2) studies related to MG and COVID-19: different variants of COVID-19 (e.g., Omicron) on MG adverse outcome events; assessment of the safety of different COVID-19 vaccines for different subtypes of MG.", 
    "abstract": "This paper synthesizes a new sliding mode controller (SMC) approach to enhance tracking and regulation tasks by following dual-mode concepts. The new control law consists of two distinct types of operation, using the combination of higher gain to large error signals (transient) and lower gain to small error signals (the region around the set point). The design is presented from a dual-mode (PD-PID) sliding surface operating in concert, fulfilling desired control objectives to ensure stability and performance. Therefore, a new controller was established, and we called it a dual-mode based SMC. The proposed controller is tested by computer simulations applied to two nonlinear processes, a continuous stirred-tank reactor (CSTR) and a mixing tank with a variable dead time. The results are compared with two different alternatives of SMC. In addition, the merits and drawbacks of the control schemes are analyzed using radial graphs, comparing the control methods with various performance measures for set points and disturbances changes. The ITSE (integral of time multiplied by the squared error), TVu (total variation of control effort) indices, Mp (maximum overshoot), and ts (settling time) were the indices used for performance analysis and comparisons.", 
    "abstract": "At the beginning of the COVID-19 pandemic, it was foreseen that the number of face-to-face psychiatry consultations would suffer a reduction. In order to compensate, the Australian Government introduced new Medicare-subsidized telephone and video-linked consultations. This study investigates how these developments affected the pre-existing inequity of psychiatry service delivery in Australia.\nThe study analyses five and a half years of national Medicare data listing all subsidized psychiatry consultation consumption aggregated to areas defined as Statistical Area level 3 (SA3s; which have population sizes of 30 k-300 k). Face-to-face, video-linked and telephone consultations are considered separately. The analysis consists of presenting rates of consumption, concentration graphs, and concentration indices to quantify inequity, using Socio Economic Indexes for Areas (SEIFA) scores to rank the SA3 areas according to socio-economic disadvantage.\nThere is a 22% drop in the rate of face-to-face psychiatry consultation consumption across Australia in the final study period compared with the last study period predating the COVID-19 pandemic. However, the loss is made up by the introduction of the new subsidized telephone and video-linked consultations. Referring to the same time periods, there is a reduction in the inequity of the distribution of face-to-face consultations, where the concentration index reduces from 0.166 to 0.129. The new subsidized video-linked consultations are distributed with severe inequity in the great majority of subpopulations studied. Australia-wide, video-linked consultations are also distributed with gross inequity, with a concentration index of 0.356 in the final study period. The effect of this upon overall inequity was to cancel out the reduction of inequity resulting from the reduction of face-to face appointments.\nAustralian subsidized video-linked psychiatry consultations have been distributed with gross inequity and have been a significant exacerbator of the overall inequity of psychiatric service provision. Future policy decisions wishing to reduce this inequity should take care to reduce the risk posed by expanding telepsychiatry.", 
    "abstract": "As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, SHAPEGGEN, which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows SHAPEGGEN to mimic the data in various real-world areas. We include SHAPEGGEN and several real-world graph datasets in a graph explainability library, GRAPHXAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, GRAPHXAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.", 
    "abstract": "Pharmacokinetic natural product-drug interactions (NPDIs) occur when botanical or other natural products are co-consumed with pharmaceutical drugs. With the growing use of natural products, the risk for potential NPDIs and consequent adverse events has increased. Understanding mechanisms of NPDIs is key to preventing or minimizing adverse events. Although biomedical knowledge graphs (KGs) have been widely used for drug-drug interaction applications, computational investigation of NPDIs is novel. We constructed NP-KG as a first step toward computational discovery of plausible mechanistic explanations for pharmacokinetic NPDIs that can be used to guide scientific research.\nWe developed a large-scale, heterogeneous KG with biomedical ontologies, linked data, and full texts of the scientific literature. To construct the KG, biomedical ontologies and drug databases were integrated with the Phenotype Knowledge Translator framework. The semantic relation extraction systems, SemRep and Integrated Network and Dynamic Reasoning Assembler, were used to extract semantic predications (subject-relation-object triples) from full texts of the scientific literature related to the exemplar natural products green tea and kratom. A literature-based graph constructed from the predications was integrated into the ontology-grounded KG to create NP-KG. NP-KG was evaluated with case studies of pharmacokinetic green tea- and kratom-drug interactions through KG path searches and meta-path discovery to determine congruent and contradictory information in NP-KG compared to ground truth data. We also conducted an error analysis to identify knowledge gaps and incorrect predications in the KG.\nThe fully integrated NP-KG consisted of 745,512 nodes and 7,249,576 edges. Evaluation of NP-KG resulted in congruent (38.98% for green tea, 50% for kratom), contradictory (15.25% for green tea, 21.43% for kratom), and both congruent and contradictory (15.25% for green tea, 21.43% for kratom) information compared to ground truth data. Potential pharmacokinetic mechanisms for several purported NPDIs, including the green tea-raloxifene, green tea-nadolol, kratom-midazolam, kratom-quetiapine, and kratom-venlafaxine interactions were congruent with the published literature.\nNP-KG is the first KG to integrate biomedical ontologies with full texts of the scientific literature focused on natural products. We demonstrate the application of NP-KG to identify known pharmacokinetic interactions between natural products and pharmaceutical drugs mediated by drug metabolizing enzymes and transporters. Future work will incorporate context, contradiction analysis, and embedding-based methods to enrich NP-KG. NP-KG is publicly available at https://doi.org/10.5281/zenodo.6814507. The code for relation extraction, KG construction, and hypothesis generation is available at https://github.com/sanyabt/np-kg.", 
    "abstract": "We consider random walks evolving on two models of connected and undirected graphs and study the exact large deviations of a local dynamical observable. We prove, in the thermodynamic limit, that this observable undergoes a first-order dynamical phase transition (DPT). This is interpreted as a \"coexistence\" of paths in the fluctuations that visit the highly connected bulk of the graph (delocalization) and paths that visit the boundary (localization). The methods we used also allow us to characterize analytically the scaling function that describes the finite-size crossover between the localized and delocalized regimes. Remarkably, we also show that the DPT is robust with respect to a change in the graph topology, which only plays a role in the crossover regime. All results support the view that a first-order DPT may also appear in random walks on infinite-size random graphs.", 
    "abstract": "We present analytical results for the distribution of the number of cycles in directed and undirected random 2-regular graphs (2-RRGs) consisting of N nodes. In directed 2-RRGs each node has one inbound link and one outbound link, while in undirected 2-RRGs each node has two undirected links. Since all the nodes are of degree k=2, the resulting networks consist of cycles. These cycles exhibit a broad spectrum of lengths, where the average length of the shortest cycle in a random network instance scales with lnN, while the length of the longest cycle scales with N. The number of cycles varies between different network instances in the ensemble, where the mean number of cycles \u3008S\u3009 scales with lnN. Here we present exact analytical results for the distribution P_{N}(S=s) of the number of cycles s in ensembles of directed and undirected 2-RRGs, expressed in terms of the Stirling numbers of the first kind. In both cases the distributions converge to a Poisson distribution in the large N limit. The moments and cumulants of P_{N}(S=s) are also calculated. The statistical properties of directed 2-RRGs are equivalent to the combinatorics of cycles in random permutations of N objects. In this context our results recover and extend known results. In contrast, the statistical properties of cycles in undirected 2-RRGs have not been studied before.", 
    "abstract": "According to the evolutionary death-birth protocol, a player is chosen randomly to die and neighbors compete for the available position proportional to their fitness. Hence, the status of the focal player is completely ignored and has no impact on the strategy update. In this paper, we revisit and generalize this rule by introducing a weight factor to compare the payoff values of the focal and invading neighbors. By means of evolutionary graph theory, we analyze the model on joint transitive graphs to explore the possible consequences of the presence of a weight factor. We find that focal weight always hinders cooperation under weak selection strength. Surprisingly, the results show a nontrivial tipping point of the weight factor where the threshold of cooperation success shifts from positive to negative infinity. Once focal weight exceeds this tipping point, cooperation becomes unreachable. Our theoretical predictions are confirmed by Monte Carlo simulations on a square lattice of different sizes. We also verify the robustness of the conclusions to arbitrary two-player prisoner's dilemmas, to dispersal graphs with arbitrary edge weights, and to interaction and dispersal graphs overlapping arbitrarily.", 
    "abstract": "Anthrax is a zoonotic disease caused by the Bacillus anthracis bacteria, which is one of the top five important livestock diseases and the second top priority zoonotic disease, next to rabies, in Ethiopia, which remains a major problem for animals and public health in Ethiopia. This study was conducted to verify the existence of the outbreak, determine risk factors, and implement measures to control the anthrax outbreak in Farta woreda, South Gondar zone, Northwest Ethiopia in 2019.\nA community-based case-control study was conducted from March 25 to April 1, 2019. A structured questionnaire was used to collect data and for review of documents and discussion with livestock and health office staff. The collected data were analyzed by SPSS and presented in tables and graphs.\nA total of 20 human anthrax cases with an attack rate of 2.5 per 1000 population were reported from the affected kebele. The age of the cases ranged from 1 month to 65 years (median age\u2009=\u200937.5 years). Of the total cases, 66.7% were male and 77.8% were 15 and older. The probability of developing anthrax among people who had unvaccinated animals was higher than in those who didn't have unvaccinated animals with an AOR\u2009=\u20098.113 (95% CI 1.685-39.056) and the probability of getting anthrax in relation to people's awareness of anthrax was AOR\u2009=\u20090.114 (95% CI 0.025-0.524).\nAn anthrax outbreak occurred in Wawa Mengera Kebele of Farta woreda. The presence of unvaccinated animals in a household was found to be a risk factor for anthrax cases. Timely animal vaccination and strengthening health education on the vaccination of animals, mode of transmission, and disposal of dead animals are essential for preventing anthrax cases.", 
    "abstract": "Several scientists are interested in recent developments in nanotechnology and nanoscience. Grease is an essential component of many machines and engines because it helps keep them cool by reducing friction between their various elements. In sealed life applications including centralized lubrication systems, electrical motors, bearings, logging and mining machinery, truck wheel hubs, construction, landscaping, and gearboxes, greases are also utilized. Nanoparticles are added to convectional grease to improve its cooling and lubricating properties. More specifically, the current study goal is to investigate open channel flow while taking grease into account as a Maxwell fluid with MoS", 
    "abstract": "Clear communication of medical risk helps to ensure proper patient understanding of healthcare options and supports informed decision-making. Communication involving visual and written risk typically conveys risk more effectively than conversations alone between a patient and a clinician. However, perception of risk is context-dependent, and the efficacy of and preferences for commonly-used risk communication formats are not well-understood during pregnancy, which is a time of complex decision-making. We sought to address this knowledge gap.\nThis study aimed to assess pregnant and recently pregnant people's understanding and preferences for different risk communication formats.\nWe conducted an open online REDCap survey of pregnant and recently pregnant people over a 1-month period in 2022. Study participants were aged 16 to 49 years, pregnant or recently pregnant, and able to provide informed consent in English. Data collected included demographics, measurements of accuracy of understanding including both gist accuracy (general understanding) and verbatim accuracy (numeric quantification), and preferences for risk communication formats including icon arrays, pie charts, bar graphs, and text. Descriptive analyses of the proportion of correctly answered questions were calculated.\nA total of 247 participants completed \u22651 item on accuracy and risk communication preferences, and 230 provided complete responses. Gist (general) understanding was accurate between 74% and 89% of the time for most graphical formats. Verbatim understanding (exact numeric quantification) was approximately 90% accurate for most formats. Respondents preferred that figures be used over circles to display risk in icon arrays, both for themselves and for infants, although figures generated more worry. However, participants substantially preferred pie charts over bar graphs (59%-70% vs 19%-25%). Respondents preferred risk to be expressed with a lower denominator of 200 rather than a higher denominator of 1000 (79% vs 13%, although the lower denominator generated more worry), and in terms of chance of survival rather than chance of death (50% vs 33%).\nIn a survey of pregnant and recently pregnant people, most respondents preferred pie charts over other graph formats, and lower rather than higher denominators in text. Presentations of survival rather than death estimates were also preferred. Approximately 75% to 90% of respondents accurately understood risk presented with visual and written communication. For the remaining participants, for whom accurate understanding was challenging, new strategies need to be developed.", 
    "abstract": "To provide a research review of the components and outcomes of routine outcome monitoring (ROM) and recommendations for research and therapeutic practice.\nA narrative review of the three phases of ROM - data collection, feeding back data, and adapting therapy - and an overview of patient outcomes from 11 meta-analytic studies.\nPatients support ROM when its purpose is clear and integrated within therapy. Greater frequency of data collection is more important for shorter-term therapies, and use of graphs, greater specificity of feedback, and alerts are helpful. Overall effects on patient outcomes are statistically significant (\nROM offers a low-cost method for enhancing patient outcomes, on average resulting in an \u2248 8% advantage (success rate difference; SRD) over standard care. CSTs are particularly effective for not-on-track patients (SRD between \u2248 20% and 29%), but ROM does not work for all patients and successful implementation is a major challenge, along with securing appropriate cultural adaptations.", 
    "abstract": "Health workers exposed to ionizing radiation account for \u2009+ 50% of workers exposed to man-made radiation in France. Over the last decade, the use of radiation in medicine has increased due to the introduction of new practices. The EXposition des Professionnels de santE aux RayonnemenTs ioniSants study aims to evaluate and characterize the trends in radiation exposure of health workers in France between 2009 and 2019.\nThis retrospective study includes all health workers with at least one dosimetric record in the system for occupational dosimetry registration (Syst\u00e8me d'information de la surveillance de l'exposition aux rayonnements ionisants) database for each of the years 2009, 2014, and 2019, in the hospitals included in the study. Individual external doses and socio-professional data were collected. Statistical analyses include descriptions, graphs, and logistic regressions.\nA total of 1457 workers were included (mean age: 39.8\u00a0years, 59% women). The average exposure significantly decreased between 2009 and 2019 (-0.008\u00a0mSv/year, p\u2009<\u20090.05). There were large discrepancies in trends according to professions, departments, hospitals, and gender. Over the 10-year study period, radiologic technologists and physicians were the most exposed (0.15\u00a0mSv (95%CI 0.14-0.16) and 0.13\u00a0mSv (0.06-0.21), respectively), but their exposure tended to decrease. Workers in nuclear medicine departments had the highest radiation exposure (0.36\u00a0mSv (0.33-0.39)), which remained stable over time. Thirty-eight percent of recorded doses were nonzero in 2009, decreasing to 20% in 2019.\nThis study allowed to identify physicians and radiologic technologists in nuclear medicine departments as the most exposed medical workers in France, and to show an overall decrease trend in radiation exposure. This should be instructive for radiation monitoring and safety of exposed medical workers.\n\u2022 Radiation exposure of healthcare workers in most medical departments has steadily decreased between 2009 and 2019 in several French hospitals. \u2022 The number of zero doses consistently increased during the study period. \u2022 Workers in nuclear medicine departments are the most exposed, especially radiologic technologists and physicians.", 
    "abstract": "Thin membranes are highly sought-after for nanopore-based single-molecule sensing, and fabrication of such membranes becomes challenging in the \u227210 nm thickness regime where a plethora of useful molecule information can be acquired by nanopore sensing. In this work, we present a scalable and controllable method to fabricate silicon nitride (Si", 
    "abstract": "We estimated the effect of social media use on adolescents aged 14 years and risk of cigarette, e-cigarette, and dual use when aged 17 years. Data was from the UK Millennium Cohort Study, which followed up approximately 19\u2008000 children born between 2000 and 2002.\nRepresentative longitudinal data were collected at ages 14 years and 17 years. Directed acyclic graphs identified confounders (eg, demographics, mental health, in-person interactions, cognitive ability, risk-taking, antisocial behaviour, previous or current parental cigarette or e-cigarette use, and socioeconomic circumstances). The relationship between self-reported social media use per weekday (reference category: from 1 min to under 30 mins) and cigarette, e-cigarette, and dual use was examined using longitudinal analysis to estimate adjusted odds ratios (OR) or relative risk ratios (RRRs). A complete case sample was used; weights accounted for sample design and attrition. This study was a secondary data analysis of the UK Millennium Cohort Study (prospective longitudinal study). Ethical approval was received from a Research Ethics Committee at each study sweep.\nIn total, 6234 individuals (168\u2008314 observations) were included. 5778 (92\u00b77%) reported social media use, 1730 (27\u00b78%) cigarette use, 1389 (22\u00b73%) e-cigarette use, and 479 (7\u00b768%) dual use. Social media use was associated with all outcomes in a dose-response manner. For cigarette use, ORs increased from 1\u00b767 (95% CI 1\u00b726-2\u00b721) for 30 mins to 1 h, to 3\u00b709 (2\u00b743-3\u00b791) for 2 h or longer of social media use. For e-cigarette use, ORs increased from 1\u00b790 (1\u00b741-2\u00b755) for 30 mins to 1 h, to 3\u00b734 (2\u00b760-4\u00b728) for 2 h or longer of social media use. For dual use, RRRs increased from 1\u00b791 (1\u00b716-3\u00b715) for 30 mins to 1 h, to 4\u00b726 (2\u00b781-6\u00b746) for 2 h or longer of social media use. For e-cigarette and dual use, associations were stronger for males than for females; the opposite was found for cigarette use.\nAfter accounting for observed confounders and reverse causality, our findings suggest social media use, although only measured at one point in time, is associated with increased risk of cigarette, e-cigarette, and dual use. The greatest risk was observed in those who used social media for 2 h or longer. Given the potential health harms of social media use, guidance for parents and caregivers about safe social media use and regulation on time spent on social media is required.\nMedical Research Council, Chief Scientist Office, Wellcome Trust.", 
    "abstract": "This study aimed to optimize the processing of probiotic yogurt supplemented with cumin essential oil (CEO), vitamin C, D3 (Vit D), and reduction of fermentation time using response surface methodology as a new functional food for diabetics with desirable sensory properties. The central composite design (CCD) was used to analyze the effect of these independent variables on the growth of the Lactobacillus plantarum A7 (LA7), starter culture, and overall acceptability. Differences between treatments were analyzed. The data were evaluated by analysis of variance at the significance level of 0.05. The effective concentration of CEO and fermentation time had the significant effect on the Lactobacillus plantarum A7 (LA7) number. Variance analysis and three-dimensional graphs show that almost the only effective factor on the overall acceptability of probiotic yogurt containing essential oil and vitamin D3 was CEO. According to the obtained data from the analysis, the optimal amount of independent variables for probiotic yogurt formulation such as CEO, D3, and fermentation time was 0.02% (v/v), 400\u00a0IU, and 9\u00a0h, respectively. This functional product can be considered an efficient food to reduce or eliminate the complications of diabetes.", 
    "abstract": "The potential for synthetic data to act as a replacement for real data in research has attracted attention in recent months due to the prospect of increasing access to data and overcoming data privacy concerns when sharing data. The field of generative artificial intelligence and synthetic data is still early in its development, with a research gap evidencing that synthetic data can adequately be used to train algorithms that can be used on real data. This study compares the performance of a series machine learning models trained on real data and synthetic data, based on the National Diet and Nutrition Survey (NDNS).\nFeatures identified to be potentially of relevance by directed acyclic graphs were isolated from the NDNS dataset and used to construct synthetic datasets and impute missing data. Recursive feature elimination identified only four variables needed to predict mean arterial blood pressure: age, sex, weight and height. Bayesian generalised linear regression, random forest and neural network models were constructed based on these four variables to predict blood pressure. Models were trained on the real data training set (n = 2408), a synthetic data training set (n = 2408) and larger synthetic data training set (n = 4816) and a combination of the real and synthetic data training set (n = 4816). The same test set (n = 424) was used for each model.\nSynthetic datasets demonstrated a high degree of fidelity with the real dataset. There was no significant difference between the performance of models trained on real, synthetic or combined datasets. Mean average error across all models and all training data ranged from 8.12 To 8.33. This indicates that synthetic data was capable of training equally accurate machine learning models as real data.\nFurther research is needed on a variety of datasets to confirm the utility of synthetic data to replace the use of potentially identifiable patient data. There is also further urgent research needed into evidencing that synthetic data can truly protect patient privacy against adversarial attempts to re-identify real individuals from the synthetic dataset.", 
    "abstract": "The active global SARS-CoV-2 pandemic caused more than 426 million cases and 5.8 million deaths worldwide. The development of completely new drugs for such a novel disease is a challenging, time intensive process. Despite researchers around the world working on this task, no effective treatments have been developed yet. This emphasizes the importance of drug repurposing, where treatments are found among existing drugs that are meant for different diseases. A common approach to this is based on knowledge graphs, that condense relationships between entities like drugs, diseases and genes. Graph neural networks (GNNs) can then be used for the task at hand by predicting links in such knowledge graphs. Expanding on state-of-the-art GNN research, Doshi et al. recently developed the Dr-COVID model. We further extend their work using additional output interpretation strategies. The best aggregation strategy derives a top-100 ranking of 8,070 candidate drugs, 32 of which are currently being tested in COVID-19-related clinical trials. Moreover, we present an alternative application for the model, the generation of additional candidates based on a given pre-selection of drug candidates using collaborative filtering. In addition, we improved the implementation of the Dr-COVID model by significantly shortening the inference and pre-processing time by exploiting data-parallelism. As drug repurposing is a task that requires high computation and memory resources, we further accelerate the post-processing phase using a new emerging hardware-we propose a new approach to leverage the use of high-capacity Non-Volatile Memory for aggregate drug ranking.", 
    "abstract": "Universal salt iodization was started before decades but there are communities using the un-iodized salt till now. More than one-tenth of the Ethiopian community uses un-iodized salt.\nThis study aimed to identify the hotspots and associate factors of un-iodized salt availability in Ethiopia based on Ethiopian national household survey data.\nWe conducted an in-depth analysis of the Ethiopian Demographic and Health Survey 2016 data. A total of 15,567 households were included in the final analysis. We cleaned and weighed the data using Stata version 16 software and descriptive outputs were reported in graphs and tables. We computed the weighted prevalence of un-iodized salt and prepared it for spatial analysis. Global-level spatial autocorrelation, hotspot analysis using the Getis-Ord Gi* statistics, and spatial interpolation using empirical Bayesian interpolation were executed using ArcGIS 10.3 to predict the magnitude of un-iodized salt at the national level. The binary logistics regression model was used to identify the contributing factors of un-iodized salt utilization. Model goodness of fit was tested with Hosmer and Lemeshow goodness-of-fit test (P\u2009=\u20090.96). Finally, the adjusted odds ratio (AOR) with 95% CI was reported to identify significant factors.\nThe magnitude of un-iodized salt availability was 14.19% (95% CI: 13.65, 14.75) among Ethiopian households. Un-iodized salt hotspots were found in Afar, Somalia, and Benishangul Gumuz regions. Compared to poorest wealth index: poorer (AOR\u2009=\u20090.55, 95% CI: 0.48, 0.64), middle (AOR\u2009=\u20090.51, 95% CI: 0.44, 0.60), richer (AOR\u2009=\u20090.55, 95% CI: 0.47, 0.64), and richest (AOR\u2009=\u20090.61, 95% CI: 0.50, 0.75); compared to uneducated household head: heads with secondary (AOR\u2009=\u20090.72, 95% CI: 0.60, 0.67) and above secondary (AOR\u2009=\u20090.54, 95% CI: 0.43, 0.67) education reduced the odds of un-iodized salt viability, while households living in highland (AOR\u2009=\u20091.16, 95% CI: 1.05, 1.29) had increased the odds of un-iodized salt availability.\nMore than a tenth of the households in Ethiopia uses un-iodized salt. Hotspots of un-iodized salt availability were found in Somali and Afar regions of Ethiopia. Better wealth index and education of the household heads reduces the odds of un-iodized salt availability while living in a high altitude above 2200\u00a0m increases the odds of un-iodized salt availability in Ethiopia.", 
    "abstract": "In many real, directed networks, the strongly connected component of nodes which are mutually reachable is very small. This does not fit with current theory, based on random graphs, according to which strong connectivity depends on mean degree and degree-degree correlations. And it has important implications for other properties of real networks and the dynamical behavior of many complex systems. We find that strong connectivity depends crucially on the extent to which the network has an overall direction or hierarchical ordering-a property measured by trophic coherence. Using percolation theory, we find the critical point separating weakly and strongly connected regimes and confirm our results on many real-world networks, including ecological, neural, trade, and social networks. We show that the connectivity structure can be disrupted with minimal effort by a targeted attack on edges which run counter to the overall direction. This means that many dynamical processes on networks can depend significantly on a small fraction of edges.", 
    "abstract": "Sudden unexpected death in epilepsy (SUDEP) is the leading seizure-related cause of death in epilepsy patients. There are no validated biomarkers of SUDEP risk. Here, we explored peri-ictal differences in topological brain network properties from scalp EEG recordings of SUDEP victims. Functional connectivity networks were constructed and examined as directed graphs derived from undirected delta and high frequency oscillation (HFO) EEG coherence networks in eight SUDEP and 14 non-SUDEP epileptic patients. These networks were proxies for information flow at different spatiotemporal scales, where low frequency oscillations coordinate large-scale activity driving local HFOs. The clustering coefficient and global efficiency of the network were higher in the SUDEP group pre-ictally, ictally and post-ictally (", 
    "abstract": "In this paper, we studied the time-domain irreversibility of time series, which is a fundamental property of systems in a nonequilibrium state. We analyzed a subgroup of the databases provided by University of Rochester, namely from the THEW Project. Our data consists of LQTS (Long QT Syndrome) patients and healthy persons. LQTS may be associated with an increased risk of sudden cardiac death (SCD), which is still a big clinical problem. ECG-based artificial intelligence methods can identify sudden cardiac death with a high accuracy. It follows that heart rate variability contains information about the possibility of SCD, which may be extracted, provided that appropriate methods are developed for this purpose. Our aim was to assess the complexity of both groups using visibility graph (VG) methods. Multivariate analysis of connection patterns of graphs built from time series was performed using multiplex visibility graph methods. For univariate time series, time irreversibility of the ECG interval QT of patients with LQTS was lower than for the healthy. However, we did not observe statistically significant difference in the comparison of RR intervals time series of the two groups studied. The connection patterns retrieved from multiplex VGs have more similarity with each other in the case of LQTS patients. This observation may be used to develop better methods for SCD risk stratification.", 
    "abstract": "The Blasius equation for laminar flow comes from the Prandtl boundary layer equations. In this article, we establish a new and generic Blasius equation for turbulent flow derived from the turbulent boundary layer equation that can be used for turbulent as well as laminar flow. The analytical and numerical solutions have been investigated under specific conditions to the developed new Blasius equation. The analytical and numerical results have been compared through tables and graphs to validate the established model. In fluid dynamics, analytical solutions to complicated systems are tedious and time-consuming. Changing one or more constraints can introduce new challenges. In this case, symbolic computation software provides an easier and more flexible solution for fluid dynamical systems, even if boundary conditions are adjusted to explain reality. Therefore, the MATLAB code is used to investigate the new third-order Blasius equation. The comparison and graphical representations demonstrate that the achieved results are encouraging.", 
    "abstract": "Typical liquids aren't great for engineering because of their low heat conductivity. To enhance heat transfer capabilities in industries as diverse as computers, pharmaceuticals, and molten metals, researchers and scientists have developed nanofluids, which are composed of nanoparticles distributed in a base fluid.\nMathematical modeling of micropolar \nology: We have used suitable transformations to arrive at a system of nonlinear ODEs, which we then solve numerically in MATHEMATICA using Runge-Kutta methods of the fourth order coupled with shooting approaches.\nTables and graphs are used to examine the effects of immersed flow and display profiles of physical parameters of interest. This includes velocities, temperatures, skin friction, and Nusselt numbers. The average heat transfer rate increased to ", 
    "abstract": "Postpartum Family Planning (PPFP), which aims to prevent high risk unintended and closely spaced pregnancies during the first year following childbirth, is one of the highest impact interventions to avoid increased risk of premature birth, low birth weight, fetal and neonatal death, and adverse maternal health outcomes. This study aimed to assess the combined effect of a package of interventions on the use and quality of PPFP services at Y12HMC, Addis Ababa Ethiopia.\nCross-sectional study design was conducted to evaluate the effect of the package of chosen interventions: creating a private counseling space near the postpartum ward and providing training for health care providers on the WHO decision tool kit and Long Acting Contraceptive Methods (LACM). Interviews were conducted with 470 women (235 before and 235 after the intervention). Frequency tables and graphs were used to describe the study variables and statistical significance between pre and post intervention indicators was declared at \nFrom 470 participants, 421 respondents participated which makes response rate of 90%. The majority of the participants were in the age category 20 to 29\u2009years, married/lived together, completed at least primary education and had more than one child both at baseline and post intervention. The proportion of women who chose PPFP increased from 55.8% at baseline to 69% after the intervention. The most preferred contraceptive method was implant. The overall service satisfaction level of the study participants was 95.4% post intervention, significantly higher than at baseline (78%, \nThis study demonstrated that quality improvement interventions like creating a private counseling space and providing training using WHO decision tool kit can achieve significant improvement on satisfaction of clients and PPFP uptake.", 
    "abstract": "This dataset describes the analysis of aflatoxins, macroelement and microelement concentration, oxidative stability and fatty acid profile of infant formula milk powder. Gas chromatography (CG) was used to identity 14 fatty acid methyl esters in in five samples of oils. The Racimat 893 method (induction times), Thermogravimetry (TG), Derivative Thermogravimetry (DTG) and Differential Scanning Calorimetry (DSC) were used to estimate the oxidative stability of oils. In addition, UV-VIS spectroscopic techniques were employed to obtain graphs of the absorption of each oil. The data presented can be useful in identifying compounds available in oils used to promote wound healing and understand the degradation mechanism.", 
    "abstract": "Allostery is an important regulatory mechanism of protein functions. Among allosteric proteins, certain protein structure types are more observed. However, how allosteric regulation depends on protein topology remains elusive. In this study, we extracted protein topology graphs at the fold level and found that known allosteric proteins mainly contain multiple domains or subunits and allosteric sites reside more often between two or more domains of the same fold type. Only a small fraction of fold-fold combinations are observed in allosteric proteins, and homo-fold-fold combinations dominate. These analyses imply that the locations of allosteric sites including cryptic ones depend on protein topology. We further developed TopoAlloSite, a novel method that uses the kernel support vector machine to predict the location of allosteric sites on the overall protein topology based on the subgraph-matching kernel. TopoAlloSite successfully predicted known cryptic allosteric sites in several allosteric proteins like phosphopantothenoylcysteine synthetase, spermidine synthase, and sirtuin 6, demonstrating its power in identifying cryptic allosteric sites without performing long molecular dynamics simulations or large-scale experimental screening. Our study demonstrates that protein topology largely determines how its function can be allosterically regulated, which can be used to find new druggable targets and locate potential binding sites for rational allosteric drug design.", 
    "abstract": "Four accurate, green, uncomplicated, and fast spectrophotometric procedures were established for the purpose of resolving as well as quantifying a ternary combination prescribed for cardiovascular patients, such as aspirin, atorvastatin, and ramipril. Method (A) is based on the first derivative zero-crossing spectrophotometry for the determination of aspirin and atorvastatin at 247.4\u00a0nm and 302.6\u00a0nm, respectively. Ramipril was determined using the second derivative at 211\u00a0nm. Method (B) depends on the ratio spectra first derivative (RDS) where the absorption spectrum of the ternary combination was divided by the spectrum of one of the analytes. When treated similarly, the concentrations of the other two analytes were measured using their corresponding calibration graphs. For the determination of ASP and RAM, ATR was used as a divisor with a concentration of 26\u00a0\u00b5g/mL, and the RDS values at 272.0 and 225.8\u00a0nm, respectively, were plotted against the ASP and RAM concentrations. Using 40\u00a0\u00b5g/mL ASP as a divisor, ATR was analyzed, and the RDS values at 295\u00a0nm were plotted versus the ATR concentration. Method (C) is based on the double divisor-ratio spectra derivative technique. In this technique, the derivative of the ratio spectrum is computed by dividing the absorption spectra of the studied combination by the standard spectrum of abinary combination of two of the three analytes being studied. The concentrations of the three analytes in the mixture were assayed by determining the absorbance either at the positive or the negative amplitude. For the determination of ASP, ATR, and RAM, the wavelengths used were 244, 295, and 220\u00a0nm, respectively. Method (D) was a hybrid double divisor-ratio spectra technique based on convolving the double divisor-ratio spectra with trigonometric Fourier functions. The magnitudes of the Fourier function coefficients at either maximum or minimum points were correlated to the concentration of each drug in the mixture. The specificity of the suggested methods was tested by analyzing synthetic laboratory-prepared combinations and laboratory-made tablets. Furthermore, the accuracy and precision were ensured by statistically comparing the obtained results with those obtained from comparison method using Bartlett's Test for Equality of Variances and ANOVA test.", 
    "abstract": "The interdependence of hospitals is underappreciated in patient outcomes studies. We used a network science approach to foreground this interdependence. Specifically, within two large state-based interhospital networks, we examined the relationship of a hospital's network position with in-hospital mortality and length of stay.\nWe constructed interhospital network graphs using data from the Healthcare Cost and Utilization Project and the American Hospital Association Annual Survey for Florida (2014) and California (2011). The exposure of interest was hospital centrality, defined as weighted degree (sum of all ties to a given hospital from other hospitals). The outcomes were in-hospital mortality and length of stay with sub-analyses for four acute medical conditions: pneumonia, heart failure, ischemic stroke, myocardial infarction. We compared outcomes for each quartile of hospital centrality relative to the most central quartile (Q4), independent of patient- and hospital-level characteristics, in this retrospective cross-sectional study.\nThe inpatient cohorts had 1,246,169 patients in Florida and 1,415,728 in California. Compared to Florida's central hospitals which had an overall mortality 1.60%, peripheral hospitals had higher in-hospital mortality (1.97%, adjusted OR (95%CI): Q1 1.61 (1.37, 1.89), p<0.001). Hospitals in the middle quartiles had lower in-hospital mortality compared to central hospitals (%, adjusted OR (95% CI): Q2 1.39%, 0.79 (0.70, 0.89), p<0.001; Q3 1.33%, 0.78 (0.70, 0.87), p<0.001). Peripheral hospitals had longer lengths of stay (adjusted incidence rate ratio (95% CI): Q1 2.47 (2.44, 2.50), p<0.001). These findings were replicated in California, and in patients with heart failure and pneumonia in Florida. These results show a u-shaped distribution of outcomes based on hospital network centrality quartile.\nThe position of hospitals within an inter-hospital network is associated with patient outcomes. Specifically, hospitals located in the peripheral or central positions may be most vulnerable to diminished quality outcomes due to the network. Results should be replicated with deeper clinical data.", 
    "abstract": "In clinical settings and research studies, childbirth experience is often measured using a single-item question about overall experience. Little is known about what women include in this rating, which complicates the design of adequate follow-up, as well as the interpretation of research findings based on ratings of overall childbirth experience. The aim of this study was to examine which known dimensions of childbirth experience women include in the rating on a single-item measure.\nRatings of overall childbirth experience on a 10-point numeric rating scale (NRS) from 2953 women with spontaneous or induced onset of labour at two Swedish hospitals were evaluated against the validated Childbirth Experience Questionnaire 2 (CEQ2), completed on one of the first days postpartum. The CEQ2 measures four childbirth experience domains: own capacity, perceived safety, professional support and participation. Internal consistency for CEQ2 was evaluated by calculating Cronbach's alpha. NRS ratings were explored in relation to CEQ2 using empirical cumulative distribution function graphs, where childbirth experience was defined as negative (NRS ratings 1-4), mixed (NRS ratings 5-6) or positive (NRS ratings 7-10). A multiple linear regression analysis, presented as beta coefficients (B) and 95% confidence intervals (CI), was also performed to explore the relationship between the four domains of the CEQ2 and overall childbirth experience.\nThe prevalence of negative childbirth experience was 6.3%. All CEQ2-subscales reached high or acceptable reliability (Cronbach's alpha\u2009=\u20090.78; 0.81; 0.69 and 0.66, respectively). Regardless of overall childbirth experience, the majority of respondents scored high on the CEQ2 subscale representing professional support. Overall childbirth experience was mainly explained by perceived safety (B\u2009=\u20091.60, CI 1.48-1.73), followed by own capacity (B\u2009=\u20090.65, CI 0.53-0.77) and participation (B\u2009=\u20090.43, CI 0.29-0.56).\nIn conclusion, overall childbirth experience rated by a single-item measurement appears to mainly capture experiences of perceived safety, and to a lesser extent own capacity and participation, but appears not to reflect professional support. CEQ2 shows good psychometric properties for use shortly after childbirth, and among women with induced onset of labour, which increases the usability of the instrument.", 
    "abstract": "A dominating set of a graph [Formula: see text] is a subset U of its vertices V, such that any vertex of G is either in U, or has a neighbor in U. The dominating-set problem is to find a minimum dominating set in G. Dominating sets are of critical importance for various types of networks/graphs, and find therefore potential applications in many fields. Particularly, in the area of communication, dominating sets are prominently used in the efficient organization of large-scale wireless ad hoc and sensor networks. However, the dominating set problem is also a hard optimization problem and thus currently is not efficiently solvable on classical computers. Here, we propose a biomolecular and a quantum algorithm for this problem, where the quantum algorithm provides a quadratic speedup over any classical algorithm. We show that the dominating set problem can be solved in [Formula: see text] queries by our proposed quantum algorithm, where n is the number of vertices in G. We also demonstrate that our quantum algorithm is the best known procedure to date for this problem. We confirm the correctness of our algorithm by executing it on IBM Quantum's qasm simulator and the Brooklyn superconducting quantum device. And lastly, we show that molecular solutions obtained from solving the dominating set problem are represented in terms of a unit vector in a finite-dimensional Hilbert space.", 
    "abstract": "Metagenome assembly is an efficient approach to reconstruct microbial genomes from metagenomic sequencing data. Although short-read sequencing has been widely used for metagenome assembly, linked- and long-read sequencing have shown their advancements in assembly by providing long-range DNA connectedness. Many metagenome assembly tools were developed to simplify the assembly graphs and resolve the repeats in microbial genomes. However, there remains no comprehensive evaluation of metagenomic sequencing technologies, and there is a lack of practical guidance on selecting the appropriate metagenome assembly tools. This paper presents a comprehensive benchmark of 19 commonly used assembly tools applied to metagenomic sequencing datasets obtained from simulation, mock communities or human gut microbiomes. These datasets were generated using mainstream sequencing platforms, such as Illumina and BGISEQ short-read sequencing, 10x Genomics linked-read sequencing, and PacBio and Oxford Nanopore long-read sequencing. The assembly tools were extensively evaluated against many criteria, which revealed that long-read assemblers generated high contig contiguity but failed to reveal some medium- and high-quality metagenome-assembled genomes (MAGs). Linked-read assemblers obtained the highest number of overall near-complete MAGs from the human gut microbiomes. Hybrid assemblers using both short- and long-read sequencing were promising methods to improve both total assembly length and the number of near-complete MAGs. This paper also discussed the running time and peak memory consumption of these assembly tools and provided practical guidance on selecting them.", 
    "abstract": "There is growing interest in the role of structural variants (SVs) as drivers of local adaptation and speciation. From a biodiversity genomics perspective, the characterization of genome-wide SVs provides an exciting opportunity to complement single nucleotide polymorphisms (SNPs). However, little is known about the impacts of SV discovery and genotyping strategies on the characterization of genome-wide SV diversity within and among populations. Here, we explore a near whole-species resequence data set, and long-read sequence data for a subset of highly represented individuals in the critically endangered k\u0101k\u0101p\u014d (Strigops habroptilus). We demonstrate that even when using a highly contiguous reference genome, different discovery and genotyping strategies can significantly impact the type, size and location of SVs characterized genome-wide. Further, we found that the mean number of SVs in each of two k\u0101k\u0101p\u014d lineages differed both within and across generations. These combined results suggest that genome-wide characterization of SVs remains challenging at the population-scale. We are optimistic that increased accessibility to long-read sequencing and advancements in bioinformatic approaches including multireference approaches like genome graphs will alleviate at least some of the challenges associated with resolving SV characteristics below the species level. In the meantime, we address caveats, highlight considerations, and provide recommendations for the characterization of genome-wide SVs in biodiversity genomic research.", 
    "abstract": "Intestinal parasite infections are one of the most serious public health issues in the globe. Individuals' health is harmed by a high prevalence of intestinal parasite infections, which primarily affect physical and mental growth, resulting in malnutrition, anemia, stunting, cognitive impairment, reduced educational achievement, and inefficiency.\nThe aim of this study is to determine the prevalence of intestinal parasite infection and associated factors among food handlers in Feres Bet town, North West Amhara, Ethiopia, 2021.\nFrom March to April 2021, a facility-based cross-sectionalstudy was undertaken in Feres Bet town among food handlers and drinking establishments. A total of 370 study participants were enrolled in the study utilizing a basic random sampling procedure. Epi data version 3.1 was used to enter the data, and statistical product service solution version 25 was used to clean and analyze it. To show the relationship between dependent and independent variables, a binary logistic regression statistical model was built. Adjested odd ratios with a 95% confidence interval and a p-value of less than 0.05 were declared statistically significant. Tables and graphs were used to convey the report.\nThe study included 370 food handlers out of a total of 399. Approximately 166 (44.9%) of food workers tested positive for at least one intestinal parasite. Hand washing practice after toilet [(AOR: 6.25, 95% CI: 2.05-19.02)], hand washing practice after touching dirty materials [(AOR: 3.71, 95% CI: 1.72-8.03)], hand washing practice before meal (AOR\u00a0=\u00a012.49, 95% CI\u00a0=\u00a04.92-31.72), periodic medical checkup [(AOR\u00a0=\u00a03.42, 95% CI\u00a0=\u00a01.29-9.06)], were variables significantly associated with intestinal parasite.\nThe prevalence of intestinal parasite infection among food and drink handlers was found to be 44.9% in this investigation.", 
    "abstract": "The varying experience of surgeons and ultrasound physicians, and their collaboration with physicians, may affect operation time and efficiency. We evaluated the learning curve of ultrasound-guided vacuum-assisted excision (VAE) of breast lesion with collaboration between different physicians, and assessed characteristics associated with operation time.\nThe sample population of this retrospective study was divided into two groups: 49 consecutive patient surgeries completed by skilled surgeons and novice ultrasound physicians (U group); and 30 consecutive patient surgeries completed by skilled ultrasound physicians and novice surgeons (S group). Cumulative summation graphs were used to evaluate operation time and calculate the turning point of the learning curve. Patients in the U and S groups were divided into exploration stage and proficiency stage according to the turning point, and the differences in influencing factors were compared. A total of 548 patients who underwent vacuum-assisted breast excision performed by a combination of skilled surgeons and skilled ultrasound physicians were selected as the reference group (R group). The differences among the three groups were compared. The relationship between the operation time and other factors in the different groups was analyzed using linear regression.\nThe best learning curve of the sample population was the quadratic fitting equation, and the turning point was the 19th case in the U group and the 14th case in the S group. The total operation times in the proficiency stage were significantly shorter than those in the exploration stage in the U and S groups (P=0.012 and P=0.003, separately). Patient age, long diameter, short diameter, and depth of masses related to the operation time.\nOur data suggest the existence of different learning curves in ultrasound-guided vacuum-assisted excision for the collaborations of different stages surgeons and ultrasound physicians. Through the accumulation of experience, it is feasible to safely perform ultrasound-guided VAE of breast lesions.", 
    "abstract": "Understanding the role of natural selection in driving evolutionary change requires accurate estimates of the strength of selection acting at the genetic level in the wild. This is challenging to achieve but may be easier in the case of populations in migration-selection balance. When two populations are at equilibrium under migration-selection balance, there exist loci whose alleles are selected different ways in the two populations. Such loci can be identified from genome sequencing by their high values of F", 
    "abstract": "Understanding the common topological characteristics of the human brain network across a population is central to understanding brain functions. The abstraction of human connectome as a graph has been pivotal in gaining insights on the topological properties of the brain network. The development of group-level statistical inference procedures in brain graphs while accounting for the heterogeneity and randomness still remains a difficult task. In this study, we develop a robust statistical framework based on persistent homology using the order statistics for analyzing brain networks. The use of order statistics greatly simplifies the computation of the persistent barcodes. We validate the proposed methods using comprehensive simulation studies and subsequently apply to the resting-state functional magnetic resonance images. We found a statistically significant topological difference between the male and female brain networks.", 
    "abstract": "We used a large convenience sample (", 
    "abstract": "The loss of health workers through death is of great importance and interest to the public, media and the medical profession as it has very profound social and professional consequences on the delivery of health services.\nTo describe the profile, causes and patterns of death among medical doctors and dental surgeons in Uganda between 1986 and 2016.\nWe conducted a retrospective descriptive study of mortality among registered medical doctors and dental surgeons. Information on each case was collected using a standard questionnaire and analysed. Cause of death was determined using pathology reports, and if unavailable, verbal autopsies. We summarized our findings across decades using means and standard deviations, proportions and line graphs as appropriate. Cuzick's test for trend was used to assess crude change in characteristics across the three decades. To estimate the change in deaths across decades adjusted for age and sex, we fit a logistic regression model, and used the margins command with a dy/dx option. All analyses were done in Stata version 14.0 (Stata Corp, College Station, TX).\nThere were 489 deaths registered between 1986 and 2016. Of these, 59 (12.1%) were female. The mean age at death was 48.8 years (Standard Deviation (SD) 15.1) among male and 40.1 years (SD 12.8) among females. We ascertained the cause of death for 468/489 (95.7%). The most common causes of death were HIV/AIDS (218/468, 46.6%), cancer (68/468, 14.5%), non-communicable diseases (62/48, 13.3%), alcohol related deaths (36, 7.7%), road traffic accidents (34, 7.3%), gunshots (11, 2.4%), among others. After adjusting for age and sex, HIV/AIDs attributable deaths decreased by 33 percentage points between the decade of 1986 to1995 and that of 2006 to 2016 -0.33 (-0.44, -0.21. During the same period, cancer attributable deaths increased by 13 percentage periods 0.13 (0.05,0.20).\nThe main causes of death were HIV/AIDS, cancer, non-communicable diseases, alcohol-related diseases and road traffic accidents. There was a general downward trend in the HIV/AIDS related deaths and a general upward trend in cancer related deaths. Doctors should be targeted for preventive and support services especially for both communicable and non-communicable diseases.", 
    "abstract": "A ", 
    "abstract": "Focusing on the theme of \"osteoporosis-related research in adolescents,\" a systematic visualization of the developmental lineage, current research status, hot spots, and trends of adolescent osteoporosis was conducted to provide a reference for subsequent related research, clinical diagnosis, and treatment.\nThe Web of Science core database was used as the data source to retrieve the relevant literature and the bibliometrics method. An online bibliometric platform, CiteSpace, and VOSviewer software were used to conduct co-occurrence analysis on the authors, scientific research institutions, national cooperation, keywords, and funding sources to draw the relevant knowledge map.\nA total of 1,199 publications from the Web of Science core database were included in this study. The number of published adolescent osteoporosis (AOP) studies has shown an upward trend over the past 29 years, with the United States being the major contributor to the field with the highest number of publications (291, 24.3%) and the highest number of citations (12,186). The international collaboration map shows that the United States is the country most focused on international collaborative exchanges, with the closest collaboration between the United States and Canada. The most influential research institutions and authors are Children's Hospital and Rauch F. the United States is the primary funding source for this research area. Research hotspots were mainly focused on \"bone density,\" \"osteoporosis,\" and \"children.\"\nThese knowledge maps review the research hotpots in adolescent osteoporosis research over time, analyze and summarize the research process over the past 29 years, and predict future research directions.", 
    "abstract": "Times of crisis such as the COVID-19 pandemic are expected to compromise mental health. Despite a large number of studies, evidence on the development of mental health in general populations during the pandemic is inconclusive. One reason may be that representative data spanning the whole pandemic and allowing for comparisons to pre-pandemic data are scarce.\nWe analyzed representative data from telephone surveys of Germany's adults. Three mental health indicators were observed in ~1,000 and later up to 3,000 randomly sampled participants monthly until June 2022: symptoms of depression (observed since April 2019, PHQ-2), symptoms of anxiety (GAD-2), and self-rated mental health (latter two observed since March 2021). We produced time series graphs including estimated three-month moving means and proportions of positive screens (PHQ/GAD-2 score \u2265 3) and reports of very good/excellent mental health, as well as smoothing curves. We also compared time periods between years. Analyses were stratified by sex, age, and level of education.\nWhile mean depressive symptom scores declined from the first wave of the pandemic to summer 2020, they increased from October 2020 and remained consistently elevated throughout 2021 with another increase between 2021 and 2022. Correspondingly, the proportion of positive screens first decreased from 11.1% in spring/summer 2019 to 9.3% in the same period in 2020 and then rose to 13.1% in 2021 and to 16.9% in 2022. While depressive symptoms increased in all subgroups at different times, developments among women (earlier increase), the youngest (notable increase in 2021) and eldest adults, as well as the high level of education group (both latter groups: early, continuous increases) stand out. However, the social gradient in symptom levels between education groups remained unchanged. Symptoms of anxiety also increased while self-rated mental health decreased between 2021 and 2022.\nElevated symptom levels and reduced self-rated mental health at the end of our observation period in June 2022 call for further continuous mental health surveillance. Mental healthcare needs of the population should be monitored closely. Findings should serve to inform policymakers and clinicians of ongoing dynamics to guide health promotion, prevention, and care.", 
    "abstract": "Since life expectancy has increased significantly over the past century, society is being forced to discover innovative ways to support active aging and elderly care. The e-VITA project, which receives funding from both the European Union and Japan, is built on a cutting edge method of virtual coaching that focuses on the key areas of active and healthy aging. The requirements for the virtual coach were ascertained through a process of participatory design in workshops, focus groups, and living laboratories in Germany, France, Italy, and Japan. Several use cases were then chosen for development utilising the open-source Rasa framework. The system uses common representations such as Knowledge Bases and Knowledge Graphs to enable the integration of context, subject expertise, and multimodal data, and is available in English, German, French, Italian, and Japanese.", 
    "abstract": "In the structural analysis of discrete geometric data, graph kernels have a great track record of performance. Using graph kernel functions provides two significant advantages. First, a graph kernel is capable of preserving the graph's topological structures by describing graph properties in a high-dimensional space. Second, graph kernels allow the application of machine learning methods to vector data that are rapidly evolving into graphs. In this paper, the unique kernel function for similarity determination procedures of point cloud data structures, which are crucial for several applications, is formulated. This function is determined by the proximity of the geodesic route distributions in graphs reflecting the discrete geometry underlying the point cloud. This research demonstrates the efficiency of this unique kernel for similarity measures and the categorization of point clouds.", 
    "abstract": "The parameters of the improved design of the pressure mechanism of a roller technological machine for squeezing wet materials are investigated in this article. The factors influencing the parameters of the pressure mechanism, which provide the required force between the working rolls of a technological machine during the processing of moisture-saturated fibrous materials, such as wet leather, were studied. The processed material is drawn in the vertical direction between the working rolls under their pressure. This study aimed to determine the parameters that make it possible to create the required pressure of the working rolls depending on the change in the thickness of the material being processed. A pressure mechanism of working rolls mounted on levers is proposed. In the design of the proposed device, the length of the levers does not change due to the movement of the sliders when turning the levers; this provides a horizontal direction of the sliders. The change in the pressure force of the working rolls is determined depending on the variation in the nip angle, the coefficient of friction, and other factors. Based on theoretical studies concerning the feed of the semi-finished leather product between the squeezing rolls, graphs were plotted, and conclusions were drawn. An experimental roller stand designated for pressing multi-layer leather semi-finished products has been developed and manufactured. An experiment was carried out to determine the factors affecting the technological process of squeezing excess moisture from wet semi-finished leather products with their multilayer package together with moisture-removing materials by means of their vertical supply on a base plate between rotating squeezing shafts also covered with moisture-removing materials. According to the results of the experiment, the optimal process parameters were selected. It is recommended to carry out the process of squeezing the moisture from two wet semi-finished leather products at a pass rate more than twice as high and with a pressing force of the working shafts two times lower compared to the analog. According to the results of the study, the optimal parameters for the process of squeezing the moisture from two layers of wet leather semi-finished products were chosen, namely the feed rate of 0.34 m/s and a pressing force of the squeezing rollers of 32 kN/m. The use of the proposed roller device allowed an increase of two times or more in the productivity of the process of processing wet leather semi-finished products on the basis of the proposed technique compared to known roller wringers.", 
    "abstract": "Implementing green development is important to realizing a harmonious relationship between humans and nature, and has attracted the attention of governments all over the world. This paper uses the PMC (Policy Modeling Consistency) model to make a quantitative evaluation of 21 representative green development policies issued by the Chinese government. The research finds: firstly, the overall evaluation grade of green development is good and the average PMC index of China's 21 green development policies is 6.59. Second, the evaluation of 21 green development policies can be divided into four different grades. Most grades of the 21 policies are excellent and good; the values of five first-level indicators about policy nature, policy function, content evaluation, social welfare, and policy object are high, which indicates that the 21 green development policies in this paper are relatively comprehensive and complete. Third, most green development policies are feasible. In twenty-one green development policies, there are: one perfect-grade policy, eight excellent-grade policies, ten good-grade policies, and two bad-grade policies. Fourthly, this paper analyzes the advantages and disadvantages of policies in different evaluation grades by drawing four PMC surface graphs. Finally, based on the research findings, this paper puts forward suggestions to optimize the green development policy-making of China.", 
    "abstract": "Cancer is a disease that causes abnormal cell formation and spreads throughout the body, causing harm to other organs. Breast cancer is the most common kind among many of cancers worldwide. Breast cancer affects women due to hormonal changes or genetic mutations in DNA. Breast cancer is one of the primary causes of cancer worldwide and the second biggest cause of cancer-related deaths in women. Metastasis development is primarily linked to mortality. Therefore, it is crucial for public health that the mechanisms involved in metastasis formation are identified. Pollution and the chemical environment are among the risk factors that are being indicated as impacting the signaling pathways involved in the construction and growth of metastatic tumor cells. Due to the high risk of mortality of breast cancer, breast cancer is potentially fatal, more research is required to tackle the deadliest disease. We considered different drug structures as chemical graphs in this research and computed the partition dimension. This can help to understand the chemical structure of various cancer drugs and develop formulation more efficiently.", 
    "abstract": "Spurt upsurge in violent protest and armed conflict in populous, civil areas has upstretched momentous concern worldwide. The unrelenting strategy of the law enforcement agencies focuses on thwarting the conspicuous impact of violent events. Increased surveillance using a widespread visual network supports the state actors in maintaining vigilance. Minute, simultaneous monitoring of numerous surveillance feeds is a workforce-intensive, idiosyncratic, and otiose method. Significant advancements in Machine Learning (ML) show potential in realizing precise models to detect suspicious activities in the mob. Existing pose estimation techniques have privations in detecting weapon operation activity. The paper proposes a comprehensive, customized human activity recognition approach using human body skeleton graphs. The VGG-19 backbone extracted 6600 body coordinates from the customized dataset. The methodology categorizes human activities into eight classes experienced during violent clashes. It facilitates alarm triggers in a specific activity, i.e., stone pelting or weapon handling while walking, standing, and kneeling is considered a regular activity. The end-to-end pipeline presents a robust model for multiple human tracking, mapping a skeleton graph for each person in consecutive surveillance video frames with the improved categorization of suspicious human activities, realizing effective crowd management. LSTM-RNN Network, trained on a customized dataset superimposed with Kalman filter, attained 89.09% accuracy for real-time pose identification.", 
    "abstract": "In this paper, we perform analytical and statistical studies of Revan indices on graphs $ G $: $ R(G) = \\sum_{uv \\in E(G)} F(r_u, r_v) $, where $ uv $ denotes the edge of $ G $ connecting the vertices $ u $ and $ v $, $ r_u $ is the Revan degree of the vertex $ u $, and $ F $ is a function of the Revan vertex degrees. Here, $ r_u = \\Delta + \\delta - d_u $ with $ \\Delta $ and $ \\delta $ the maximum and minimum degrees among the vertices of $ G $ and $ d_u $ is the degree of the vertex $ u $. We concentrate on Revan indices of the Sombor family, i.e., the Revan Sombor index and the first and second Revan $ (a, b) $-$ KA $ indices. First, we present new relations to provide bounds on Revan Sombor indices which also relate them with other Revan indices (such as the Revan versions of the first and second Zagreb indices) and with standard degree-based indices (such as the Sombor index, the first and second $ (a, b) $-$ KA $ indices, the first Zagreb index and the Harmonic index). Then, we extend some relations to index average values, so they can be effectively used for the statistical study of ensembles of random graphs.", 
    "abstract": "In this study we analyzed the flow, heat and mass transfer behavior of Casson nanofluid past an exponentially stretching surface under the impact of activation energy, Hall current, thermal radiation, heat source/sink, Brownian motion and thermophoresis. Transverse magnetic field with the assumption of small Reynolds number is implemented vertically. The governing partial nonlinear differential equations\u00a0of the flow, heat and mass transfer are transformed into ordinary differential equations by using similarity transformation and solved numerically by using Matlab bvp4c package. The impact of each of the Hall current parameter, thermal radiation parameter, heat source/sink parameter, Brownian motion parameter, Prandtl number, thermophoresis parameter and magnetic parameter on velocity, concentration and temperature, is discussed through graphs. The skin friction coefficient along the x-and z-directions, the local Nusselt number and the Sherwood number are calculated numerically to look into the inside behavior of the emerging parameters. It is witnessed that the flow velocity is a diminishing function of the thermal radiation parameter and the behavior has observed in the case of Hall parameter. Moreover, mounting values of Brownian motion parameter reduce the nanoparticle concentration profile.", 
    "abstract": "The assessment and treatment of wounds are nurses' and their teams' responsibilities, as it is up to the nurses to outline a therapeutic plan for tissue repair. For the evaluation process, the nurse must be scientifically trained and use reliable instruments.\nWebsite development for wound assessment.\nThis is a methodological study that developed a website to evaluate wounds based on an assessment questionnaire called Expected Results of the Evaluation of Chronic Wound Healing (RESVECH 2.0), which consists of an adapted and validated instrument.\nThe website construction followed the basic flowchart of elaboration. To use it, the professionals create their login and subsequently register their patients. Then, they answer six questionnaires that form the evaluation process according to RESVECH 2.0. The website allows nurses to monitor the patient's evolution through graphs and previous assessments that are filed in a database. For the evaluation process, the professional needs to have a technological internet-accessed device, such as a tablet or cell phone, in order to make wound care assistance more practical and efficient.\nthe findings demonstrate the importance of adding technology to assistance in the treatment of wounds and may provide more qualified service and more resolutive treatment.", 
    "abstract": "Knowledge graph completion (KGC) has attracted significant research interest in applying knowledge graphs (KGs). Previously, many works have been proposed to solve the KGC problem, such as a series of translational and semantic matching models. However, most previous methods suffer from two limitations. First, current models only consider the single form of relations, thus failing to simultaneously capture the semantics of multiple relations (direct, multi-hop and rule-based). Second, the data-sparse problem of knowledge graphs would make part of relations challenging to embed. This paper proposes a novel translational knowledge graph completion model named multiple relation embedding (MRE) to address the above limitations. We attempt to embed multiple relations to provide more semantic information for representing KGs. To be more specific, we first leverage PTransE and AMIE+ to extract multi-hop and rule-based relations. Then, we propose two specific encoders to encode extracted relations and capture semantic information of multiple relations. We note that our proposed encoders can achieve interactions between relations and connected entities in relation encoding, which is rarely considered in existing methods. Next, we define three energy functions to model KGs based on the translational assumption. At last, a joint training method is adopted to perform KGC. Experimental results illustrate that MRE outperforms other baselines on KGC, demonstrating the effectiveness of embedding multiple relations for advancing knowledge graph completion.", 
    "abstract": "The utilization of molecular structure topological indices is currently a standing operating procedure in the structure-property relations research, especially in QSPR/QSAR study. In the past several year, generous molecular topological indices related to some chemical and physical properties of chemical compounds were put forward. Among these topological indices, the VDB topological indices rely only on the vertex degree of chemical molecular graphs. The VDB topological index of an $ n $-order graph $ G $ is defined as TI(G) = \\sum\\limits_{1\\leq i\\leq j\\leq n-1}m_{ij}\\psi_{ij}, $ where $ \\{\\psi_{ij}\\} $ is a set of real numbers, $ m_{ij} $ is the quantity of edges linking an $ i $-vertex and another $ j $-vertex. Numerous famous topological indices are special circumstance of this expression. f-benzenoids are a kind of polycyclic aromatic hydrocarbons, present in large amounts in coal tar. Studying the properties of f-benzenoids via topological indices is a worthy task. In this work the extremum $ TI $ of f-benzenoids with given number of edges were determined. The main idea is to construct f-benzenoids with maximal number of inlets and simultaneously minimal number of hexagons in $ \\Gamma_{m} $, where $ \\Gamma_{m} $ is the collection of f-benzenoids with exactly $ m $ $ (m\\geq19) $ edges. As an application of this result, we give a unified approach of VDB topological indices to predict distinct chemical and physical properties such as the boiling point, $ \\pi $-electrom energy, molecular weight and vapour pressure etc. of f-benzenoids with fixed number of edges.", 
    "abstract": "Neutrosophic soft set theory is one of the most developed interdisciplinary research areas, with multiple applications in various fields such as computational intelligence, applied mathematics, social networks, and decision science. In this research article, we introduce the powerful framework of single-valued neutrosophic soft competition graphs by integrating the powerful technique of single-valued neutrosophic soft set with competition graph. For dealing with different levels of competitive relationships among objects in the presence of parametrization, the novel concepts are defined which include single-valued neutrosophic soft k-competition graphs and p-competition single-valued neutrosophic soft graphs. Several energetic consequences are presented to obtain strong edges of the above-referred graphs. The significance of these novel concepts is investigated through application in professional competition and also an algorithm is developed to address this decision-making problem.", 
    "abstract": "Parkinson's disease (PD) is a neurodegenerative disease with a broad spectrum of motor and non-motor symptoms. The great heterogeneity of clinical symptoms, biomarkers, and neuroimaging and lack of reliable progression markers present a significant challenge in predicting disease progression and prognoses.\nWe propose a new approach to disease progression analysis based on the mapper algorithm, a tool from topological data analysis. In this paper, we apply this method to the data from the Parkinson's Progression Markers Initiative (PPMI). We then construct a Markov chain on the mapper output graphs.\nThe resulting progression model yields a quantitative comparison of patients' disease progression under different usage of medications. We also obtain an algorithm to predict patients' UPDRS III scores.\nBy using mapper algorithm and routinely gathered clinical assessments, we developed a new dynamic models to predict the following year's motor progression in the early stage of PD. The use of this model can predict motor evaluations at the individual level, assisting clinicians to adjust intervention strategy for each patient and identifying at-risk patients for future disease-modifying therapy clinical trials.", 
    "abstract": "Simulation and programming of current quantum computers as Noisy Intermediate-Scale Quantum (NISQ) devices represent a hot topic at the border of current physical and information sciences. The quantum walk process represents a basic subroutine in many quantum algorithms and plays an important role in studying physical phenomena. Simulating quantum walk processes is computationally challenging for classical processors. With an increasing improvement in qubits fidelity and qubits number in a single register, there is a potential to improve quantum walks simulations substantially. However, efficient ways to simulate quantum walks in qubit registers still have to be explored. Here, we explore the relationship between quantum walk on graphs and quantum circuits. Firstly, we discuss ways to obtain graphs provided quantum circuit. We then explore techniques to represent quantum walk on a graph as a quantum circuit. Specifically, we study hypercube graphs and arbitrary graphs. Our approach to studying the relationship between graphs and quantum circuits paves way for the efficient implementation of quantum walks algorithms on quantum computers.", 
    "abstract": "Percolation establishes the connectivity of complex networks and is one of the most fundamental critical phenomena for the study of complex systems. On simple networks, percolation displays a second-order phase transition; on multiplex networks, the percolation transition can become discontinuous. However, little is known about percolation in networks with higher-order interactions. Here, we show that percolation can be turned into a fully fledged dynamical process when higher-order interactions are taken into account. By introducing signed triadic interactions, in which a node can regulate the interactions between two other nodes, we define triadic percolation. We uncover that in this paradigmatic model the connectivity of the network changes in time and that the order parameter undergoes a period doubling and a route to chaos. We provide a general theory for triadic percolation which accurately predicts the full phase diagram on random graphs as confirmed by extensive numerical simulations. We find that triadic percolation on real network topologies reveals a similar phenomenology. These results radically change our understanding of percolation and may be used to study complex systems in which the functional connectivity is changing in time dynamically and in a non-trivial way, such as in neural and climate networks.", 
    "abstract": "In highly connected financial networks, the failure of a single institution can cascade into additional bank failures. This systemic risk can be mitigated by adjusting the loans, holding shares, and other liabilities connecting institutions in a way that prevents cascading of failures. We are approaching the systemic risk problem by attempting to optimize the connections between the institutions. In order to provide a more realistic simulation environment, we have incorporated nonlinear/discontinuous losses in the value of the banks. To address scalability challenges, we have developed a two-stage algorithm where the networks are partitioned into modules of highly interconnected banks and then the modules are individually optimized. We developed a new algorithms for classical and quantum partitioning for directed and weighed graphs (first stage) and a new methodology for solving Mixed Integer Linear Programming problems with constraints for the systemic risk context (second stage). We compare classical and quantum algorithms for the partitioning problem. Experimental results demonstrate that our two-stage optimization with quantum partitioning is more resilient to financial shocks, delays the cascade failure phase transition, and reduces the total number of failures at convergence under systemic risks with reduced time complexity.", 
    "abstract": "Entity alignment refers to matching entities with the same realistic meaning in different knowledge graphs. The structure of a knowledge graph provides the global signal for entity alignment. But in the real world, a knowledge graph provides insufficient structural information in general. Moreover, the problem of knowledge graph heterogeneity is common. The semantic and string information can alleviate the problems caused by the sparse and heterogeneous nature of knowledge graphs, yet both of them have not been fully utilized by most existing work. Therefore, we propose an entity alignment model based on multiple information (EAMI), which employs structural, semantic and string information. EAMI learns the structural representation of a knowledge graph by using multi-layer graph convolutional networks. To acquire more accurate entity vector representation, we incorporate the attribute semantic representation into the structural representation. In addition, to further improve entity alignment, we study the entity name string information. There is no training required to calculate the similarity of entity names. Our model is tested on publicly available cross-lingual datasets and cross-resource datasets, and the experimental results demonstrate the effectiveness of our model.", 
    "abstract": "The bacteria Mycobacterium tuberculosis is responsible for the infectious disease Tuberculosis. Targeting the tubercule bacteria is an important challenge in developing the antimycobacterials. The glyoxylate cycle is considered as a potential target for the development of anti-tuberculosis agents, due to its absence in the humans. Humans only possess tricarboxylic acid cycle, while this cycle gets connected to glyoxylate cycle in microbes. Glyoxylate cycle is essential to the Mycobacterium for its growth and survival. Due to this reason, it is considered as a potential therapeutic target for the development of anti-tuberculosis agents. Here, we explore the effect on the behavior of the tricarboxylic acid cycle, glyoxylate cycle and their integrated pathway with the bioenergetics of the Mycobacterium, under the inhibition of key glyoxylate cycle enzymes using Continuous Petri net. Continuous Petri net is a special Petri net used to perform the quantitative analysis of the networks. We first study the tricarboxylic acid cycle and glyoxylate cycle of the tubercule bacteria by simulating its Continuous Petri net model under different scenarios. Both the cycles are then integrated with the bioenergetics of the bacteria and the integrated pathway is again simulated under different conditions. The simulation graphs show the metabolic consequences of inhibiting the key glyoxylate cycle enzymes and adding the uncouplers on the individual as well as integrated pathway. The uncouplers that inhibit the synthesis of adenosine triphosphate, play an important role as anti-mycobacterials. The simulation study done here validates the proposed Continuous Petri net model as compared with the experimental outcomes and also explains the consequences of the enzyme inhibition on the biochemical reactions involved in the metabolic pathways of the mycobacterium.", 
    "abstract": "Extensive reviews of functional analysis literature were conducted 10 (Beavers et al.,\u00a02013) and 20 (Hanley et al.,\u00a02003) years ago; we expanded this review to capture the vast and innovative functional analysis research that has occurred over the past decade. Our review produced 1,333 functional analysis outcomes from 326 studies on the functional analysis of problem behavior between June 2012 and May 2022. Some characteristics of functional analysis studies were similar across the current and previous two reviews (e.g., child participants, developmental disability diagnosis, use of line graphs depicting session means, differentiated response outcomes). Other characteristics deviated from the previous two reviews (e.g., increase in autistic representation, outpatient settings, use of supplementary assessments, the inclusion of tangible conditions, and multiple function outcomes; decrease in session durations). We update previously reported participant and methodological characteristics, summarize outcomes, comment on recent trends, and propose future directions in the functional analysis literature.", 
    "abstract": "Read-overlap-based graph data structures play a central role in computing de novo genome assembly. Most long-read assemblers use Myers's string graph model to sparsify overlap graphs. Graph sparsification improves assembly contiguity by removing spurious and redundant connections. However, a graph model must be coverage-preserving, i.e. it must ensure that there exist walks in the graph that spell all chromosomes, given sufficient sequencing coverage. This property becomes even more important for diploid genomes, polyploid genomes, and metagenomes where there is a risk of losing haplotype-specific information.\nWe develop a novel theoretical framework under which the coverage-preserving properties of a graph model can be analyzed. We first prove that de Bruijn graph and overlap graph models are guaranteed to be coverage-preserving. We next show that the standard string graph model lacks this guarantee. The latter result is consistent with prior work suggesting that removal of contained reads, i.e. the reads that are substrings of other reads, can lead to coverage gaps during string graph construction. Our experiments done using simulated long reads from HG002 human diploid genome show that 50 coverage gaps are introduced on average by ignoring contained reads from nanopore datasets. To remedy this, we propose practical heuristics that are well-supported by our theoretical results and are useful to decide which contained reads should be retained to avoid coverage gaps. Our method retains a small fraction of contained reads (1-2%) and closes majority of the coverage gaps.\nSource code is available through GitHub (https://github.com/at-cg/ContainX) and Zenodo with doi: 10.5281/zenodo.7687543.", 
    "abstract": "Habitat restoration is an effective method for improving landscape connectivity, which can reduce habitat fragmentation. Maintaining landscape connectivity could promote connections between habitat, which is extremely essential to preserve gene flow and population viability. This study proposes a methodological framework to analyze landscape connectivity for Asian elephant habitat conservation, aiming to provide practical options for reducing habitat fragmentation and improving habitat connectivity. Our approach involved combining a species distribution model using MaxEnt and landscape functional connectivity models using graph theory to assess the impact on connectivity improvement via farmland/plantation restoration as habitat. The results showed that: (1) there were 119 suitable habitat patches of Asian elephant covering a total area of 1952.41 km", 
    "abstract": "A simple, portable, economical low-temperature atmospheric plasma (LTAP) for bactericidal efficacy of Gram-negative bacteria (Pseudomonas aeruginosa) with different carrier gases (argon, helium, and nitrogen) using the quality by design (QbD) approach, design of experiments (DoE), and response surface graphs (RSG) is presented. Box-Behnken design was used as the DoE to narrow down and further optimize the experimental factors of LTAP. Plasma exposure time, input DC voltage, and carrier gas flow rate were varied to examine the bactericidal efficacy using the zone of inhibition (ZOI). A higher bactericidal efficacy was achieved under the optimal bactericidal factors having ZOI of 50.837\u00a0\u00b1\u00a02.418\u2009mm", 
    "abstract": "An instance of the non-preemptive tree packing problem consists of an undirected graph ", 
    "abstract": "Risk evaluation has always been of great interest for individuals wanting to invest in various businesses, especially in the marketing and product sale centres. A finely detailed evaluation of the risk factor can lead to better returns in terms of investment in a particular business. Considering this idea, this paper aims to evaluate the risk factor of investing in different nature of products in a supermarket for a better proportioning of investment based on the product's sales. This is achieved using novel Picture fuzzy Hypersoft Graphs. Picture Fuzzy Hypersoft set (PFHSs) is employed in this technique, a hybrid structure of Picture Fuzzy set and Hypersoft Set. These structures work best for evaluating uncertainty using membership, non-membership, neutral, and multi-argument functions, making them ideal for Risk Evaluation studies. Also, the concept of the PFHS graph with the help of the PFHS set is introduced with some operations like the cartesian product, composition, union, direct product, and lexicographic product. This method presented in the paper provides new insight into product sale risk analysis with a pictorial representation of its associated factors.", 
    "abstract": "Drug discovery is accelerated with computational methods such as alchemical simulations to estimate ligand affinities. In particular, relative binding free energy (RBFE) simulations are beneficial for lead optimization. To use RBFE simulations to compare prospective ligands ", 
    "abstract": "Education and training delivered within ambulance services is vital to clinicians maintaining competence, confidence and currency. Simulation and debrief in medical education aims to imitate clinical experience and provide real-time feedback. The South Western Ambulance Service NHS Foundation Trust employs senior doctors in their learning and development (L&D) team to support the development of 'train the trainer' courses for L&D officers (LDOs). This short report of a quality improvement initiative describes the implementation and evaluation of a simulation-debrief model of paramedic education.\nA quality improvement design was adopted. The train the trainer scenarios for simulation-debrief were designed and written following the trust's training needs analysis by the L&D team. The course ran for two days, and each scenario was facilitated by faculty experienced in simulation (both doctors and paramedics). Low-fidelity mannequins and standard ambulance training kit was used (including response bags, training monitor and defibrillator). Participants' pre- and post-scenario self-reported confidence scores were recorded, and qualitative feedback requested. Numerical data were analysed, and collated into graphs using Excel. Thematic analysis of comments was used to present qualitative themes. The SQUIRE 2.0 checklist for reporting quality improvement initiatives was used to frame this short report.\nForty-eight LDOs attended across three courses. All participants reported improved confidence scores in the clinical topic covered after each simulation-debrief scenario, with a minority reporting equivocal scores. Formal qualitative feedback from participants indicated an overwhelmingly positive response to the introduction of simulation-debrief as an education method, and a move away from summative, assessment-based training. The positive value of a multidisciplinary faculty was also reported.\nThe simulation-debrief model of paramedic education represents a move away from the use of didactic teaching and 'tick box'-style assessments in previous train the trainer courses. The introduction of simulation-debrief teaching methodology has had a positive impact on paramedics' confidence in the selected clinical topics, and is seen by LDOs as an effective and valuable education method.", 
    "abstract": "Humans tend to systematically underestimate exponential growth and perceive it in linear terms, which can have severe consequences in a variety of fields. Recent studies attempted to examine the origins of this bias and to mitigate it by using the logarithmic vs. the linear scale in graphical representations. However, they yielded conflicting results as to which scale induces more perceptual errors. In the current study, in an experiment with a short educational intervention, we further examine the factors modulating the exponential bias in graphs and suggest a theoretical explanation for our findings. Specifically, we test the hypothesis that each of the scales can induce misperceptions in a particular context. In addition to this, we explore the effect of mathematical education by testing two groups of participants (with a background in humanities vs. formal sciences). The results of this study confirm that when used in an inadequate context, these scales can have a dramatic effect on the interpretation of visualizations representing exponential growth. In particular, while the log scale leads to more errors in graph description tasks, the linear scale misleads people when they have to make predictions on the future trajectory of exponential growth. The second part of the study revealed that the difficulties with both scales can be reduced by means of a short educational intervention. Importantly, while no difference between participants groups was observed prior to the intervention, participants with a better mathematical education showed a stronger learning effect at posttest. The findings of this study are discussed in light of a dual-process model.", 
    "abstract": "", 
    "abstract": "To determine the prevalence and determinants of different types of echocardiographic left ventricular remodeling in African black hypertensive patients.\nA transversal descriptive study was conducted from January 1, 2015, to March 31, 2016, in the external explorations department of the Abidjan Heart Institute-C\u00f4te d'Ivoire. Transthoracic cardiac echo-graphs were performed in 524 (251 women) hypertensive subjects according to the American Society of Echocardiography convention.\nTwenty-nine percent of hypertensive patients had cardiac remodeling (in women and men, respectively, concentric remodeling 14.7 % and 15.7 %; concentric hypertrophy 6 % and 10.3 %; eccentric hypertrophy 7.6 % and 3.7 %). Only systolic and diastolic blood pressure levels were significantly correlated with left ventricular mass indexed to body surface area.\nThis study showed a significant proportion of hypertensives with abnormal left ventricular geometry and confirmed the relationship between blood pressure level and left ventricular geometry change.", 
    "abstract": "Epilepsy is a brain disorder consisting of abnormal electrical discharges of neurons resulting in epileptic seizures. The nature and spatial distribution of these electrical signals make epilepsy a field for the analysis of brain connectivity using artificial intelligence and network analysis techniques since their study requires large amounts of data over large spatial and temporal scales. For example, to discriminate states that would otherwise be indistinguishable from the human eye. This paper aims to identify the different brain states that appear concerning the intriguing seizure type of epileptic spasms. Once these states have been differentiated, an attempt is made to understand their corresponding brain activity.\nThe representation of brain connectivity can be done by graphing the topology and intensity of brain activations. Graph images from different instants within and outside the actual seizure are used as input to a deep learning model for classification purposes. This work uses convolutional neural networks to discriminate the different states of the epileptic brain based on the appearance of these graphs at different times. Next, we apply several graph metrics as an aid to interpret what happens in the brain regions during and around the seizure.\nResults show that the model consistently finds distinctive brain states in children with epilepsy with focal onset epileptic spasms that are indistinguishable under the expert visual inspection of EEG traces. Furthermore, differences are found in brain connectivity and network measures in each of the different states.\nComputer-assisted discrimination using this model can detect subtle differences in the various brain states of children with epileptic spasms. The research reveals previously undisclosed information regarding brain connectivity and networks, allowing for a better understanding of the pathophysiology and evolving characteristics of this particular seizure type. From our data, we speculate that the prefrontal, premotor, and motor cortices could be more involved in a hypersynchronized state occurring in the few seconds immediately preceding the visually evident EEG and clinical ictal features of the first spasm in a cluster. On the other hand, a disconnection in centro-parietal areas seems a relevant feature in the predisposition and repetitive generation of epileptic spasms within clusters.", 
    "abstract": "The prevalence of intra-articular knee injuries and reparative surgeries is increasing in many countries. Alarmingly, there is a risk of developing post-traumatic osteoarthritis (PTOA) after sustaining a serious intra-articular knee injury. Although physical inactivity is suggested as a risk factor contributing to the high prevalence of the condition, there is a paucity of research characterising the association between physical activity and joint health. Consequently, the primary aim of this review will be to identify and present available empirical evidence regarding the association between physical activity and joint degeneration after intra-articular knee injury and summarise the evidence using an adapted Grading of Recommendations Assessment, Development and Evaluations. The secondary aim will be to identify potential mechanistic pathways through which physical activity could influence PTOA pathogenesis. The tertiary aim will be to highlight gaps in current understanding of the association between physical activity and joint degeneration following joint injury.\nA scoping review will be conducted using the Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews checklist and best-practice recommendations. The review will be guided by the following research question: what is the role of physical activity in the trajectory from intra-articular knee injury to PTOA in young men and women? We will identify primary research studies and grey literature by searching the electronic databases Scopus, Embase: Elsevier, PubMed, Web of Science: all databases, and Google Scholar. Reviewing pairs will screen abstracts, full texts and will extract data. Data will be presented descriptively using charts, graphs, plots and tables.\nThis research does not require ethical approval due to the data being published and publicly available. This review will be submitted for publication in a peer-reviewed sports medicine journal irrespective of discoveries and disseminated through scientific conference presentations and social media.\nhttps://osf.io/84pnh/.", 
    "abstract": "Local health departments (LHDs) need timely county-level and subcounty-level data to monitor health-related trends, identify health disparities, and inform areas of highest need for interventions as part of their ongoing assessment responsibilities; yet, many health departments rely on secondary data that are not timely and cannot provide subcounty insights.\nWe developed and evaluated a mental health dashboard in Tableau for an LHD audience featuring statewide syndromic surveillance emergency department (ED) data in North Carolina from the North Carolina Disease Event Tracking and Epidemiologic Collection Tool (NC DETECT).\nWe developed a dashboard that provides counts, crude rates, and ED visit percentages at statewide and county levels, as well as breakdowns by zip code, sex, age group, race, ethnicity, and insurance coverage for 5 mental health conditions. We evaluated the dashboards through semistructured interviews and a Web-based survey that included the standardized usability questions from the System Usability Scale.\nConvenience sample of LHD public health epidemiologists, health educators, evaluators, and public health informaticians.\nSix semistructured interview participants successfully navigated the dashboard but identified usability issues when asked to compare county-level trends displayed in different outputs (eg, tables vs graphs). Thirty respondents answered all questions on the System Usability Scale for the dashboard, which received an above average score of 86.\nThe dashboards scored well on the System Usability Scale, but more research is needed to identify best practices in disseminating multiyear syndromic surveillance ED visit data on mental health conditions to LHDs.", 
    "abstract": "The Gene Ontology (GO) knowledgebase (http://geneontology.org) is a comprehensive resource concerning the functions of genes and gene products (proteins and noncoding RNAs). GO annotations cover genes from organisms across the tree of life as well as viruses, though most gene function knowledge currently derives from experiments carried out in a relatively small number of model organisms. Here, we provide an updated overview of the GO knowledgebase, as well as the efforts of the broad, international consortium of scientists that develops, maintains, and updates the GO knowledgebase. The GO knowledgebase consists of three components: (1) the GO-a computational knowledge structure describing the functional characteristics of genes; (2) GO annotations-evidence-supported statements asserting that a specific gene product has a particular functional characteristic; and (3) GO Causal Activity Models (GO-CAMs)-mechanistic models of molecular \"pathways\" (GO biological processes) created by linking multiple GO annotations using defined relations. Each of these components is continually expanded, revised, and updated in response to newly published discoveries and receives extensive QA checks, reviews, and user feedback. For each of these components, we provide a description of the current contents, recent developments to keep the knowledgebase up to date with new discoveries, and guidance on how users can best make use of the data that we provide. We conclude with future directions for the project.", 
    "abstract": "Primary postpartum hemorrhage is still the main cause of maternal death worldwide, especially in low-resource nations like Ethiopia where there are insufficient healthcare facilities and a shortage of skilled medical personnel. Data on the prevalence of primary postpartum hemorrhage in the study population are scarce or non-existent.\nThe aim of this study was to assess the prevalence of primary postpartum hemorrhage and its associated factors among delivering women in Gedeo Zone, Southern Ethiopia, in 2021.\nA facility-based cross-sectional study was carried out from January 1 to March 30, 2021, in public health facilities in the Gedeo Zone. A randomly selected 577 participants were involved in the study. Data were gathered using an interview-administered, pre-tested, structured questionnaire. The gathered information was imported into Epi Info 3.5.1 and analyzed with SPSS 23. Descriptive data was presented using tables and graphs. A logistic regression model was fitted. A bivariable and multivariable logistic regression model was computed to identify the presence and strength of association. To run multivariable logistic regression analyses, variables with \nThe magnitude of primary postpartum hemorrhage was 4.2% (95% CI: 2.4-6.0). Postpartum hemorrhage was significantly associated with current antepartum hemorrhage (AOR = 11.67; 95%CI: 7.17-16.17), twin delivery (AOR = 6.59, 95%CI: 1.48-11.70), uterine atony (AOR = 8.45, 95%CI: 4.35-12.55), and prolonged labor (AOR = 5.6, 95%CI: 2.9-8.50).\nThe prevalence of primary postpartum hemorrhages in the Gedeo Zone, Southern Ethiopia was 4.2%. Current ante partum hemorrhage, twin delivery, uterine atony, and prolonged labor were predictors of primary postpartum hemorrhage. The results back up the necessity for care in the early postpartum period so that clinicians may quickly identify any issues, prevent and start treating excessive blood loss early, and, taking into account the aforementioned factors, possibly reduce the frequency of primary postpartum hemorrhage.", 
    "abstract": "Annotation of the mass signals is still the biggest bottleneck for the untargeted mass spectrometry analysis of complex mixtures. Molecular networks are being increasingly adopted by the mass spectrometry community as a tool to annotate large-scale experiments. We have previously shown that the process of propagating annotations from spectral library matches on molecular networks can be automated using Network Annotation Propagation (NAP). One of the limitations of NAP is that the information for the spectral matches is only propagated locally, to the first neighbor of a spectral match. Here, we show that annotation propagation can be expanded to nodes not directly connected to spectral matches using random walks on graphs, introducing the ChemWalker python library.\nSimilarly to NAP, ChemWalker relies on combinatorial in silico fragmentation results, performed by MetFrag, searching biologically relevant databases. Departing from the combination of a spectral network and the structural similarity among candidate structures, we have used MetFusion Scoring function to create a weight function, producing a weighted graph. This graph was subsequently used by the random walk to calculate the probability of 'walking' through a set of candidates, departing from seed nodes (represented by spectral library matches). This approach allowed the information propagation to nodes not directly connected to the spectral library match. Compared with NAP, ChemWalker has a series of improvements, on running time, scalability and maintainability and is available as a standalone python package.\nChemWalker is freely available at https://github.com/computational-chemical-biology/ChemWalker.\nridasilva@usp.br.\nSupplementary data are available at Bioinformatics online.", 
    "abstract": "Osteoarthritis (OA) and lower back pain (LBP) are most common health problems which lead to pain and disability. This study aimed to systematically review the evidence to find any relationship between knee osteoarthritis (KOA) and LBP or any potential causation.\nThe databases of Scopus, MEDLINE, and Embase were searched from inception to 01 October 2022. Any study published in English assessing live humans over 18 years with KOA and LBP was eligible to be included. Studies were independently screened by two researchers. Data of the included studies were extracted based on the participants, outcomes related to knee and lumbar spine, reported association or causation between LBP and KOA, and study design. Data were narratively analyzed and presented as graphs and table. Methodology quality was assessed.\nOf 9953 titles and abstracts, duplicates were removed, and 7552 were screened. Altogether, 88 full texts were screened, and 13 were eligible for the final inclusion. There were some biomechanical and clinical causations were observed for the concurrent presence of LBP and KOA. Biomechanically, high pelvic incidence is a risk factor for development of spondylolisthesis and KOA. Clinically, knee pain intensity was higher in KOA when presents with LBP. Less than 20% of studies have justified their sample size during the quality assessment.\nDevelopment and progression of KOA in patients with degenerative spondylolisthesis may be induced by significantly greater mismatches of lumbo-pelvic sagittal alignment. Elderly patients with degenerative lumbar spondylolisthesis and severe KOA reported a different pelvic morphology, increased sagittal malalignment with a lack of lumbar lordosis due to double-level listhesis, and greater knee flexion contracture than in patients with no to mild and moderate KOA. People with concurrent LBP and KOA have reported poor function with more disability. Both LBP and lumbar kyphosis indicate functional disability and knee symptoms in patients with KOA.\nDifferent biomechanical and clinical causations were revealed for the concurrent existence of KOA and LBP. Therefore, careful assessment of both back and knee joints should be considered when treating KOA and vice versa.\nPROSPERO CRD42022238571.", 
    "results": "Of 9953 titles and abstracts, duplicates were removed, and 7552 were screened. Altogether, 88 full texts were screened, and 13 were eligible for the final inclusion. There were some biomechanical and clinical causations were observed for the concurrent presence of LBP and KOA. Biomechanically, high pelvic incidence is a risk factor for development of spondylolisthesis and KOA. Clinically, knee pain intensity was higher in KOA when presents with LBP. Less than 20% of studies have justified their sample size during the quality assessment.", 
    "abstract": "In this communication, the joint impacts of the process of melting as well as wedge angle entity on hydromagnetic hyperbolic tangent nanofluid flow owing to permeable wedge-shaped surface in the incidence of suspended nanoparticles along with radiation, Soret and Dufour numbers are scrutinized. The mathematical model which represents the system consists of a system of highly non-linear coupled partial differential equations. These equations are solved using a finite-difference-based MATLAB solver which implements the Lobatto IIIa collocation formula and is fourth-order accurate. Further, the comparison of computed results is carried out with the previously reported articles and outstanding conformity is recorded. Emerged physical entities affecting the bearings of tangent hyperbolic MHD nanofluid velocity, distribution of temperature, and concentration of nanoparticles are visualized in graphs. In another line, shearing stress, the surface gradient of heat transfer, and volumetric rate of concentration are recorded in tabular form. Most interestingly, momentum boundary layer thickness and thicknesses of thermal as well as solutal boundary layers enhance with an increment of Weissenberg number. Moreover, an increment on tangent hyperbolic nanofluid velocity and decrement on the thickness of momentum boundary layer is visualized for the increment of numerical values of power-law index entity, which can determine the behavior of shear-thinning fluids.This study has applications for coating materials used in chemical engineering, such as strong paints, aerosol manufacturing, and thermal treatment of water-soluble solutions.", 
    "abstract": "", 
    "abstract": "Current analysis highlights the aspects of different nanoparticles in peristalsis with entropy generation. Mathematical equations of considered problem are modelled via conservation laws for mass, momentum and energy. Such equations contain variable viscosity, nonlinear thermal radiation, viscous dissipation, heat generation/absorption and mixed convection aspects. Boundary conditions comprise the second order velocity and first order thermal slip effects. Entropy expression is obtained by utilization thermodynamics. Simplified and dimensionless forms of the considered conservative laws are obtained through lubrication technique. Resulting system of equations subject to the considered boundary conditions is solved numerically via built-in shooting procedure in Mathematica. Such numerical procedure is very suitable to obtain numerical results directly and fastly in the form of graphs. Further all the considered flow quantities are discussed graphically for the significant parameters of interest in detail. Both velocity and temperature are decreasing against large volume fraction parameter. Increasing temperature dependent viscosity effects decrease the entropy and enhance the Bejan number.", 
    "abstract": "Advances in spatial transcriptomics enlarge the use of single cell technologies to unveil the expression landscape of the tissues with valuable spatial context. Here, we propose an unsupervised and manifold learning-based algorithm, Spatial Transcriptome based cEll typE cLustering (STEEL), which identifies domains from spatial transcriptome by clustering beads exhibiting both highly similar gene expression profiles and close spatial distance in the manner of graphs. Comprehensive evaluation of STEEL on spatial transcriptomic datasets from 10X Visium platform demonstrates that it not only achieves a high resolution to characterize fine structures of mouse brain but also enables the integration of multiple tissue slides individually analyzed into a larger one. STEEL outperforms previous methods to effectively distinguish different cell types/domains of various tissues on Slide-seq datasets, featuring in higher bead density but lower transcript detection efficiency. Application of STEEL on spatial transcriptomes of early-stage mouse embryos (E9.5-E12.5) successfully delineates a progressive development landscape of tissues from ectoderm, mesoderm and endoderm layers, and further profiles dynamic changes on cell differentiation in heart and other organs. With the advancement of spatial transcriptome technologies, our method will have great applicability on domain identification and gene expression atlas reconstruction.", 
    "abstract": "Electronic health record (EHR) data are a valuable resource for population health research but lack critical information such as relationships between individuals. Emergency contacts in EHRs can be used to link family members, creating a population that is more representative of a community than traditional family cohorts.\nWe revised a published algorithm: relationship inference from the electronic health record (RIFTEHR). Our version, Pythonic RIFTEHR (P-RIFTEHR), identifies a patient's emergency contacts, matches them to existing patients (when available) using network graphs, checks for conflicts, and infers new relationships. P-RIFTEHR was run on December 15, 2021 in the Northwestern Medicine Electronic Data Warehouse (NMEDW) on approximately 2.95 million individuals and was validated using the existing link between children born at NM hospitals and their mothers. As proof-of-concept, we modeled the association between parent and child obesity using logistic regression.\nThe P-RIFTEHR algorithm matched 1\u200a157\u200a454 individuals in 448\u200a278 families. The median family size was 2, the largest was 32 persons, and 247 families spanned 4 generations or more. Validation of the mother-child pairs resulted in 95.1% sensitivity. Children were 2 times more likely to be obese if a parent is obese (OR: 2.30; 95% CI, 2.23-2.37).\nP-RIFTEHR can identify familiar relationships in a large, diverse population in an integrated health system. Estimates of parent-child inheritability of obesity using family structures identified by the algorithm were consistent with previously published estimates from traditional cohort studies.", 
    "abstract": "Visual analysis is the primary method of analyzing single-case research data, yet relatively little is known about the variables that influence raters' decisions and rater agreement. Previous research has suggested that trend, variability, and autocorrelation may negatively affect interrater agreement, but studies have been limited by small numbers of graphs and participants whose knowledge of single-case research was not described. The purpose of this study was to examine the main and interaction effects of two values of each of six data characteristics (e.g., level, trend, and number of data points) on agreement among visual analysts. Using data from Lanovaz and Hranchuk\u00a0(2021), we examined odds ratios to identify data characteristics that influence interrater agreement. Results suggest that trend and effect size, and to a lesser extent variability, have the largest effects on interrater agreement. We discuss the implications of our results for future research on improving interrater agreement among visual analysts.", 
    "abstract": "Kawasaki disease (KD) is a febrile disease that affects children under 5\u00a0years of age and leads to serious cardiovascular complications such as coronary artery disease. The development of markers that can predict early is important to reduce the under- and misdiagnosis of KD. The aim of this research was to develop a diagnostic predictive model to differentiate Kawasaki disease (KD) from other febrile diseases using eosinophil-to-lymphocyte ratio (ELR) and other biomarkers. We recruited a total of 190 children with KD and 1604 children with other febrile diseases. We retrospectively collected clinical information from the children, which included laboratory data on the day of admission, such as white blood cells (WBC), hemoglobin (HGB), calcitoninogen (PCT), hypersensitive c-reactive protein (CRP), snake prognostic nutritional index (PNI), peripheral blood neutrophil-lymphocyte ratio (NLR), platelet-lymphocyte ratio (PLR), and ELR. We performed analyses using univariate analysis, multivariate logistic regression, and column line plots, and evaluated the diagnostic parameters of the predictive models. ELR was significantly increased in patients with KD. After multivariate logistic regression, WBC, HGB, CRP, NLR, ELR and PNI were finally included as indicators for constructing the prediction model. The ROC curve analysis suggested that the C-index of the diagnostic prediction model was 0.921. The calibration curve showed good diagnostic performance of the columnar graph model. The cut-off value of ELR alone for KD was 0.04, the area under the ROC curve was 0.809. Kids with KD show highly expressive level of ELR compared to children with febrile disease, which can be used to diagnose KD, and column line graphs constructed together with other indicators can help pediatricians to identify KD more effectively from febrile children.", 
    "abstract": "In this paper, the dataset values of the Instrumented Charpy V-notch impact tests (ICITs) of base material (BM), heat affected zone (HAZ) and fusion zone (FZ) of the electron beam welded (EBW) joints of S960M high strength steel (HSS) of the related article have been presented. This dataset provides the force obtained by the ICITs, which can be used to further plot figures and describes the force (F)-displacement (s) graphs of the individual tested samples of the article. The absorbed impact energy measurements in each sample provide information on the material's behaviour under the impact load. The obtained\u00a0absorbed impact energy indicates the material's toughness and whether the material failure will be ductile or brittle under impact load. The force-displacement curves from axial tensile loading of S960M specimens are presented. The graphs give information about the highest load and behaviour of load-displacement in axial tensile load testing. In addition, the microstructure images of the base material, fusion zone and different heat-affected subzones were taken by the optical microscopic and are the other parts of the data. ICITs data were collected during in situ impact testing of high strength structural steel S960M using Heckert instrumented impact testing equipment connected to a four-channel digital oscilloscope. A more detailed interpretation of the data presented in this article. The presented data are produced as part of the main work entitled \"Experimental assessment of microstructure and mechanical properties of electron beam welded S960M high strength structural steel\".", 
    "abstract": "The recognition of hypoxia symptoms is a critical part of physiological training in military aviation. Acute exposure protocols have been designed in hypobaric chambers to train aircrews to recognize hypoxia and quickly take corrective actions. The goal of the acute hypoxia test is to know the time of useful consciousness and the minimal arterial oxygen saturation tolerated. Currently, there is no computer system specifically designed to analyze the physiological variables obtained during the test. This paper reports the development and analytical capabilities of a computational tool specially designed for these purposes. The procedure was designed using the Igor Pro 8.01 language, which processes oxygen saturation and heart rate signals. To accomplish this, three functional boards are displayed. The first allows the loading and processing of the data. The second generates graphs that allow for a rapid visual examination to determine the validity of individual records and calculate slopes on selected segments of the recorded signal. Finally, the third can apply filters to generate data groups for analysis. In addition, this tool makes it possible to propose new study variables that are derived from the raw signals and can be applied simultaneously to large data sets. The program can generate graphs accompanied by basic statistical parameters and heat maps that facilitate data visualization. Moreover, there is a possibility of adding other signals during the test, such as the oxygenation level in vital organs, electrocardiogram, or electroencephalogram, which illustrates the test's excellent potential for application in aerospace medicine and for helping us develop a better understanding of complex physiological phenomena.", 
    "abstract": "This paper presents a method for the multi-criteria classification of data in terms of identifying pneumatic wheel imbalance on the basis of vehicle body vibrations in normal operation conditions. The paper uses an expert system based on search graphs that apply source features of objects and distances from points in the space of classified objects (the metric used). Rules generated for data obtained from tests performed under stationary and road conditions using a chassis dynamometer were used to develop the expert system. The recorded linear acceleration signals of the vehicle body were analyzed in the frequency domain for which the power spectral density was determined. The power field values for selected harmonics of the spectrum consistent with the angular velocity of the wheel were adopted for further analysis. In the developed expert system, the Kamada-Kawai model was used to arrange the nodes of the decision tree graph. Based on the developed database containing learning and testing data for each vehicle speed and wheel balance condition, the probability of the wheel imbalance condition was determined. As a result of the analysis, it was determined that the highest probability of identifying wheel imbalance equal to almost 100% was obtained in the vehicle speed range of 50 km/h to 70 km/h. This is known as the pre-resonance range in relation to the eigenfrequency of the wheel vibrations. As the vehicle speed increases, the accuracy of the data classification for identifying wheel imbalance in relation to the learning data decreases to 50% for the speed of 90 km/h.", 
    "abstract": "The large bandwidths that are available at millimeter-wave frequencies enable fixed wireless access (FWA) applications, in which fixed point-to-point wireless links are used to provide internet connectivity. In FWA networks, a wireless mesh is created and data are routed from the customer premises equipment (CPE) towards the point of presence (POP), which is the interface with the wired internet infrastructure. The performance of the wireless links depends on the radio propagation characteristics, as well as the wireless technology that is used. The radio propagation characteristics depend on the environment and on the considered frequency. In this work, we analyzed the network characteristics of FWA networks using radio propagation models for different wireless technologies using millimeter-wave (mmWave) frequencies of 28 GHz, 60 GHz, and 140 GHz. Different scenarios and environments were considered, and the influence of rain, vegetation, and the number of subscribers was investigated. A network planning algorithm is presented that defines a route for each CPE towards the POP based on a predefined location of customer devices and considering the available capacity of the wireless links. Rain does not have a considerable effect on the system capacity. Even though the higher frequencies exhibit a larger path loss, resulting in a lower power of the received signal, the larger bandwidths enable a higher channel capacity.", 
    "abstract": "The basic types of multi-stable energy harvesters are bistable energy harvesting systems (BEH) and tristable energy harvesting systems (TEH). The present investigations focus on the analysis of BEH and TEH systems, where the corresponding depth of the potential well and the width of their characteristics are the same. The efficiency of energy harvesting for TEH and BEH systems assuming similar potential parameters is provided. Providing such parameters allows for reliable formulation of conclusions about the efficiency in both types of systems. These energy harvesting systems are based on permanent magnets and a cantilever beam designed to obtain energy from vibrations. Starting from the bond graphs, we derived the nonlinear equations of motion. Then, we followed the bifurcations along the increasing frequency for both configurations. To identify the character of particular solutions, we estimated their corresponding phase portraits, Poincare sections, and Lyapunov exponents. The selected solutions are associated with their voltage output. The results in this numerical study clearly show that the bistable potential is more efficient for energy harvesting provided the corresponding excitation amplitude is large enough. However, the tristable potential could work better in the limits of low-level and low-frequency excitations.", 
    "abstract": "The neuroscience community has developed many convolutional neural networks (CNNs) for the early detection of Alzheimer's disease (AD). Population graphs are thought of as non-linear structures that capture the relationships between individual subjects represented as nodes, which allows for the simultaneous integration of imaging and non-imaging information as well as individual subjects' features. Graph convolutional networks (GCNs) generalize convolution operations to accommodate non-Euclidean data and aid in the mining of topological information from the population graph for a disease classification task. However, few studies have examined how GCNs' input properties affect AD-staging performance. Therefore, we conducted three experiments in this work. Experiment 1 examined how the inclusion of demographic information in the edge-assigning function affects the classification of AD versus cognitive normal (CN). Experiment 2 was designed to examine the effects of adding various neuropsychological tests to the edge-assigning function on the mild cognitive impairment (MCI) classification. Experiment 3 studied the impact of the edge assignment function. The best result was obtained in Experiment 2 on multi-class classification (AD, MCI, and CN). We applied a novel framework for the diagnosis of AD that integrated CNNs and GCNs into a unified network, taking advantage of the excellent feature extraction capabilities of CNNs and population-graph processing capabilities of GCNs. To learn high-level anatomical features, DenseNet was used; a set of population graphs was represented with nodes defined by imaging features and edge weights determined by different combinations of imaging or/and non-imaging information, and the generated graphs were then fed to the GCNs for classification. Both binary classification and multi-class classification showed improved performance, with an accuracy of 91.6% for AD versus CN, 91.2% for AD versus MCI, 96.8% for MCI versus CN, and 89.4% for multi-class classification. The population graph's imaging features and edge-assigning functions can both significantly affect classification accuracy.", 
    "abstract": "Efficient navigation in a socially compliant manner is an important and challenging task for robots working in dynamic dense crowd environments. With the development of artificial intelligence, deep reinforcement learning techniques have been widely used in the robot navigation. Previous model-free reinforcement learning methods only considered the interactions between robot and humans, not the interactions between humans and humans. To improve this, we propose a decentralized structured RNN network with coarse-grained local maps (LM-SRNN). It is capable of modeling not only Robot-Human interactions through spatio-temporal graphs, but also Human-Human interactions through coarse-grained local maps. Our model captures current crowd interactions and also records past interactions, which enables robots to plan safer paths. Experimental results show that our model is able to navigate efficiently in dense crowd environments, outperforming state-of-the-art methods.", 
    "abstract": "Smart healthcare systems that make use of abundant health data can improve access to healthcare services, reduce medical costs and provide consistently high-quality patient care. Medical dialogue systems that generate medically appropriate and human-like conversations have been developed using various pre-trained language models and a large-scale medical knowledge base based on Unified Medical Language System (UMLS). However, most of the knowledge-grounded dialogue models only use local structure in the observed triples, which suffer from knowledge graph incompleteness and hence cannot incorporate any information from dialogue history while creating entity embeddings. As a result, the performance of such models decreases significantly. To address this problem, we propose a general method to embed the triples in each graph into large-scalable models and thereby generate clinically correct responses based on the conversation history using the recently recently released MedDialog(EN) dataset. Given a set of triples, we first mask the head entities from the triples overlapping with the patient's utterance and then compute the cross-entropy loss against the triples' respective tail entities while predicting the masked entity. This process results in a representation of the medical concepts from a graph capable of learning contextual information from dialogues, which ultimately aids in leading to the gold response. We also fine-tune the proposed Masked Entity Dialogue (MED) model on smaller corpora which contain dialogues focusing only on the Covid-19 disease named as the Covid Dataset. In addition, since UMLS and other existing medical graphs lack data-specific medical information, we re-curate and perform plausible augmentation of knowledge graphs using our newly created Medical Entity Prediction (MEP) model. Empirical results on the MedDialog(EN) and Covid Dataset demonstrate that our proposed model outperforms the state-of-the-art methods in terms of both automatic and human evaluation metrics.", 
    "abstract": "", 
    "abstract": "Re-shaping of thermodynamics with the graph theory and Ramsey theory is suggested. Maps built of thermodynamic states are addressed. Thermodynamic states may be attainable and non-attainable by the thermodynamic process in the system of constant mass. We address the following question how large should be a graph describing connections between discrete thermodynamic states to guarantee the appearance of thermodynamic cycles? The Ramsey theory supplies the answer to this question. Direct graphs emerging from the chains of irreversible thermodynamic processes are considered. In any complete directed graph, representing the thermodynamic states of the system the Hamiltonian path is found. Transitive thermodynamic tournaments are addressed. The entire transitive thermodynamic tournament built of irreversible processes does not contain a cycle of length 3, or in other words, the transitive thermodynamic tournament is acyclic and contains no directed thermodynamic cycles.", 
    "abstract": "Recent studies have focused on the similarity between ", 
    "abstract": "Interval-valued data is an effective way to represent complex information where uncertainty, inaccuracy etc. are involved in the data space and they are worthy of taking into account. Interval analysis together with neural network has proven to work well on Euclidean data. However, in real-life scenarios, data follows a much more complex structure and is often represented as graphs, which is non-Euclidean in nature. Graph Neural Network is a powerful tool to handle graph like data with countable feature space. So, there is a research gap between the interval-valued data handling approaches and existing GNN model. No model in GNN literature can handle a graph with interval-valued features and, on the other hand, Multi Layer Perceptron (MLP) based on interval mathematics can not process the same due to non-Euclidean structure behind the graph. This article proposes an Interval-Valued Graph Neural Network, a novel GNN model where, for the first time, we relax the restriction of the feature space being countable without compromising the time complexity of the best performing GNN model in the literature. Our model is much more general than existing models as any countable set is always a subset of the universal set ", 
    "abstract": "The dataset presented in this paper consists of a network of interpersonal lending relations from a single village of a deprivated area of Hungary. The data are originated from quantitative surveys from May 2014 to June 2014. The data collection was embedded in a Participatory Action Research (PAR) which aimed to investigate the financial survival strategies of low-income households in a Hungarian village in a disadvantaged region. The directed graphs of lending and borrowing are a unique dataset that empirically captures a hidden and informal financial activity between households. The network contains 164 households and 281 credit connections among them.", 
    "abstract": "This paper seeks to identify the causal impact of educational human capital on social distancing behavior at workplace in Turkey using district-level data for the period of April 2020 - February 2021. We adopt a unified causal framework, predicated on domain knowledge, theory-justified constraints anda data-driven causal structure discovery using causal graphs. We answer our causal query by employing machine learning prediction algorithms; instrumental variables in the presence of latent confounding and Heckman's model in the presence of selection bias. Results show that educated regions are able to distance-work and educational human capital is a key factor in reducing workplace mobility, possibly through its impact on employment. This pattern leads to higher workplace mobility for less educated regions and translates into higher Covid-19 infection rates. The future of the pandemic lies in less educated segments of developing countries and calls for public health action to decrease its unequal and pervasive impact.", 
    "abstract": "The present study aimed to perform the validity and reliability study of the Food Frequency Questionnaire (FFQ) on the frequency of foods rich in antioxidant nutrients and used in Age-Related Eye Diseases (AREDs). In the first interview of the study, the first application of FFQ was carried out, and blank forms of Dietary Records (DRs) were given. For the validity of the FFQ, a total of 12 d (3 days * 4 weeks) of DR were taken. For the reliability of the FFQ, a test-retest application was made with an interval of 4 weeks. The daily intake means of antioxidant nutrients, omega 3 and total antioxidant capacity data obtained from both the FFQ and DR were calculated, and the concordance between the two methods was evaluated with the Pearson Correlation Coefficient (PCC) and Bland-Altman graphs. The present study was carried out at Ege University \u0130zmir/Turkey, Department of Ophthalmology, Retina Unit. The study was conducted with individuals aged \u226550 years who suffered from Age-Related Macular Degeneration (", 
    "abstract": "The history of Earth's biodiversity is punctuated episodically by mass extinctions. These are characterized by major declines of taxon richness, but the accompanying ecological collapse has rarely been evaluated quantitatively. The Permian-Triassic mass extinction (PTME; \u223c252 mya), as the greatest known extinction, permanently altered marine ecosystems and paved the way for the transition from Paleozoic to Mesozoic evolutionary faunas. Thus, the PTME offers a window into the relationship between taxon richness and ecological dynamics of ecosystems during a severe extinction. However, the accompanying ecological collapse through the PTME has not been evaluated in detail. Here, using food-web models and a marine paleocommunity dataset spanning the PTME, we show that after the first extinction phase, community stability decreased only slightly despite the loss of more than half of taxonomic diversity, while community stability significantly decreased in the second phase. Thus, taxonomic and ecological changes were unequivocally decoupled, with species richness declining severely \u223c61 ka earlier than the collapse of marine ecosystem stability, implying that in major catastrophes, a biodiversity crash may be the harbinger of a more devastating ecosystem collapse.", 
    "abstract": "Predicting patients' survival from gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To learn effective WSI representations for survival prediction, existing deep learning methods have explored utilizing graphs to describe the complex structure inner WSIs, where graph node is respective to WSI patch. However, these graphs are often densely-connected or static, leading to some redundant or missing patch correlations. Moreover, these methods cannot be directly scaled to the very-large WSI with more than 10,000 patches. To address these, this paper proposes a scalable graph convolution network, GraphLSurv, which can efficiently learn adaptive and sparse structures to better characterize WSIs for survival prediction.\nGraphLSurv has three highlights in methodology: (1) it generates adaptive and sparse structures for patches so that latent patch correlations could be captured and adjusted dynamically according to prediction tasks; (2) based on the generated structure and a given graph, GraphLSurv further aggregates local microenvironmental cues into a non-local embedding using the proposed hybrid message passing network; (3) to make this network suitable for very large-scale graphs, it adopts an anchor-based technique to reduce theorical computation complexity.\nThe experiments on 2268 WSIs show that GraphLSurv achieves a concordance-index of 0.66132 and 0.68348, with an improvement of 3.79% and 3.41% compared to existing methods, on NLST and TCGA-BRCA, respectively.\nGraphLSurv could often perform better than previous methods, which suggests that GraphLSurv could provide an important and effective means for WSI survival prediction. Moreover, this work empirically shows that adaptive and sparse structures could be more suitable than static or dense ones for modeling WSIs.", 
    "abstract": "Drug-targeted therapies are promising approaches to treating tumors, and research on receptor-ligand interactions for discovering high-affinity targeted drugs has been accelerating drug development. This study presents a mechanism-driven deep learning-based computational model to learn double drug sequences, protein sequences, and drug graphs to project drug-target affinities (DTAs), which was termed the DoubleSG-DTA. We deployed lightweight graph isomorphism networks to aggregate drug graph representations and discriminate between molecular structures, and stacked multilayer squeeze-and-excitation networks to selectively enhance spatial features of drug and protein sequences. What is more, cross-multi-head attentions were constructed to further model the non-covalent molecular docking behavior. The multiple cross-validation experimental evaluations on various datasets indicated that DoubleSG-DTA consistently outperformed all previously reported works. To showcase the value of DoubleSG-DTA, we applied it to generate promising hit compounds of Non-Small Cell Lung Cancer harboring EGFRT790M mutation from natural products, which were consistent with reported laboratory studies. Afterward, we further investigated the interpretability of the graph-based \"black box\" model and highlighted the active structures that contributed the most. DoubleSG-DTA thus provides a powerful and interpretable framework that extrapolates for potential chemicals to modulate the systemic response to disease.", 
    "abstract": "In recent years, nanoparticles have been highly investigated in the laboratory. However, only a few laboratory discoveries have been translated into clinical practice. These findings in the laboratory are limited by trial-and-error methods to determine the optimum formulation for successful drug delivery. A new paradigm is required to ease the translation of lab discoveries to clinical practice. Due to their previous success in antiviral activity, it is vital to accelerate the discovery of novel drugs to treat and manage viruses. Machine learning is a subfield of artificial intelligence and consists of computer algorithms which are improved through experience. It can generate predictions from data inputs via an algorithm which includes a method built from inputs and outputs. Combining nanotherapeutics and well-established machine-learning algorithms can simplify antiviral-drug development systems by automating the analysis. Other relationships in bio-pharmaceutical networks would eventually aid in reaching a complex goal very easily. From previous laboratory experiments, data can be extracted and input into machine learning algorithms to generate predictions. In this study, poly (lactic-co-glycolic acid) (PLGA) nanoparticles were investigated in antiviral drug delivery. Data was extracted from research articles on nanoparticle size, polydispersity index, drug loading capacity and encapsulation efficiency. The Gaussian Process, a form of machine learning algorithm, could be applied to this data to generate graphs with predictions of the datasets. The Gaussian Process is a probabilistic machine learning model which defines a prior over function. The mean and variance of the data can be calculated via matrix multiplications, leading to the formation of prediction graphs-the graphs generated in this study which could be used for the discovery of novel antiviral drugs. The drug load and encapsulation efficiency of a nanoparticle with a specific size can be predicted using these graphs. This could eliminate the trial-and-error discovery method and save laboratory time and ease efficiency.", 
    "abstract": "The hazardous pest known as rice leaf roller (", 
    "abstract": "The aim of this study is to examine the Darcy-Forchheimer flow = of H2O-based Al-Al2O3/Cu-Al2O3 hybrid nanofluid past a heated stretchable plate including heat consumption/ generation and non-linear radiation impacts. The governing flow equations are formulated using the Naiver-Stokes equation. These flow equations are re-framed by using the befitted transformations. The MATLAB bvp4c scheme is utilized to compute the converted flow equations numerically. The graphs, tables, and charts display the vicissitudes in the hybrid nanofluid velocity, hybrid nanofluid temperature, skin friction coefficient, and local Nusselt number via relevant flow factors. It can be seen that the hybrid nanofluid velocity decreased as the magnetic field parameter was increased. The hybrid nanofluid temperature tended to rise as the heat absorption/generation, nanoparticle volume friction, and nonlinear radiation parameters were increased. The surface drag force decreased when the quantity of the magnetic parameter increased. The larger size of the radiation parameter led to enrichment of the heat transmission gradient.", 
    "abstract": "Multidisciplinary clinical decision-making has become increasingly important for complex diseases, such as cancers, as medicine has become very specialized. Multiagent systems (MASs) provide a suitable framework to support multidisciplinary decisions. In the past years, a number of agent-oriented approaches have been developed on the basis of argumentation models. However, very limited work has focused, thus far, on systematic support for argumentation in communication among multiple agents spanning various decision sites and holding varying beliefs. There is a need for an appropriate argumentation scheme and identification of recurring styles or patterns of multiagent argument linking to enable versatile multidisciplinary decision applications. We propose, in this paper, a method of linked argumentation graphs and three types of patterns corresponding to scenarios of agents changing the minds of others (argumentation) and their own (belief revision): the collaboration pattern, the negotiation pattern, and the persuasion pattern. This approach is demonstrated using a case study of breast cancer and lifelong recommendations, as the survival rates of diagnosed cancer patients are rising and comorbidity is the norm.", 
    "abstract": "Linear codes with a few weights have been extensively studied due to their wide applications in secret sharing schemes, strongly regular graphs, association schemes, and authentication codes. In this paper, we choose the defining sets from two distinct weakly regular plateaued balanced functions, based on a generic construction of linear codes. Then we construct a family of linear codes with at most five nonzero weights. Their minimality is also examined and the result shows that our codes are helpful in secret sharing schemes.", 
    "abstract": "In this paper, the Sun and its behavior are studied by means of complex networks. The complex network was built using the Visibility Graph algorithm. This method maps time series into graphs in which every element of the time series is considered as a node and a visibility criterion is defined in order to connect them. Using this method, we construct complex networks for magnetic field and sunspots time series encompassing four solar cycles, and various measures such as degree, clustering coefficient, mean path length, betweenness centrality, eigenvector centrality and decay exponents were calculated. In order to study the system in several time scales, we perform both a global, where the network contains information on the four solar cycles, and a local analysis, involving moving windows. Some metrics correlate with solar activity, while others do not. Interestingly, those metric which seem to respond to varying levels of solar activity in the global analysis, also do in the moving windows analysis. Our results suggest that complex networks can provide a useful way to follow solar activity, and reveal new features on solar cycles.", 
    "abstract": "A discrete version of opinion dynamics systems, based on the Biswas-Chatterjee-Sen (BChS) model, has been studied on Barab\u00e1si-Albert networks (BANs). In this model, depending on a pre-defined noise parameter, the mutual affinities can assign either positive or negative values. By employing extensive computer simulations with Monte Carlo algorithms, allied with finite-size scaling hypothesis, second-order phase transitions have been observed. The corresponding critical noise and the usual ratios of the critical exponents have been computed, in the thermodynamic limit, as a function of the average connectivity. The effective dimension of the system, defined through a hyper-scaling relation, is close to one, and it turns out to be connectivity-independent. The results also indicate that the discrete BChS model has a similar behavior on ", 
    "abstract": "To treat diseases caused by genetic variants, it is necessary to identify disease-causing variants in patients. However, since there are a large number of disease-causing variants, the application of AI is required. We propose AI to solve this problem and report the results of its application in identifying disease-causing variants.\nTo assist physicians in their task of identifying disease-causing variants, we propose an explainable AI (XAI) that combines high estimation accuracy with explainability using a knowledge graph. We integrated databases for genomic medicine and constructed a large knowledge graph that was used to achieve the XAI.\nWe compared our XAI with random forests and decision trees.\nWe propose an XAI that uses knowledge graphs for explanation. The proposed method achieves high estimation performance and explainability. This will support the promotion of genomic medicine.", 
    "abstract": "Epilepsy surgery is a viable therapy option for patients with pharmacoresistant focal epilepsies. A prerequisite for postoperative seizure freedom is the localization of the epileptogenic zone, e.g., using electro- and magnetoencephalography (EEG/MEG). Evidence shows that resting state MEG contains subtle alterations, which may add information to the workup of epilepsy surgery. Here, we investigate node degree (ND), a graph-theoretical parameter of functional connectivity, in relation to the seizure onset zone (SOZ) determined by invasive EEG (iEEG) in a consecutive series of 50 adult patients. Resting state data were subjected to whole brain, all-to-all connectivity analysis using the imaginary part of coherence. Graphs were described using parcellated ND. SOZ localization was investigated on a lobar and sublobar level. On a lobar level, all frequency bands except alpha showed significantly higher maximal ND (mND) values inside the SOZ compared to outside (ratios 1.11-1.20, alpha 1.02). Area-under-the-curve (AUC) was 0.67-0.78 for all expected alpha (0.44, ns). On a sublobar level, mND inside the SOZ was higher for all frequency bands (1.13-1.38, AUC 0.58-0.78) except gamma (1.02). MEG ND is significantly related to SOZ in delta, theta and beta bands. ND may provide new localization tools for presurgical evaluation of epilepsy surgery.", 
    "abstract": "Named entity recognition aims to identify entities from unstructured text and is an important subtask for natural language processing and building knowledge graphs. Most of the existing entity recognition methods use conditional random fields as label decoders or use pointer networks for entity recognition. However, when the number of tags is large, the computational cost of method based on conditional random fields is high and the problem of nested entities cannot be solved. The pointer network uses two modules to identify the first and the last of the entities separately, and a single module can only focus on the information of the first or the last of the entities, but cannot pay attention to the global information of the entities. In addition, the neural network model has the problem of local instability. To solve mentioned problems, a named entity recognition model based on global pointer and adversarial training is proposed. To obtain global entity information, global pointer is used to decode entity information, and rotary relative position information is considered in the model designing to improve the model's perception of position; to solve the model's local instability problem, adversarial training is used to improve the robustness and generalization of the model. The experimental results show that the F1 score of the model are improved on several public datasets of OntoNotes5, MSRA, Resume, and Weibo compared with the existing mainstream models.", 
    "abstract": "Graph theoretical molecular descriptors alias topological indices are a convenient means for expressing in numerical form the chemical structure encoded in a molecular graph. The structure descriptors derived from molecular graphs are widely used in quantitative structure-property relationship (QSPR) and quantitative structure-activity relationship (QSAR) studies. The reason for introducing new indices is to obtain predictions of target properties of considered molecules that are better than the predictions obtained using already known indices. In this paper, we apply the reduced reverse degree based indices introduced in 2021 by Vignesh et al. In the QSPR analysis, we first compute the reduced reverse degree based indices for a family of benzenoid hydrocarbon molecules and then we obtain the correlation with the Physico-chemical properties of the considered molecules. We show that all the properties taken into consideration for the benzenoid hydrocarbons can be very effectively predicted by the reduced reverse degree based indices. Also, we have compared the predictive capability of reduced reverse degree based topological descriptors against 16 existing degree based indices. Further, we compute the defined reduced reverse degree based topological indices for Hyaluronic Acid-Paclitaxel Conjugates [Formula: see text], [Formula: see text].", 
    "abstract": "Local environmental interactions are a major factor in determining the success of a new mutant in structured populations. Spatial variations in the concentration of genotype-specific resources change the fitness of competing strategies locally and thus can drastically change the outcome of evolutionary processes in unintuitive ways. The question is how such local environmental variations in network population structures change the condition for selection and fixation probability of an advantageous (or deleterious) mutant. We consider linear graph structures and focus on the case where resources have a spatial periodic pattern. This is the simplest model with two parameters, length scale and fitness scales, representing heterogeneity. We calculate fixation probability and fixation times for a constant population birth-death process as fitness heterogeneity and period vary. Fixation probability is affected by not only the level of fitness heterogeneity but also spatial scale of resources variations set by period of distribution T. We identify conditions for which a previously a deleterious mutant (in a uniform environment) becomes beneficial as fitness heterogeneity is increased. We observe cases where the fixation probability of both mutant and resident types are more than their neutral value, 1/N, simultaneously. This coincides with exponential increase in time to fixation which points to potential coexistence of resident and mutant types. Finally, we discuss the effect of the 'fitness shift' where the fitness function of two types has a phase difference. We observe significant increases (or decreases) in the fixation probability of the mutant as a result of such phase shift.", 
    "abstract": "The structure of social networks strongly affects how different phenomena spread in human society, from the transmission of information to the propagation of contagious diseases. It is well-known that heterogeneous connectivity strongly favors spread, but a precise characterization of the redundancy present in social networks and its effect on the robustness of transmission is still lacking. This gap is addressed by the metric backbone, a weight- and connectivity-preserving subgraph that is sufficient to compute all shortest paths of weighted graphs. This subgraph is obtained via algebraically-principled axioms and does not require statistical sampling based on null-models. We show that the metric backbones of nine contact networks obtained from proximity sensors in a variety of social contexts are generally very small, 49% of the original graph for one and ranging from about 6% to 20% for the others. This reflects a surprising amount of redundancy and reveals that shortest paths on these networks are very robust to random attacks and failures. We also show that the metric backbone preserves the full distribution of shortest paths of the original contact networks-which must include the shortest inter- and intra-community distances that define any community structure-and is a primary subgraph for epidemic transmission based on pure diffusion processes. This suggests that the organization of social contact networks is based on large amounts of shortest-path redundancy which shapes epidemic spread in human populations. Thus, the metric backbone is an important subgraph with regard to epidemic spread, the robustness of social networks, and any communication dynamics that depend on complex network shortest paths.", 
    "abstract": "Although the Internet and social media provide people with a range of opportunities and benefits in a variety of ways, the proliferation of fake news has negatively affected society and individuals. Many efforts have been invested to detect the fake news. However, to learn the representation of fake news by context information, it has brought many challenges for fake news detection due to the feature sparsity and ineffectively capturing the non-consecutive and long-range context. In this paper, we have proposed Intra-graph and Inter-graph Joint Information Propagation Network (abbreviated as IIJIPN) with Third-order Text Graph Tensor for fake news detection. Specifically, data augmentation is firstly utilized to solve the data imbalance and strengthen the small corpus. In the stage of feature extraction, Third-order Text Graph Tensor with sequential, syntactic, and semantic features is proposed to describe contextual information at different language properties. After constructing the text graphs for each text feature, Intra-graph and Inter-graph Joint Information Propagation is used for encoding the text: intra-graph information propagation is performed in each graph to realize homogeneous information interaction, and high-order homogeneous information interaction in each graph can be achieved by stacking propagation layer; inter-graph information propagation is performed among text graphs to realize heterogeneous information interaction by connecting the nodes across the graphs. Finally, news representations are generated by attention mechanism consisting of graph-level attention and node-level attention mechanism, and then news representations are fed into a fake news classifier. The experimental results on four public datasets indicate that our model has outperformed state-of-the-art methods. Our source code is available at https://github.com/cuibenkuan/IIJIPN.", 
    "abstract": "Real-time neuron detection and neural activity extraction are critical components of real-time neural decoding. They are modeled effectively in dataflow graphs. However, these graphs and the components within them in general have many parameters, including hyper-parameters associated with machine learning sub-systems. The dataflow graph parameters induce a complex design space, where alternative configurations (design points) provide different trade-offs involving key operational metrics including accuracy and time-efficiency. In this paper, we propose a novel optimization framework that automatically configures the parameters in different neural decoders. The proposed optimization framework is evaluated in depth through two case studies. Significant performance improvement in terms of accuracy and efficiency is observed in both case studies compared to the manual parameter optimization that was associated with the published results of those case studies. Additionally, we investigate the application of efficient multi-threading strategies to speed-up the running time of our parameter optimization framework. Our proposed optimization framework enables efficient and effective estimation of parameters, which leads to more powerful neural decoding capabilities and allows researchers to experiment more easily with alternative decoding models.", 
    "abstract": "Can we apply graph representation learning algorithms to identify autism spectrum disorder (ASD) patients within a large brain imaging dataset? ASD is mainly identified by brain functional connectivity patterns. Attempts to unveil the common neural patterns emerged in ASD are the essence of ASD classification. We claim that graph representation learning methods can appropriately extract the connectivity patterns of the brain, in such a way that the method can be generalized to every recording condition, and phenotypical information of subjects. These methods can capture the whole structure of the brain, both local and global properties.\nThe investigation is done for the worldwide brain imaging multi-site database known as ABIDE I and II (Autism Brain Imaging Data Exchange). Among different graph representation techniques, we used AWE, Node2vec, Struct2vec, multi node2vec, and Graph2Img. The best approach was Graph2Img, in which after extracting the feature vectors representative of the brain nodes, the PCA algorithm is applied to the matrix of feature vectors. The classifier adapted to the features embedded in graphs is an LeNet deep neural network.\nAlthough we could not outperform the previous accuracy of 10-fold cross-validation in the identification of ASD versus control patients in this dataset, for leave-one-site-out cross-validation, we could obtain better results (our accuracy: 80%). The result is that graph embedding methods can prepare the connectivity matrix more suitable for applying to a deep network.", 
    "abstract": "In transferable black-box attacks, adversarial samples remain adversarial across multiple models and are more likely to attack unknown models. From this view, acquiring and exploiting multiple models is the key to improving transferability. For exploiting multiple models, existing approaches concentrate on differences among models but ignore the underlying complex dependencies. This exacerbates the issue of unbalanced and inadequate attacks on multiple models. To this problem, this paper proposes a novel approach, called Relational Graph Ensemble Attack (RGEA), to exploit the dependencies among multiple models. Specifically, we redefine the multi-model ensemble attack as a multi-objective optimization and create a sub-optimization problem to compute the optimal attack direction, but there are serious time-consuming problems. For this time-consuming problem, we define the vector representation of the model, extract the dependency matrix, and then equivalently simplify the sub-optimization problem by utilizing the dependency matrix. Finaly, we theoretically extend to investigate the connection between RGEA and the traditional multiple gradient descent algorithm (MGDA). Notably, combining RGEA further enhances the transferability of existing gradient-based attacks. The experiments using ten normal training models and ten defensive models on the labeled face in the wild (LFW) dataset demonstrate that RGEA improves the success rate of white-box attacks and further boosts the transferability of black-box attacks.", 
    "abstract": "This study introduces the application of deep-learning technologies in automatically generating guidance for independent reading. The study explores and demonstrates how to incorporate the latest advances in deep-learning-based natural language processing technologies in the three reading stages, namely, the pre-reading stage, the while-reading stage, and the post-reading stage. As a result, the novel design and implementation of a prototype system based on deep learning technologies are presented. This system includes connections to prior knowledge with knowledge graphs and summary-based question generation, the breakdown of complex sentences with text simplification, and the auto-grading of readers' writing regarding their comprehension of the reading materials. Experiments on word sense disambiguation, named entity recognition and question generation with real-world materials in the prototype system show that the selected deep learning models on these tasks obtain favorable results, but there are still errors to be overcome before their direct usage in real-world applications. Based on the experiment results and the reported performance of the deep learning models on reading-related tasks, the study reveals the challenges and limitations of deep learning technologies, such as inadequate performance, domain transfer issues, and low explain ability, for future improvement.", 
    "abstract": "Published reports of chemical compounds often contain multiple machine-readable descriptions which may supplement each other in order to yield coherent and complete chemical representations. This publication presents a method to cross-check such descriptions using a canonical representation and isomorphism of molecular graphs. If immediate agreement between compound descriptions is not found, the algorithm derives the minimal set of simplifications required for both descriptions to arrive to a matching form (if any). The proposed algorithm is used to cross-check chemical descriptions from the Crystallography Open Database to identify coherently described entries as well as those requiring further curation.", 
    "abstract": "Personalized treatment planning in Molecular Radiotherapy (MRT) with accurately determining the absorbed dose is highly desirable. The absorbed dose is calculated based on the Time-Integrated Activity (TIA) and the dose conversion factor. A crucial unresolved issue in MRT dosimetry is which fit function to use for the TIA calculation. A data-driven population-based fitting function selection could help solve this problem. Therefore, this project aims to develop and evaluate a method for accurately determining TIAs in MRT, which performs a Population-Based Model Selection within the framework of the Non-Linear Mixed-Effects (NLME-PBMS) model.\nBiokinetic data of a radioligand for the Prostate-Specific Membrane Antigen (PSMA) for cancer treatment were used. Eleven fit functions were derived from various parameterisations of mono-, bi-, and tri-exponential functions. The functions' fixed and random effects parameters were fitted (in the NLME framework) to the biokinetic data of all patients. The goodness of fit was assumed acceptable based on the visual inspection of the fitted curves and the coefficients of variation of the fitted fixed effects. The Akaike weight, the probability that the model is the best among the whole set of considered models, was used to select the fit function most supported by the data from the set of functions with acceptable goodness of fit. NLME-PBMS Model Averaging (MA) was performed with all functions having acceptable goodness of fit. The Root-Mean-Square Error (RMSE) of the calculated TIAs from individual-based model selection (IBMS), a shared-parameter population-based model selection (SP-PBMS) reported in the literature, and the functions from NLME-PBMS method to the TIAs from MA were calculated and analysed. The NLME-PBMS (MA) model was used as the reference as this model considers all relevant functions with corresponding Akaike weights.\nThe function [Formula: see text] was selected as the function most supported by the data with an Akaike weight of (54\u202f\u00b1\u202f11) %. Visual inspection of the fitted graphs and the RMSE values show that the NLME model selection method has a relatively better or equivalent performance than the IBMS or SP-PBMS methods. The RMSEs of the IBMS, SP-PBMS, and NLME-PBMS (f\nA procedure including fitting function selection in a population-based method was developed to determine the best fit function for calculating TIAs in MRT for a given radiopharmaceutical, organ and set of biokinetic data. The technique combines standard practice approaches in pharmacokinetics, i.e. an Akaike-weight-based model selection and the NLME model framework.", 
    "abstract": "Physicians establish diagnosis by assessing a patient's signs, symptoms, age, sex, laboratory test findings and the disease history. All this must be done in limited time and against the backdrop of an increasing overall workload. In the era of evidence-based medicine it is utmost important for a clinician to be abreast of the latest guidelines and treatment protocols which are changing rapidly. In resource limited settings, the updated knowledge often does not reach the point-of-care. This paper presents an artificial intelligence (AI)-based approach for integrating comprehensive disease knowledge, to support physicians and healthcare workers in arriving at accurate diagnoses at the point-of-care. We integrated different disease-related knowledge bodies to construct a comprehensive, machine interpretable diseasomics knowledge-graph that includes the Disease Ontology, disease symptoms, SNOMED CT, DisGeNET, and PharmGKB data. The resulting disease-symptom network comprises knowledge from the Symptom Ontology, electronic health records (EHR), human symptom disease network, Disease Ontology, Wikipedia, PubMed, textbooks, and symptomology knowledge sources with 84.56% accuracy. We also integrated spatial and temporal comorbidity knowledge obtained from EHR for two population data sets from Spain and Sweden respectively. The knowledge graph is stored in a graph database as a digital twin of the disease knowledge. We use node2vec (node embedding) as digital triplet for link prediction in disease-symptom networks to identify missing associations. This diseasomics knowledge graph is expected to democratize the medical knowledge and empower non-specialist health workers to make evidence based informed decisions and help achieve the goal of universal health coverage (UHC). The machine interpretable knowledge graphs presented in this paper are associations between various entities and do not imply causation. Our differential diagnostic tool focusses on signs and symptoms and does not include a complete assessment of patient's lifestyle and health history which would typically be necessary to rule out conditions and to arrive at a final diagnosis. The predicted diseases are ordered according to the specific disease burden in South Asia. The knowledge graphs and the tools presented here can be used as a guide.", 
    "abstract": "The increasing prevalence and magnitude of harmful effects of substance use disorders (SUDs) in low- and middle-income countries (LMICs) make it imperative to embrace interventions which are acceptable, feasible, and effective in reducing this burden. Globally, the use of telehealth interventions is increasingly being explored as possible effective approaches in the management of SUDs. Using a scoping review of literature, this article summarizes and evaluates evidence for the acceptability, feasibility, and effectiveness of telehealth interventions for SUDs in LMICs. Searches were conducted in five bibliographic databases: PubMed, Psych INFO, Web of Science, Cumulative Index of Nursing and Allied Professionals and the Cochrane database of systematic review. Studies from LMICs which described a telehealth modality, identified at least one psychoactive substance use among participants, and methods that either compared outcomes using pre- and post-intervention data, treatment versus comparison groups, post-intervention data, behavioral or health outcome, and outcome of either acceptability, feasibility, and/or effectiveness were included. Data is presented in a narrative summary using charts, graphs, and tables. The search produced 39 articles across 14 countries which fulfilled our eligibility criteria over a period of 10 years (2010 to 2020). Research on this topic increased remarkably in the latter five years with the highest number of studies in 2019. The identified studies were heterogeneous in their methods and various telecommunication modalities were used to evaluate substance use disorder, with cigarette smoking as the most assessed. Most studies used quantitative methods. The highest number of included studies were from China and Brazil, and only two studies from Africa assessed telehealth interventions for SUDs. There has been an increasingly significant body of literature which evaluates telehealth interventions for SUDs in LMICs. Overall, telehealth interventions showed promising acceptability, feasibility, and effectiveness for SUDs. This article identifies gaps and strengths and suggests directions for future research.", 
    "abstract": "Molecular dynamics (MD) simulations are used in diverse scientific and engineering fields such as drug discovery, materials design, separations, biological systems, and reaction engineering. These simulations generate highly complex data sets that capture the 3D spatial positions, dynamics, and interactions of thousands of molecules. Analyzing MD data sets is key for understanding and predicting emergent phenomena and in identifying key drivers and tuning design knobs of such phenomena. In this work, we show that the Euler characteristic (EC) provides an effective topological descriptor that facilitates MD analysis. The EC is a versatile, low-dimensional, and easy-to-interpret descriptor that can be used to reduce, analyze, and quantify complex data objects that are represented as graphs/networks, manifolds/functions, and point clouds. Specifically, we show that the EC is an informative descriptor that can be used for machine learning and data analysis tasks such as classification, visualization, and regression. We demonstrate the benefits of the proposed approach through case studies that aim to understand and predict the hydrophobicity of self-assembled monolayers and the reactivity of complex solvent environments.", 
    "abstract": "Aotearoa New Zealand (AoNZ) guidelines suggest surveillance colonoscopy should be carefully considered after age 75. The authors noted a cluster of patients presenting in their 8th and 9th decade of life with a new colorectal cancer (CRC) having previously been declined surveillance colonoscopy.\nA 7-year retrospective analysis was performed of patients who underwent a colonoscopy aged between 71 and 75\u2009years in the period between 2006 and 2012. Kaplan-Meier graphs were created with survival measured from the time of index colonoscopy. Log rank tests were used to determine any difference in survival distribution. Relative risk (RR) was calculated, and 95% confidence intervals (CI) reported.\nA total of 623 patients met inclusion criteria; 461 (74%) had no indication for surveillance colonoscopy and 162 (26%) had an indication. Of the 162 patients with an indication, 91 (56.2%) underwent surveillance colonoscopies after the age of 75. Twenty-three (3.7%) patients were diagnosed with a new CRC. Eighteen (78.2%) patients diagnosed with a new CRC underwent surgery. The median survival overall was 12.9\u2009years (95% CI 12.2-13.5). This did not differ between patients with (13.1, 95% CI 12.1-14.1) or without (12.6, 95% CI 11.2-14.0) an indication for surveillance.\nThis study found one quarter of patients who had a colonoscopy between the ages of 71-75 had an indication for surveillance colonoscopy. Most patients with a new CRC underwent surgery. This study suggests it may be appropriate to update the AoNZ guidelines and consider adopting a risk stratification tool to aid decision making.", 
    "abstract": "The present investigation is carried out to predict the flow characteristics of a micropolar liquid that is infused with ternary nanoparticles across a stretching/shrinking surface under the impact of chemical reactions and radiation. Here, three dissimilarly shaped nanoparticles (copper oxide, graphene and copper nanotubes) are suspended in H", 
    "abstract": "Nowadays, water pollution and energy crises worldwide force researchers to develop multi-functional and highly efficient nanomaterials. In this scenario, the present work reports a dual-functional La", 
    "abstract": "The etiology of congenital solitary functioning kidney (CSFK) is largely unknown but likely includes various risk factors. We performed a case-control study to compare exposure to environmental and parental risk factors during embryonic kidney development between children with CSFK and healthy controls.\nWe included 434 children with CSFK and 1302 healthy controls from the AGORA data- and biobank matched on year of birth. Exposure to potential risk factors was investigated using parental questionnaire data. Crude and adjusted odds ratios (aORs) with 95% confidence intervals (CIs) were estimated for each potential risk factor. Multiple imputation was used to deal with missing values. Confounders for each potential risk factor were selected using directed acyclic graphs.\nMaternal stress was newly identified as a risk factor for CSFK (aOR 2.1, 95% CI 1.2-3.5). Known associations with conception using in vitro fertilization/intracytoplasmic sperm injection (aOR 1.8, 95% CI 1.0-3.2), maternal infections during pregnancy (aOR 2.5, 95% CI 1.4-4.7), smoking during pregnancy (aOR 1.4, 95% CI 1.0-2.0), and parental CAKUT (aOR 6.6, 95% CI 2.9-15.1) were confirmed, but previous associations with diabetes and obesity could not be replicated. Folic acid supplement use and younger maternal age seemed to reduce the risk of CSFK (aORs 0.7, 95% CI 0.5-1.0, and 0.8, 95% CI 0.6-1.0, respectively).\nEnvironmental and parental risk factors are likely to be involved in the development of CSFK and future studies should combine genetic, environmental, and gene-environment interaction analyses. Women wanting to become pregnant should consider optimizing their health and lifestyle. A higher-resolution version of the Graphical abstract is available as Supplementary information.", 
    "abstract": "With the emergence of enormous amounts of data, numerous ways to visualize such data have been used. Bar, circular, line, radar and bubble graphs that are ubiquitous were investigated for their effectiveness. Fourteen participants performed four types of evaluations: between categories (cities), within categories (transport modes within a city), all categories, and a direct reading within a category from a graph. The representations were presented in random order and participants were asked to respond to sixteen questions to the best of their ability after visually scanning the related graph. There were two trials on two separate days for each participant. Eye movements were recorded using an eye tracker. Bar and line graphs show superiority over circular and radial graphs in effectiveness, efficiency, and perceived ease of use primarily due to eye saccades. The radar graph had the worst performance. \"Vibration-type\" fill pattern could be improved by adding colors and symbolic fills. Design guidelines are proposed for the effective representation of data so that the presentation and communication of information are effective.", 
    "abstract": "Graph neural network (GNN) is a powerful model for learning from graph data. However, existing GNNs may have limited expressive power, especially in terms of capturing adequate structural and positional information of input graphs. Structure properties and node position information are unique to graph-structured data, but few GNNs are capable of capturing them. This paper proposes Structure- and Position-aware Graph Neural Networks (SP-GNN), a new class of GNNs offering generic and expressive power of graph data. SP-GNN enhances the expressive power of GNN architectures by incorporating a near-isometric proximity-aware position encoder and a scalable structure encoder. Further, given a GNN learning task, SP-GNN can be used to analyze positional and structural awareness of GNN tasks using the corresponding embeddings computed by the encoders. The awareness scores can guide fusion strategies of the extracted positional and structural information with raw features for better performance of GNNs on downstream tasks. We conduct extensive experiments using SP-GNN on various graph datasets and observe significant improvement in classification over existing GNN models.", 
    "abstract": "Frameworks for selecting exposures in high-dimensional environmental datasets, while considering confounding, are lacking. We present a two-step approach for exposure selection with subsequent confounder adjustment for statistical inference.\nWe measured cognitive ability in 338 children using the Woodcock-Mu\u00f1oz General Intellectual Ability (GIA) score, and potential associated features across several environmental domains. Initially, 111 variables theoretically associated with GIA score were introduced into a Least Absolute Shrinkage and Selection Operator (LASSO) in a 50% feature selection subsample. Effect estimates for selected features were subsequently modeled in linear regressions in a 50% inference (hold out) subsample, first adjusting for sex and age and later for covariates selected via directed acyclic graphs (DAGs). All models were adjusted for clustering by school.\nOf the 15 LASSO selected variables, eleven were not associated with GIA score following our inference modeling approach. Four variables were associated with GIA scores, including: serum ferritin adjusted for inflammation (inversely), mother's IQ (positively), father's education (positively), and hours per day the child works on homework (positively). Serum ferritin was not in the expected direction.\nOur two-step approach moves high-dimensional feature selection a step further by incorporating DAG-based confounder adjustment for statistical inference.", 
    "abstract": "Gene regulatory networks define the interactions between DNA products and other substances in cells. Increasing knowledge of these networks improves the level of detail with which the processes that trigger different diseases are described and fosters the development of new therapeutic targets. These networks are usually represented by graphs, and the primary sources for their correct construction are usually time series from differential expression data. The inference of networks from this data type has been approached differently in the literature. Mostly, computational learning techniques have been implemented, which have finally shown some specialization in specific datasets. For this reason, the need arises to create new and more robust strategies for reaching a consensus based on previous results to gain a particular capacity for generalization. This paper presents GENECI (GEne NEtwork Consensus Inference), an evolutionary machine learning approach that acts as an organizer for constructing ensembles to process the results of the main inference techniques reported in the literature and to optimize the consensus network derived from them, according to their confidence levels and topological characteristics. After its design, the proposal was confronted with datasets collected from academic benchmarks (DREAM challenges and IRMA network) to quantify its accuracy. Subsequently, it was applied to a real-world biological network of melanoma patients whose results could be contrasted with medical research collected in the literature. Finally, it has been proved that its ability to optimize the consensus of several networks leads to outstanding robustness and accuracy, gaining a certain generalization capacity after facing the inference of multiple datasets. The source code is hosted in a public repository at GitHub under MIT license: https://github.com/AdrianSeguraOrtiz/GENECI. Moreover, to facilitate its installation and use, the software associated with this implementation has been encapsulated in a python package available at PyPI: https://pypi.org/project/geneci/.", 
    "abstract": "CARA is a five-year Health Research Board (HRB) project. Superbugs cause resistant infections that are difficult to treat and pose a serious threat to human health. Providing tools to explore the prescription of antibiotics by GPs may help identify gaps where improvements can be made. CARA's aim is to combine, link and visualise data on infections, prescribing and other healthcare information.\nThe CARA team is creating a dashboard to provide GPs with a tool to visualise their own practice data and compare this with other GPs in Ireland. Anonymous patient data can be uploaded and visualised to show details, current trends and changes in infections and prescribing. The CARA platform will also provide easy options to generate audit reports.\nAfter registration, a tool for anonymous data upload will be provided. Through this uploader, data will be used to create instant graphs and overviews as well as comparisons with other GP practices. With selection options, graphical presentations can be further explored or audits generated. Currently, few GPs are involved in the development of the dashboard to ensure it will be efficient. Examples of the dashboard will be shown at the conference.\nThe CARA project will provide GPs with a tool to access, analyse and understand their patient data. GPs will have secure accounts accessible through the CARA website to allow easy anonymous data upload in a few steps. The dashboard will show comparisons of their prescribing with other (unknown) practices, identify areas for improvement and conduct audit reports.", 
    "abstract": "This study focused on modeling and density functional theory (DFT) analysis of reference (AI1) and designed structures (AI11-AI15), based on the thieno-imidazole core, in order to create profitable candidates for solar cells. All the optoelectronic properties of the molecular geometries were computed using DFT and time dependent-DFT approaches. The influence of terminal acceptors on the bandgaps, absorption, hole and electron mobilities, charge transfer capabilities, fill factor, dipole moment, etc. Of the recently designed structures (AI11-AI15), as well as reference (AI1), were evaluated. Optoelectronics and chemical parameters of newly architecture geometries were shown to be superior to the cited molecule. The FMOs and DOS graphs also demonstrated that the linked acceptors remarkably improved the dispersion of charge density in the geometries under study, particularly in AI11 and AI14. Calculated values of binding energy and chemical potential confirmed the thermal stability of the molecules. All the derived geometries surpassed the AI1 (Reference) molecule in terms of maximum absorbance ranging from 492 to 532\u00a0nm (in chlorobenzene solvent) and a narrower bandgap ranging from 1.76 to 1.99eV. AI15 had the lowest exciton dissociation energy of 0.22eV as well as lowest electrons and hole dissociation energies, while AI11 and AI14 showed highest VOC, fill factor, power conversion efficiency (PCE), IP and EA (owing to presence of strong electron pulling cyano (CN) moieties at their acceptor portions and extended conjugation) than all the examined molecules, implying that they could be used to build elite solar cells with enhanced photovoltaic attributes.", 
    "abstract": "The aim of this study is to find the knowledge, attitude and awareness of biocompatibility of orthodontic materials among dental students.A survey of 13 questions was created using Google Forms. The questions on the survey cover a wide range of topics including the awareness of cytotoxicity of orthodontic materials, their corrosive potential, ways of preventing corrosion and release of subsequent substances, and various means by which these physiological actions occur. This survey was circulated among dental undergraduate students and the responses recorded were then transferred to SPSS software. Here we conducted descriptive analysis to produce a pie chart and Chi square test to determine the association and statistical significance.The results were noted and examined in the form of bar graphs. The p value for this study was found to be 0.01, which makes this study statistically significant. The awareness about the biocompatibility of orthodontic materials among dental students was found to be low. Continuing dental education programs can be conducted to constantly improve the knowledge regarding the judicious and ecofriendly use of all newly available dental materials.", 
    "abstract": "Across many problems in science and engineering, it is important to consider how much the output of a given system changes due to perturbations of the input. Here, we investigate the glassy phase of \u00b1J spin glasses at zero temperature by calculating the robustness of the ground states to flips in the sign of single interactions. For random graphs and the Sherrington-Kirkpatrick model, we find relatively large sets of bond configurations that generate the same ground state. These sets can themselves be analyzed as subgraphs of the interaction domain, and we compute many of their topological properties. In particular, we find that the robustness, equivalent to the average degree, of these subgraphs is much higher than one would expect from a random model. Most notably, it scales in the same logarithmic way with the size of the subgraph as has been found in genotype-phenotype maps for RNA secondary structure folding, protein quaternary structure, gene regulatory networks, as well as for models for genetic programming. The similarity between these disparate systems suggests that this scaling may have a more universal origin.", 
    "abstract": "Node role explainability in complex networks is very difficult yet is crucial in different application domains such as social science, neurosciences, or computer science. Many efforts have been made on the quantification of hubs revealing particular nodes in a network using a given structural property. Yet, in several applications, when multiple instances of networks are available and several structural properties appear to be relevant, the identification of node roles remains largely unexplored. Inspired by the node automorphically equivalence relation, we define an equivalence relation on graph nodes associated with any collection of nodal statistics (i.e., any functions on the node set). This allows us to define new graph global measures: the power coefficient and the orthogonality score to evaluate the parsimony and heterogeneity of a given nodal statistics collection. In addition, we introduce a new method based on structural patterns to compare graphs that have the same vertices set. This method assigns a value to a node to determine its role distinctiveness in a graph family. Extensive numerical results of our method are conducted on both generative graph models and real data concerning human brain functional connectivity. The differences in nodal statistics are shown to be dependent on the underlying graph structure. Comparisons between generative models and real networks combining two different nodal statistics reveal the complexity of human brain functional connectivity with differences at both global and nodal levels. Using a group of 200 healthy controls connectivity networks, our method computes high correspondence scores among the whole population to detect homotopy and finally quantify differences between comatose patients and healthy controls.", 
    "abstract": "Big Data science has significantly furthered our understanding of complex systems by harnessing large volumes of data, generated at high velocity and in great variety. However, there is a risk that Big Data collection is prioritised to the detriment of 'Small Data' (data with few observations). This poses a particular risk to ecology where Small Data abounds. Machine learning experts are increasingly looking to Small Data to drive the next generation of innovation, leading to development in methods for Small Data such as transfer learning, knowledge graphs, and synthetic data. Meanwhile, meta-analysis and causal reasoning approaches are evolving to provide new insights from Small Data. These advances should add value to high-quality Small Data catalysing future insights for ecology.", 
    "abstract": "Tick-borne encephalitis (TBE) is a growing public health problem with an average of 361 cases notified annually to Germany's passive surveillance system since 2001. We aimed to assess clinical manifestations and identify covariates associated with severity.\nWe included cases notified 2018-2020 in a prospective cohort study and collected data with telephone interviews, questionnaires to general practitioners, and hospital discharge summaries. Covariates' causal associations with severity were evaluated with multivariable logistic regression, adjusted for variables identified via directed acyclic graphs.\nOf 1220 eligible cases, 581 (48%) participated. Of these, 97.1% were not (fully) vaccinated. TBE was severe in 20.3% of cases (children: 9.1%, \u226570-year-olds: 48.6%). Routine surveillance data underreported the proportion of cases with central nervous system involvement (56% vs. 84%). Ninety percent required hospitalization, 13.8% intensive care, and 33.4% rehabilitation. Severity was most notably associated with age (odds ratio (OR): 1.04, 95% confidence interval (CI): 1.02-1.05), hypertension (OR: 2.27, 95%CI: 1.37-3.75), and monophasic disease course (OR: 1.67, 95%CI: 1.08-2.58).\nWe observed substantial TBE burden and health service utilization, suggesting that awareness of TBE severity and vaccine preventability should be increased. Knowledge of severity-associated factors may help inform patients' decision to get vaccinated.", 
    "abstract": "The aim of this study was to analyze the value of the unadjusted CUSUM graph of liver surgical injury and discard rates in organ procurement in the Netherlands.\nUnadjusted CUSUM graphs were plotted for surgical injury (C event) and discard rate (C2 event) from procured livers accepted for transplantation for each local procurement team compared with the total national cohort. The average incidence for each outcome was used as benchmark based on procurement quality forms (Sep 2010-Oct 2018). The data from the five Dutch procuring teams were blind-coded.\nThe C and C2 event rate were 17% and 1.9%, respectively (n\u00a0=\u00a01265). A total of 12 CUSUM charts were plotted for the national cohort and the five local teams. National CUSUM charts showed an overlapping \"alarm signal.\" This overlapping signal for both C and C2, albeit a different time period, was only found in one local team. The other CUSUM alarm signal went off for two separate local teams, but only for C events or C2 events respectively, and at different points in time. The other remaining CUSUM charts showed no alarm signaling.\nThe unadjusted CUSUM chart is a simple and effective monitoring tool in following performance quality of organ procurement for liver transplantation. Both national and local recorded CUSUMs are useful to see the implication of national and local effects on organ procurement injury. Both procurement injury and organ\u00a0discard are equally important in this analysis and need to be separately CUSUM charted.", 
    "abstract": "", 
    "abstract": "The rapid accumulation of high-throughput sequence data demands the development of effective and efficient data-driven computational methods to functionally annotate proteins. However, most current approaches used for functional annotation simply focus on the use of protein-level information but ignore inter-relationships among annotations.\nHere, we established PFresGO, an attention-based deep-learning approach that incorporates hierarchical structures in Gene Ontology (GO) graphs and advances in natural language processing algorithms for the functional annotation of proteins. PFresGO employs a self-attention operation to capture the inter-relationships of GO terms, updates its embedding accordingly and uses a cross-attention operation to project protein representations and GO embedding into a common latent space to identify global protein sequence patterns and local functional residues. We demonstrate that PFresGO consistently achieves superior performance across GO categories when compared with 'state-of-the-art' methods. Importantly, we show that PFresGO can identify functionally important residues in protein sequences by assessing the distribution of attention weightings. PFresGO should serve as an effective tool for the accurate functional annotation of proteins and functional domains within proteins.\nPFresGO is available for academic purposes at https://github.com/BioColLab/PFresGO.\nSupplementary data are available at Bioinformatics online.", 
    "abstract": "Quality control (QC) is a necessary, but often an under-appreciated, part of FMRI processing. Here we describe procedures for performing QC on acquired or publicly available FMRI datasets using the widely used AFNI software package. This work is part of the Research Topic, \"Demonstrating Quality Control (QC) Procedures in fMRI.\" We used a sequential, hierarchical approach that contained the following major stages: (1) GTKYD (getting to know your data, esp. its basic acquisition properties), (2) APQUANT (examining quantifiable measures, with thresholds), (3) APQUAL (viewing qualitative images, graphs, and other information in systematic HTML reports) and (4) GUI (checking features interactively with a graphical user interface); and for task data, and (5) STIM (checking stimulus event timing statistics). We describe how these are complementary and reinforce each other to help researchers stay close to their data. We processed and evaluated the provided, publicly available resting state data collections (7 groups, 139 total subjects) and task-based data collection (1 group, 30 subjects). As specified within the Topic guidelines, each subject's dataset was placed into one of three categories: Include, exclude or uncertain. The main focus of this paper, however, is the detailed description of QC procedures: How to understand the contents of an FMRI dataset, to check its contents for appropriateness, to verify processing steps, and to examine potential quality issues. Scripts for the processing and analysis are freely available.", 
    "abstract": "Incomplete immunization and non-immunization increase the risk of disease and death among children. This study aims to assess childhood vaccination practices and associated factors among mothers and caregivers in Debre Tabor town, Amhara region, Ethiopia.\nA community-based cross-sectional study design was conducted between February 30 and April 30, 2022. The study participants were proportionally allocated to all six kebeles found in the town. A systematic random sampling technique was used to select the study participants. The collected data were checked and coded and then entered into EpiData Version 3.1 and exported into SPSS Version 26. The results were organized using frequency tables, graphs, and charts, and bivariate and multivariable logistic regression were used to test the association of covariates with childhood vaccination practices.\nApproximately 422 study mothers and caregivers participated in the study, with a response rate of 100%. The mean age was 30.63 years (11.74), which ranged from 18 to 58 years. More than half of the study participants (56.4%) expressed fears about the side effects of vaccination. A majority (78.4%) of the study participants availed of counseling services about vaccination, and 71.1% of them received regular antenatal care. This study found that approximately 280 [66.4%, 95% confidence interval (CI): 61.8-70.6] mothers/caregivers had a history of good childhood vaccination practices. The factors of the fear of side effects [adjusted odds ratio (AOR)\u2009=\u20093.34; 95% CI: 1.72-6.49], no workload (AOR\u2009=\u20096.08; 95% CI: 1.74-21.22), medium workload (AOR\u2009=\u20094.80; 95% CI: 1.57-14.71), being a mother of child/children (AOR\u2009=\u20092.55; 95% CI: 1.27-5.13), positive attitude (AOR\u2009=\u20092.25; 95% CI: 1.32-3.82), and sound knowledge (AOR\u2009=\u20093.88; 95% CI: 2.26-6.68) were significantly associated with childhood vaccination practices.\nMore than half of the study participants had a history of good childhood vaccination practices. However, the rate of such practices was low among mothers and caregivers. The fear of side effects, workload, motherhood, attitude, and knowledge were all factors associated with childhood vaccination practices. Awareness creation and a consideration of the workload of mothers would be helpful in dispelling fears and increasing the rate of good practices among mothers and caregivers.", 
    "abstract": "The impact of chemical reaction and activation energy plays a vital role in the analysis of fluid dynamics and its thermal properties. The application of the flow of fluid is significantly considered in nuclear reactors, automobiles, manufacturing setups, electronic appliances etc. This study explores the impacts of activation energy and chemical reaction on the magnetohydrodynamic Darcy-Forchheimer squeezed Casson fluid flow through a porous material across the horizontal channel where the two parallel plates are assumed to be in motion. By using similarity variables, partial differential equations are converted to ordinary differential equations. Numerical method is applied using MATLAB to solve the problems and acquire the data for velocity field, thermal distribution, and concentration distribution. The graphs indicate that fluid velocity and temperature increases as the plates are brought closer. In addition, there was a correlation between a rise in the Hartmann number and a decrease in the fluid's velocity because of the existence of strong Lorentz forces. The temperature and the concentration of the liquid will increase due to the Brownian motion. When the Darcy-Forchheimer and activation energy parameters are both increased, the velocity and concentration decreases.", 
    "abstract": "Dental caries is defined as a dynamic diet microbial disease of teeth, which results in localized dissolution and destruction of the mineralized tissues of the teeth. Dental caries develops when there is a susceptible tooth exposed to pathogenic bacteria in the presence of substrate. Under these conditions, the bacteria metabolize substrate to form acid, which decalcifies teeth. Dental caries is among the top oral health problem in both developing and developed nations affecting around 20-50% of the population globally.\nThis study was conducted to assess the magnitude, associated factors, and antimicrobial susceptibility pattern of bacterial isolates among adult dental caries patients visiting Hiwot Fana specialized university hospital dental clinic from April 23 to-June 23, 2021.\nAn institutional-based cross-sectional study was conducted among 290 study participants. Convenient sampling techniques were used to select the study participants. Data was entered into Epi-info version 7.2.4.0 and exported to Statistical Package for the Social Sciences version 20 for analysis. The result was explained by using summary measures of texts, tables, and graphs after analysis by using bivariate and multivariate logistic regression. Statistical significance was defined at a p-value of less than 0.05.\nThe overall magnitude of bacteria among dental caries patients was 68.3%. S mutans 74(37.4%) and Lactobacillus spp 58(29.3%) were the most predominant isolated bacteria. Lack of teeth brushing (AOR: 2.8, 95% CI:1.6, 4.6), the habit of chewing khat always (AOR:4.8, 95%CI:2.10,8.80), the habit of chewing khat sometimes (AOR: 3.8: 95% CI: 2.520,9.48) and consumption of soft drinks (AOR: 1.9, 95% CI:1.2,3.1) were significantly associated with bacterial dental caries. Almost, all bacterial isolates were susceptible to ceftriaxone and ciprofloxacin compared to Amoxicillin, Azithromycin, chloramphenicol, clindamycin, doxycycline, erythromycin, gentamycin, penicillin, tetracycline, vancomycin and tobramycin.\nTeeth brushing habit, consumption of soft drink and a habit of chewing khat affects dental health and they are associated with bacterial dental caries. Harari regional health bureau better to focus by giving health education to the community about dental caries based on identified associated factors with dental caries.", 
    "abstract": "Graph models are standard for representing mutual relationships between sets of entities. Often, graphs deal with a large number of entities with a small number of connections (e.g. social media relationships, infectious disease spread). The distances or similarities between such large graphs are known to be well established by the Graphlet Correlation Distance (GCD). This paper deals with small graphs (with potentially high densities of connections) that have been somewhat neglected in the literature but that concern important fora like sociology, ecology and fisheries, to mention some examples. First, based on numerical experiments, we study the conditions under which Erd\u0151s-R\u00e9nyi, Fitness Scale-Free, Watts-Strogatz small-world and geometric graphs can be distinguished by a specific GCD measure based on 11 orbits, the GCD11. This is done with respect to the density and the order (i.e. the number of nodes) of the graphs when comparing graphs with the same and different orders. Second, we develop a randomization statistical test based on the GCD11 to compare empirical graphs to the four possible null models used in this analysis and apply it to a fishing case study where graphs represent pairwise proximity between fishing vessels. The statistical test rules out independent pairing within the fleet studied which is a standard assumption in fisheries. It also illustrates the difficulty to identify similarities between real-world small graphs and graph models.", 
    "abstract": "Graph analytical approaches permit identifying novel genes involved in complex diseases, but are limited by (i) inferring structural network similarity of connected gene nodes, ignoring potentially relevant unconnected nodes; (ii) using homogeneous graphs, missing gene-disease associations' complexity; (iii) relying on disease/gene-phenotype associations' similarities, involving highly incomplete data; (iv) using binary classification, with gene-disease edges as positive training samples, and non-associated gene and disease nodes as negative samples that may include currently unknown disease genes; or (v) reporting predicted novel associations without systematically evaluating their accuracy. Addressing these limitations, we develop the Heterogeneous Integrated Graph for Predicting Disease Genes (HetIG-PreDiG) model that includes gene-gene, gene-disease, and gene-tissue associations. We predict novel disease genes using low-dimensional representation of nodes accounting for network structure, and extending beyond network structure using the developed Gene-Disease Prioritization Score (GDPS) reflecting the degree of gene-disease association via gene co-expression data. For negative training samples, we select non-associated gene and disease nodes with lower GDPS that are less likely to be affiliated. We evaluate the developed model's success in predicting novel disease genes by analyzing the prediction probabilities of gene-disease associations. HetIG-PreDiG successfully predicts (Micro-F1 = 0.95) gene-disease associations, outperforming baseline models, and is validated using published literature, thus advancing our understanding of complex genetic diseases.", 
    "abstract": "Protein-protein interactions (PPIs) play crucial roles in almost all biological processes from cell-signaling and membrane transport to metabolism and immune systems. Efficient characterization of PPIs at the molecular level is key to the fundamental understanding of PPI mechanisms. Even with the gigantic amount of PPI models from graphs, networks, geometry and topology, it remains as a great challenge to design functional models that efficiently characterize the complicated multiphysical information within PPIs. Here we propose persistent Tor-algebra (PTA) model for a unified algebraic representation of the multiphysical interactions. Mathematically, our PTA is inherently algebraic data analysis. In our PTA model, protein structures and interactions are described as a series of face rings and Tor modules, from which PTA model is developed. The multiphysical information within/between biomolecules are implicitly characterized by PTA and further represented as PTA barcodes. To test our PTA models, we consider PTA-based ensemble learning for PPI binding affinity prediction. The two most commonly used datasets, i.e. SKEMPI and AB-Bind, are employed. It has been found that our model outperforms all the existing models as far as we know. Mathematically, our PTA model provides a highly efficient way for the characterization of molecular structures and interactions.", 
    "abstract": "To evaluate three of the main verbal models that have been proposed to explain the relationship between fluctuating asymmetry and fitness in humans: the \"good genes,\" the \"good development,\" and the \"growth\" hypotheses.\nA formal model was generated for each verbal model following three steps. First, based on the literature, a theoretical causal model and the theoretical object of inquiry were outlined. Second, an empirical causal model and the targets of inference were defined using observational data of facial asymmetries and life-history traits related to fitness. Third, generalized linear models and causal inference were used as the estimation strategy.\nThe results suggest that the theoretical and empirical assumptions of the \"good genes\" hypothesis should be reformulated. The results were compatible with most of the empirical assumptions of \"the good development\" hypothesis but suggest that further discussion of its theoretical assumptions is needed. The results were less informative about the \"growth\" hypothesis, both theoretically and empirically. There was a positive association between facial fluctuating asymmetry and the number of offspring that was not compatible with any of the empirical causal models evaluated.\nAlthough the three hypotheses focus on different aspects of the link between asymmetry and fitness, their overlap opens the possibility of a unified theory on the subject. The results of this study make explicit which assumptions need to be updated and discussed, facilitating the advancement of this area of research. Overall, this study elucidates the potential benefit of using formal models for theory revision and development.", 
    "abstract": "A fitness landscape is a mapping from a space of discrete genotypes to the real numbers. A path in a fitness landscape is a sequence of genotypes connected by single mutational steps. Such a path is said to be accessible if the fitness values of the genotypes encountered along the path increase monotonically. We study accessible paths on random fitness landscapes of the House-of-Cards type, on which fitness values are independent, identically and continuously distributed random variables. The genotype space is taken to be a Cartesian power graph [Formula: see text], where [Formula: see text] is the number of genetic loci and the allele graph [Formula: see text] encodes the possible allelic states and mutational transitions on one locus. The probability of existence of accessible paths between two genotypes at a distance linear in [Formula: see text] displays a transition from 0 to a positive value at a threshold [Formula: see text] for the fitness difference between the initial and final genotype. We derive a lower bound on [Formula: see text] for general [Formula: see text] and show that this bound is tight for a large class of allele graphs. Our results generalize previous results for accessibility percolation on the biallelic hypercube, and compare favorably to published numerical results for multiallelic Hamming graphs.", 
    "abstract": "We present scTenifoldXct, a semi-supervised computational tool for detecting ligand-receptor (LR)-mediated cell-cell interactions and mapping cellular communication graphs. Our method is based on manifold alignment, using LR pairs as inter-data correspondences to embed ligand and receptor genes expressed in interacting cells into a unified latent space. Neural networks are employed to minimize the distance between corresponding genes while preserving the structure of gene regression networks. We apply scTenifoldXct to real datasets for testing and demonstrate that our method detects interactions with high consistency compared with other methods. More importantly, scTenifoldXct uncovers weak but biologically relevant interactions overlooked by other methods. We also demonstrate how scTenifoldXct can be used to compare different samples, such as healthy vs. diseased and wild type vs. knockout, to identify differential interactions, thereby revealing functional implications associated with changes in cellular communication status.", 
    "abstract": "Few-shot knowledge graph completion (KGC) is an important and common task in real applications, which aims to predict unseen facts when only few samples are available for each relation in the knowledge graph (KG). Previous methods on few-shot KGC mainly focus on static KG, however, many KG in real-world applications are dynamic and develop over time. In this work, we consider few-shot KGC in temporal knowledge graphs (TKGs), where the fact may only hold for a specific timestamp. We propose a Few-Shot Completion model in TKG (TFSC), which compare the input query to the given few-shot references to make predictions. Specifically, in order to enhance the representation of entities in the case of few samples, we use the attention mechanism to model the neighbor entities of the task entity with timestamp information, and generate expressive time-aware entity pair representations through the Transformer encoder. A comprehensive set of experiments is finally carried out to demonstrate the effectiveness a of our proposed model TFSC.", 
    "abstract": "Admixture graphs are mathematical structures that describe the ancestry of populations in terms of divergence and merging (admixing) of ancestral populations as a graph. An admixture graph consists of a graph topology, branch lengths, and admixture proportions. The branch lengths and admixture proportions can be estimated using numerous numerical optimization methods, but inferring the topology involves a combinatorial search for which no polynomial algorithm is known. In this paper, we present a reversible jump MCMC algorithm for sampling high-probability admixture graphs and show that this approach works well both as a heuristic search for a single best-fitting graph and for summarizing shared features extracted from posterior samples of graphs. We apply the method to 11 Native American and Siberian populations and exploit the shared structure of high-probability graphs to characterize the relationship between Saqqaq, Inuit, Koryaks, and Athabascans. Our analyses show that the Saqqaq is not a good proxy for the previously identified gene flow from Arctic people into the Na-Dene speaking Athabascans.", 
    "abstract": "This study explored the relationship between sleep quality and next-day pain intensity for people with low back pain and investigated whether there was any evidence that this relationship was causal.\nWe conducted a secondary analysis of an observational study that investigated sleep quality in people with low back pain. People with low back pain were recruited from primary care and the community. Sleep quality was measured with subjective (self-report) and objective (polysomnography (PSG)) measures. PSG analysis classifies sleep into stages, of which slow-wave sleep (SWS) is thought to have a key role in maintaining or increasing pain intensity. We drew directed acyclic graphs to identify possible confounders of the relationship between both measures of sleep quality, and pain intensity. We constructed two linear regression models to explore the effect of subjective and objective sleep quality on next-day pain intensity before and after confounder adjustment.\nThirty-nine participants were included in the study. For participants with low back pain, self-reported better quality sleep \u03b2=-0.38 (95% CI\u00a0-0.63 to\u00a0-0.13), or spending a greater proportion of time in SWS \u03b2=-0.12 (95% CI\u00a0-0.22 to\u00a0-0.02) was associated with lower next day pain intensity. After confounder adjustment, the effect reduced and was no longer significant for either self-reported \u03b2=-0.18 (95% CI\u00a0-0.46 to 0.10), or SWS \u03b2=-0.08 (95% CI\u00a0-0.18 to 0.03).\nSleep quality, whether measured by self-report or proportion of time in SWS, was associated with next day pain intensity for people with low back pain. However, this relationship is likely to be confounded and therefore not likely to be causal.", 
    "abstract": "Researchers faced with incomplete data are encouraged to consider whether their data are 'missing completely at random' (MCAR), 'missing at random' (MAR) or 'missing not at random' (MNAR) when planning their analysis. However, there are two major problems with this classification as originally defined by Rubin in the 1970s. First, when there are missing data in multiple variables, the plausibility of the MAR assumption is difficult to assess using substantive knowledge and is more stringent than is generally appreciated. Second, although MCAR and MAR are sufficient conditions for consistent estimation with specific methods, they are not necessary conditions and therefore this categorization does not directly determine the best approach for handling the missing data in an analysis. How best to handle missing data depends on the assumed causal relationships between variables and their missingness, and what these relationships imply in terms of the 'recoverability' of the target estimand (the population parameter that encodes the answer to the underlying research question). Recoverability is defined as whether the estimand can be consistently estimated from the patterns and associations in the observed data without needing to invoke external information on the extent to which the distribution of missing values might differ from that of observed values. In this manuscript we outline an approach for deciding which method to use to handle multivariable missing data in an analysis, using directed acyclic graphs to depict missingness assumptions and determining the implications in terms of recoverability of the target estimand.", 
    "abstract": "Networks have risen to prominence as intellectual technologies and graphical representations, not only in science, but also in journalism, activism, policy, and online visual cultures. Inspired by approaches taking trouble as occasion to (re)consider and reflect on otherwise implicit knowledge practices, in this article we explore how problems with network practices can be taken as invitations to attend to the diverse settings and situations in which network graphs and maps are created and used in society. In doing so, we draw on cases from our research, engagement and teaching activities involving making networks, making sense of networks, making networks public, and making network tools. As a contribution to \"critical data practice,\" we conclude with some approaches for slowing down and caring for network practices and their associated troubles to elicit a richer picture of what is involved in making networks work as well as reconsidering their role in collective forms of inquiry.", 
    "abstract": "Graph theory models a network by its nodes (the fundamental unit by which graphs are formed) and connections. 'Degree' hubs reflect node centrality (the connection rate), while 'connector' hubs are those linked to several clusters of nodes (mainly long-range connections).\nHere, we compared hubs modeled from measures of interdependencies of between-electrode resting-state eyes-closed electroencephalography (rsEEG) rhythms in normal elderly (Nold) and Alzheimer's disease dementia (ADD) participants. At least 5\u2009min of rsEEG was recorded and analyzed. As ADD is considered a 'network disease' and is typically associated with abnormal rsEEG delta (<4\u2009Hz) and alpha rhythms (8-12\u2009Hz) over associative posterior areas, we tested the hypothesis of abnormal posterior hubs from measures of interdependencies of rsEEG rhythms from delta to gamma bands (2-40\u2009Hz) using eLORETA bivariate and multivariate-directional techniques in ADD participants versus Nold participants. Three different definitions of 'connector' hub were used.\nConvergent results showed that in both the Nold and ADD groups there were significant parietal 'degree' and 'connector' hubs derived from alpha rhythms. These hubs had a prominent outward 'directionality' in the two groups, but that 'directionality' was lower in ADD participants than in Nold participants.\nIn conclusion, independent methodologies and hub definitions suggest that ADD patients may be characterized by low outward 'directionality' of partially preserved parietal 'degree' and 'connector' hubs derived from rsEEG alpha rhythms.", 
    "abstract": "The class of multi-relational graph convolutional networks (MRGCNs) is a recent extension of standard graph convolutional networks (GCNs) to handle heterogenous graphs with multiple types of relationships. MRGCNs have been shown to yield results superior than traditional GCNs in various machine learning tasks. The key idea is to introduce a new kind of convolution operated on tensors that can effectively exploit correlations exhibited in multiple relationships. The main objective of this paper is to analyze the algorithmic stability and generalization guarantees of MRGCNs to confirm the usefulness of MRGCNs. Our contributions are of three folds. First, we develop a matrix representation of various tensor operations underneath MRGCNs to simplify the analysis significantly. Next, we prove the uniform stability of MRGCNs and deduce the convergence of the generalization gap to support the usefulness of MRGCNs. The analysis sheds lights on the design of MRGCNs, for instance, how the data should be scaled to achieve the uniform stability of the learning process. Finally, we provide experimental results to demonstrate the stability results.", 
    "abstract": "The development of deep learning techniques has greatly benefited CNN-based object detectors, leading to unprecedented progress in recent years. However, the distribution variance between training and testing domains causes significant performance degradation. Labeling data for new scenarios is costly and time-consuming, so most existing domain adaptation methods perform feature alignment through adversarial training. While this can improve the accuracy of detectors in unlabeled target domains, the unconstrained domain alignment also negatively transfers the feature distribution, which compromises the recognition ability of the model. To address this problem, we propose the Knowledge Transfer Network (KTNet), which consists of object intrinsic knowledge mining and category relational knowledge constraint modules. Specifically, a binary classifier shared by the source and target domains is designed to extract common attribute knowledge of objects, which can align foreground and background features from different data domains adaptively. Then, we construct relational knowledge graphs to explicitly constrain the category correlations in the source, target, and cross-domain settings. These two modules guide the detector to learn object-related and domain-invariant representations, enabling the proposed KTNet to perform well in four commonly-used cross-domain scenarios. Furthermore, the ablation experiments show that our method is scalable to more complex backbone networks and different detection architectures.", 
    "abstract": "The purpose of this research is to determine the impact of innovation, economic growth, financial development, trade, foreign direct investment (FDI), electricity consumption, and urbanization on the environmental degradations in Pakistan. This study has employed the dynamic autoregressive distributed lag model (ARDL), to investigate the actual change in the independent variables and its impact on the dependent variable through graphs. The findings demonstrate that energy consumption, GDP growth, urbanization, and trade negatively influence the carbon emissions in the short term. On the other hand, the findings indicate that in the long term, only GDP growth and trade had a significantly negative impact on emissions. Urbanization has a positive and considerable impact on the emissions of carbon dioxide in the long run. On the other hand, financial development and foreign direct investment (FDI) help reduce the environmental degradation in the short term and long term. Moreover, innovation positively affects the carbon emissions in both the long and short run. Policy recommendations are given based on the findings of this study.", 
    "abstract": "The study of brain connectivity plays an important role in understanding the functional organizations of the brain. It also helps to identify connectivity signatures that can be used for evaluating neural disorders and monitoring treatment efficacy. In this work, age-related changes in brain connectivity are studied to obtain aging signatures based on various modeling techniques. These include an energy-based machine learning technique to identify brain network interaction differences between two age groups with a large (30 years) age gap between them. Disconnectivity graphs and activation maps of the seven prominent resting-state networks (RSN) were obtained from functional MRI data of old and young adult subjects. Two-sample ", 
    "abstract": "Various relations existing in Electroencephalogram (EEG) data are significant for EEG feature representation. Thus, studies on the graph-based method focus on extracting relevancy between EEG channels. The shortcoming of existing graph studies is that they only consider a single relationship of EEG electrodes, which results an incomprehensive representation of EEG data and relatively low accuracy of emotion recognition. In this paper, we propose a fusion graph convolutional network (FGCN) to extract various relations existing in EEG data and fuse these extracted relations to represent EEG data more comprehensively for emotion recognition. First, the FGCN mines brain connection features on topology, causality, and function. Then, we propose a local fusion strategy to fuse these three graphs to fully utilize the valuable channels with strong topological, causal, and functional relations. Finally, the graph convolutional neural network is adopted to represent EEG data for emotion recognition better. Experiments on SEED and SEED-IV demonstrate that fusing different relation graphs are effective for improving the ability in emotion recognition. Furthermore, the emotion recognition accuracy of 3-class and 4-class is higher than that of other state-of-the-art methods.", 
    "abstract": "Machine learning (ML) and deep learning (DL), in particular, are common tools for anomaly detection (AD). With the rapid increase in the number of Internet-connected devices, the growing desire for Internet of Things (IoT) devices in the home, on our person, and in our vehicles, and the transition to smart infrastructure and the Industrial IoT (IIoT), anomaly detection in these devices is critical. This paper is a survey of anomaly detection in sensor networks/the IoT. This paper defines what an anomaly is and surveys multiple sources based on those definitions. The goal of this survey was to highlight how anomaly detection is being performed on the Internet of Things and sensor networks, identify anomaly detection approaches, and outlines gaps in the research in this domain.", 
    "abstract": "The present study focuses on a crosswise stream of liquid-holding nano-sized particles over an elongating (stretching) surface. Tiny particles of copper are added into base liquid (water). The influence of the micro rotation phenomenon is also considered. By means of appropriate transformations non-linear coupled ordinary differential equations are attained that govern the flow problem. The Runge-Kutta-Fehlberg scheme, together with the shooting method, is engaged to acquire results numerically. Micropolar coupling parameter, microelements concentration and nanoparticles volume fraction effects are examined over the profiles of velocity, temperature and micro-rotation. Moreover, heat flux and shear stress are computed against pertinent parameters and presented through bar graphs. Outcomes revealed that material constant has increasing effects on normal components of flow velocity; however, it decreasingly influences the tangential velocity, micro-rotation components and temperature profile. Temperature profile appeared to be higher for weak concentration of microelements. It is further noticed that normal velocity profile is higher in magnitude for the case of strong concentration (n = 0) of microelements, whereas tangential velocity profile is higher near the surface for the case of weak concentration (n = 0.5) of microelements. An increase of 3.74% in heat flux is observed when the volume fraction of nanoparticles is increased from 1 to 5%.", 
    "abstract": "Big Data analytics is a technique for researching huge and varied datasets and it is designed to uncover hidden patterns, trends, and correlations, and therefore, it can be applied for making superior decisions in healthcare. Drug-drug interactions (DDIs) are a main concern in drug discovery. The main role of precise forecasting of DDIs is to increase safety potential, particularly, in drug research when multiple drugs are co-prescribed. Prevailing conventional method machine learning (ML) approaches mainly depend on handcraft features and lack generalization. Today, deep learning (DL) techniques that automatically study drug features from drug-related networks or molecular graphs have enhanced the capability of computing approaches for forecasting unknown DDIs. Therefore, in this study, we develop a sparrow search optimization with deep learning-based DDI prediction (SSODL-DDIP) technique for healthcare decision making in big data environments. The presented SSODL-DDIP technique identifies the relationship and properties of the drugs from various sources to make predictions. In addition, a multilabel long short-term memory with an autoencoder (MLSTM-AE) model is employed for the DDI prediction process. Moreover, a lexicon-based approach is involved in determining the severity of interactions among the DDIs. To improve the prediction outcomes of the MLSTM-AE model, the SSO algorithm is adopted in this work. To assure better performance of the SSODL-DDIP technique, a wide range of simulations are performed. The experimental results show the promising performance of the SSODL-DDIP technique over recent state-of-the-art algorithms.", 
    "abstract": "Shared decision making near end of life is a balancing act of communicating prognosis to patients and their surrogates/families and engaging them in considering value-concordant management choices. This cross-sectional survey aimed to determine the format in which older patients with chronic illnesses would prefer to receive prognostic information on their treatment options and disease progression, and their desired level of engagement in decision making. With a 60% participation rate, 139 inpatients in two hospitals and five surrogates were presented with six hypothetical scenarios with a randomly assigned sequence: verbal and written summary, graph, table, photo, video, and pamphlet. The majority (76%) of respondents chose the traditional verbal communication of prognosis by their doctor with a written summary as a reference and to share with family; the second choice was a condition-specific pamphlet (63%). Many found the graph and photo to be distressing (36% and 42%, respectively). Most (71%) wanted to know everything about their condition trajectory, and 63% chose shared decision making rather than completely autonomous or full delegation to clinicians or family. There were no gender differentials between wanting to know it all, supporting shared decision making or the preferred format for breaking news (", 
    "abstract": "When quantifying differences in health outcomes between immigrants and non-immigrants, it is common practice to adjust for observed differences in outcome risk factors between the groups being compared. However, as some of these outcome risk factors may act as mediators on the causal path between the exposure and outcome, adjusting for these may remove effects of factors that characterize the immigrants rather than removing a bias between immigrants and non-immigrants.\nThis study investigates the underlying conditions for which adjusting for outcome risk factors in regression models can lead to the estimation of either total or direct effect for the difference in health outcomes between immigrants and non-immigrants. For this investigation, we use modern tools in causal inference to construct causal models that we believe are highly relevant in an immigrant dataset. In these models, the outcome risk factor is modeled either as a mediator, a selection factor, or a combined mediator/selection factor. Unlike mediators, selection factors are variables that affect the probability of being in the immigrant dataset and may contribute to a bias when comparing immigrants and non-immigrants.\nWhen the outcome risk factor acts both as a mediator and selection factor, the adjustment for the risk factor in regression models leads to the estimation of what is known as a \"controlled\" direct effect. When the outcome risk factor is either a selection factor or a mediator alone, the adjustment for the risk factor in regression models leads to the estimation of a total effect or a controlled direct effect, respectively. In all regression analyses, also adjusting for various confounding paths, including mediator-outcome confounding, may be necessary to obtain valid controlled direct effects or total effects.\nDepending on the causal role of the outcome risk factors in immigrant datasets, regression adjustment for these may result in the estimation of either total effects or controlled direct effects for the difference in outcomes between immigrants and non-immigrants. Because total and controlled direct effects are interpreted differently, we advise researchers to clarify to the readers which types of effects are presented when adjusting for outcome risk factors in immigrant datasets.", 
    "abstract": "Researchers must utilize exploratory data techniques to clearly present findings to a target audience and create appropriate graphs and figures. Researchers can determine if outliers exist, data are missing, and statistical assumptions will be upheld by understanding data. Additionally, it is essential to comprehend these data when describing them in conclusions of a paper, in a meeting with colleagues invested in the findings, or while reading others\u2019 work.", 
