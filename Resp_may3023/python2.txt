    "abstract": "Color patterns in nonavian reptiles are beautifully diverse, but little is known about the genetics and development of these patterns. Here, we investigated color patterning in pet ball pythons (Python regius), which have been bred to show color phenotypes that differ dramatically from the wildtype form. We report that several color phenotypes in pet animals are associated with putative loss-of-function variants in the gene encoding endothelin receptor EDNRB1: (1) frameshift variants in EDNRB1 are associated with conversion of the normal mottled color pattern to skin that is almost fully white, (2) missense variants affecting conserved sites of the EDNRB1 protein are associated with dorsal, longitudinal stripes, and (3) substitutions at EDNRB1 splice donors are associated with subtle changes in patterning compared to wildtype. We propose that these phenotypes are caused by loss of specialized color cells (chromatophores), with loss ranging from severe (fully white) to moderate (dorsal striping) to mild (subtle changes in patterning). Our study is the first to describe variants affecting endothelin signaling in a nonavian reptile and suggests that reductions in endothelin signaling in ball pythons can produce a variety of color phenotypes, depending on the degree of color cell loss.",

    "abstract": "TRviz is an open-source Python library for decomposing, encoding, aligning and visualizing tandem repeat (TR) sequences. TRviz takes a collection of alleles (TR containing sequences) and one or more motifs as input and generates a plot showing the motif composition of the TR sequences.\nTRviz is an open-source Python library and freely available at https://github.com/Jong-hun-Park/trviz. Detailed documentation is available at https://trviz.readthedocs.io.\nSupplementary data are available at ",

    "abstract": "Since the NHGRI-EBI Catalog of human genome-wide association studies was established by NHGRI in 2008, research on it has attracted more and more researchers as the amount of data has grown rapidly. Easy-to-use, open-source, general-purpose programs for accessing the NHGRI-EBI Catalog of human genome-wide association studies are in great demand for current Python data analysis pipeline.\nIn this work we present pandasGWAS, a Python package that provides programmatic access to the NHGRI-EBI Catalog of human genome-wide association studies. Instead of downloading all data locally, pandasGWAS queries data based on input criteria and handles paginated data gracefully. The data is then transformed into multiple associated pandas.DataFrame objects according to its hierarchical relationships, which makes it easy to integrate into current Python-based data analysis toolkits.\npandasGWAS is an open-source Python package that provides the first Python client interface to the GWAS Catalog REST API. Compared with existing tools, the data structure of pandasGWAS is more consistent with the design specification of GWAS Catalog REST API, and provides many easy-to-use mathematical symbol operations.",

    "abstract": "We present pyGOMoDo, a Python library to perform homology modeling and docking, specifically designed for human GPCRs. pyGOMoDo is a python wrap-up of the updated functionalities of GOMoDo web server (https://molsim.sci.univr.it/gomodo). It was developed having in mind its usage through Jupyter notebooks, where users can create their own protocols of modeling and docking of GPCRs. In this article, we focus on the internal structure and general capabilities of pyGOMoDO and on how it can be useful for carrying out structural biology studies of GPCRs.\nThe source code is freely available at https://github.com/rribeiro-sci/pygomodo under the Apache 2.0 license. Tutorial notebooks containing minimal working examples can be found at https://github.com/rribeiro-sci/pygomodo/tree/main/examples.",

    "abstract": "'PascalX' is a Python library providing fast and accurate tools for mapping SNP-wise GWAS summary statistics. Specifically, it allows for scoring genes and annotated gene sets for enrichment signals based on data from, both, single GWAS and pairs of GWAS. The gene scores take into account the correlation pattern between SNPs. They are based on the cumulative density function of a linear combination of \u03c72 distributed random variables, which can be calculated either approximately or exactly to high precision. Acceleration via multithreading and GPU is supported. The code of PascalX is fully open source and well suited as a base for method development in the GWAS enrichment test context.\nThe source code is available at https://github.com/BergmannLab/PascalX and archived under doi://10.5281/zenodo.4429922. A user manual with usage examples is available at https://bergmannlab.github.io/PascalX/.",

    "abstract": "Large-scale multi-ethnic DNA sequencing data is increasingly available owing to decreasing cost of modern sequencing technologies. Inference of the population structure with such sequencing data is fundamentally important. However, the ultra-dimensionality and complicated linkage disequilibrium patterns across the whole genome make it challenging to infer population structure using traditional principal component analysis based methods and software.\nWe present the ERStruct Python Package, which enables the inference of population structure using whole-genome sequencing data. By leveraging parallel computing and GPU acceleration, our package achieves significant improvements in the speed of matrix operations for large-scale data. Additionally, our package features adaptive data splitting capabilities to facilitate computation on GPUs with limited memory.\nOur Python package ERStruct is an efficient and user-friendly tool for estimating the number of top informative principal components that capture population structure from whole genome sequencing data.",

    "abstract": "The aim was to develop in-house software that is able to calculate and generate the biological plan evaluation of the esophagus treatment plan using the Niemierko model for normal tissue complication probability and tumor control probability. The Niemierko model can be applied for esophagus cancer treatment plan to estimate the tumor control probability (TCP) and the normal tissue complication probability (NTCP) using different planning techniques. The equivalent uniform dose (EUD) and effective volume parameters were compared with organ at risk. Subsequently, EUD and TCP parameter were compared with tumor volume for all five different planning techniques.\nTen cases for esophageal cancer were included in this study. For each patient, five treatment plans were generated. The Anisotropic analytical algorithms (AAA) were used for dose calculation for the three-dimensional conformal radiation therapy (3D-CRT), intensity modulated radiation therapy (IMRT) and volumetric modulated arc therapy (VMAT) techniques. The in-house developed radiobiological plan evaluation software using python programming is used for this study which takes a dose volume histogram (DVH) text file as an input file for biological plan evaluation.\nEUD, NTCP, TCP and effective volume were calculated from the Niemierko model using the in-house developed python based software and compared with treatment monitor units (MU) with all five different treatment plan. The best technique is quantified as benchmarked out of other different qualities of treatment. The four field 3D-CRT treatment plan is found to be the best suited from the perspective of biological plan index evaluation among the other planning techniques.",

    "abstract": "WaveTrain is an open-source software for numerical simulations of chain-like quantum systems with nearest-neighbor (NN) interactions only. The Python package is centered around tensor train (TT, or matrix product) format representations of Hamiltonian operators and (stationary or time-evolving) state vectors. It builds on the Python tensor train toolbox Scikit_tt, which provides efficient construction methods and storage schemes for the TT format. Its solvers for eigenvalue problems and linear differential equations are used in WaveTrain for the time-independent and time-dependent Schr\u00f6dinger equations, respectively. Employing efficient decompositions to construct low-rank representations, the tensor-train ranks of state vectors are often found to depend only marginally on the chain length N. This results in the computational effort growing only slightly more than linearly with N, thus mitigating the curse of dimensionality. As a complement to the classes for full quantum mechanics, WaveTrain also contains classes for fully classical and mixed quantum-classical (Ehrenfest or mean field) dynamics of bipartite systems. The graphical capabilities allow visualization of quantum dynamics \"on the fly,\" with a choice of several different representations based on reduced density matrices. Even though developed for treating quasi-one-dimensional excitonic energy transport in molecular solids or conjugated organic polymers, including coupling to phonons, WaveTrain can be used for any kind of chain-like quantum systems, with or without periodic boundary conditions and with NN interactions only. The present work describes version 1.0 of our WaveTrain software, based on version 1.2 of scikit_tt, both of which are freely available from the GitHub platform where they will also be further developed. Moreover, WaveTrain is mirrored at SourceForge, within the framework of the WavePacket project for numerical quantum dynamics. Worked-out demonstration examples with complete input and output, including animated graphics, are available.",

    "abstract": "Molecular dynamics simulations, at different scales, have been exploited for investigating complex mechanisms ruling biologically inspired systems. Nonetheless, with recent advances and unprecedented achievements, the analysis of molecular dynamics simulations requires customized workflows. In 2018, we developed Morphoscanner to retrieve structural relations within self-assembling peptide systems. In particular, we conceived Morphoscanner for tracking the emergence of \u03b2-structured domains in self-assembling peptide systems. Here, we introduce Morphoscanner2.0. Morphoscanner2.0 is an object-oriented library for structural and temporal analysis of atomistic and coarse-grained molecular dynamics (CG-MD) simulations written in Python. The library leverages MDAnalysis, PyTorch and NetworkX to perform the pattern recognition of secondary structure patterns, and interfaces with Pandas, Numpy and Matplotlib to make the results accessible to the user. We used Morphoscanner2.0 on both simulation trajectories and protein structures. Because of its dependencies on the MDAnalysis package, Morphoscanner2.0 can read several file formats generated by widely-used molecular simulation packages such as NAMD, Gromacs, OpenMM. Morphoscanner2.0 also includes a routine for tracking the alpha-helix domain formation.",

    "abstract": "Evolutionary Game Theory (EGT) provides an important framework to study collective behavior. It combines ideas from evolutionary biology and population dynamics with the game theoretical modeling of strategic interactions. Its importance is highlighted by the numerous high level publications that have enriched different fields, ranging from biology to social sciences, in many decades. Nevertheless, there has been no open source library that provided easy, and efficient, access to these methods and models. Here, we introduce EGTtools, an efficient hybrid C++/Python library which provides fast implementations of both analytical and numerical EGT methods. EGTtools is able to analytically evaluate a system based on the replicator dynamics. It is also able to evaluate any EGT problem resorting to finite populations and large-scale Markov processes. Finally, it resorts to C++ and MonteCarlo simulations to estimate many important indicators, such as stationary or strategy distributions. We illustrate all these methodologies with concrete examples and analysis.",

    "abstract": "Genome-wide association studies (GWASs) have identified numerous genetic variants associated with complex disease risk; however, most of these associations are non-coding, complicating identifying their proximal target gene. Transcriptome-wide association studies (TWASs) have been proposed to mitigate this gap by integrating expression quantitative trait loci (eQTL) data with GWAS data. Numerous methodological advancements have been made for TWAS, yet each approach requires ad hoc simulations to demonstrate feasibility. Here, we present twas_sim, a computationally scalable and easily extendable tool for simplified performance evaluation and power analysis for TWAS methods.\nSoftware and documentation are available at https://github.com/mancusolab/twas_sim.",

    "abstract": "The Python research environment for radiation therapy (PyRERT) is a set of business software for hospital physicists to conduct radiation therapy research.\nChoose the open source Enthought Tool Suite\uff08ETS\uff09 as the core external dependency library of PyRERT. PyRERT is divided into base layer, content layer and interaction layer, and each layer is composed of different functional modules.\nPyRERT V1.0 provide a good development environment for scientific research programming in DICOM RT file processing, batch processing of water tank scan data, digital phantom creation, 3D medical image volume visualization, virtual radiotherapy equipment driver, and film scan image analysis.\nPyRERT enables the results of the research group to be iteratively inherited in the form of software. It's reusable basic classes and functional modules greatly improve the efficiency of scientific research task programming.",

    "abstract": null,

    "abstract": "The DynaSig-ML ('Dynamical Signatures-Machine Learning') Python package allows the efficient, user-friendly exploration of 3D dynamics-function relationships in biomolecules, using datasets of experimental measures from large numbers of sequence variants. It does so by predicting 3D structural dynamics for every variant using the Elastic Network Contact Model (ENCoM), a sequence-sensitive coarse-grained normal mode analysis model. Dynamical Signatures represent the fluctuation at every position in the biomolecule and are used as features fed into machine learning models of the user's choice. Once trained, these models can be used to predict experimental outcomes for theoretical variants. The whole pipeline can be run with just a few lines of Python and modest computational resources. The compute-intensive steps are easily parallelized in the case of either large biomolecules or vast amounts of sequence variants. As an example application, we use the DynaSig-ML package to predict the maturation efficiency of human microRNA miR-125a variants from high-throughput enzymatic assays.\nDynaSig-ML is open-source software available at https://github.com/gregorpatof/dynasigml_package.",

    "abstract": "Evolutionary processes happen gradually over time and are, thus, considered time dependent. In addition, several evolutionary processes are either adaptations to local habitats or changing habitats, otherwise restricted thereby. Since evolutionary processes driving speciation take place within the landscape of environmental and temporal bounds, several published studies have aimed at providing accurate, fossil-calibrated, estimates of the divergence times of both extant and extinct species. Correct calibration is critical towards attributing evolutionary adaptations and speciation both to the time and paleogeography that contributed to it. Data from more than 4000 studies and nearly 1,50,000 species are available from a central TimeTree resource and provide opportunities of retrieving divergence times, evolutionary timelines, and time trees in various formats for most vertebrates. These data greatly enhance the ability of researchers to investigate evolution. However, there is limited functionality when studying lists of species that require batch retrieval. To overcome this, a PYTHON package termed Python-Automated Retrieval of TimeTree data (PAReTT) was created to facilitate a biologist-friendly interaction with the TimeTree resource. Here, we illustrate the use of the package through three examples that includes the use of timeline data, time tree data, and divergence time data. Furthermore, PAReTT was previously used in a meta-analysis of candidate genes to illustrate the relationship between divergence times and candidate genes of migration. The PAReTT package is available for download from GitHub or as a pre-compiled Windows executable, with extensive documentation on the package available on GitHub wiki pages regarding dependencies, installation, and implementation of the various functions.",

    "abstract": "PyHMMER provides Python integration of the popular profile Hidden Markov Model software HMMER via Cython bindings. This allows the annotation of protein sequences with profile HMMs and building new ones directly with Python. PyHMMER increases flexibility of use, allowing creating queries directly from Python code, launching searches, and obtaining results without I/O, or accessing previously unavailable statistics like uncorrected P-values. A new parallelization model greatly improves performance when running multithreaded searches, while producing the exact same results as HMMER.\nPyHMMER supports all modern Python versions (Python 3.6+) and similar platforms as HMMER (x86 or PowerPC UNIX systems). Pre-compiled packages are released via PyPI (https://pypi.org/project/pyhmmer/) and Bioconda (https://anaconda.org/bioconda/pyhmmer). The PyHMMER source code is available under the terms of the open-source MIT licence and hosted on GitHub (https://github.com/althonos/pyhmmer); its documentation is available on ReadTheDocs (https://pyhmmer.readthedocs.io).",

    "abstract": "Construction of kinship matrices among individuals is an important step for both association studies and prediction studies based on different levels of omic data. Methods for constructing kinship matrices are becoming diverse and different methods have their specific appropriate scenes. However, software that can comprehensively calculate kinship matrices for a variety of scenarios is still in an urgent demand.\nIn this study, we developed an efficient and user-friendly python module, PyAGH, that can accomplish (1) conventional additive kinship matrces construction based on pedigree, genotypes, abundance data from transcriptome or microbiome; (2) genomic kinship matrices construction in combined population; (3) dominant and epistatic effects kinship matrices construction; (4) pedigree selection, tracing, detection and visualization; (5) visualization of cluster, heatmap and PCA analysis based on kinship matrices. The output from PyAGH can be easily integrated in other mainstream software based on users' purposes. Compared with other softwares, PyAGH integrates multiple methods for calculating the kinship matrix and has advantages in terms of speed and data size compared to other software. PyAGH is developed in python and C\u2009+\u2009\u2009+\u2009and can be easily installed by pip tool. Installation instructions and a manual document can be freely available from https://github.com/zhaow-01/PyAGH .\nPyAGH is a fast and user-friendly Python package for calculating kinship matrices using pedigree, genotype, microbiome and transcriptome data as well as processing, analyzing and visualizing data and results. This package makes it easier to perform predictions and association studies processes based on different levels of omic data.",

    "abstract": "Primer design is a routine practice for modern molecular biology labs. Bioinformatics tools like primer3 and primer-blast have standardized the primer design for a specific region. However, large-scale primer design, especially for genome-wide screening, is still a labor-intensive job for most wet-lab researchers using these pipelines.\nHere, we present the primerdiffer pipeline, which can be used to batch design primers that differentiate haplotypes on a large scale with precise false priming checking. This command-line interface (CLI) pipeline includes greedy primer search, local and global in silico PCR-based false priming checking, and automated best primer selection. The local CLI application provides flexibility to design primers with the user's own genome sequences and specific parameters. Some species-specific primers designed to genotype the hybrid introgression strains from Caenorhabditis briggsae and Caenorhabditis nigoni have been validated using single-worm PCR. This pipeline provides the first CLI-based large-scale primer design tool to differentiate haplotypes in any targeted region.\nThe open-source python modules are available at github (https://github.com/runsheng/primerdiffer, https://github.com/runsheng/primervcf) and Python package index (https://pypi.org/project/primerdiffer/, https://pypi.org/project/primervcf/).",

    "abstract": "Predicting complex traits from genotypic information is a major challenge in various biological domains. With easyPheno, we present a comprehensive Python framework enabling the rigorous training, comparison and analysis of phenotype predictions for a variety of different models, ranging from common genomic selection approaches over classical machine learning and modern deep learning-based techniques. Our framework is easy-to-use, also for non-programming-experts, and includes an automatic hyperparameter search using state-of-the-art Bayesian optimization. Moreover, easyPheno provides various benefits for bioinformaticians developing new prediction models. easyPheno enables to quickly integrate novel models and functionalities in a reliable framework and to benchmark against various integrated prediction models in a comparable setup. In addition, the framework allows the assessment of newly developed prediction models under pre-defined settings using simulated data. We provide a detailed documentation with various hands-on tutorials and videos explaining the usage of easyPheno to novice users.\neasyPheno is publicly available at https://github.com/grimmlab/easyPheno and can be easily installed as Python package via https://pypi.org/project/easypheno/ or using Docker. A comprehensive documentation including various tutorials complemented with videos can be found at https://easypheno.readthedocs.io/.\nSupplementary data are available at ",

    "abstract": "Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.",

    "abstract": "Bioinformatics education can be defined as the teaching and learning of how to use software tools, along with mathematical and statistical analysis, to solve biological problems. Although many resources are available, most students still struggle to understand even the simplest sequence alignment algorithms. Applying visualizations to these topics benefits both lecturers and students. Unfortunately, educational software for visualizing step-bystep processes in the user experience of sequence alignment algorithms is rare. In this article, an educational visualization tool for biological sequence alignment is presented, and the source code is released in order to encourage the collaborative power of open-source software, with the expectation of further contributions from the community in the future. Two different modules are integrated to enable a student to investigate the characteristics of alignment algorithms.",

    "abstract": "OncoPrint, the plot to visualize an overview of genetic variants in sequencing data, has been widely used in the field of cancer genomics. However, still, there have been no Python libraries capable to generate OncoPrint yet, a big hassle to plot OncoPrints within Python-based genetic variants analysis pipelines. This paper introduces a new Python package PyOncoPrint, which can be easily used to plot OncoPrints in Python. The package is based on the existing widely used scientific plotting library Matplotlib, the resulting plots are easy to be adjusted for various needs.",

    "abstract": "Seeking probabilistic motifs in a sequence is a common task to annotate putative transcription factor binding sites or other RNA/DNA binding sites. Useful motif representations include position weight matrices (PWMs), dinucleotide PWMs (di-PWMs), and hidden Markov models (HMMs). Dinucleotide PWMs not only combine the simplicity of PWMs-a matrix form and a cumulative scoring function-but also incorporate dependency between adjacent positions in the motif (unlike PWMs which disregard any dependency). For instance to represent binding sites, the HOCOMOCO database provides di-PWM motifs derived from experimental data. Currently, two programs, SPRy-SARUS and MOODS, can search for occurrences of di-PWMs in sequences.\nWe propose a Python package called dipwmsearch, which provides an original and efficient algorithm for this task (it first enumerates matching words for the di-PWM, and then searches these all at once in the sequence, even if the latter contains IUPAC codes). The user benefits from an easy installation via Pypi or conda, a comprehensive documentation, and executable scripts that facilitate the use of di-PWMs.\ndipwmsearch is available at https://pypi.org/project/dipwmsearch/ and https://gite.lirmm.fr/rivals/dipwmsearch/ under Cecill license.",

    "abstract": "We present two open-source Python packages: \"electron spectro-microscopy\" (espm) and \"electron microscopy tables\" (emtables). The espm software enables the simulation of scanning transmission electron microscopy energy-dispersive X-ray spectroscopy datacubes, based on user-defined chemical compositions and spatial abundance maps of constituent phases. The simulation process uses X-ray emission cross-sections generated via state-of-the-art calculations made with emtables. These tables are designed to be easily modifiable, either manually or using espm. The simulation framework is designed to test the application of decomposition algorithms for the analysis of STEM-EDX spectrum images with access to a known ground truth. We validate our approach using the case of a complex geology-related sample, comparing raw simulated and experimental datasets and the outputs of their non-negative matrix factorization. In addition to testing machine learning algorithms, our packages will also help experimental design, for instance, predicting dataset characteristics or establishing minimum counts needed to measure nanoscale features.",

    "abstract": "Here we introduce chiLife, a Python package for site-directed spin label (SDSL) modeling for electron paramagnetic resonance (EPR) spectroscopy, in particular double electron-electron resonance (DEER). It is based on in silico attachment of rotamer ensemble representations of spin labels to protein structures. chiLife enables the development of custom protein analysis and modeling pipelines using SDSL EPR experimental data. It allows the user to add custom spin labels, scoring functions and spin label modeling methods. chiLife is designed with integration into third-party software in mind, to take advantage of the diverse and rapidly expanding set of molecular modeling tools available with a Python interface. This article describes the main design principles of chiLife and presents a series of examples.",

    "abstract": "Low-affinity interactions among multivalent biomolecules may lead to the formation of molecular complexes that undergo phase transitions to become extra-large clusters. Characterizing the physical properties of these clusters is important in recent biophysical research. Due to weak interactions such clusters are highly stochastic, demonstrating a wide range of sizes and compositions. We have developed a Python package to perform multiple stochastic simulation runs using NFsim (Network-Free stochastic simulator), characterize and visualize the distribution of cluster sizes, molecular composition, and bonds across molecular clusters and individual molecules of different types.\nThe software is implemented in Python. A detailed Jupyter notebook is provided to enable convenient running. Code, user guide and examples are freely available at https://molclustpy.github.io/.\nachattaraj007@gmail.com , blinov@uchc.edu.\nAvailable at https://molclustpy.github.io/.",

    "abstract": "Deep neural network models (DNNs) are essential to modern AI and provide powerful models of information processing in biological neural networks. Researchers in both neuroscience and engineering are pursuing a better understanding of the internal representations and operations that undergird the successes and failures of DNNs. Neuroscientists additionally evaluate DNNs as models of brain computation by comparing their internal representations to those found in brains. It is therefore essential to have a method to easily and exhaustively extract and characterize the results of the internal operations of any DNN. Many models are implemented in PyTorch, the leading framework for building DNN models. Here we introduce ",

    "abstract": "MR image classification in datasets collected from multiple sources is complicated by inconsistent and missing DICOM metadata. Therefore, we aimed to establish a method for the efficient automatic classification of MR brain sequences.\nDeep convolutional neural networks (DCNN) were trained as one-vs-all classifiers to differentiate between six classes: T1 weighted (w), contrast-enhanced T1w, T2w, T2w-FLAIR, ADC, and SWI. Each classifier yields a probability, allowing threshold-based and relative probability assignment while excluding images with low probability (label: unknown, open-set recognition problem). Data from three high-grade glioma (HGG) cohorts was assessed; C1 (320 patients, 20,101 MRI images) was used for training, while C2 (197, 11,333) and C3 (256, 3522) were for testing. Two raters manually checked images through an interactive labeling tool. Finally, MR-Class' added value was evaluated via radiomics model performance for progression-free survival (PFS) prediction in C2, utilizing the concordance index (C-I).\nApproximately 10% of annotation errors were observed in each cohort between the DICOM series descriptions and the derived labels. MR-Class accuracy was 96.7% [95% Cl: 95.8, 97.3] for C2 and 94.4% [93.6, 96.1] for C3. A total of 620 images were misclassified; manual assessment of those frequently showed motion artifacts or alterations of anatomy by large tumors. Implementation of MR-Class increased the PFS model C-I by 14.6% on average, compared to a model trained without MR-Class.\nWe provide a DCNN-based method for the sequence classification of brain MR images and demonstrate its usability in two independent HGG datasets.",

    "abstract": "Virtual screening using molecular docking is now routinely used for the rapid evaluation of very large ligand libraries in early stage drug discovery. As the size of compound libraries which can feasibly be screened grows, so do the challenges in result management and storage. Here we introduce Ringtail, a new Python tool in the AutoDock Suite for efficient storage and analysis of virtual screening data based on portable SQLite databases. Ringtail is designed to work with AutoDock-GPU and AutoDock Vina out-of-the-box. Its modular design also allows for easy extension to support input file types from other docking software, different storage solutions, and incorporation into other applications. Ringtail's SQLite database output can dramatically reduce the required disk storage (36-46 fold) by selecting individual poses to store and by taking advantage of the relational database format. Filtering times are also dramatically reduced, requiring minutes to filter millions of ligands. Thus, Ringtail is a tool that can immediately integrate into existing virtual screening pipelines using AutoDock-GPU and Vina, and is scriptable and modifiable to fit specific user needs.",

    "abstract": "Obtaining nitrous oxide isotopocule measurements with isotope ratio mass spectrometry (IRMS) involves analyzing the ion current ratios of the nitrous oxide parent ion (N\nWe developed a user-friendly Python package (pyisotopomer) to determine two coefficients (\u03b3 and \u03ba) that describe scrambling in the IRMS ion source, and then used this calibration to obtain intramolecular isotope deltas in N\nWith two appropriate reference materials, \u03b3 and \u03ba can be determined robustly and accurately for a given IRMS system. An additional third reference material is needed to define the zero-point of the delta scale. We show that IRMS scrambling behavior can vary with time, necessitating regular calibrations. Finally, we present an intercalibration between two IRMS laboratories, using pyisotopomer to calculate \u03b3 and \u03ba, and to obtain intramolecular N\nGiven these considerations, we discuss how to use pyisotopomer to obtain high-quality N",

    "abstract": "Biochemical reaction prediction tools leverage enzymatic promiscuity rules to generate reaction networks containing novel compounds and reactions. The resulting reaction networks can be used for multiple applications such as designing novel biosynthetic pathways and annotating untargeted metabolomics data. It is vital for these tools to provide a robust, user-friendly method to generate networks for a given application. However, existing tools lack the flexibility to easily generate networks that are tailor-fit for a user's application due to lack of exhaustive reaction rules, restriction to pre-computed networks, and difficulty in using the software due to lack of documentation.\nHere we present Pickaxe, an open-source, flexible software that provides a user-friendly method to generate novel reaction networks. This software iteratively applies reaction rules to a set of metabolites to generate novel reactions. Users can select rules from the prepackaged JN1224min ruleset, derived from MetaCyc, or define their own custom rules. Additionally, filters are provided which allow for the pruning of a network on-the-fly based on compound and reaction properties. The filters include chemical similarity to target molecules, metabolomics, thermodynamics, and reaction feasibility filters. Example applications are given to highlight the capabilities of Pickaxe: the expansion of common biological databases with novel reactions, the generation of industrially useful chemicals from a yeast metabolome database, and the annotation of untargeted metabolomics peaks from an E. coli dataset.\nPickaxe predicts novel metabolic reactions and compounds, which can be used for a variety of applications. This software is open-source and available as part of the MINE Database python package ( https://pypi.org/project/minedatabase/ ) or on GitHub ( https://github.com/tyo-nu/MINE-Database ). Documentation and examples can be found on Read the Docs ( https://mine-database.readthedocs.io/en/latest/ ). Through its documentation, pre-packaged features, and customizable nature, Pickaxe allows users to generate novel reaction networks tailored to their application.",

    "abstract": "We describe a new open-source Python-based package for high accuracy correlated electron calculations using quantum Monte Carlo (QMC) in real space: PyQMC. PyQMC implements modern versions of QMC algorithms in an accessible format, enabling algorithmic development and easy implementation of complex workflows. Tight integration with the PySCF environment allows for a simple comparison between QMC calculations and other many-body wave function techniques, as well as access to high accuracy trial wave functions.",

    "abstract": "Various inaccurate traditional models have resulted in major ambiguities and gaps in the interpretation of Anatolian plate deformation directions. To address this issue, a GIS-based spatial statistical analysis method was used for the first time to detect the directional distribution of deformation along the Anatolian Plate in Turkey. Two strategies were used in this study: firstly, identifying the abnormally active seismic areas by detecting significant hotspot and cold spot clusters and confirming this detection using optimized hotspot analysis for earthquake events that occurred from 1900 to the end of 2019. Secondly, detecting the directional distribution of deformation using a Standard Deviational Ellipse (SDE) by calculating the standard deviation of the x and y coordinates from the mean center for each set of earthquake events in the Anaconda Python Platform and ArcGIS 10.8 software. Our improved geostatistical analysis results confirmed the existence of abnormal seismic hazard zones within the study area and three deformation directions: the east-west trend, the southeast-northwest trend, and the south-north trend.",

    "abstract": "Here, we develop and show the use of an open-source Python library to control commercial potentiostats. It standardizes the commands for different potentiostat models, opening the possibility to perform automated experiments independently of the instrument used. At the time of this writing, we have included potentiostats from CH Instruments (models 1205B, 1242B, 601E, and 760E) and PalmSens (model Emstat Pico), although the open-source nature of the library allows for more to be included in the future. To showcase the general workflow and implementation of a real experiment, we have automated the Randles-\u0160ev\u010d\u0131\u0301k methodology to determine the diffusion coefficient of a redox-active species in solution using cyclic voltammetry. This was accomplished by writing a Python script that includes data acquisition, data analysis, and simulation. The total run time was 1 min and 40 s, well below the time it would take even an experienced electrochemist to apply the methodology in a traditional manner. Our library has potential applications that expand beyond the automation of simple repetitive tasks; for example, it can interface with peripheral hardware and well-established third-party Python libraries as part of a more complex and intelligent setup that relies on laboratory automation, advanced optimization, and machine learning.",

    "abstract": "Protex is an open-source program that enables proton exchanges of solvent molecules during molecular dynamics simulations. While conventional molecular dynamics simulations do not allow for bond breaking or formation, protex offers an easy-to-use interface to augment these simulations and define multiple proton sites for (de-)protonation using a single topology approach with two different ",

    "abstract": "Annotation of nonmodel organisms is an open problem, especially the detection of untranslated regions (UTRs). Correct annotation of UTRs is crucial in transcriptomic analysis to accurately capture the expression of each gene yet is mostly overlooked in annotation pipelines. Here we present peaks2utr, an easy-to-use Python command line tool that uses the UTR enrichment of single-cell technologies, such as 10\u00d7 Chromium, to accurately annotate 3' UTRs for a given canonical annotation.\npeaks2utr is implemented in Python 3 (\u22653.8). It is available via PyPI at https://pypi.org/project/peaks2utr and GitHub at https://github.com/haessar/peaks2utr. It is licensed under GNU GPLv3.",

    "abstract": "As part of a larger effort to aid in seamless integration of Fourier-based multiplexed ion mobility with a range mass analyzers, we have developed an all-in-one graphical user interface tool for FT-IM-MS data analysis that runs directly within a web browser. This tool, FTflow, accepts mzML files and displays necessary information such as mass spectra and extracted ion chromatograms in order to reconstruct arrival time distributions. It also extracts the corresponding mobility-related information (e.g., ",

    "abstract": "",

    "abstract": "In the era where transcriptome profiling moves towards single-cell and spatial resolutions, the traditional co-expression analysis lacks the power to fully utilize such rich information to unravel spatial gene associations. Here we present a Python package called Spatial Enrichment Analysis of Gene Associations using L-index (SEAGAL) to detect and visualize spatial gene correlations at both single-gene and gene-set levels. Our package takes spatial transcriptomics data sets with gene expression and the aligned spatial coordinates as input. It allows for analyzing and visualizing spatial correlations at both single-gene and gene-set levels. The output could be visualized as volcano plots and heatmaps with a few lines of code, thus providing an easy-yet-comprehensive tool for mining spatial gene associations.\nThe Python package SEAGAL can be installed using pip: https://pypi.org/project/seagal/ . The source code and step-by-step tutorials are available at: https://github.com/linhuawang/SEAGAL .\nlinhuaw@bcm.edu.",

    "abstract": "Conformational heterogeneity is a defining hallmark of intrinsically disordered proteins and protein regions (IDRs). The functions of IDRs and the emergent cellular phenotypes they control are associated with sequence-specific conformational ensembles. Simulations of conformational ensembles that are based on atomistic and coarse-grained models are routinely used to uncover the sequence-specific interactions that may contribute to IDR functions. These simulations are performed either independently or in conjunction with data from experiments. Functionally relevant features of IDRs can span a range of length scales. Extracting these features requires analysis routines that quantify a range of properties. Here, we describe a new analysis suite SOURSOP, an object-oriented and open-source toolkit designed for the analysis of simulated conformational ensembles of IDRs. SOURSOP implements several analysis routines motivated by principles in polymer physics, offering a unique collection of simple-to-use functions to characterize IDR ensembles. As an extendable framework, SOURSOP supports the development and implementation of new analysis routines that can be easily packaged and shared.",

    "abstract": "Deep learning has shown great promise as the backbone of clinical decision support systems. Synthetic data generated by generative models can enhance the performance and capabilities of data-hungry deep learning models. However, there is (1)\u00a0limited availability of (synthetic) datasets and (2)\u00a0generative models are complex to train, which hinders their adoption in research and clinical applications. To reduce this entry barrier, we explore generative model sharing to allow more researchers to access, generate, and benefit from synthetic data.\nWe propose \nThe scalability and design of the library are demonstrated by its growing number of integrated and readily-usable pretrained generative models, which include 21 models utilizing nine different generative adversarial network architectures trained on 11 different datasets. We further analyze three ",

    "abstract": "Leptospirosis is a potentially life-threatening zoonosis caused by pathogenic Leptospira. The major hurdle of the diagnosis of Leptospirosis lies in the issues associated with current methods of detection, which are time-consuming, tedious and the need for sophisticated, special equipments. Restrategizing the diagnostics of Leptospirosis may involve considerations of the direct detection of the outer membrane protein, which can be faster, cost-saving and require fewer equipments. One such promising marker is LipL32, which is an antigen with high amino acid sequence conservation among all the pathogenic strains. In this study, we endeavored to isolate an aptamer against LipL32 protein via a modified SELEX strategy known as tripartite-hybrid SELEX, based on 3 different partitioning strategies. In this study, we also demonstrated the deconvolution of the candidate aptamers by using in-house Python-aided unbiased data sorting in examining multiple parameters to isolate potent aptamers. We have successfully generated an RNA aptamer against LipL32 of Leptospira, LepRapt-11, which is applicable in a simple direct ELASA for the detection of LipL32. LepRapt-11 can be a promising molecular recognition element for the diagnosis of leptospirosis by targeting LipL32.",

    "abstract": "Despite the creation of thousands of machine learning (ML) models, the promise of improving patient care with ML remains largely unrealized. Adoption into clinical practice is lagging, in large part due to disconnects between how ML practitioners evaluate models and what is required for their successful integration into care delivery. Models are just one component of care delivery workflows whose constraints determine clinicians' abilities to act on models' outputs. However, methods to evaluate the usefulness of models in the context of their corresponding workflows are currently limited. To bridge this gap we developed APLUS, a reusable framework for quantitatively assessing via simulation the utility gained from integrating a model into a clinical workflow. We describe the APLUS simulation engine and workflow specification language, and apply it to evaluate a novel ML-based screening pathway for detecting peripheral artery disease at Stanford Health Care.",

    "abstract": "MoSDeF-GOMC is a python interface for the Monte Carlo software GOMC to the Molecular Simulation Design Framework (MoSDeF) ecosystem. MoSDeF-GOMC automates the process of generating initial coordinates, assigning force field parameters, and writing coordinate (PDB), connectivity (PSF), force field parameter, and simulation control files. The software lowers entry barriers for novice users while allowing advanced users to create complex workflows that encapsulate simulation setup, execution, and data analysis in a single script. All relevant simulation parameters are encoded within the workflow, ensuring reproducible simulations. MoSDeF-GOMC's capabilities are illustrated through a number of examples, including prediction of the adsorption isotherm for CO",

    "abstract": "Here, we introduce FRETpredict, a Python software program to predict FRET efficiencies from ensembles of protein conformations. FRETpredict uses an established Rotamer Library Approach to describe the FRET probes covalently bound to the protein. The software efficiently operates on large conformational ensembles such as those generated by molecular dynamics simulations to facilitate the validation or refinement of molecular models and the interpretation of experimental data. We demonstrate the performance and accuracy of the software for different types of systems: a relatively structured peptide (polyproline 11), an intrinsically disordered protein (ACTR), and three folded proteins (HiSiaP, SBD2, and MalE). We also describe a general approach to generate new rotamer libraries for FRET probes of interest. FRETpredict is open source (GPLv3) and is available at github.com/KULL-Centre/FRETpredict and as a Python PyPI package at pypi.org/project/FRETpredict .\nWe present FRETpredict, an open-source software to calculate FRET observables from protein structures. Using a previously developed Rotamer Library Approach, FRETpredict helps place multiple conformations of the selected FRET probes at the labeled sites, and use these to calculate FRET efficiencies. Through several case studies, we illustrate the ability of FRETpredict to interpret experimental results and validate protein conformations. We also explain a methodology for generating new rotamer libraries of FRET probes of interest.",

    "abstract": "Plasma ionization is rapidly gaining popularity for mass spectrometry (MS)-based studies of volatiles and aerosols. However, data from plasma ionization are delicate to interpret as competing ionization pathways in the plasma create numerous ion species. There is no tool for detection of adducts and in-source fragments from plasma ionization data yet, which makes data evaluation ambiguous.\nWe developed DBDIpy, a Python library for processing and formal analysis of untargeted, time-sensitive plasma ionization MS datasets. Its core functionality lies in the identification of in-source fragments and identification of rivaling ionization pathways of the same analytes in time-sensitive datasets. It further contains elementary functions for processing of untargeted metabolomics data and interfaces to an established ecosystem for analysis of MS data in Python.\nDBDIpy is implemented in Python (Version \u2265 3.7) and can be downloaded from PyPI the Python package repository (https://pypi.org/project/DBDIpy) or from GitHub (https://github.com/leopold-weidner/DBDIpy).\nSupplementary data are available at Bioinformatics online.",

    "abstract": "\u2003\n\u2003The objective of the present study was to report on follicular stage changes in two ball pythons, \n\u2003Two female pythons - one weighing 2.8\u2009kg, the other weighing 2.5\u2009kg, and neither with a history of reproduction - were examined by ultrasound to enable viewing of ovarian follicles in different phases and sizes. \n\u2003Even given the slow metabolism of reptiles, ultrasound revealed an improvement in follicle homogeneity between 6 and 15 days after the start of homeopathy in both snakes; there was also improved weight gain in both animals. The MOdified NARanjo Criteria for Homeopathy (MONARCH) score was +8 in each of the cases, suggesting a causal relationship between the use of homeopathic medicine and clinical outcome.\n\u2003",

    "abstract": "Among the most relevant themes of modernity, using renewable resources to produce biofuels attracts several countries' attention, constituting a vital part of the global geopolitical chessboard since humanity's energy needs will grow faster and faster. Fortunately, advances in personal computing associated with free and open-source software production facilitate this work of prospecting and understanding complex scenarios. Thus, for the development of this work, the keywords \"biofuel\" and \"nanocatalyst\" were delivered to the Scopus database, which returned 1071 scientific articles. The titles and abstracts of these papers were saved in Research Information Systems (RIS) format and submitted to automatic analysis via the Visualization of Similarities Method implemented in VOSviewer 1.6.18 software. Then, the data extracted from the VOSviewer were processed by software written in Python, which allowed the use of the network data generated by the Visualization of Similarities Method. Thus, it was possible to establish the relationships for the pair between the nodes of all clusters classified by Link Strength Between Items or Terms (LSBI) or by year. Indeed, other associations should arouse particular interest in the readers. However, here, the option was for a numerical criterion. However, all data are freely available, and stakeholders can infer other specific connections directly. Therefore, this innovative approach allowed inferring that the most recent pairs of terms associate the need to produce biofuels from microorganisms' oils besides cerium oxide nanoparticles to improve the performance of fuel mixtures by reducing the emission of hydrocarbons (HC) and oxides of nitrogen (NOx).",

    "abstract": "With the oncoming age of big data, biologists are encountering more use cases for cloud-based computing to streamline data processing and storage. Unfortunately, cloud platforms are difficult to learn, and there are few resources for biologists to demystify them. We have developed a guide for experimental biologists to set up cloud processing on Amazon Web Services to cheaply outsource data processing and storage. Here we provide a guide for setting up a computing environment in the cloud and showcase examples of using Python and Julia programming languages. We present example calcium imaging data in the zebrafish brain and corresponding analysis using suite2p software. Tools for budget and user management are further discussed in the attached protocol. Using this guide, researchers with limited coding experience can get started with cloud-based computing or move existing coding infrastructure into the cloud environment.",

    "abstract": "We introduce Shennong, a Python toolbox and command-line utility for audio speech features extraction. It implements a wide range of well-established state-of-the-art algorithms: spectro-temporal filters such as Mel-Frequency Cepstral Filterbank or Predictive Linear Filters, pre-trained neural networks, pitch estimators, speaker normalization methods, and post-processing algorithms. Shennong is an open source, reliable and extensible framework built on top of the popular Kaldi speech processing library. The Python implementation makes it easy to use by non-technical users and integrates with third-party speech modeling and machine learning tools from the Python ecosystem. This paper describes the Shennong software architecture, its core components, and implemented algorithms. Then, three applications illustrate its use. We first present a benchmark of speech features extraction algorithms available in Shennong on a phone discrimination task. We then analyze the performances of a speaker normalization model as a function of the speech duration used for training. We finally compare pitch estimation algorithms on speech under various noise conditions.",

    "abstract": "Molecular simulation users are sometimes discouraged from using specific molecular models because of the inconvenience of finding the force field parameters and preparing and validating the topology files. To facilitate this process and make the accurate anisotropic force field AUA4 available to molecular dynamics users, we have created and validated an automated topology and coordinate file creation routine for the GROMACS molecular simulation software. In the present work, we describe the AUA4, explain its particularities and how it was implemented, thoroughly validating the implementation, and for the first time, perform a molecular dynamics benchmark for this transferable force field. Several properties were computed, namely, liquid density, vapor pressure, and vaporization enthalpy by conducting explicit vapor-liquid interface simulations. The results evidence the correct implementation showing slight deviations from the parametrization studies. The benchmark shows the superior predictive capability of the AUA4 in recreating liquid density (RMSD equal to 17.0 kg/m",

    "abstract": "The traditional watershed segmentation methods usually suffer from over segmentation for irregularly shaped particles. This is because the distance map of an irregularly shaped particle contains multiple local maxima, and over segmentation would happen if these local maxima were used as seeds for watershed segmentation. In this work, several methods based on morphological reconstruction, including h-dome transform, h-maxima, and area-reconstruction h-dome transform, are introduced to merge, or erase redundant local maxima, and the performance of these methods in avoiding over segmentation is compared. The results show that the area-reconstruction h-dome transform is the most effective method in controlling over segmentation among the evaluated methods. However, the area-reconstruction h-dome transform is achieved by superposition of binary reconstructions at each grayscale level, which is extremely time-consuming and impractical for batch processing. A hybrid pixel-queue algorithm is applied to accelerate the area-reconstruction h-dome transform, and the algorithm is implemented in Cython to further improve the computational efficiency. For a 2592\u2009\u00d7\u20091944 pixel image, on a PC with an Intel Core i5 2.4GHz processor and 8\u00a0GB RAM, the processing time of the area-reconstruction h-dome transform after acceleration is about 549\u2009ms, which is 249 times faster than the unaccelerated algorithm and 4 times faster than the reconstruction function in the Scikit-image library (an open-source image processing library for the Python programming language) which performs reconstruction by dilation. The accelerated area-reconstruction h-dome transform algorithm was successfully applied to the segmentation of rubber particles in a thermoplastic polyolefin (TPO) compound. RESEARCH HIGHLIGHTS: Techniques for segmenting particles with irregular shapes based on morphological reconstruction are reviewed. A fast algorithm for area-reconstruction h-dome transform is introduced based on Vincent's first approach combined with the pixel queue algorithm and Cython acceleration. The accelerated reconstruction algorithm is 249 times faster than the unaccelerated algorithm. The fast area-reconstruction h-dome transform algorithm is successfully applied to rubber particle segmentation of a thermal plastic polyolefin.",

    "abstract": "Artificial intelligence (AI) methods are changing all areas of research and have a variety of capabilities of analysis in ophthalmology, specifically in visual fields (VFs) to detect or predict vision loss progression. Whereas most of the AI algorithms are implemented in Python language, which offers numerous open-source functions and algorithms, the majority of algorithms in VF analysis are offered in the R language. This paper introduces PyVisualFields, a developed package to address this gap and make available VF analysis in the Python language.\nFor the first version, the R libraries for VF analysis provided by vfprogression and visualFields packages are analyzed to define the overlaps and distinct functions. Then, we defined and translated this functionality into Python with the help of the wrapper library rpy2. Besides maintaining, the subsequent versions' milestones are established, and the third version will be R-independent.\nThe developed Python package is available as open-source software via the GitHub repository and is ready to be installed from PyPI. Several Jupyter notebooks are prepared to demonstrate and describe the capabilities of the PyVisualFields package in the categories of data presentation, normalization and deviation analysis, plotting, scoring, and progression analysis.\nWe developed a Python package and demonstrated its functionality for VF analysis and facilitating ophthalmic research in VF statistical analysis, illustration, and progression prediction.\nUsing this software package, researchers working on VF analysis can more quickly create algorithms for clinical applications using cutting-edge AI techniques.",

    "abstract": "Density-based clustering procedures are widely used in a variety of data science applications. Their advantage lies in the capability to find arbitrarily shaped and sized clusters and robustness against outliers. In particular, they proved effective in the analysis of molecular dynamics simulations, where they serve to identify relevant, low-energetic molecular conformations. As such, they can provide a convenient basis for the construction of kinetic (core-set) Markov-state models. Here we present the open-source Python project CommonNNClustering, which provides an easy-to-use and efficient reimplementation of the common-nearest-neighbor (CommonNN) method. The package provides functionalities for hierarchical clustering and an evaluation of the results. We put our emphasis on a generic API design to keep the implementation flexible and open for customization.",

    "abstract": "Despite fast evolution cycles in deep learning methodologies for medical imaging in radiotherapy, auto-segmentation solutions rarely run in clinics due to the lack of open-source frameworks feasible for processing DICOM RT Structure Sets. Besides this shortage, available open-source DICOM RT Structure Set converters rely exclusively on 2D reconstruction approaches leading to pixelated contours with potentially low acceptance by healthcare professionals. PyRaDiSe, an open-source, deep learning framework independent Python package, addresses these issues by providing a framework for building auto-segmentation solutions feasible to operate directly on DICOM data. In addition, PyRaDiSe provides profound DICOM RT Structure Set conversion and processing capabilities; thus, it applies also to auto-segmentation-related tasks, such as dataset construction for deep learning model training.\nThe PyRaDiSe package follows a holistic approach and provides DICOM data handling, deep learning model inference, pre-processing, and post-processing functionalities. The DICOM data handling allows for highly automated and flexible handling of DICOM image series, DICOM RT Structure Sets, and DICOM registrations, including 2D-based and 3D-based conversion from and to DICOM RT Structure Sets. For deep learning model inference, extending given skeleton classes is straightforwardly achieved, allowing for employing any deep learning framework. Furthermore, a profound set of pre-processing and post-processing routines is included that incorporate partial invertibility for restoring spatial properties, such as image origin or orientation.\nThe PyRaDiSe package, characterized by its flexibility and automated routines, allows for fast deployment and prototyping, reducing efforts for auto-segmentation pipeline implementation. Furthermore, while deep learning model inference is independent of the deep learning framework, it can easily be integrated into famous deep learning frameworks such as PyTorch or Tensorflow. The developed package has successfully demonstrated its capabilities in a research project at our institution for organs-at-risk segmentation in brain tumor patients. Furthermore, PyRaDiSe has shown its conversion performance for dataset construction.\nThe PyRaDiSe package closes the gap between data science and clinical radiotherapy by enabling deep learning segmentation models to be easily transferred into clinical research practice. PyRaDiSe is available on https://github.com/ubern-mia/pyradise and can be installed directly from the Python Package Index using pip install pyradise.",

    "abstract": "Isotopic composition modelling is a key aspect in many environmental studies. This work presents Isocompy, an open source Python library that estimates isotopic compositions through machine learning algorithms with user-defined variables. Isocompy includes dataset preprocessing, outlier detection, statistical analysis, feature selection, model validation and calibration and postprocessing. This tool has the flexibility to operate with discontinuous inputs in time and space. The automatic decision-making procedures are knitted in different stages of the algorithm, although it is possible to manually complete each step. The extensive output reports, figures and maps generated by Isocompy facilitate the comprehension of stable water isotope studies. The functionality of Isocompy is demonstrated with an application example involving the meteorological features and isotopic composition of precipitation in N Chile, which are compared with the results produced in previous studies. In essence, Isocompy offers an open source foundation for isotopic studies that ensures reproducible research in environmental fields.",

    "abstract": "PyGenePlexus is a Python package that enables a user to gain insight into any gene set of interest through a molecular interaction network informed supervised machine learning model. PyGenePlexus provides predictions of how associated every gene in the network is to the input gene set, offers interpretability by comparing the model trained on the input gene set to models trained on thousands of known gene sets, and returns the network connectivity of the top predicted genes.\nhttps://pypi.org/project/geneplexus/ and https://github.com/krishnanlab/PyGenePlexus.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "PACKMAN-molecule is a Structural Bioinformatics toolbox in the form of an Application Programming Interface that contains several utilities that can be used for structural bioinformatics applications. It has already been used in several applications, and its added features and unique object hierarchy make it readily extensible, feature-rich and user-friendly. The tutorial for it is available at: https://py-packman.readthedocs.io/en/latest/tutorials/molecule.html.\nPACKMAN-Molecule is freely available with an MIT license on GitHub at https://github.com/Pranavkhade/PACKMAN.",

    "abstract": "Mass spectrometry-based proteomics is increasingly employed in biology and medicine. To generate reliable information from large datasets and ensure comparability of results, it is crucial to implement and standardize the quality control of the raw data, the data processing steps and the statistical analyses. MSPypeline provides a platform for importing MaxQuant output tables, generating quality control reports, data preprocessing including normalization and performing exploratory analyses by statistical inference plots. These standardized steps assess data quality, provide customizable figures and enable the identification of differentially expressed proteins to reach biologically relevant conclusions.\nThe source code is available under the MIT license at https://github.com/siheming/mspypeline with documentation at https://mspypeline.readthedocs.io. Benchmark mass spectrometry data are available on ProteomeXchange (PXD025792).\nSupplementary data are available at ",

    "abstract": "Neurotechnologies have great potential to transform our society in ways that are yet to be uncovered. The rate of development in this field has increased significantly in recent years, but there are still barriers that need to be overcome before bringing neurotechnologies to the general public. One of these barriers is the difficulty of performing experiments that require complex software, such as brain-computer interfaces (BCI) or cognitive neuroscience experiments. Current platforms have limitations in terms of functionality and flexibility to meet the needs of researchers, who often need to implement new experimentation settings. This work was aimed to propose a novel software ecosystem, called MEDUSA\u00a9, to overcome these limitations.\nWe followed strict development practices to optimize MEDUSA\u00a9 for research in BCI and cognitive neuroscience, making special emphasis in the modularity, flexibility and scalability of our solution. Moreover, it was implemented in Python, an open-source programming language that reduces the development cost by taking advantage from its high-level syntax and large number of community packages.\nMEDUSA\u00a9 provides a complete suite of signal processing functions, including several deep learning architectures or connectivity analysis, and ready-to-use BCI and neuroscience experiments, making it one of the most complete solutions nowadays. We also put special effort in providing tools to facilitate the development of custom experiments, which can be easily shared with the community through an app market available in our website to promote reproducibility.\nMEDUSA\u00a9 is a novel software ecosystem for modern BCI and neurotechnology experimentation that provides state-of-the-art tools and encourages the participation of the community to make a difference for the progress of these fields. Visit the official website at https://www.medusabci.com/ to know more about this project.",

    "abstract": "Currently, remote laboratories have gained relevance in engineering education as tools to support active learning, experimentation, and motivation of students. Nonetheless, the costs and issues regarding their implementation and deployment limit the access of the students and educators to their advantages and features such as technical and educational. In this line, this study describes a fully open-source remote laboratory in hardware and software for education in automatic control systems employing Raspberry Pi and Python language with an approximate cost of USD 461. Even, by changing some components, the cost can be reduced to USD 420 or less. To illustrate the functionalities of the laboratory, we proposed a low-cost tank control system with its respective instrumentation, signal conditioning, identification, and control, which are exposed in this document. However, other experiments can be easily scalable and adaptable to the remote laboratory. Concerning the interface of the laboratory, we designed a complete user-friendly web interface with real-time video for the users to perform the different activities in automatic control such as identification or controller implementation through the programming language Python. The instructions to build and replicate the hardware and software are indicated in the open repositories provided for the project as well as in this paper. Our intention with this project is to offer a complete low-cost and open-source remote laboratory that can be adapted and used for the students, educators, and stakeholders to learn, experiment, and teach in the field of automatic control systems.",

    "abstract": "Gos is a declarative Python library designed to create interactive multiscale visualizations of genomics and epigenomics data. It provides a consistent and simple interface to the flexible Gosling visualization grammar. Gos hides technical complexities involved with configuring web-based genome browsers and integrates seamlessly within computational notebooks environments to enable new interactive analysis workflows.\nGos is released under the MIT License and available on the Python Package Index (PyPI). The source code is publicly available on GitHub (https://github.com/gosling-lang/gos), and documentation with examples can be found at https://gosling-lang.github.io/gos.",

    "abstract": "spectrum_utils is a Python package for mass spectrometry data processing and visualization. Since its introduction, spectrum_utils has grown into a fundamental software solution that powers various applications in proteomics and metabolomics, ranging from spectrum preprocessing prior to spectrum identification and machine learning applications to spectrum plotting from online data repositories and assisting data analysis tasks for dozens of other projects. Here, we present updates to spectrum_utils, which include new functionality to integrate mass spectrometry community data standards, enhanced mass spectral data processing, and unified mass spectral data visualization in Python. spectrum_utils is freely available as open source at https://github.com/bittremieux/spectrum_utils.",

    "abstract": "The genome-wide association study (GWAS) is a popular genomic approach that identifies genomic regions associated with a phenotype and, thus, aims to discover causative mutations (CM) in the genes underlying the phenotype. However, GWAS discoveries are limited by many factors and typically identify associated genomic regions without the further ability to compare the viability of candidate genes and actual CMs. Therefore, the current methodology is limited to CM identification. In our recent work, we presented a novel approach to an empowered \"GWAS to Genes\" strategy that we named Synthetic phenotype to causative mutation (SP2CM). We established this strategy to identify CMs in soybean genes and developed a web-based tool for accuracy calculation (AccuTool) for a reference panel of soybean accessions. Here, we describe our further development of the tool that extends its utilization for other species and named it AccuCalc. We enhanced the tool for the analysis of datasets with a low-frequency distribution of a rare phenotype by automated formatting of a synthetic phenotype and added another accuracy-based GWAS evaluation criterion to the accuracy calculation. We designed AccuCalc as a Python package for GWAS data analysis for any user-defined species-independent variant calling format (vcf) or HapMap format (hmp) as input data. AccuCalc saves analysis outputs in user-friendly tab-delimited formats and also offers visualization of the GWAS results as Manhattan plots accentuated by accuracy. Under the hood of Python, AccuCalc is publicly available and, thus, can be used conveniently for the SP2CM strategy utilization for every species.",

    "abstract": "Here, we present sbml2hyb, an easy-to-use standalone Python tool that facilitates the conversion of existing mechanistic models of biological systems in Systems Biology Markup Language (SBML) into hybrid semiparametric models that combine mechanistic functions with machine learning (ML). The so-formed hybrid models can be trained and stored back in databases in SBML format. The tool supports a user-friendly export interface with an internal format validator. Two case studies illustrate the use of the sbml2hyb tool. Additionally, we describe HMOD, a new model format designed to support and facilitate hybrid models building. It aggregates the mechanistic model information with the ML information and follows as close as possible the SBML rules. We expect the sbml2hyb tool and HMOD to greatly facilitate the widespread usage of hybrid modeling techniques for biological systems analysis.\nThe Python interface, source code and the example models used for the case studies are accessible at: https://github.com/r-costa/sbml2hyb.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "GSEL is a computational framework for calculating the enrichment of signatures of diverse evolutionary forces in a set of genomic regions. GSEL can flexibly integrate any sequence-based evolutionary metric and analyze sets of human genomic regions identified by genome-wide assays (e.g. GWAS, eQTL, *-seq). The core of GSEL's approach is the generation of empirical null distributions tailored to the allele frequency and linkage disequilibrium structure of the regions of interest. We illustrate the application of GSEL to variants identified from a GWAS of body mass index, a highly polygenic trait.\nGSEL is implemented as a fast, flexible and user-friendly python package. It is available with demonstration data at https://github.com/abraham-abin13/gsel_vec.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "JBrowse Jupyter is a package that aims to close the gap between Python programming and genomic visualization. Web-based genome browsers are routinely used for publishing and inspecting genome annotations. Historically they have been deployed at the end of bioinformatics pipelines, typically decoupled from the analysis itself. However, emerging technologies such as Jupyter notebooks enable a more rapid iterative cycle of development, analysis and visualization.\nWe have developed a package that provides a Python interface to JBrowse 2's suite of embeddable components, including the primary Linear Genome View. The package enables users to quickly set up, launch and customize JBrowse views from Jupyter notebooks. In addition, users can share their data via Google's Colab notebooks, providing reproducible interactive views.\nJBrowse Jupyter is released under the Apache License and is available for download on PyPI. Source code and demos are available on GitHub at https://github.com/GMOD/jbrowse-jupyter.",

    "abstract": "The Green Tree Python (",

    "abstract": "COVID-19 was originally diagnosed in Wuhan, China, in late December 2019; it subsequently expanded internationally, affecting around 7 million people and caused 300,000 deaths by May 2020. An Application that could surveillance the Covid-19 in Indonesia is needed. Therefore, we proposed a prediction application for the COVID-19 pandemic situation in Indonesia by referring to public compliance with surveillance policies using the SPCIRD model. The Levenberg Marquardt curve fitting method was chosen because it is simple and produces a reasonably good model. According the result of questionnaire and black box testing our application performed really well. The result of our SPCIRD model with Levenberg-Marquardt optimization achieved R2 with the score -1.248 in active cases, -0.235 in recovered and -3.982 in death for 281 number of iterations. We are also achieved 93.3% that agree and very agree with the application usability to understand and to predict pattern from COVID-19. It was also had 83.3\\% respondent that satisfy with the prototype.",

    "abstract": "Snake strikes are some of the most rapid accelerations in terrestrial vertebrates. Generating rapid body accelerations requires high ground reaction forces, but on flat surfaces snakes must rely on static friction to prevent slip. We hypothesize that snakes may be able to take advantage of structures in the environment to prevent their body from slipping, potentially allowing them to generate faster and more forceful strikes. To test this hypothesis, we captured high-speed video and forces from defensive strikes of juvenile blood pythons (Python brongersmai) on a platform that was either open on all sides or with two adjacent walls opposite the direction of the strike. Contrary to our predictions, snakes maintained high performance on open platforms by imparting rearward momentum to the posterior body and tail. This compensatory behavior increases robustness to changes in their strike conditions and could allow them to exploit variable environments.",

    "abstract": "Item parameter estimation is a crucial step when conducting item factor analysis (IFA). From the view of frequentist estimation, marginal maximum likelihood (MML) seems to be the gold standard. However, fitting a high-dimensional IFA model by MML is still a challenging task. The current study demonstrates that with the help of a GPU (graphics processing unit) and carefully designed vectorization, the computational time of MML could be largely reduced for large-scale IFA applications. In particular, a Python package called xifa (accelerated item factor analysis) is developed, which implements a vectorized Metropolis-Hastings Robbins-Monro (VMHRM) algorithm. Our numerical experiments show that the VMHRM on a GPU may run 33 times faster than its CPU version. When the number of factors is at least five, VMHRM (on GPU) is much faster than the Bock-Aitkin expectation maximization, MHRM implemented by mirt (on CPU), and the importance-weighted autoencoder (on GPU). The GPU-implemented VMHRM is most appropriate for high-dimensional IFA with large data sets. We believe that GPU computing will play a central role in large-scale psychometric modeling in the near future.",

    "abstract": "Current PET datasets are becoming larger, thereby increasing the demand for fast and reproducible processing pipelines. This paper presents a freely available, open source, Python-based software package called NiftyPAD, for versatile analyses of static, full or dual-time window dynamic brain PET data. The key novelties of NiftyPAD are the analyses of dual-time window scans with reference input processing, pharmacokinetic modelling with shortened PET acquisitions through the incorporation of arterial spin labelling (ASL)-derived relative perfusion measures, as well as optional PET data-based motion correction. Results obtained with NiftyPAD were compared with the well-established software packages PPET and QModeling for a range of kinetic models. Clinical data from eight subjects scanned with four different amyloid tracers were used to validate the computational performance. NiftyPAD achieved [Formula: see text] correlation with PPET, with absolute difference [Formula: see text] for linearised Logan and MRTM2 methods, and [Formula: see text] correlation with QModeling, with absolute difference [Formula: see text] for basis function based SRTM and SRTM2 models. For the recently published SRTM ASL method, which is unavailable in existing software packages, high correlations with negligible bias were observed with the full scan SRTM in terms of non-displaceable binding potential ([Formula: see text]), indicating reliable model implementation in NiftyPAD. Together, these findings illustrate that NiftyPAD is versatile, flexible, and produces comparable results with established software packages for quantification of dynamic PET data. It is freely available ( https://github.com/AMYPAD/NiftyPAD ), and allows for multi-platform usage. The modular setup makes adding new functionalities easy, and the package is lightweight with minimal dependencies, making it easy to use and integrate into existing processing pipelines.",

    "abstract": "The exploration of chemical systems occurs on complex energy landscapes. Comprehensively sampling rugged energy landscapes with many local minima is a common problem for molecular dynamics simulations. These multiple local minima trap the dynamic system, preventing efficient sampling. This is a particular challenge for large biochemical systems with many degrees of freedom. Replica exchange molecular dynamics (REMD) is an approach that accelerates the exploration of the conformational space of a system, and thus can be used to enhance the sampling of complex biomolecular processes. In parallel, the empirical valence bond (EVB) approach is a powerful approach for modeling chemical reactivity in biomolecular systems. Here, we present an open-source Python-based tool that interfaces with the Q simulation package, and increases the sampling efficiency of the EVB free energy perturbation/umbrella sampling approach by means of REMD. This approach, Q-RepEx, both decreases the computational cost of the associated REMD-EVB simulations, and opens the door to more efficient studies of biochemical reactivity in systems with significant conformational fluctuations along the chemical reaction coordinate.",

    "abstract": "Openly sharing data with sensitive attributes and privacy restrictions is a challenging task. In this document we present the implementation of pyCANON, a Python library and command line interface (CLI) to check and assess the level of anonymity of a dataset through some of the most common anonymization techniques: k-anonymity, (\u03b1,k)-anonymity, \u2113-diversity, entropy \u2113-diversity, recursive (c,\u2113)-diversity, t-closeness, basic \u03b2-likeness, enhanced \u03b2-likeness and \u03b4-disclosure privacy. For the case of more than one sensitive attribute, two approaches are proposed for evaluating these techniques. The main strength of this library is to obtain a full report of the parameters that are fulfilled for each of the techniques mentioned above, with the unique requirement of the set of quasi-identifiers and sensitive attributes. The methods implemented are presented together with the attacks they prevent, the description of the library, examples of the different functions' usage, as well as the impact and the possible applications that can be developed. Finally, some possible aspects to be incorporated in future updates are proposed.",

    "abstract": "In order for mathematical models to make credible contributions, it is essential for them to be verified and validated. Currently, verification and validation (V&V) of these models does not meet the expectations of the system biology and systems pharmacology communities. Partially as a result of this shortfall, systemic V&V of existing models currently requires a lot of time and effort. In order to facilitate systemic V&V of chosen hypothalamic-pituitary-adrenal (HPA) axis models, we have developed a computational framework named VeVaPy-taking care to follow the recommended best practices regarding the development of mathematical models. VeVaPy includes four functional modules coded in Python, and the source code is publicly available. We demonstrate that VeVaPy can help us efficiently verify and validate the five HPA axis models we have chosen. Supplied with new and independent data, VeVaPy outputs objective V&V benchmarks for each model. We believe that VeVaPy will help future researchers with basic modeling and programming experience to efficiently verify and validate mathematical models from the fields of systems biology and systems pharmacology.",

    "abstract": "In this paper, we propose an advanced scripting approach using Python and R for satellite image processing and modelling terrain in C\u00f4te d'Ivoire, West Africa. Data include Landsat 9 OLI/TIRS C2 L1 and the SRTM digital elevation model (DEM). The EarthPy library of Python and 'raster' and 'terra' packages of R are used as tools for data processing. The methodology includes computing vegetation indices to derive information on vegetation coverage and terrain modelling. Four vegetation indices were computed and visualised using R: the Normalized Difference Vegetation Index (NDVI), Enhanced Vegetation Index 2 (EVI2), Soil-Adjusted Vegetation Index (SAVI) and Atmospherically Resistant Vegetation Index 2 (ARVI2). The SAVI index is demonstrated to be more suitable and better adjusted to the vegetation analysis, which is beneficial for agricultural monitoring in C\u00f4te d'Ivoire. The terrain analysis is performed using Python and includes slope, aspect, hillshade and relief modelling with changed parameters for the sun azimuth and angle. The vegetation pattern in C\u00f4te d'Ivoire is heterogeneous, which reflects the complexity of the terrain structure. Therefore, the terrain and vegetation data modelling is aimed at the analysis of the relationship between the regional topography and environmental setting in the study area. The upscaled mapping is performed as regional environmental analysis of the Yamoussoukro surroundings and local topographic modelling of the Kossou Lake. The algorithms of the data processing include image resampling, band composition, statistical analysis and map algebra used for calculation of the vegetation indices in C\u00f4te d'Ivoire. This study demonstrates the effective application of the advanced programming algorithms in Python and R for satellite image processing.",

    "abstract": "To compare the rate of energy expenditure of low efficiency walking with high efficiency walking.\nLaboratory based experimental study.\nUnited States.\n13 healthy adults (six women, seven men) with no known gait disorder, mean (\u00b1standard deviation) age 34.2\u00b116.1 years, height 174.2\u00b112.6 cm, weight 78.2\u00b122.5 kg, and body mass index 25.6\u00b16.0.\nParticipants performed three, five minute walking trials around an indoor 30 m course. The first trial consisted of walking at a freely chosen walking speed in the participant's usual style. The next two trials consisted of low efficiency walks in which participants were asked to duplicate the walks of Mr Teabag and Mr Putey (acted by John Cleese and Michael Palin, respectively) in the legendary Monty Python Ministry of Silly Walks (MoSW) skit that first aired in 1970. Distance covered during the five minute walks was used to calculate average speed. Ventilation and gas exchange were collected throughout to determine oxygen uptake (V\u0307O\nV\u0307O\nV\u0307O\nFor adults with no known gait disorder who average approximately 5000 steps/day, exchanging about 22%-34% of their daily steps with higher energy, low efficiency walking in Teabag style-requiring around 12-19 min-could increase daily EE by 100 kcal. Adults could achieve 75 minutes of vigorous intensity physical activity per week by walking inefficiently for about 11 min/day. Had an initiative to promote inefficient movement been adopted in the early 1970s, we might now be living among a healthier society. Efforts to promote higher energy-and perhaps more joyful-walking should ensure inclusivity and inefficiency for all.",

    "abstract": "Ontologies contain formal and structured information about a domain and are widely used in bioinformatics for annotation and integration of data. Several methods use ontologies to provide background knowledge in machine learning tasks, which is of particular importance in bioinformatics. These methods rely on a set of common primitives that are not readily available in a software library; a library providing these primitives would facilitate the use of current machine learning methods with ontologies and the development of novel methods for other ontology-based biomedical applications.\nWe developed mOWL, a Python library for machine learning with ontologies formalized in the Web Ontology Language (OWL). mOWL implements ontology embedding methods that map information contained in formal knowledge bases and ontologies into vector spaces while preserving some of the properties and relations in ontologies, as well as methods to use these embeddings for similarity computation, deductive inference and zero-shot learning. We demonstrate mOWL on the knowledge-based prediction of protein-protein interactions using the gene ontology and gene-disease associations using phenotype ontologies.\nmOWL is freely available on https://github.com/bio-ontology-research-group/mowl and as a Python package in PyPi.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "First-Order, Reduced and Controlled Error (FORCE) learning and its variants are widely used to train chaotic recurrent neural networks (RNNs), and outperform gradient methods on certain tasks. However, there is currently no standard software framework for FORCE learning. We present tension, an object-oriented, open-source Python package that implements a TensorFlow / Keras API for FORCE. We show how rate networks, spiking networks, and networks constrained by biological data can all be trained using a shared, easily extensible high-level API. With the same resources, our implementation outperforms a conventional RNN in loss and published FORCE implementations in runtime. Our work here makes FORCE training chaotic RNNs accessible and simple to iterate, and facilitates modeling of how behaviors of interest emerge from neural dynamics.",

    "abstract": "A plethora of proteomics search engine output file formats are in circulation. This lack of standardized output files greatly complicates generic downstream processing of peptide-spectrum matches (PSMs) and PSM files. While standards exist to solve this problem, these are far from universally supported by search engines. Moreover, software libraries are available to read a selection of PSM file formats, but a package to parse PSM files into a unified data structure has been missing. Here, we present psm_utils, a Python package to read and write various PSM file formats and to handle peptidoforms, PSMs, and PSM lists in a unified and user-friendly Python-, command line-, and web-interface. psm_utils was developed with pragmatism and maintainability in mind, adhering to community standards and relying on existing packages where possible. The Python API and command line interface greatly facilitate handling various PSM file formats. Moreover, a user-friendly web application was built using psm_utils that allows anyone to interconvert PSM files and retrieve basic PSM statistics. psm_utils is freely available under the permissive Apache2 license at https://github.com/compomics/psm_utils.",

    "abstract": "We report the development of a python-based auxiliary-field quantum Monte Carlo (AFQMC) program, ipie, with preliminary timing benchmarks and new AFQMC results on the isomerization of [Cu",

    "abstract": "Surface reflectance is an essential product from remote sensing Earth observations critical for a wide variety of applications, including consistent land cover mapping and change, and estimation of vegetation attributes. From 2000 to 2017 the Earth Observing-1 Hyperion instrument acquired the first satellite based hyperspectral image archive from space resulting in over 83,138 publicly available images. Hyperion imagery however requires significant preprocessing to derive surface reflectance. SUREHYP is a Python package designed to process batches of Hyperion images, bringing together a number of published algorithms and methods to correct at sensor radiance and derive surface reflectance. In this paper, we present the SUREHYP workflow and demonstrate its application on Hyperion imagery. Results indicate SUREHYP produces flat terrain surface reflectance results comparable to commercially available software, with reflectance values for the whole spectral range almost entirely within 10% of the software's over a reference target, yet it is publicly available and open source, allowing the exploitation of this valuable hyperspectral archive on a global scale.",

    "abstract": "Large-scale kinetic models are an invaluable tool to understand the dynamic and adaptive responses of biological systems. The development and application of these models have been limited by the availability of computational tools to build and analyze large-scale models efficiently. The toolbox presented here provides the means to implement, parameterize and analyze large-scale kinetic models intuitively and efficiently.\nWe present a Python package (SKiMpy) bridging this gap by implementing an efficient kinetic modeling toolbox for the semiautomatic generation and analysis of large-scale kinetic models for various biological domains such as signaling, gene expression and metabolism. Furthermore, we demonstrate how this toolbox is used to parameterize kinetic models around a steady-state reference efficiently. Finally, we show how SKiMpy can implement multispecies bioreactor simulations to assess biotechnological processes.\nThe software is available as a Python 3 package on GitHub: https://github.com/EPFL-LCSB/SKiMpy, along with adequate documentation.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "The electrical resistivity tomography (ERT) technique was conducted for the geophysical survey of a landslide on the southern slope of Jbel Tghat, north of the city of Fez, Morocco. Nine electrical resistivity tomography profiles were implemented to: (a) characterize the geometry of the dipping zone; (b) characterize their internal structures; and (c) highlight the faulting zone between the marly deposits and the conglomerate formation. The measured data sets were processed using EarthImager\u2122 2D (Advanced Geosciences, Inc), and BERT (Boundless Electrical Resistivity Tomography) software packages that offer a simple workflow from data import to inversion and visualization, while offering full control over inversion parameters. Moreover, BERT software is a Python-based open-source inversion software package. Both ERT processing software allows obtaining 2D subsurface electrical models associated with the distribution of the subsurface apparent electrical resistivity property, in Ohm.m units. Those 2D subsurface electrical models are retrieved using the same inversion parameters to determine the distribution of geoelectric layers and their defining parameters (e.g., electrical resistivity, thickness, and depth), giving access to certain characteristics exclusive to one of the two processing techniques, comparing the inversion findings to better understand the process's limits, as well as evaluating the capabilities of the two inversion methods.",

    "abstract": "Modeling in systems and synthetic biology relies on accurate parameter estimates and predictions. Accurate model calibration relies, in turn, on data and on how well suited the available data are to a particular modeling task. Optimal experimental design (OED) techniques can be used to identify experiments and data collection procedures that will most efficiently contribute to a given modeling objective. However, implementation of OED is limited by currently available software tools that are not well suited for the diversity of nonlinear models and non-normal data commonly encountered in biological research. Moreover, existing OED tools do not make use of the state-of-the-art numerical tools, resulting in inefficient computation. Here, we present the NLoed software package and demonstrate its use with in vivo data from an optogenetic system in ",

    "abstract": "NMR spectroscopy is an inherently insensitive technique with respect to the amount of observable signal. A common element in all NMR spectra is random thermal noise that is often characterized by a signal-to-noise ratio (SNR). SNR can be generically improved experimentally with repetitive signal averaging or during post-processing with apodization; the former of which often results in long experimental times and the latter results in the loss of spectral resolution. Denoising techniques can instead be used during post-processing to enhance SNR without compromising resolution. The most common approach relies on the singular-value decomposition (SVD) to discard noisy components of NMR data. SVD-based approaches work well, such as Cadzow and PCA, but are computationally expensive when used for large datasets that are often encountered in NMR (e.g., Carr-Purcell/Meiboom-Gill and nD datasets). Herein, we describe the implementation of a new wavelet transform (WT) routine for the fast and robust denoising of 1D and 2D NMR spectra. Several simulated and experimental datasets are denoised with both SVD-based Cadzow or PCA and WT's, and the resulting SNR enhancements and spectral uniformity are compared. WT denoising offers similar and improved denoising compared with SVD and operates faster by several orders-of-magnitude in some cases. All denoising and processing routines used in this work are included in a free and open-source Python library called DESPERATE.",

    "abstract": "High-dimensional LASSO (Hi-LASSO) is a powerful feature selection tool for high-dimensional data. Our previous study showed that Hi-LASSO outperformed the other state-of-the-art LASSO methods. However, the substantial cost of bootstrapping and the lack of experiments for a parametric statistical test for feature selection have impeded to apply Hi-LASSO for practical applications. In this paper, the Python package and its Spark library are efficiently designed in a parallel manner for practice with real-world problems, as well as providing the capability of the parametric statistical tests for feature selection on high-dimensional data. We demonstrate Hi-LASSO's outperformance with various intensive experiments in a practical manner. Hi-LASSO will be efficiently and easily performed by using the packages for feature selection. Hi-LASSO packages are publicly available at https://github.com/datax-lab/Hi-LASSO under the MIT license. The packages can be easily installed by Python PIP, and additional documentation is available at https://pypi.org/project/hi-lasso and https://pypi.org/project/Hi-LASSO-spark.",

    "abstract": "Kilovoltage radiotherapy dose calculations are generally performed with manual point dose calculations based on water dosimetry. Tissue heterogeneities, irregular surfaces, and introduction of lead cutouts for treatment are either not taken into account or crudely approximated in manual calculations. Full Monte Carlo (MC) simulations can account for these limitations but require a validated treatment unit model, accurately segmented patient tissues and a treatment planning interface (TPI) to facilitate the simulation setup and result analysis. EGSnrc was used in this work to create a model of Xstrahl kilovoltage unit extending the range of energies, applicators, and validation parameters previously published. The novel functionality of the Python-based framework developed in this work allowed beam modification using custom lead cutouts and shields, commonly present in kilovoltage treatments, as well as absolute dose normalization using the output of the unit. 3D user-friendly planning interface of the developed framework facilitated non-co-planar beam setups for CT phantom MC simulations in DOSXYZnrc. The MC models of 49 clinical beams showed good agreement with measured and reference data, to within 2% for percentage depth dose curves, 4% for beam profiles at various depths, 2% for backscatter factors, 0.5 mm of absorber material for half-value layers, and 3% for output\u00a0factors. End-to-end testing of the framework using custom lead cutouts resulted in good agreement to within 3% of absolute dose distribution between simulations and EBT3 GafChromic film measurements. Gamma analysis demonstrated poor agreement at the field edges which was attributed to the limitations of simulating smooth cutout shapes. Dose simulated in a heterogeneous phantom agreed to within 7% with measured values converted using the ratio of mass energy absorption coefficients of appropriate tissues and\u00a0air.",

    "abstract": "Fanpy is a free and open-source Python library for developing and testing multideterminant wavefunctions and related ab initio methods in electronic structure theory. The main use of Fanpy is to quickly prototype new methods by making it easier to convert the mathematical formulation of a new wavefunction ans\u00e4tze to a working implementation. Fanpy is designed based on our recently introduced Flexible Ansatz for N-electron Configuration Interaction (FANCI) framework, where multideterminant wavefunctions are represented by their overlaps with Slater determinants of orthonormal spin-orbitals. In the simplest case, a new wavefunction ansatz can be implemented by simply writing a function for evaluating its overlap with an arbitrary Slater determinant. Fanpy is modular in both implementation and theory: the wavefunction model, the system's Hamiltonian, and the choice of objective function are all independent modules. This modular structure makes it easy for users to mix and match different methods and for developers to quickly explore new ideas. Fanpy is written purely in Python with standard dependencies, making it accessible for various operating systems. In addition, it adheres to principles of modern software development, including comprehensive documentation, extensive testing, quality assurance, and continuous integration and delivery protocols. This article is considered to be the official release notes for the Fanpy library.",

    "abstract": "Electrocardiogram (ECG) and photoplethysmogram (PPG) are commonly used to determine the vital signs of heart rate, respiratory rate, and oxygen saturation in patient monitoring. In addition to simple observation of those summarized indexes, waveform signals can be analyzed to provide deeper insights into disease pathophysiology and support clinical decisions. Such data, generated from continuous patient monitoring from both conventional bedside and low-cost wearable monitors, are increasingly accessible. However, the recorded waveforms suffer from considerable noise and artifacts and, hence, are not necessarily used prior to certain quality control (QC) measures, especially by those with limited programming experience. Various signal quality indices (SQIs) have been proposed to indicate signal quality. To facilitate and harmonize a wider usage of SQIs in practice, we present a Python package, named vital_sqi, which provides a unified interface to the state-of-the-art SQIs for ECG and PPG signals. The vital_sqi package provides with seven different peak detectors and access to more than 70 SQIs by using different settings. The vital_sqi package is designed with pipelines and graphical user interfaces to enable users of various programming fluency to use the package. Multiple SQI extraction pipelines can take the PPG and ECG waveforms and generate a bespoke SQI table. As these SQI scores represent the signal features, they can be input in any quality classifier. The package provides functions to build simple rule-based decision systems for signal segment quality classification using user-defined SQI thresholds. An experiment with a carefully annotated PPG dataset suggests thresholds for relevant PPG SQIs.",

    "abstract": "Gene set enrichment analysis (GSEA) is a commonly used algorithm for characterizing gene expression changes. However, the currently available tools used to perform GSEA have a limited ability to analyze large datasets, which is particularly problematic for the analysis of single-cell data. To overcome this limitation, we developed a GSEA package in Python (GSEApy), which could efficiently analyze large single-cell datasets.\nWe present a package (GSEApy) that performs GSEA in either the command line or Python environment. GSEApy uses a Rust implementation to enable it to calculate the same enrichment statistic as GSEA for a collection of pathways. The Rust implementation of GSEApy is 3-fold faster than the Numpy version of GSEApy (v0.10.8) and uses >4-fold less memory. GSEApy also provides an interface between Python and Enrichr web services, as well as for BioMart. The Enrichr application programming interface enables GSEApy to perform over-representation analysis for an input gene list. Furthermore, GSEApy consists of several tools, each designed to facilitate a particular type of enrichment analysis.\nThe new GSEApy with Rust extension is deposited in PyPI: https://pypi.org/project/gseapy/. The GSEApy source code is freely available at https://github.com/zqfang/GSEApy. Also, the documentation website is available at https://gseapy.rtfd.io/.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Super-resolution optical fluctuation imaging (SOFI) is a highly democratizable technique that provides optical super-resolution without requirement of sophisticated imaging instruments. Easy-to-use open-source packages for SOFI are important to support the utilization and community adoption of the SOFI method, they also encourage the participation and further development of SOFI by new investigators. In this work, we developed ",

    "abstract": "The Boelen's Python (",

    "abstract": "The artificial intelligence-based structure prediction program AlphaFold-Multimer enabled structural modelling of protein complexes with unprecedented accuracy. Increasingly, AlphaFold-Multimer is also used to discover new protein-protein interactions (PPIs). Here, we present AlphaPulldown, a Python package that streamlines PPI screens and high-throughput modelling of higher-order oligomers using AlphaFold-Multimer. It provides a convenient command-line interface, a variety of confidence scores and a graphical analysis tool.\nAlphaPulldown is freely available at https://www.embl-hamburg.de/AlphaPulldown.\nSupplementary note is available at Bioinformatics online.",

    "abstract": "scFates provides an extensive toolset for the analysis of dynamic trajectories comprising tree learning, feature association testing, branch differential expression and with a focus on cell biasing and fate splits at the level of bifurcations. It is meant to be fully integrated into the scanpy ecosystem for seamless analysis of trajectories from single-cell data of various modalities (e.g. RNA and ATAC).\nscFates is released as open-source software under the BSD 3-Clause 'New' License and is available from the Python Package Index at https://pypi.org/project/scFates/. The source code is available on GitHub at https://github.com/LouisFaure/scFates/. Code reproduction and tutorials on published datasets are available on GitHub at https://github.com/LouisFaure/scFates_notebooks.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Cross-linking and immunoprecipitation (CLIP) is a technology to map the binding sites of RNA-binding proteins (RBPs). The region where an RBP binds within RNA is often indicative of its molecular function in RNA processing. As an example, the binding sites of splicing factors are found within or proximal to alternatively spliced exons. To better reveal the function of RBPs, we developed a tool to visualize the distribution of CLIP signals around various transcript features.\nHere, we present Metadensity (https://github.com/YeoLab/Metadensity), a software that allows users to generate metagene plots. Metadensity allows users to input features such as branchpoints and preserves the near-nucleotide resolution of CLIP technologies by not scaling the features by length. Metadensity normalizes immunoprecipitated libraries with background controls, such as size-matched inputs, then windowing in various user-defined features. Finally, the signals are averaged across a provided set of transcripts.\nMetadensity is available at https://github.com/YeoLab/Metadensity, with example notebooks at https://metadensity.readthedocs.io/en/latest/tutorial.html.\nSupplementary data are available at ",

    "abstract": "Quantum mechanical/molecular mechanics (QM/MM) methods are important tools in molecular modeling as they are able to couple an extended phase space sampling with an accurate description of the electronic properties of the system. Here, we describe a Python software package, called PyMM, which has been developed to apply a QM/MM approach, the perturbed matrix method, in a simple and efficient way. PyMM requires a classical atomic trajectory of the whole system and a set of unperturbed electronic properties of the ground and electronic excited states. The software output includes a set of the most common perturbed properties, such as the electronic excitation energies and the transitions dipole moments, as well as the eigenvectors describing the perturbed electronic states, which can be then used to estimate whatever electronic property. The software is composed of a simple and complete command-line interface, a set of internal input validation, and three main analyses focusing on (i) the perturbed eigenvector behavior, (ii) the calculation of the electronic absorption spectrum, and (iii) the estimation of the free energy differences along a reaction coordinate.",

    "abstract": "Calculation of the Size Specific Dose Estimate (SSDE) requires accurate delineation of the skin boundary of patient CT slices. The AAPM recommendation for SSDE evaluation at every CT slice is too time intensive for manual contouring, prohibiting real-time or bulk processing; an automated approach is therefore desirable. Previous automated delineation studies either did not fully disclose the steps of the algorithm or did not always manage to fully isolate the patient. The purpose of this study was to develop a validated, freely available, fast, vendor-independent open-source tool to automatically and accurately contour and calculate the SSDE for the abdomino-pelvic region for entire studies in real-time, including flagging of patient-truncated images.\nThe Python tool, CTContour, consists of a sequence of morphological steps and scales over multiple cores for speed. Tool validation was achieved on 700 randomly selected slices from abdominal and abdomino-pelvic studies from public datasets. Contouring accuracy was assessed visually by four medical physicists using a 1-5 Likert scale (5 indicating perfect contouring). Mean SSDE values were validated via manual calculation.\nContour accuracy validation produced a score of four of five for 98.5\u00a0% of the images. A 300 slice exam was contoured and truncation flagged in 6.3\u00a0s on a six-core laptop.\nThe algorithm was accurate even for complex clinical scenarios and when artefacts were present. Fast execution makes it possible to automate the calculation of SSDE in real time. The tool has been published on GitHub under the GNU-GPLv3 license.",

    "abstract": "The systems biology markup language (SBML) is an extensible standard format for exchanging biochemical models. One of the extensions for SBML is the SBML Layout and Render package. This allows modelers to describe a biochemical model as a pathway diagram. However, up to now, there has been little support to help users easily add and retrieve such information from SBML. In this application note, we describe a new Python package called SBMLDiagrams. This package allows a user to add a layout and render information or retrieve it using a straightforward Python API. The package uses skia-python to support the rendering of the diagrams, allowing export to commons formats such as PNG or PDF.\nSBMLDiagrams is publicly available and licensed under the liberal MIT open-source license. The package is available for all major platforms. The source code has been deposited at GitHub (github.com/sys-bio/SBMLDiagrams). Users can install the package using the standard pip installation mechanism: pip install SBMLDiagrams.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Ligating two or more DNA fragments is a regular operation for the subcloning and the engineering of vectors. The overlap extension PCR serves as a straightforward method to solve this issue. However, it takes a relatively long time to design the appropriate overlapping primers and the primers for the full-length sequence, and there has not been a professional offline software for such kind of primer design. Here, we propose a Python script to search, calculate and sort thousands of combinations of primers for users according to the predefined parameters. The results of script running and experimental validation show that this script is capable of generating the optimal pairs of primers based on the proper melting temperatures and lengths of the primers, which facilitates gene modification in research.",

    "abstract": "Molecular simulations such as molecular dynamics (MD) and Monte Carlo (MC) simulations are powerful tools allowing the prediction of experimental observables in the study of systems such as proteins, membranes, and polymeric materials. The quality of predictions based on molecular simulations depend on the validity of the underlying physical assumptions. physical_validation allows users of molecular simulation programs to perform simple yet powerful tests of physical validity on their systems and setups. It can also be used by molecular simulation package developers to run representative test systems during development, increasing code correctness. The theoretical foundation of the physical validation tests were established by Merz & Shirts (2018), in which the physical_validation package was first mentioned.",

    "abstract": "Quantifying industry consumption or production of resources, wastes, emissions, and losses-collectively called flows-is a complex and evolving process. The attribution of flows to industries often requires allocating multiple data sources that span spatial and temporal scopes and contain varied levels of aggregation. Once calculated, datasets can quickly become outdated with new releases of source data. The US Environmental Protection Agency (USEPA) developed the open-source Flow Sector Attribution (FLOWSA) Python package to address the challenges surrounding attributing flows to US industrial and final-use sectors. Models capture flows drawn from or released to the environment by sectors, as well as flow transfers between sectors. Data on flow use and generation by source-defined activities are imported from providers and transformed into standardized tables but are otherwise numerically unchanged in preparation for modeling. FLOWSA sector attribution models allocate primary data sources to industries using secondary data sources and file mapping activities to sectors. Users can modify methodological, spatial, and temporal parameters to explore and compare the impact of sector attribution methodological changes on model results. The standardized data outputs from these models are used as the environmental data inputs into the latest version of USEPA's US Environmentally Extended Input-Output (USEEIO) models, life cycle models of US goods and services for ~400 categories. This communication demonstrates FLOWSA's capability by describing how to build models and providing select model results for US industry use of water, land, and employment. FLOWSA is available on GitHub, and many of the data outputs are available on the USEPA's Data Commons.",

    "abstract": "Non-traditional animal models present an opportunity to discover novel biology that has evolved to allow such animals to survive in extreme environments. One striking example is the Burmese python (Python molurus bivittatus), which exhibits extreme physiological adaptation in various metabolic organs after consuming a large meal following long periods of fasting. The response to such a large meal in pythons involves a dramatic surge in metabolic rate, lipid overload in plasma, and massive but reversible organ growth through the course of digestion. Multiple studies have reported the physiological responses in post-prandial pythons, while the specific molecular control of these processes is less well-studied. Investigating the mechanisms that coordinate organ growth and adaptive responses offers the opportunity to gain novel insight that may be able to treat various pathologies in humans. Here, we summarize past research on the post-prandial physiological changes in the Burmese python with a focus on the gastrointestinal tract, heart, and liver. Specifically, we address our recent molecular discoveries in the post-prandial python liver which demonstrate transient adaptations that may reveal new therapeutic targets. Lastly, we explore new biology of the aquaporin 7 gene that is potently upregulated in mammalian cardiac myocytes by circulating factors in post-prandial python plasma.",

    "abstract": "Determining the correct localization of post-translational modifications (PTMs) on peptides aids in interpreting their effect on protein function. While most algorithms for this task are available as standalone applications or incorporated into software suites, improving their versatility through access from popular scripting languages facilitates experimentation and incorporation into novel workflows. Here we describe pyAscore, an efficient and versatile implementation of the Ascore algorithm in Python for scoring the localization of user defined PTMs in data dependent mass spectrometry. pyAscore can be used from the command line or imported into Python scripts and accepts standard file formats from popular software tools used in bottom-up proteomics. Access to internal objects for scoring and working with modified peptides adds to the toolbox for working with PTMs in Python. pyAscore is available as an open source package for Python 3.6+ on all major operating systems and can be found at pyascore.readthedocs.io.",

    "abstract": "Self-shading in fields of two-axis tracking collectors typically ranges from 1% to 6% of the annual incident irradiation. It is thus essential to account for shading in order to obtain accurate yield estimates and financing for such solar projects. The present study presents the free and open-source Python package ",

    "abstract": "The human upper respiratory tract is the reservoir of a diverse community of commensals and potential pathogens (pathobionts), including ",

    "abstract": "In gene expression data analysis framework, ultrahigh dimensionality and measurement error are ubiquitous features. Therefore, it is crucial to correct measurement error effects and make variable selection when fitting a regression model. In this paper, we introduce a python package BOOME, which refers to BOOsting algorithm for Measurement Error in binary responses and ultrahigh-dimensional predictors. We primarily focus on logistic regression and probit models with responses, predictors, or both contaminated with measurement error. The BOOME aims to address measurement error effects, and employ boosting procedure to make variable selection and estimation.",

    "abstract": "Natural scenes are composed of a wide range of edge angles and spatial frequencies, with a strong overrepresentation of vertical and horizontal edges. Correspondingly, many mammalian species are much better at discriminating these cardinal orientations compared to obliques. A potential reason for this increased performance could be an increased number of neurons in the visual cortex that are tuned to cardinal orientations, which is likely to be an adaptation to the natural scene statistics. Such biased angular tuning has recently been shown in the mouse primary visual cortex. However, it is still unknown if mice also show a perceptual dominance of cardinal orientations. Here, we describe the design of a novel custom-built touchscreen chamber that allows testing natural scene perception and orientation discrimination performance by applying different task designs. Using this chamber, we applied an iterative convergence towards orientation discrimination thresholds for cardinal or oblique orientations in different cohorts of mice. Surprisingly, the expert discrimination performance was similar for both groups but showed large inter-individual differences in performance and training time. To study the discrimination of cardinal and oblique stimuli in the same mice, we, therefore, applied, a different training regime where mice learned to discriminate cardinal and oblique gratings in parallel. Parallel training revealed a higher task performance for cardinal orientations in an early phase of the training. The performance for both orientations became similar after prolonged training, suggesting that learning permits equally high perceptual tuning towards oblique stimuli. In summary, our custom-built touchscreen chamber offers a flexible tool to test natural visual perception in rodents and revealed a training-induced increase in the perception of oblique gratings. The touchscreen chamber is entirely open-source, easy to build, and freely available to the scientific community to conduct visual or multimodal behavioral studies. It is also based on the FAIR principles for data management and sharing and could therefore serve as a catalyst for testing the perception of complex and natural visual stimuli across behavioral labs.",

    "abstract": "Gene expression and SNPs data hold great potential for a new understanding of disease prognosis, drug sensitivity, and toxicity evaluations. Cluster analysis is used to analyze data that do not contain any specific subgroups. The goal is to use the data itself to recognize meaningful and informative subgroups. In addition, cluster investigation helps data reduction purposes, exposes hidden patterns, and generates hypotheses regarding the relationship between genes and phenotypes. Cluster analysis could also be used to identify bio-markers and yield computational predictive models. The methods used to analyze microarrays data can profoundly influence the interpretation of the results. Therefore, a basic understanding of these computational tools is necessary for optimal experimental design and meaningful data analysis. This manuscript provides an analysis protocol to effectively analyze gene expression data sets through the K-means and DBSCAN algorithms. The general protocol enables analyzing omics data to identify subsets of features with low redundancy and high robustness, speeding up the identification of new bio-markers through pathway enrichment analysis. In addition, to demonstrate the effectiveness of our clustering analysis protocol, we analyze a real data set from the GEO database. Finally, the manuscript provides some best practice and tips to overcome some issues in the analysis of omics data sets through unsupervised learning.",

    "abstract": "DADApy is a Python software package for analyzing and characterizing high-dimensional data manifolds. It provides methods for estimating the intrinsic dimension and the probability density, for performing density-based clustering, and for comparing different distance metrics. We review the main functionalities of the package and exemplify its usage in a synthetic dataset and in a real-world application. DADApy is freely available under the open-source Apache 2.0 license.",

    "abstract": "FCMpy is an open-source Python module for building and analyzing Fuzzy Cognitive Maps (FCMs). The module provides tools for end-to-end projects involving FCMs. It is able to derive fuzzy causal weights from qualitative data or simulating the system behavior. Additionally, it includes machine learning algorithms (",

    "abstract": "Color morphs in ball pythons (Python regius) provide a unique and largely untapped resource for understanding the genetics of coloration in reptiles. Here we use a community-science approach to investigate the genetics of three color morphs affecting production of the pigment melanin. These morphs-Albino, Lavender Albino, and Ultramel-show a loss of melanin in the skin and eyes, ranging from severe (Albino) to moderate (Lavender Albino) to mild (Ultramel). To identify genetic variants causing each morph, we recruited shed skins of pet ball pythons via social media, extracted DNA from the skins, and searched for putative loss-of-function variants in homologs of genes controlling melanin production in other vertebrates. We report that the Albino morph is associated with missense and non-coding variants in the gene TYR. The Lavender Albino morph is associated with a deletion in the gene OCA2. The Ultramel morph is associated with a missense variant and a putative deletion in the gene TYRP1. Our study is one of the first to identify genetic variants associated with color morphs in ball pythons and shows that pet samples recruited from the community can provide a resource for genetic studies in this species.",

    "abstract": null,

    "abstract": "G-quadruplexes (G4s) are non-canonical DNA and RNA secondary structures that control gene regulation. A single nucleotide polymorphism (SNP) is a small genetic variation occurring within a DNA sequence and accounting for the variabilities between individuals. While the majority of SNPs, especially those frequent in the population, are considered as benign genetic variations, few others can lead to diseases. SNPs occurring in G4 sequences were reported to modulate gene regulation. In order to find overlaps between predicted G4 sequences and SNPs located in the genomic regions, we developed two complementary computational python codes (SNP-locator and G4-overlap). The codes map a mutation to the overlapping/closest G4 sequences, based on the genetic variant name and the FASTA format of the corresponding gene. We validated these two codes on a set of 31 SNP variants occurring in cytochromes ",

    "abstract": "pyDHM is an open-source Python library aimed at Digital Holographic Microscopy (DHM) applications. The pyDHM is a user-friendly library written in the robust programming language of Python that provides a set of numerical processing algorithms for reconstructing amplitude and phase images for a broad range of optical DHM configurations. The pyDHM implements phase-shifting approaches for in-line and slightly off-axis systems and enables phase compensation for telecentric and non-telecentric systems. In addition, pyDHM includes three propagation algorithms for numerical focusing complex amplitude distributions in DHM and digital holography (DH) setups. We have validated the library using numerical and experimental holograms.",

    "abstract": "S* is a widely used statistic for detecting archaic admixture from population genetic data. Previous studies used freezing-archer to apply S*, which is only directly applicable to the specific case of Neanderthal and Denisovan introgression in Papuans. Here, we implemented sstar for a more general purpose. Compared with several tools, including SPrime, SkovHMM, and ArchaicSeeker2.0, for detecting introgressed fragments with simulations, our results suggest that sstar is robust to differences in demographic models, including ghost introgression and two-source introgression. We believe sstar will be a useful tool for detecting introgressed fragments in various scenarios and in non-human species.",

    "abstract": "Currently, there are many publicly available Next Generation Sequencing tools developed for variant annotation and classification. However, as modern sequencing technology produces more and more sequencing data, a more efficient analysis program is desired, especially for variant analysis. In this study, we updated SNPAAMapper, a variant annotation pipeline by converting perl codes to python for generating annotation output with an improved computational efficiency and updated information for broader applicability. The new pipeline written in Python can classify variants by region (Coding Sequence, Untranslated Regions, upstream, downstream, intron), predict amino acid change type (missense, nonsense, etc.), and prioritize mutation effects (e.g., synonymous > non-synonymous) while being faster and more efficient. Our new pipeline works in five steps. First, exon annotation files are generated. Next, the exon annotation files are processed, and gene mapping and feature information files are produced. Afterward, the python scrips classify the variants based on genomic regions and predict the amino acid change category. Lastly, another python script prioritizes and ranks the mutation effects of variants to output the result file. The Python version of SNPAAMapper accomplished the overall speed by running most annotation steps in a substantially shorter time. The Python script can classify variants by region in 53 s compared to 166 s for the Perl script in a test sample run on a Latitude 7480 Desktop computer with 8GB RAM and an Intel Core i5-6300 CPU @ 2.4Ghz. Steps of predicting amino acid change type and prioritizing mutation effects of variants were executed within 1 s for both pipelines. SNPAAMapper-Python was developed and tested on the ClinVar database, a NCBI database of information on genomic variation and its relationship to human health. We believe our developed Python version of SNPAAMapper variant annotation pipeline will benefit the community by elucidating the variant consequence and speed up the discovery of causative genetic variants through whole genome/exome sequencing. Source codes, test data files, instructions, and further explanations are available on the web at https://github.com/BaiLab/SNPAAMapper-Python.",

    "abstract": null,

    "abstract": "One of the most important applications of sensors is feedback control, in which an algorithm is applied to data that are collected from sensors in order to drive system actuators and achieve the desired outputs of the target plant. One of the most challenging applications of this control is represented by magnetic confinement fusion, in which real-time systems are responsible for the confinement of plasma at a temperature of several million degrees within a toroidal container by means of strong electromagnetic fields. Due to the fast dynamics of the underlying physical phenomena, data that are collected from electromagnetic sensors must be processed in real time. In most applications, real-time systems are implemented in C++; however, Python applications are now becoming more and more widespread, which has raised potential interest in their applicability in real-time systems. In this study, a framework was set up to assess the applicability of Python in real-time systems. For this purpose, a reference operating system configuration was chosen, which was optimized for real time, together with a reference framework for real-time data management. Within this framework, the performance of modules that computed PID control and FFT transforms was compared for C++ and Python implementations, respectively. Despite the initial concerns about Python applicability in real-time systems, it was found that the worst-case execution time (WCET) could also be safely defined for modules that were implemented in Python, thereby confirming that they could be considered for real-time applications.",

    "abstract": "Various constraint-based optimization approaches have been developed for the computational analysis and design of metabolic networks. Herein, we present StrainDesign, a comprehensive Python package that builds upon the COBRApy toolbox and integrates the most popular metabolic design algorithms, including nested strain optimization methods such as OptKnock, RobustKnock and OptCouple as well as the more general minimal cut sets approach. The optimization approaches are embedded in individual modules, which can also be combined for setting up more elaborate strain design problems. Advanced features, such as the efficient integration of GPR rules and the possibility to consider gene and reaction additions or regulatory interventions, have been generalized and are available for all modules. The package uses state-of-the-art preprocessing methods, supports multiple solvers and provides a number of enhanced tools for analyzing computed intervention strategies including 2D and 3D plots of user-selected metabolic fluxes or yields. Furthermore, a user-friendly interface for the StrainDesign package has been implemented in the GUI-based metabolic modeling software CNApy. StrainDesign provides thus a unique and rich framework for computational strain design in Python, uniting many algorithmic developments in the field and allowing modular extension in the future.\nThe StrainDesign package can be retrieved from PyPi, Anaconda and GitHub (https://github.com/klamt-lab/straindesign) and is also part of the latest CNApy package.",

    "abstract": "AEON.py is a Python library for the analysis of the long-term behaviour in very large asynchronous Boolean networks. It provides significant computational improvements over the state-of-the-art methods for attractor detection. Furthermore, it admits the analysis of partially specified Boolean networks with uncertain update functions. It also includes techniques for identifying viable source-target control strategies and the assessment of their robustness with respect to parameter perturbations.\nAll relevant results are available in Supplementary Materials. The tool is accessible through https://github.com/sybila/biodivine-aeon-py.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "A programming workshop has been developed for biochemists and molecular biologists to introduce them to the power and flexibility of solving problems with Python. The workshop is designed to move users beyond a \"plug-and-play\" approach that is based on spreadsheets and web applications in their teaching and research to writing scripts to parse large collections of data and to perform dynamic calculations. The live-coding workshop is designed to introduce specific coding skills, as well as provide insight into the broader array of open-access resources and libraries that are available for scientific computation.",

    "abstract": "Vision is among the oldest and arguably most important sensory modalities for animals to interact with their external environment. Although many different eye types exist within the animal kingdom, mounting evidence indicates that the genetic networks required for visual system formation and function are relatively well conserved between species. This raises the question as to how common developmental programs are modified in functionally different eye types. Here, we approached this issue through EyeVolve, an open-source PYTHON-based model that recapitulates eye development based on developmental principles originally identified in ",

    "abstract": "Paired- and single-chain T cell receptor (TCR) sequencing are now commonly used techniques for interrogating adaptive immune responses. TCRs targeting the same epitope frequently share motifs consisting of critical contact residues. Here we illustrate the key features of tcrdist3, a new Python package for distance-based TCR analysis through a series of three interactive examples. In the first example, we illustrate how tcrdist3 can integrate sequence similarity networks, gene-usage plots, and background-adjusted CDR3 logos to identify TCR sequence features conferring antigen specificity among sets of peptide-MHC-multimer sorted receptors. In the second example, we show how the TCRjoin feature in tcrdist3 can be used to flexibly query receptor sequences of interest against bulk repertoires or libraries of previously annotated TCRs based on matching of similar sequences. In the third example, we show how the TCRdist metric can be leveraged to identify candidate polyclonal receptors under antigenic selection in bulk repertoires based on sequence neighbor enrichment testing, a statistical approach similar to TCRNET and ALICE algorithms, but with added flexibility in how the neighborhood can be defined.",

    "abstract": "Propensity score matching (PSM) is a technique used in retrospective investigation of cohort matching as an alternative approach to the prospective matching that is typically used by a randomized control trial (RCT). The process of selecting untreated cases that are the best match to the treated cases is the focus of this research. We created a PSM package for the python environment, termed PsmPy, to carry out this task. The PsmPy package debuted and proposed here is based on a logistic regression logit score where a match is selected using k-nearest neighbors (k-NN). Additional plotting and arguments are available to the user and are also described. To benchmark our method, we compared it with the existing R package, MatchIt, and evaluated our covariates' residual effect sizes with respect to the treatment condition before and after matching. Using a Mann-Whitney statistical test, we showed that our method significantly outperformed MatchIt in cohort matching (U=49, p<0.0001) when comparing residual effect sizes of the covariates. The PsmPy demonstrated a 10-fold average improvement in residual effect sizes amongst covariates when compared with the package MatchIt, suggesting that it is a viable alternative for use in propensity matching studies.",

    "abstract": "Gene expression is regulated through cis-regulatory elements (CREs), among which are promoters, enhancers, Polycomb/Trithorax Response Elements (PREs), silencers and insulators. Computational prediction of CREs can be achieved using a variety of statistical and machine learning methods combined with different feature space formulations. Although Python packages for DNA sequence feature sets and for machine learning are available, no existing package facilitates the combination of DNA sequence feature sets with machine learning methods for the genome-wide prediction of candidate CREs. We here present Gnocis, a Python package that streamlines the analysis and the modelling of CRE sequences by providing extensible APIs and implementing the glue required for combining feature sets and models for genome-wide prediction. Gnocis implements a variety of base feature sets, including motif pair occurrence frequencies and the k-spectrum mismatch kernel. It integrates with Scikit-learn and TensorFlow for state-of-the-art machine learning. Gnocis additionally implements a broad suite of tools for the handling and preparation of sequence, region and curve data, which can be useful for general DNA bioinformatics in Python. We also present Deep-MOCCA, a neural network architecture inspired by SVM-MOCCA that achieves moderate to high generalization without prior motif knowledge. To demonstrate the use of Gnocis, we applied multiple machine learning methods to the modelling of D. melanogaster PREs, including a Convolutional Neural Network (CNN), making this the first study to model PREs with CNNs. The models are readily adapted to new CRE modelling problems and to other organisms. In order to produce a high-performance, compiled package for Python 3, we implemented Gnocis in Cython. Gnocis can be installed using the PyPI package manager by running 'pip install gnocis'. The source code is available on GitHub, at https://github.com/bjornbredesen/gnocis.",

    "abstract": "In physiological signal analysis, identifying meaningful relationships and inherent patterns in signals can provide valuable information regarding subjects' physiological state and changes. Although MATLAB has been widely used in signal processing and feature analysis, Python has recently dethroned MATLAB with the rise of data science, machine learning and artificial intelligence. Hence, there is a compelling need for a Python package for physiological feature analysis and extraction to achieve compatibility with downstream models often trained in Python. Thus, we present a novel visualization and feature analysis Python toolbox, PySio, to enable rapid, efficient and user-friendly analysis of physiological signals. First, the user should import the signal-of-interest with the corresponding sampling rate. After importing, the user can either analyze the signal as it is, or can choose a specific region for more detailed analysis. PySio enables the user to (i) visualize and analyze the physiological signals (or user-selected segments of the signals) in time domain, (ii) study the signals (or user-selected segments of the signals) in frequency domain through discrete Fourier transform and spectrogram representations, and (iii) investigate and extract the most common time (energy, entropy, zero crossing rate and peaks) and frequency (spectral entropy, rolloff, centroid, spread, peaks and bandpower) domain features, all with one click. Clinical relevance- As the physiological signals originate directly from the underlying physiological events, proper analysis of the signal patterns can provide valuable information in personalized treatment and wearable technology applications.",

    "abstract": null,

    "abstract": "The i2b2 platform is used at major academic health institutions and research consortia for querying for electronic health data. However, a major obstacle for wider utilization of the platform is the complexity of data loading that entails a steep curve of learning the platform's complex data schemas. To address this problem, we have developed the i2b2-etl package that simplifies the data loading process, which will facilitate wider deployment and utilization of the platform.\nWe have implemented i2b2-etl as a Python application that imports ontology and patient data using simplified input file schemas and provides inbuilt record number de-identification and data validation. We describe a real-world deployment of i2b2-etl for a population-management initiative at MassGeneral Brigham.\ni2b2-etl is a free, open-source application implemented in Python available under the Mozilla 2 license. The application can be downloaded as compiled docker images. A live demo is available at https://i2b2clinical.org/demo-i2b2etl/ (username: demo, password: Etl@2021).\nSupplementary data are available at Bioinformatics online.",

    "abstract": "There is general awareness of artificial selection and its potential implications on the health and welfare of animals. Despite growing popularity and increasing numbers of reptile breeds of atypical colour and pattern variants, only a few studies have investigated the appearance and causes of diseases associated with colour morphs. Ball pythons (Python regius) are among the most frequently bred reptiles and breeders have selected for a multitude of different colour and pattern morphs. Among those colour variants, the spider morph of the ball python is frequently associated with wobble syndrome. The aim of this study was to determine whether a morphological variant can be found and associated with the clinical occurrence of wobble syndrome in spider ball pythons, using computed tomography and magnetic resonance imaging as in-vivo diagnostic methods. Data from five spider and three wild type ball pythons was assessed and evaluated comparatively. We were able to identify distinctive structural differences in inner ear morphology in spider ball pythons, which were highly likely related to wobble syndrome. To our knowledge, this is the first report of these anomalies and provides a basis for further anatomical and genetic studies and discussion of the implications for animal welfare in reptile breeding.",

    "abstract": "PyVaporation-a freely available Python package with an open-source code for modelling and studying pervaporation processes-is introduced. The theoretical background of the solution, its applicability and limitations are discussed. The usability of the package is evaluated using various examples of working with and modelling experimental data. A general equation for the representation of a component's permeance as a function of feed composition, temperature and initial feed composition is proposed and implemented in the developed package. The suggested general permeance equation may be used for the description of an extremal character of permeance as a function of process temperature and feed composition, allowing the description of processes with a high degree of non-ideality. The application of the package allowed modelling experimental points of various sets of hydrophilic pervaporation data and data on membrane performance from independent sources with a relative root mean square deviation of not more than 9% for flux and not more than 5% for a separated mixture concentration. The application of the facilitated parameter approach allowed the prediction of the components' permeance as a function of feed concentration at various initial feed concentrations with a relative root mean square error of 3-26%. The package was proven useful for modelling isothermal and adiabatic time and length-dependent pervaporation processes. The comparison of the models obtained with PyVaporation with models provided in the literature indicated similar accuracy of the obtained results, thereby proving the applicability of the developed package.",

    "abstract": "Earth observation data have revolutionized Earth science and significantly enhanced the ability to forecast weather, climate and natural hazards. The storage format of the majority of Earth observation data can be classified into swath, grid or point structures. Earth science studies frequently involve resampling between swath, grid and point data when combining measurements from multiple instruments, which can provide more insights into geophysical processes than using any single instrument alone. As the amount of Earth observation data increases each day, the demand for a high computational efficient tool to resample and fuse Earth observation data has never been greater. We present a software tool, called pytaf, that resamples Earth observation data stored in swath, grid or point structures using a novel block indexing algorithm. This tool is specially designed to process large scale datasets. The core functions of pytaf were implemented in C with OpenMP to enable parallel computations in a shared memory environment. A user-friendly python interface was also built. The tool has been extensively tested on supercomputers and successfully used to resample the data from five instruments on the EOS-Terra platform at a mission-wide scale.",

    "abstract": "Computational systems biology analyses typically make use of multiple software and their dependencies, which are often run across heterogeneous compute environments. This can introduce differences in performance and reproducibility. Capturing metadata (e.g. package versions, GPU model) currently requires repetitious code and is difficult to store centrally for analysis. Even where virtual environments and containers are used, updates over time mean that versioning metadata should still be captured within analysis pipelines to guarantee reproducibility.\nMicrobench is a simple and extensible Python package to automate metadata capture to a file or Redis database. Captured metadata can include execution time, software package versions, environment variables, hardware information, Python version and more, with plugins. We present three case studies demonstrating Microbench usage to benchmark code execution and examine environment metadata for reproducibility purposes.\nInstall from the Python Package Index using pip install microbench. Source code is available from https://github.com/alubbock/microbench.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Three-dimensional (3D) spheroid cultures are generating increasing interest in cancer research, e.g. for the evaluation of pharmacological effects of novel small molecule inhibitors. This is mainly due to the fact that such 3D structures reflect physiological characteristics of tumours and the cellular microenvironments they reside in more faithfully than two-dimensional (2D) cell cultures; in addition, they allow the reduction of animal experiments while providing significantly relevant human-based models. Quantification of such organoid structures as well as the mainly slice-based acquisition and thus forced 2D representation of 3D spheroids provide a challenge for the interpretation of the associated generated data. Here, we provide a novel open-source workflow to reconstruct a 3D entity from slice-recorded microscopical images with or without treatment with anti-migratory small molecule inhibitors. This reconstruction produces distinct point clouds as basis for subsequent comparison of basic readout parameters using average computer processor, memory and graphics resources within an acceptable time frame. We were able to validate the usefulness of this workflow using 3D data generated by various imaging techniques, including ",

    "abstract": "Machine learning (ML) is revolutionizing image-based diagnostics in pathology and radiology. ML models have shown promising results in research settings, but the lack of interoperability between ML systems and enterprise medical imaging systems has been a major barrier for clinical integration and evaluation. The DICOM",

    "abstract": null,

    "abstract": "The design of X-ray optics based on diffraction from crystals depends on the accurate calculation of the structure factors of their Bragg reflections over a wide range of temperatures. In general, the temperature dependence of the lattice parameters, the atomic positions and the atomic thermal vibrations is both anisotropic and nonlinear. Implemented here is a software package for precise and flexible calculation of structure factors for dynamical diffraction. \u03b1-Quartz is used as an example because it presents the challenges mentioned above and because it is being considered for use in high-resolution X-ray spectroscopy. The package is designed to be extended easily to other crystals by adding new material files, which are kept separate from the package's stable core. Python 3 was chosen as the language to allow the easy integration of this code into existing packages. The importance of a correct anisotropic treatment of the atomic thermal vibrations is demonstrated by comparison with an isotropic Debye model. Discrepancies between the two models can be as much as 5% for strong reflections and considerably larger (even to the level of 100%) for weak reflections. A script for finding Bragg reflections that backscatter X-rays of a given energy within a given temperature range is demonstrated. The package and example scripts are available on request. Also discussed, in detail, are the various conventions related to the proper description of chiral quartz.",

    "abstract": "Developing machine learning algorithms for time-series data often requires manual annotation of the data. To do so, graphical user interfaces (GUIs) are an important component. Existing Python packages for annotation and analysis of time-series data have been developed without addressing adaptability, usability, and user experience. Therefore, we developed a generic open-source Python package focusing on adaptability, usability, and user experience. The developed package, Machine Learning and Data Analytics (MaD) GUI, enables developers to rapidly create a GUI for their specific use case. Furthermore, MaD GUI enables domain experts without programming knowledge to annotate time-series data and apply algorithms to it. We conducted a small-scale study with participants from three international universities to test the adaptability of MaD GUI by developers and to test the user interface by clinicians as representatives of domain experts. MaD GUI saves up to 75% of time in contrast to using a state-of-the-art package. In line with this, subjective ratings regarding usability and user experience show that MaD GUI is preferred over a state-of-the-art package by developers and clinicians. MaD GUI reduces the effort of developers in creating GUIs for time-series analysis and offers similar usability and user experience for clinicians as a state-of-the-art package.",

    "abstract": "Theory of mind (ToM) is considered crucial for understanding social-cognitive abilities and impairments. However, verbal theories of the mechanisms underlying ToM are often criticized as under-specified and mutually incompatible. This leads to measures of ToM being unreliable, to the extent that even canonical experimental tasks do not require representation of others' mental states. There have been attempts at making computational models of ToM, but these are not easily available for broad research application. In order to help meet these challenges, we here introduce the Python package tomsup: Theory of mind simulations using Python. The package provides a computational eco-system for investigating and comparing computational models of hypothesized ToM mechanisms and for using them as experimental stimuli. The package notably includes an easy-to-use implementation of the variational recursive Bayesian k-ToM model developed by (Devaine, Hollard, & Daunizeau, 2014b) and of simpler non-recursive decision models, for comparison. We provide a series of tutorials on how to: (i) simulate agents relying on the k-ToM model and on a range of simpler types of mechanisms; (ii) employ those agents to generate online experimental stimuli; (iii) analyze the data generated in such experimental setup, and (iv) specify new custom ToM and heuristic cognitive models.",

    "abstract": "Modeling and understanding properties of materials from first principles require knowledge of the underlying atomistic structure. This entails knowing the individual chemical identity and position of all atoms involved. Obtaining such information for macro-molecules, nano-particles, and clusters and for the surface, interface, and bulk phases of amorphous and solid materials represents a difficult high-dimensional global optimization problem. The rise of machine learning techniques in materials science has, however, led to many compelling developments that may speed up structure searches. The complexity of such new methods has prompted a need for an efficient way of assembling them into global optimization algorithms that can be experimented with. In this paper, we introduce the Atomistic Global Optimization X (AGOX) framework and code as a customizable approach that enables efficient building and testing of global optimization algorithms. A modular way of expressing global optimization algorithms is described, and modern programming practices are used to enable that modularity in the freely available AGOX Python package. A number of examples of global optimization approaches are implemented and analyzed. This ranges from random search and basin-hopping to machine learning aided approaches with on-the-fly learnt surrogate energy landscapes. The methods are applied to problems ranging from supported clusters over surface reconstructions to large carbon clusters and metal-nitride clusters incorporated into graphene sheets.",

    "abstract": "The Florida Everglades is a unique and fragile coastal wetland ecosystem that is undergoing a decades-long, large-scale ecological restoration. This freshwater ecosystem in southern Florida has been stressed by diminishment of freshwater flow and water diversion due to agricultural activities and urbanization. The health of this vast ecosystem is also threatened by the presence of a large number of invasive species, including the Burmese python. These large constrictors were introduced to South Florida through the pet trade; first sightings in Everglades National Park occurred in the 1980s. Pythons are naturally camouflaged in the Everglades, which turns out to be an excellent environment for propagation of these huge predators. This top predator has severely disrupted the food web, consuming mammals, birds and even other reptiles. In this paper, the current population control efforts implemented by various management agencies are assessed. While more paid professional hunters should be retained to join the search and removal efforts, innovative control measures are necessary.",

    "abstract": "The recent discovery of two independently evolved XX/XY sex determination systems in the snake genera ",

    "abstract": "NeuroKit2 is a Python Toolbox for Neurophysiological Signal Processing. The presented method is an adaptation of NeuroKit2 to simplify and automate computation of the various mathematical estimates of heart rate variability (HRV) or similar time series. By default, the present approach accepts as input electrocardiogram's R-R intervals (RRIs) or peak times, i.e., timestamp of each consecutive R peak, but the RRIs or peak times can also stem from other biosensors such as photoplethysmography (PPGs) or represent more general kinds of biological or non-biological time series oscillations. The data may be derived from a single or several sources such as conventional univariate heart rate time series or intermittently weakly coupled fetal and maternal heart rate data. The method describes preprocessing and computation of an output of 124 HRV measures including measures with a dynamic, time-series-specific optimal time delay-based complexity estimation with a user-definable time window length. I also provide an additional layer of HRV estimation looking at the temporal fluctuations of the HRV estimates themselves, an approach not yet widely used in the field, yet showing promise (doi: 10.3389/fphys.2017.01112). To demonstrate the application of the methodology, I present an approach to studying the dynamic relationships between sleep state architecture and multi-dimensional HRV metrics in 31 subjects. NeuroKit2's documentation is extensive. Here, I attempted to simplify things summarizing all you need to produce the most extensive HRV estimation output available to date as open source and all in one place. The presented Jupyter notebooks allow the user to run HRV analyses quickly and at scale on univariate or multivariate time-series data. I gratefully acknowledge the excellent support from the NeuroKit team.\u2022Univariate or multivariate time series input; ingestion, preprocessing, and computation of 124 HRV metrics.\u2022Estimation of intra- and inter-individual higher order temporal fluctuations of HRV metrics.\u2022Application to a sleep dataset recorded using Apple Watch and expert sleep labeling.",

    "abstract": "Mathematical models are effective in studying cancer development at different scales from metabolism to tissue. Phase Field Models (PFMs) have been shown to reproduce accurately cancer growth and other related phenomena, including expression of relevant molecules, extracellular matrix remodeling and angiogenesis. However, implementations of such models are rarely published, reducing access to these techniques. To reduce this gap, we developed Mocafe, a modular open-source Python package that implements some of the most important PFMs reported in the literature. Mocafe is designed to handle both PFMs purely based on differential equations and hybrid agent-based PFMs. Moreover, Mocafe is meant to be extensible, allowing the inclusion of new models in future releases.\nMocafe is a Python package based on FEniCS, a popular computing platform for solving partial differential equations. The source code, extensive documentation and demos are provided on GitHub at URL: https://github.com/BioComputingUP/mocafe. Moreover, we uploaded on Zenodo an archive of the package, which is available at https://doi.org/10.5281/zenodo.6366052.",

    "abstract": "The influence of metabolism on signaling, epigenetic markers, and transcription is highly complex yet important for understanding cancer physiology. Despite the development of high-resolution multi-omics technologies, it is difficult to infer metabolic activity from these indirect measurements. Fortunately, genome-scale metabolic models and constraint-based modeling provide a systems biology framework to investigate the metabolic states and define the genotype-phenotype associations by integrations of multi-omics data. Constraint-Based Reconstruction and Analysis (COBRA) methods are used to build and simulate metabolic networks using mathematical representations of biochemical reactions, gene-protein reaction associations, and physiological and biochemical constraints. These methods have led to advancements in metabolic reconstruction, network analysis, perturbation studies as well as prediction of metabolic state. Most computational tools for performing these analyses are written for MATLAB, a proprietary software. In order to increase accessibility and handle more complex datasets and models, community efforts have started to develop similar open-source tools in Python. To date there is a comprehensive set of tools in Python to perform various flux analyses and visualizations; however, there are still missing algorithms in some key areas. This review summarizes the availability of Python software for several components of COBRA methods and their applications in cancer metabolism. These tools are evolving rapidly and should offer a readily accessible, versatile way to model the intricacies of cancer metabolism for identifying cancer-specific metabolic features that constitute potential drug targets.",

    "abstract": "The availability of data is the driving force behind most of the state-of-the-art techniques for machine translation tasks. Understandably, this availability of data motivates researchers to propose new techniques and claim about the superiority of their techniques over the existing ones by using suitable evaluation measures. However, the performance of underlying learning algorithms can be greatly influenced by the correctness and the consistency of the corpus. We present our investigations for the relevance of a publicly available python to pseudo-code parallel corpus for automated documentation task, and the studies performed using this corpus. We found that the corpus had many visible issues like overlapping of instances, inconsistency in translation styles, incompleteness, and misspelled words. We show that these discrepancies can significantly influence the performance of the learning algorithms to the extent that they could have caused previous studies to draw incorrect conclusions. We performed our experimental study using statistical machine translation and neural machine translation models. We have recorded a significant difference ( ",

    "abstract": "Enrichment analyses are widely applied to investigate lists of genes of interest. However, such analyses often result in long lists of annotation terms with high redundancy, making the interpretation and reporting difficult. Long annotation lists and redundancy also complicate the comparison of results obtained from different enrichment analyses. An approach to overcome these issues is using down-sized annotation collections composed of non-redundant terms. However, down-sized collections are generic and the level of detail may not fit the user's study. Other available approaches include clustering and filtering tools, which are based on similarity measures and thresholds that can be complicated to comprehend and set.\nWe propose orsum, a Python package to filter enrichment results. orsum can filter multiple enrichment results collectively and highlight common and specific annotation terms. Filtering in orsum is based on a simple principle: a term is discarded if there is a more significant term that annotates at least the same genes; the remaining more significant term becomes the representative term for the discarded term. This principle ensures that the main biological information is preserved in the filtered results while reducing redundancy. In addition, as the representative terms are selected from the original enrichment results, orsum outputs filtered terms tailored to the study. As a use case, we applied orsum to the enrichment analyses of four lists of genes, each associated with a neurodegenerative disease.\norsum provides a comprehensible and effective way of filtering and comparing enrichment results.\u00a0It is available at https://anaconda.org/bioconda/orsum .",

    "abstract": "Meta-analysis is a central method for quality evidence generation. In particular, meta-analysis is gaining speedy momentum in the growing world of quantitative information. There are several software applications to process and output expected results. Open-source software applications generating such results are receiving more attention. This paper uses Python's capabilities to provide applicable instruction to perform a meta-analysis.\nWe used the PythonMeta package with several modifications to perform the meta-analysis on an open-access dataset from Cochrane. The analyses were complemented by employing Python's zEpid package capable of creating forest plots. Also, we developed Python scripts for contour-enhanced funnel plots to assess funnel plots asymmetry. Finally, we ran the analyses in R and STATA to check the cross-validity of the results.\nA stepwise instruction on installing the software and packages and performing meta-analysis was provided. We shared the Python codes for meta-analysts to follow and generate the standard outputs. Our results were similar to those yielded by R and STATA.\nWe successfully produced standard meta-analytic outputs using Python. This programming language has several flexibilities to improve the meta-analysis results even further.",

    "abstract": "Genome biology shows substantial progress in its analytical and computational part in the last decades. Differential gene expression is one of many computationally intense areas; it is largely developed under R programming language. Here we explain possible reasons for such dominance of R in gene expression data. Next, we discuss the prospects for Python to become competitive in this area of research in coming years. We indicate that Python can be used already in a field of a single cell differential gene expression. We pinpoint still missing parts in Python and possibilities for improvement.",

    "abstract": null,

    "abstract": "A classic approach to estimate individual theta-to-alpha transition frequency (TF) requires two electroencephalographic (EEG) recordings, one acquired in a resting state condition and one showing alpha desynchronisation due, for example, to task execution. This translates into long recording sessions that may be cumbersome in studies involving patients. Moreover, an incomplete desynchronisation of the alpha rhythm may compromise TF estimates. Here we present transfreq, a publicly available Python library that allows TF computation from resting state data by clustering the spectral profiles associated to the EEG channels based on their content in alpha and theta bands. A detailed overview of transfreq core algorithm and software architecture is provided. Its effectiveness and robustness across different experimental setups are demonstrated on a publicly available EEG data set and on in-house recordings, including scenarios where the classic approach fails to estimate TF. We conclude with a proof of concept of the predictive power of transfreq TF as a clinical marker. Specifically, we present a scenario where transfreq TF shows a stronger correlation with the mini mental state examination score than other widely used EEG features, including individual alpha peak and median/mean frequency. The documentation of transfreq and the codes for reproducing the analysis of the article with the open-source data set are available online at https://elisabettavallarino.github.io/transfreq/. Motivated by the results showed in this article, we believe our method will provide a robust tool for discovering markers of neurodegenerative diseases.",

    "abstract": "With the development of internet technology, e-learning has become an essential part of the modern education system. However, the e-learning market faces enormous competition. Consumers' continuance purchase intention has become a vital factor in the success of e-learning courses. Thus, factors that influence consumers' continuance purchase intention should be examined in the e-learning market. However, little research has focused on identifying the continuance purchase intention of an e-learning course. Based on the information system continuity model ISC), this paper develops a research model to investigate the factors influencing satisfaction and continuance purchase intention in e-learning. A cross-sectional, questionnaire-based research design was used in this study. We collected data from consumers who had enrolled in paid online Python courses. In total, 508 paid online Python course users completed the online survey. SmartPLS software was used for data analysis. The results demonstrated that perceived course quality, service quality, convenience, and usefulness significantly affect consumers' satisfaction with the experience course. Moreover, the findings show that satisfaction, self-efficacy, and e-word of mouth (e-WOM) determine the consumers' continuance purchase intention of the reminder course. This study also found that satisfaction mediates the effects of experience courses on consumers' continuance purchase intention of the online Python course. The implications for theory and practice and future research directions are discussed.",

    "abstract": "The Synthetic Biology Open Language version 3 (SBOL3) provides a data model for representation of synthetic biology information across multiple scales and throughout the design-build-test-learn workflow. To support practical use of this data model, we have developed pySBOL3, a Python library that allows programmers to create and edit SBOL3 documents. Here we describe this library and key engineering decisions in its design. The resulting implementation is a compact and maintainable core that provides both a familiar, pythonic interface for manipulating SBOL3 objects as well as mechanisms for building additional extensions and representations on this base.",

    "abstract": "Many respiratory pathogens compromise epithelial barrier function during lung infection by disrupting intercellular junctions, such as adherens junctions and tight junctions, that maintain intercellular integrity. This includes ",

    "abstract": "Rapid and repeatable polymorphism analyses have become a necessity with the current amount of genomic data that can be collected in many organisms. Traditionally, such analyses are conducted using a variety of tools in combination, often requiring numerous format translation and manipulation. Here, we present a massively updated version of our previous software package egglib, intended to alleviate such costly and error-prone tinkering with the data. egglib has been streamlined into a python package and thoroughly updated and optimized to accommodate modern-day sized dataset. We show the main characteristics of the package making it a tool of choice to perform population genetics analyses. Once the data are imported (whatever their encoding), they can be filtered, edited, analysed and compared to coalescent simulations very easily and efficiently. Furthermore, the list of diversity and polymorphism statistics that can now be calculated has been greatly expanded. The software and its full documentation are available at https://egglib.org/.",

    "abstract": "Due to its high sensitivity and resolving power, gas chromatography ion mobility spectrometry (GC-IMS) is an emerging benchtop technique for non-target screening of complex sample materials. Given the wide range of applications, such as food authenticity, custom data analysis workflows are needed. As a common basis, they necessarily share many functionalities such as file input/output, preprocessing methods, exploratory or supervised analysis and visualizations. This study introduces a new open source, fully customizable Python package for handling and analysis of GC-IMS data. A workflow to classify olive oils by geographical origin exemplarily demonstrates functionality and ease of use. Key preprocessing steps, exploratory - and supervised data analysis and feature selections are visualized. All code and detailed documentation are freely available as open source under the BSD 3-clause license at https://github.com/Charisma-Mannheim/gc-ims-tools.",

    "abstract": "Predicting transition state geometries is one of the most challenging tasks in computational chemistry, which often requires expert-based knowledge and permanent human intervention. This short communication reports technical details and preliminary results of a python-based tool (AMADAR) designed to generate any Diels-Alder (DA) transition state geometry (TS) and analyze determined IRC paths in a (quasi-)automated fashion, given the product SMILES. Two modules of the package are devoted to performing, from IRC paths, reaction force analyses (RFA) and atomic (fragment) decompositions of the reaction force F and reaction force constant [Formula: see text]. The performance of the protocol has been assessed using a dataset of 2000 DA cycloadducts retrieved from the ZINC database. The sequential location of the corresponding TSs was achieved with a success rate of 95%. RFA plots confirmed the reaction force constant [Formula: see text] to be a good indicator of the (non)synchronicity of the associated DA reactions. Moreover, the atomic decomposition of [Formula: see text] allows for the rationalization of the (a)synchronicity of each DA reaction in terms of contributions stemming from pairs of interacting atoms. The source code of the AMADAR tool is available on GitHub [ CMCDD/AMADAR(github.com) ] and can be used directly with minor customizations, mostly regarding the local working environment of the user.",

    "abstract": "Herein, we present KiMoPack, an analysis tool for the ",

    "abstract": "As efforts to computationally describe and simulate the biochemical world become more commonplace, computer programs that are capable of in silico chemistry play an increasingly important role in biochemical research. While such programs exist, they are often dependency-heavy, difficult to navigate, or not written in Python, the programming language of choice for bioinformaticians. Here, we introduce PIKAChU (Python-based Informatics Kit for Analysing CHemical Units): a cheminformatics toolbox with few dependencies implemented in Python. PIKAChU builds comprehensive molecular graphs from SMILES strings, which allow for easy downstream analysis and visualisation of molecules. While the molecular graphs PIKAChU generates are extensive, storing and inferring information on aromaticity, chirality, charge, hybridisation and electron orbitals, PIKAChU limits itself to applications that will be sufficient for most casual users and downstream Python-based tools and databases, such as Morgan fingerprinting, similarity scoring, substructure matching and customisable visualisation. In addition, it comes with a set of functions that assists in the easy implementation of reaction mechanisms. Its minimalistic design makes PIKAChU straightforward to use and install, in stark contrast to many existing toolkits, which are more difficult to navigate and come with a plethora of dependencies that may cause compatibility issues with downstream tools. As such, PIKAChU provides an alternative for researchers for whom basic cheminformatic processing suffices, and can be easily integrated into downstream bioinformatics and cheminformatics tools. PIKAChU is available at https://github.com/BTheDragonMaster/pikachu .",

    "abstract": "Analysis of intra- and extracellular dynamic like vesicles transport involves particle tracking algorithms. The design of a particle tracking pipeline is a routine but tedious task. Therefore, particle dynamics analysis is often performed by combining several pieces of software (filtering, detection, tracking, etc.) requiring many manual operations, and thus leading to poorly reproducible results. Given the new segmentation tools based on deep learning, modularity and interoperability between software have become essential in particle tracking algorithms. A good synergy between a particle detector and a tracker is of paramount importance. In addition, a user-friendly interface to control the quality of estimated trajectories is necessary. To address these issues, we developed STracking, a Python library that allows combining algorithms into standardized particle tracking pipelines.\nSTracking is available as a Python library using 'pip install' and the source code is publicly available on GitHub (https://github.com/sylvainprigent/stracking). A graphical interface is available using two napari plugins: napari-stracking and napari-tracks-reader. These napari plugins can be installed via the napari plugins menu or using 'pip install'. The napari plugin source codes are available on GitHub (https://github.com/sylvainprigent/napari-tracks-reader, https://github.com/sylvainprigent/napari-stracking).\nSupplementary data are available at Bioinformatics online.",

    "abstract": "py-MCMD, an open-source Python software, provides a robust workflow layer that manages communication of relevant system information between the simulation engines NAMD and GOMC and generates coherent thermodynamic properties and trajectories for analysis. To validate the workflow and highlight its capabilities, hybrid Monte Carlo/molecular dynamics (MC/MD) simulations are performed for SPC/E water in the isobaric-isothermal (",

    "abstract": "Open source analytical software for the analysis of electrophysiological cardiomyocyte data offers a variety of new functionalities to complement closed-source, proprietary tools. Here, we present the Cardio PyMEA application, a free, modifiable, and open source program for the analysis of microelectrode array (MEA) data obtained from cardiomyocyte cultures. Major software capabilities include: beat detection; pacemaker origin estimation; beat amplitude and interval; local activation time, upstroke velocity, and conduction velocity; analysis of cardiomyocyte property-distance relationships; and robust power law analysis of pacemaker spatiotemporal instability. Cardio PyMEA was written entirely in Python 3 to provide an accessible, integrated workflow that possesses a user-friendly graphical user interface (GUI) written in PyQt5 to allow for performant, cross-platform utilization. This application makes use of object-oriented programming (OOP) principles to facilitate the relatively straightforward incorporation of custom functionalities, e.g. power law analysis, that suit the needs of the user. Cardio PyMEA is available as an open source application under the terms of the GNU General Public License (GPL). The source code for Cardio PyMEA can be downloaded from Github at the following repository: https://github.com/csdunhamUC/cardio_pymea.",

    "abstract": "Here we developed an open-source Python-based library called Python rodent Analysis and Tracking (PyRAT). Our library analyzes tracking data to classify distinct behaviors, estimate traveled distance, speed and area occupancy. To classify and cluster behaviors, we used two unsupervised algorithms: hierarchical agglomerative clustering and t-distributed stochastic neighbor embedding (t-SNE). Finally, we built algorithms that associate the detected behaviors with synchronized neural data and facilitate the visualization of this association in the pixel space. PyRAT is fully available on GitHub: https://github.com/pyratlib/pyrat.",

    "abstract": "Electrocardiography (ECG) is the method most often used to diagnose cardiovascular diseases. To obtain a high-quality recording, the person conducting an ECG must be a trained expert. When these experts are not available, this important diagnostic tool cannot be used, consequently impacting the quality of healthcare. To avoid this problem, it must be possible for untrained healthcare professionals to record diagnostically useful ECGs so they can send the recordings to experts for diagnosis. The ECGAssess Python-based toolbox developed in this study provides feedback regarding whether ECG signals are of adequate quality. Each lead of the 12-lead recordings was classified as acceptable or unacceptable. This feedback allows people to identify and correct errors in the use of the ECG device. The toolbox classifies the signals according to stationary, heart rate, and signal-to-noise ratio. If the limits of these three criteria are exceeded, this is indicated to the user. To develop and optimize the toolbox, two annotators reviewed a data set of 1,200 ECG leads to assess their quality, and each lead was classified as acceptable or unacceptable. The evaluation of the toolbox was done with a new data set of 4,200 leads, which were annotated the same way. This evaluation shows that the ECGAssess toolbox correctly classified over 94% of the 4,200 ECG leads as either acceptable or unacceptable in comparison to the annotations.",

    "abstract": "Python and MATLAB both are common tools used for predictive modelling applications, not only in healthcare. In our predictive modelling group, both tools are widely used. None of the two tools is optimal for all tasks along the value chain of predictive modelling in healthcare.\nThe aim of this study was to explore different ways to extend our MATLAB-based toolset with Python functions.\nPre-existing interfaces between MATLAB and Python have been evaluated and more comprehensive interfaces have been designed to exchange even complex data formats such as MATLAB tables.\nThe interfaces have successfully been implemented and they were validated in a Natural Language Processing scenario based on free-text notes from a telehealth services for heart failure patients.\nIntegration of Python modules in our MATLAB toolset is possible. Further improvements especially in terms of performance, are required if large datasets need to be exchanged. A big advantage of our concept is that tabular data can be exchanged between MATLAB and Python without loss and the Python functions are called dynamically via the interface.",

    "abstract": "HTSeq 2.0 provides a more extensive application programming interface including a new representation for sparse genomic data, enhancements for htseq-count to suit single-cell omics, a new script for data using cell and molecular barcodes, improved documentation, testing and deployment, bug fixes and Python 3 support.\nHTSeq 2.0 is released as an open-source software under the GNU General Public License and is available from the Python Package Index at https://pypi.python.org/pypi/HTSeq. The source code is available on Github at https://github.com/htseq/htseq.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "LIGER (Linked Inference of Genomic Experimental Relationships) is a widely used R package for single-cell multi-omic data integration. However, many users prefer to analyze their single-cell datasets in Python, which offers an attractive syntax and highly optimized scientific computing libraries for increased efficiency.\nWe developed PyLiger, a Python package for integrating single-cell multi-omic datasets. PyLiger offers faster performance than the previous R implementation (2-5\u00d7 speedup), interoperability with AnnData format, flexible on-disk or in-memory analysis capability and new functionality for gene ontology enrichment analysis. The on-disk capability enables analysis of arbitrarily large single-cell datasets using fixed memory.\nPyLiger is available on Github at https://github.com/welch-lab/pyliger and on the Python Package Index.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "High-quality visualization of biological networks often requires both manual curation for proper alignment and programming to map external data to the graphical components. Nezzle is a network visualization software written in Python, which provides programmable and interactive interfaces for facilitating both manual and automatic curation of the graphical components of networks to create high-quality figures.\nNezzle is an open-source project under MIT license and is available from https://github.com/dwgoon/nezzle.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Design inspiration comes from the continuous stimulation of external information and the continuous accumulation of knowledge. In order to obtain an ideal design inspiration from nature, researchers have proposed a large number of biological information retrieval and knowledge acquisition methods. But how to purposefully acquire valuable biological knowledge in order to effectively stimulate design inspiration and produce the novel and feasible designs idea is still an urgent problem to be solved. This paper proposes a method for acquiring valuable biological knowledge to efficiently stimulate inspiration and quickly conceive solutions in engineering design. First, keywords, such as the functional requirements and key components of design objects, are selected as the engineering terminologies. Next, biological keywords related to the engineering terminologies are searched from the biological dictionary and biology websites. Then in order to retrieve enough biological knowledge, these biological keywords are expanded manually and automatically respectively based on Thesaurus Webpage and WordNet database, and expanded keywords are filtered according to repeated words and different forms of the same words. Finally, in the biological knowledge base, biological keywords that had been filtered are used to obtain biological knowledge with Python web crawler programming. Through an example of application for ship equipment, the effectiveness of the method is verified.",

    "abstract": "Computational pipelines have become a crucial part of modern drug discovery campaigns. Setting up and maintaining such pipelines, however, can be challenging and time-consuming-especially for novice scientists in this domain. TeachOpenCADD is a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects. We offer Python-based solutions for common tasks in cheminformatics and structural bioinformatics in the form of Jupyter notebooks, based on open source resources only. Including the 12 newly released additions, TeachOpenCADD now contains 22 notebooks that cover both theoretical background as well as hands-on programming. To promote reproducible and reusable research, we apply software best practices to our notebooks such as testing with automated continuous integration and adhering to the\u00a0idiomatic Python style. The new TeachOpenCADD website is available at https://projects.volkamerlab.org/teachopencadd and all code is deposited on GitHub.",

    "abstract": "In order to support the burgeoning field of research into intra- and interpersonal synchrony, we present an open-source software package: multiSyncPy. Multivariate synchrony goes beyond the bivariate case and can be useful for quantifying how groups, teams, and families coordinate their behaviors, or estimating the degree to which multiple modalities from an individual become synchronized. Our package includes state-of-the-art multivariate methods including symbolic entropy, multidimensional recurrence quantification analysis, coherence (with an additional sum-normalized modification), the cluster-phase 'Rho' metric, and a statistical test based on the Kuramoto order parameter. We also include functions for two surrogation techniques to compare the observed coordination dynamics with chance levels\u00a0and a windowing function to examine time-varying coordination for most of the measures. Taken together, our collation and presentation of these methods make the study of interpersonal synchronization and coordination dynamics applicable to larger, more complex and often more ecologically valid study designs. In this work, we summarize the relevant theoretical background and present illustrative practical examples, lessons learned, as well as guidance for the usage of our package - using synthetic as well as empirical data. Furthermore, we provide a discussion of our work and software and outline interesting further directions and perspectives. multiSyncPy is freely available under the LGPL license at: https://github.com/cslab-hub/multiSyncPy , and also available at the Python package index.",

    "abstract": "Burmese pythons (Python molurus bivitattus) use a unique infrared (IR) targeting system to acquire prey, avoid predators and seek thermoregulatory sites through detection of IR energy in the environment. Previous studies of sensitivity of the python IR system that relied on analysis of complex, natural behaviors lacked robust, reliable responses in animals habituated to experiments, and in vitro electrophysiological study failed to test behavioral function of the implicated protein thermoreceptor, TRPA1. The present study used conditioned discrimination procedures to analyze behavioral sensitivity and signal transduction in the python IR system. Pythons trained to behaviorally discriminate thermal stimuli averaged 70% correct choices, but failed to make correct choices when pit organs were physically occluded with IR-blocking material. The pythons exhibited greater sensitivity to thermal stimuli than previously reported, evident by correct choices that exceeded chance in response to a 14\u00a0\u00d7\u00a010",

    "abstract": "The COVID-19 pandemic had a great impact on the socio-economic stability of every country. To curb the effect and risk of transmission, governments implemented various measures including the mandatory vaccination of their citizens. However, despite these efforts, many people are still hesitant to take the vaccine because of various reasons and biases. This paper attempts to explore the perceptions of the people who have undergone vaccinations regarding the various side effects to provide inputs to vaccine manufacturers and assist people in making informed decisions in selecting the appropriate vaccine for them. The study further explored the correlation and association of age, weight category, diet category, blood type, and sleeping patterns with the severity of the selected vaccine side effects. The results revealed that vaccine side effects are associated with the vaccine type. Age, gender, weight category, diet category, blood type, and sleeping patterns have significant relationships to one or more side effects.",

    "abstract": "Mineralogy, petrology and materials science are fundamental disciplines not only for the basic knowledge and classification of solid phases but also for their technological applications, which are becoming increasingly demanding and challenging. Characterization and design of materials are of utmost importance and usually need knowledge of the thermodynamics and mechanical stability of solids. Alongside well known experimental approaches, in recent years the advances in both quantum mechanical methods and computational power have placed theoretical investigations as a complementary useful and powerful tool in this kind of study. In order to aid both theoreticians and experimentalists, an open-source Python-based software, ",

    "abstract": "Remote photoplethysmography (rPPG) aspires to automatically estimate heart rate (HR) variability from videos in realistic environments. A number of effective methods relying on data-driven, model-based and statistical approaches have emerged in the past two decades. They exhibit increasing ability to estimate the blood volume pulse (BVP) signal upon which BPMs (Beats per Minute) can be estimated. Furthermore, learning-based rPPG methods have been recently proposed. The present pyVHR framework represents a multi-stage pipeline covering the whole process for extracting and analyzing HR fluctuations. It is designed for both theoretical studies and practical applications in contexts where wearable sensors are inconvenient to use. Namely, pyVHR supports either the development, assessment and statistical analysis of novel rPPG methods, either traditional or learning-based, or simply the sound comparison of well-established methods on multiple datasets. It is built up on accelerated Python libraries for video and signal processing as well as equipped with parallel/accelerated ad-hoc procedures paving the way to online processing on a GPU. The whole accelerated process can be safely run in real-time for 30 fps HD videos with an average speedup of around 5. This paper is shaped in the form of a gentle tutorial presentation of the framework.",

    "abstract": "Lcapy is an open-source Python package for solving linear circuits symbolically. It uses a superposition of DC analysis, AC (phasor) analysis, transient (Laplace) analysis, and noise analysis. Expressions are evaluated using the computer algebra system SymPy. Lcapy can model circuits comprised of combinations of one-port and two-port networks or circuits specified using a netlist with a Spice-like notation. Lcapy can present the system of equations produced from nodal analysis, modified nodal analysis, loop analysis, and state-space analysis. Expressions can be formatted into many representations, parameterized, and transformed to other domains. Dimensional analysis is performed to reduce user errors and to present results with units. Both continuous and discrete signals are supported. Lcapy produces high-quality output. Textbook quality schematics in a number of different formats can be generated from netlists and customized for different conventions. Expressions can be formatted into LaTeX format for inclusion into a document or numerically evaluated and plotted. An overview of the features and capabilities of Lcapy is presented, along with implementation details and performance considerations.",

    "abstract": "A retrospective cohort study was conducted to analyze the application value of Python programming in general education and comprehensive quality improvement of medical students.\nA retrospective analysis was made on the application value of Python programming in the general education classroom of medical students from September 2020 to July 2021 by undergraduate students majoring in anesthesia in grade 2020, imaging in grade 2019, clinical in grade 2020, and laboratory sciences in grade 2020 in our university. A hundred students who used Python programming in general education class were divided into study group and control group. The teaching satisfaction, medical knowledge and lifelong learning ability, clinical skills, medical service ability, disease prevention, health promotion ability, interpersonal communication ability, and information management and research ability were compared between the two groups.\nIn a comparison of teaching satisfaction between the two groups, the study group was very satisfied in 89 cases, satisfactory in 10 cases, and general in 1 case, and the satisfaction rate was 100.00%; the control group was very satisfied in 54 cases, satisfactory in 23 cases, general in 13 cases, and dissatisfied in 10 cases, and the satisfaction rate was 90.00%. The teaching satisfaction in the study group was higher than that in the control group, and the difference was statistically significant (\nThe application of the Python programming method in general education and comprehensive quality improvement of medical students can effectively improve medical students' teaching satisfaction and medical knowledge such as lifelong learning ability, clinical skills, medical service ability, disease prevention, health promotion ability, interpersonal communication ability, and information management and research ability, which has a positive impact on the improvement of comprehensive quality.",

    "abstract": "Fabrication and characterization of an instrument for the high-temperature simultaneous measurement of the Seebeck coefficient (S) and thermal conductivity (\u03ba) have been carried out with Python automation. The steady-state-based Fourier's law of thermal conduction is employed for \u03ba measurement. The parallel thermal conductance technique is implemented for heat loss measurement. Introducing a thin heater and insulating heater base minimizes the heat loss and makes it easier to arrive at high temperatures. Measurement of S is carried out using the differential method. The same thermocouples are used to measure the temperature as well as voltage for S measurement. Care of temperature dependent S of the thermocouple has also been taken. Simple design, small size, and lightweight make this instrument more robust. All the components for making a sample holder are easily available in the market and can be replaced as per the user's demand. This instrument can measure samples with various dimensions and shapes in the temperature range 300-800 K. The instrument is validated using different classes of samples, such as nickel, gadolinium, Fe",

    "abstract": "Accurate detection of sequence similarity and homologous recombination are essential parts of many evolutionary analyses.\nWe have developed SimPlot++, an open-source multiplatform application implemented in Python, which can be used to produce publication quality sequence similarity plots using 63 nucleotide and 20 amino acid distance models, to detect intergenic and intragenic recombination events using \u03a6, Max-\u03c72, NSS or proportion tests, and to generate and analyze interactive sequence similarity networks. SimPlot++ supports multicore data processing and provides useful distance calculability diagnostics.\nSimPlot++ is freely available on GitHub at: https://github.com/Stephane-S/Simplot_PlusPlus, as both an executable file (for Windows) and Python scripts (for Windows/Linux/MacOS).",

    "abstract": "Current open-source Monte Carlo (MC) method implementations for light propagation modeling are many times tedious to build and require third-party licensed software that can often discourage prospective researchers in the biomedical optics community from fully utilizing the light propagation tools. Furthermore, the same drawback also limits rigorous cross-validation of physical quantities estimated by various MC codes.\nProposal of an open-source tool for light propagation modeling and an easily accessible dataset to encourage fruitful communications amongst researchers and pave the way to a more consistent comparison between the available implementations of the MC method.\nThe PyXOpto implementation of the MC method for multilayered and voxelated tissues based on the Python programming language and PyOpenCL extension enables massively parallel computation on numerous OpenCL-enabled devices. The proposed implementation is used to compute a large dataset of reflectance, transmittance, energy deposition, and sampling volume for various source, detector, and tissue configurations.\nThe proposed PyXOpto agrees well with the original MC implementation. However, further validation reveals a noticeable bias introduced by the random number generator used in the original MC implementation.\nEstablishing a common dataset is highly important for the validation of existing and development of MC codes for light propagation in turbid media.",

    "abstract": "This work presents the new template matching capabilities implemented in Pyxem, an open source Python library for analyzing four-dimensional scanning transmission electron microscopy (4D-STEM) data. Template matching is a brute force approach for deriving local crystal orientations. It works by comparing a library of simulated diffraction patterns to experimental patterns collected with nano-beam and precession electron diffraction (NBED and PED). This is a computationally demanding task, therefore the implementation combines efficiency and scalability by utilizing multiple CPU cores or a graphical processing unit (GPU). The code is built on top of the scientific Python ecosystem, and is designed to support custom and reproducible workflows that combine the image processing, template library generation, indexation and visualization all in one environment. The tools are agnostic to file size and format, which is significant in light of the increased adoption of pixelated detectors from different manufacturers. This paper details the implementation and validation of the method. The method is illustrated by calculating orientation maps of nanocrystalline materials and precipitates embedded in a crystalline matrix. The combination of speed and flexibility opens the door for automated parameter studies and real-time on-line orientation mapping inside the TEM.",

    "abstract": "The rapid accumulation of sequencing data from metagenomic studies is enabling the generation of huge collections of microbial genomes, with new challenges for mapping their functional potential. In particular, metagenome-assembled genomes are typically incomplete and harbor partial gene sequences that can limit their annotation from traditional tools. New scalable solutions are thus needed to facilitate the evaluation of functional potential in microbial genomes.\nTo resolve annotation gaps in microbial genomes, we developed KEMET, an open-source Python library devised for the analysis of Kyoto Encyclopedia of Genes and Genomes (KEGG) functional units. KEMET focuses on the in-depth analysis of metabolic reaction networks to identify missing orthologs through hidden Markov model profiles.\nWe evaluate the potential of KEMET for expanding functional annotations by simulating the effect of assembly issues on real gene sequences and showing that our approach can identify missing KEGG orthologs. Additionally, we show that recovered gene annotations can sensibly increase the quality of draft genome-scale metabolic models obtained from metagenome-assembled genomes, in some cases reaching the accuracy of models generated from complete genomes.\nKEMET therefore allows expanding genome annotations by targeted searches for orthologous sequences, enabling a better qualitative and quantitative assessment of metabolic capabilities in novel microbial organisms.",

    "abstract": "This article reports a workshop from the 2021 IUBMB/ASBMB Teaching Science with Big Data conference held virtually in June 2021 where participants learned to explore and visualize large quantities of protein PBD data using Jupyter notebooks and the Python programming language. This activity instructs participants using Jupyter notebooks, Python functions, loading data with Python, and visualize data using the matplotlib and seaborn Python plotting libraries. It also allows participants to explore large quantities of data to discover trends such amino acid abundance, dihedral angles patterns, and secondary protein structure trends. All files used in this activity, including data files, Jupyter notebooks, and completed Jupyter notebooks, are freely available at https://github.com/weisscharlesj/BiopythonRamachandran under the CC BY-NC-SA 4.0 Creative Commons license.",

    "abstract": "",

    "abstract": "Microbioreactor (MBR) devices have emerged as powerful cultivation tools for tasks of microbial phenotyping and bioprocess characterization and provide a wealth of online process data in a highly parallelized manner. Such datasets are difficult to interpret in short time by manual workflows. In this study, we present the Python package bletl and show how it enables robust data analyses and the application of machine learning techniques without tedious data parsing and preprocessing. bletl reads raw result files from BioLector I, II and Pro devices to make all the contained information available to Python-based data analysis workflows. Together with standard tooling from the Python scientific computing ecosystem, interactive visualizations and spline-based derivative calculations can be performed. Additionally, we present a new method for unbiased quantification of time-variable specific growth rate ",

    "abstract": "Epistasis is the interaction between different genes when expressing a certain phenotype. If epistasis involves more than two loci it is called high-order epistasis. High-order epistasis is an area under active research because it could be the cause of many complex traits. The most common way to specify an epistasis interaction is through a penetrance table.\nThis paper presents PyToxo, a Python tool for generating penetrance tables from any-order epistasis models. Unlike other tools available in the bibliography, PyToxo is able to work with high-order models and realistic penetrance and heritability values, achieving high-precision results in a short time. In addition, PyToxo is distributed as open-source software and includes several interfaces to ease its use.\nPyToxo provides the scientific community with a useful tool to evaluate algorithms and methods that can detect high-order epistasis to continue advancing in the discovery of the causes behind complex diseases.",

    "abstract": "Real-time fMRI (rtfMRI) has enormous potential for both mechanistic brain imaging studies or treatment-oriented neuromodulation. However, the adaption of rtfMRI has been limited due to technical difficulties in implementing an efficient computational framework. Here, we introduce a python library for real-time fMRI (rtfMRI) data processing systems, Real-Time Processing System in python (RTPSpy), to provide building blocks for a custom rtfMRI application with extensive and advanced functionalities. RTPSpy is a library package including (1) a fast, comprehensive, and flexible online fMRI image processing modules comparable to offline denoising, (2) utilities for fast and accurate anatomical image processing to define an anatomical target region, (3) a simulation system of online fMRI processing to optimize a pipeline and target signal calculation, (4) simple interface to an external application for feedback presentation, and (5) a boilerplate graphical user interface (GUI) integrating operations with RTPSpy library. The fast and accurate anatomical image processing utility wraps external tools, including FastSurfer, ANTs, and AFNI, to make tissue segmentation and region of interest masks. We confirmed that the quality of the output masks was comparable with FreeSurfer, and the anatomical image processing could complete in a few minutes. The modular nature of RTPSpy provides the ability to use it for a simulation analysis to optimize a processing pipeline and target signal calculation. We present a sample script for building a real-time processing pipeline and running a simulation using RTPSpy. The library also offers a simple signal exchange mechanism with an external application using a TCP/IP socket. While the main components of the RTPSpy are the library modules, we also provide a GUI class for easy access to the RTPSpy functions. The boilerplate GUI application provided with the package allows users to develop a customized rtfMRI application with minimum scripting labor. The limitations of the package as it relates to environment-specific implementations are discussed. These library components can be customized and can be used in parts. Taken together, RTPSpy is an efficient and adaptable option for developing rtfMRI applications. ",

    "abstract": "Wearable inertial sensors are providing enhanced insight into patient mobility and health. Significant research efforts have focused on wearable algorithm design and deployment in both research and clinical settings; however, open-source, general-purpose software tools for processing various activities of daily living are relatively scarce. Furthermore, few studies include code for replication or off-the-shelf software packages. In this work, we introduce SciKit Digital Health (SKDH), a Python software package (Python Software Foundation) containing various algorithms for deriving clinical features of gait, sit to stand, physical activity, and sleep, wrapped in an easily extensible framework. SKDH combines data ingestion, preprocessing, and data analysis methods geared toward modern data science workflows and streamlines the generation of digital endpoints in \"good practice\" environments by combining all the necessary data processing steps in a single pipeline. Our package simplifies the construction of new data processing pipelines and promotes reproducibility by following a convention over configuration approach, standardizing most settings on physiologically reasonable defaults in healthy adult populations or those with mild impairment. SKDH is open source, as well as free to use and extend under a permissive Massachusetts Institute of Technology license, and is available from GitHub (PfizerRD/scikit-digital-health), the Python Package Index, and the conda-forge channel of Anaconda.",

    "abstract": "Quantitative grading of testing has research and clinical relevance. QASAT (quantitative scale for grading of cardiovascular reflex tests, transcranial Doppler, sudomotor testing, and small fiber densities from skin biopsies) is an objective instrument for grading dysautonomia, related small fiber neuropathy and cerebral blood flow. QASAT uses established autonomic tests (deep breathing, Valsalva maneuver, tilt test, sudomotor test) and skin biopsies for assessment of small fibers. Calculations of scores are complex. This paper presents a qpack-an open source software package that implements QASAT in a Python programming language. The qpack automatically generates reproducible scores of each test and reduces calculation errors. Datasets for verifying the correct qpack implementation are provided. The goal of qpack is to facilitate availability, reproducibility, and quality of autonomic studies and skin biopsies for assessment of small fibers. Qpack is easy to use with standard Python distributions, can be incorporated into routine clinical or research autonomic testing and it is freely available.",

    "abstract": "This paper presents a new open-source program called deathdaily for predicting the number of daily COVID-19 deaths in the next 7\u00a0days. The predictions can be used by policymakers to determine whether current policies should be strengthened/mitigated or new policies should be challenged to mitigate the COVID-19 pandemic. Although vaccines have been mitigating the pandemic initially, the recent resurgence with new variants has been observed in many vaccinated countries. This paper shows how to use deathdaily to detect symptoms of resurgence. The proposed deathdaily is available in public and can be installed by a Python package manager PyPI. The deathdaily has been downloaded by 15,964 users worldwide, according to https://pepy.tech/project/deathdaily. The fact shows that the applicability, practicality, and usefulness of the proposed program have been duly evaluated.",

    "abstract": "This paper presents an implementation of the parallelization of genetic algorithms. Three models of parallelized genetic algorithms are presented, namely the Master-Slave genetic algorithm, the Coarse-Grained genetic algorithm, and the Fine-Grained genetic algorithm. Furthermore, these models are compared with the basic serial genetic algorithm model. Four modules, Multiprocessing, Celery, PyCSP, and Scalable Concurrent Operation in Python, were investigated among the many parallelization options in Python. The Scalable Concurrent Operation in Python was selected as the most favorable option, so the models were implemented using the Python programming language, RabbitMQ, and SCOOP. Based on the implementation results and testing performed, a comparison of the hardware utilization of each deployed model is provided. The results' implementation using SCOOP was investigated from three aspects. The first aspect was the parallelization and integration of the SCOOP module into the resulting Python module. The second was the communication within the genetic algorithm topology. The third aspect was the performance of the parallel genetic algorithm model depending on the hardware.",

    "abstract": "The exotic internal structure of polar topologies in multiferroic materials offers a rich landscape for materials science research. As the spatial scale of these entities is often subatomic in nature, aberration-corrected transmission electron microscopy (TEM) is the ideal characterization technique. Software to quantify and visualize the slight shifts in atomic placement within unit cells is of paramount importance due to the now routine acquisition of images at such resolution. In the previous ~decade since the commercialization of aberration-corrected TEM, many research groups have written their own code to visualize these polar entities. More recently, open-access Python packages have been developed for the purpose of TEM atomic position quantification. Building on these packages, we introduce the TEMUL Toolkit: a Python package for analysis and visualization of atomic resolution images. Here, we focus specifically on the TopoTEM module of the toolkit where we show an easy to follow, streamlined version of calculating the atomic displacements relative to the surrounding lattice and thus plotting polarization. We hope this toolkit will benefit the rapidly expanding field of topology-based nano-electronic and quantum materials research, and we invite the electron microscopy community to contribute to this open-access project.",

    "abstract": "Despite impressive success of machine learning algorithms in clinical natural language processing (cNLP), rule-based approaches still have a prominent role. In this paper, we introduce medspaCy, an extensible, open-source cNLP library based on spaCy framework that allows flexible integration of rule-based and machine learning-based algorithms adapted to clinical text. MedspaCy includes a variety of components that meet common cNLP needs such as context analysis and mapping to standard terminologies. By utilizing spaCy's clear and easy-to-use conventions, medspaCy enables development of custom pipelines that integrate easily with other spaCy-based modules. Our toolkit includes several core components and facilitates rapid development of pipelines for clinical text.",

    "abstract": "Single-molecule localization microscopy has become an important part of the super-resolution microscopy toolbox in biomedical research. Software platforms for applying analytical methods to the point-based data structures are needed that offer both routine application and flexible customization of analysis procedures. We present a python library called LOCAN that consists of well-defined data structures and analysis methods for analyzing localization data in a script or computable notebook.\nThe package source code is released open-source under a BSD-3 license at https://github.com/super-resolution/Locan. It can be installed from the Python Package Index at https://pypi.org/project/locan. Documentation is available at https://locan.readthedocs.io.",

    "abstract": "In a recent article, we described the edxia framework, a user-friendly framework to analyse the microstructure of cementitious materials using SEM-EDS hypermaps. The manual approach presented was shown to be efficient to answer the relevant scientific questions. However, it is limited for batch analysis and (semi-)automated treatments. In this article, we show how the framework can be used to customise the analysis to the problem at hand. We first present some possible extensions, and then we provide a simple example of automatic clustering, using the flexible Python scientific libraries which will allow to define more custom workflows in the future.",

    "abstract": "Global Positioning System (GPS) data have become one of the routine data streams collected by wearable devices, cell phones, and social media platforms in this digital age. Such data provide research opportunities in that they may provide contextual information to elucidate where, when, and why individuals engage in and sustain particular behavioral patterns. However, raw GPS data consisting of densely sampled time series of latitude and longitude coordinate pairs do not readily convey meaningful information concerning intra-individual dynamics and inter-individual differences; substantial data processing is required. Raw GPS data need to be integrated into a Geographic Information System (GIS) and analyzed, from which the mobility and activity patterns of individuals can be derived, a process that is unfamiliar to many behavioral scientists. In this tutorial article, we introduced GPS2space, a free and open-source Python library that we developed to facilitate the processing of GPS data, integration with GIS to derive distances from landmarks of interest, as well as extraction of two spatial features: activity space of individuals and shared space between individuals, such as members of the same family. We demonstrated functions available in the library using data from the Colorado Online Twin Study to explore seasonal and age-related changes in individuals' activity space and twin siblings' shared space, as well as gender, zygosity and baseline age-related differences in their initial levels and/or changes over time. We concluded with discussions of other potential usages, caveats, and future developments of GPS2space.",

    "abstract": "This paper presents the process of designing, fabricating, assembling, programming and optimizing a prototype nonlinear mechatronic Ball-Plate System (BPS) as a laboratory platform for engineering education STEM. Due to the nonlinearity and complexity of BPS, the task presents challenges such as: (1) difficulty in controlling the stabilization of a particular position point, known as steady-state error, (2) position resolution, known as specific distance error, and (3) adverse environmental effects-light-shadow error, which is also discussed in this paper. The laboratory prototype BPS for education was designed, manufactured and installed at Karlovac University of Applied Sciences in the Department of Mechanical Engineering, Mechatronics program. The low-cost two-degree BPS uses a USB HD camera for computer vision as a feedback sensor and two DC servo motors as actuators. Due to control problems, an advanced block diagram of the control system is proposed and discussed. An open-source control system based on Python scripts, which allows the use of ready-made functions from the library, allows the color of the ball and the parameters of the PID controller to be changed, indirectly simplifying the control system and performing mathematical calculations directly. The authors will continue their research on this BPS mechatronic platform and control algorithms.",

    "abstract": "The study of neuron morphology requires robust and comprehensive methods to quantify the differences between neurons of different subtypes and animal species. Several software packages have been developed for the analysis of neuron tracing results stored in the standard SWC format. The packages, however, provide relatively simple quantifications and their non-extendable architecture prohibit their use for advanced data analysis and visualization. We developed nGauge, a Python toolkit to support the parsing and analysis of neuron morphology data. As an application programming interface (API), nGauge can be referenced by other popular open-source software to create custom informatics analysis pipelines and advanced visualizations. nGauge defines an extendable data structure that handles volumetric constructions (e.g. soma), in addition to the SWC linear reconstructions, while remaining lightweight. This greatly extends nGauge's data compatibility.",

    "abstract": "Three-dimensional atomic-level models of polymers are the starting points for physics-based simulation studies. A capability to generate reasonable initial structural models is highly desired for this purpose. We have developed a python toolkit, namely, polymer structure predictor (psp), to generate a hierarchy of polymer models, ranging from oligomers to infinite chains to crystals to amorphous models, using a simplified molecular-input line-entry system (SMILES) string of the polymer repeat unit as the primary input. This toolkit allows users to tune several parameters to manage the quality and scale of models and computational cost. The output structures and accompanying force field (GAFF2/OPLS-AA) parameter files can be used for downstream ",

    "abstract": "Populating and depopulating cities have some degree of underutilised land. The duration of vacancy, or length of time a property remains unused, more strongly influences urban decline than the amount of vacant land. Assessment of the duration of vacancy is seldom conducted, due to a lack of linking longitudinal data. This research creates and applies a Python script to track the duration of vacancy in Minneapolis, MN, U.S.A, to create a tool that can be utilised by cities with vacant land inventories. The tool can be used globally to prioritise treatment areas for urban regeneration plans.",

    "abstract": "Automatic differentiation represents a paradigm shift in scientific programming, where evaluating both functions and their derivatives is required for most applications. By removing the need to explicitly derive expressions for gradients, development times can be shortened and calculations can be simplified. For these reasons, automatic differentiation has fueled the rapid growth of a variety of sophisticated machine learning techniques over the past decade, but is now also increasingly showing its value to support ab initio simulations of quantum systems and enhance computational quantum chemistry. Here, we present an open-source differentiable quantum chemistry simulation code and explore applications facilitated by automatic differentiation: (1) calculating molecular perturbation properties, (2) reoptimizing a basis set for hydrocarbons, (3) checking the stability of self-consistent field wave functions, and (4) predicting molecular properties via alchemical perturbations.",

    "abstract": "As synthetic biology becomes increasingly automated and data-driven, tools that help researchers implement FAIR (findable-accessible-interoperable-reusable) data management practices are needed. Crucially, in order to support machine processing and reusability of data, it is important that data artifacts are appropriately annotated with metadata drawn from controlled vocabularies. Unfortunately, adopting standardized annotation practices is difficult for many research groups to adopt, given the set of specialized database science skills usually required to interface with ontologies. In response to this need, Take Your Terms from Ontologies (Tyto) is a lightweight Python tool that supports the use of controlled vocabularies in everyday scripting practice. While Tyto has been developed for synthetic biology applications, its utility may extend to users working in other areas of bioinformatics research as well. Tyto is available as a Python package distribution or available as source at https://github.com/SynBioDex/tyto.",

    "abstract": "The mechanism of psychiatric drugs (stimulant and non-stimulant) is still unclear. Precision medication of psychiatric disorders faces challenges in pharmacogenetics and pharmacodynamics research due to difficulties in recruiting human subjects because of possibility of substance abuse and relatively small sample sizes. ",

    "abstract": "With the advent of next-generation whole-genome sequencing (WGS), the need for good-quality and well-characterised ",

    "abstract": "The wealth of protein structures collected in the Protein Data Bank enabled large-scale studies of their function and evolution. Such studies, however, require the generation of customized datasets combining the structural data with miscellaneous accessory resources providing functional, taxonomic and other annotations. Unfortunately, the functionality of currently available tools for the creation of such datasets is limited and their usage frequently requires laborious surveying of various data sources and resolving inconsistencies between their versions.\nTo address this problem, we developed localpdb, a versatile Python library for the management of protein structures and their annotations. The library features a flexible plugin system enabling seamless unification of the structural data with diverse auxiliary resources, full version control and powerful functionality of creating highly customized datasets. The localpdb can be used in a wide range of bioinformatic tasks, in particular those involving large-scale protein structural analyses and machine learning.\nlocalpdb is freely available at https://github.com/labstructbioinf/localpdb. Documentation along with the usage examples can be accessed at https://labstructbioinf.github.io/localpdb/.",

    "abstract": "The amount of data available from genomic medicine has revolutionized the approach to identify the determinants underlying many rare diseases. The task of confirming a genotype-phenotype causality for a patient affected with a rare genetic disease is often challenging. In this context, the establishment of the Matchmaker Exchange (MME) network has assumed a pivotal role in bridging heterogeneous patient information stored on different medical and research servers. MME has made it possible to solve rare disease cases by \"matching\" the genotypic and phenotypic characteristics of a patient of interest with patient data available at other clinical facilities participating in the network. Here, we present PatientMatcher (https://github.com/Clinical-Genomics/patientMatcher), an open-source Python and MongoDB-based software solution developed by Clinical Genomics facility at the Science for Life Laboratory in Stockholm. PatientMatcher is designed as a standalone MME server, but can easily communicate via REST API with external applications managing genetic analyses and patient data. The MME node is being implemented in clinical routine in collaboration with the Genomic Medicine Center Karolinska at the Karolinska University Hospital. PatientMatcher is written to implement the MME API and provides several customizable settings, including a custom-fit similarity score algorithm and adjustable matching results notifications.",

    "abstract": "We are bioinformatics trainees at the University of Michigan who started a local chapter of Girls Who Code to provide a fun and supportive environment for high school women to learn the power of coding. Our goal was to cover basic coding topics and data science concepts through live coding and hands-on practice. However, we could not find a resource that exactly met our needs. Therefore, over the past three years, we have developed a curriculum and instructional format using Jupyter notebooks to effectively teach introductory Python for data science. This method, inspired by The Carpentries organization, uses bite-sized lessons followed by independent practice time to reinforce coding concepts, and culminates in a data science capstone project using real-world data. We believe our open curriculum is a valuable resource to the wider education community and hope that educators will use and improve our lessons, practice problems, and teaching best practices. Anyone can contribute to our Open Educational Resources on GitHub.",

    "abstract": "Gmxapi provides an integrated, native Python API for both standard and advanced molecular dynamics simulations in GROMACS. The Python interface permits multiple levels of integration with the core GROMACS libraries, and legacy support is provided via an interface that mimics the command-line syntax, so that all GROMACS commands are fully available. Gmxapi has been officially supported since the GROMACS 2019 release and is enabled by default in current versions of the software. Here we describe gmxapi 0.3 and later. Beyond simply wrapping GROMACS library operations, the API permits several advanced operations that are not feasible using the prior command-line interface. First, the API allows custom user plugin code within the molecular dynamics force calculations, so users can execute custom algorithms without modifying the GROMACS source. Second, the Python interface allows tasks to be dynamically defined, so high-level algorithms for molecular dynamics simulation and analysis can be coordinated with loop and conditional operations. Gmxapi makes GROMACS more accessible to custom Python scripting while also providing support for high-level data-flow simulation algorithms that were previously feasible only in external packages.",

    "abstract": "Graphical methods have been widely used for visualization, classification, and interpretation of aqueous geochemical data to obtain a better understanding of surface and subsurface hydrologic systems. This method note presents WQChartPy, an open-source Python package developed to plot a total of 12 diagrams for analysis of aqueous geochemical data. WQChartPy can handle various data formats including Microsoft Excel, comma-separated values (CSV), and general delimited text. The 12 diagrams include eight traditional diagrams (trilinear Piper, Durov, Stiff, Chernoff face, Schoeller, Gibbs, Chadha, and Gaillardet) and four recently proposed diagrams (rectangle Piper, color-coded Piper, contour-filled Piper, and HFE-D) that have not been implemented in existing graphing software. The diagrams generated by WQChartPy can be saved as portable network graphics (PNG), scalable vector graphics (SVG), or portable document format (PDF) files for scientific publications. Jupyter and Google Colab notebooks are available online to illustrate how to use WQChartPy with example datasets. The geochemical diagrams can be generated with several lines of Python codes. Source codes of WQChartPy are publicly available at GitHub (https://github.com/jyangfsu/WQChartPy) and PyPI (https://pypi.org/project/wqchartpy/).",

    "abstract": "Quality assessment of tree-like structures obtained from a neuron reconstruction algorithm is necessary for evaluating the performance of the algorithm. The lack of user-friendly software for calculating common metrics motivated us to develop a Python toolbox called PyNeval, which is the first open-source toolbox designed to evaluate reconstruction results conveniently as far as we know. The toolbox supports popular metrics in two major categories, geometrical metrics and topological metrics, with an easy way to configure custom parameters for each metric. We tested the toolbox on both synthetic data and real data to show its reliability and robustness. As a demonstration of the toolbox in real applications, we used the toolbox to improve the performance of a tracing algorithm successfully by integrating it into an optimization procedure.",

    "abstract": "Causality defined by Granger in 1969 is a widely used concept, particularly in neuroscience and economics. As there is an increasing interest in nonlinear causality research, a Python package with a neural-network-based causality analysis approach was created. It allows performing causality tests using neural networks based on Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), or Multilayer Perceptron (MLP). The aim of this paper is to present the nonlinear method for causality analysis and the created Python package.\nThe created functions with the autoregressive (AR) and Generalized Radial Basis Functions (GRBF) neural network models were tested on simulated signals in two cases: with nonlinear dependency and with absence of causality from Y to X signal. The train-test split (70/30) was used. Errors obtained on the test set were compared using the Wilcoxon signed-rank test to determine the presence of the causality. For the chosen model, the proposed method of study the change of causality over time was presented.\nIn the case when X was a polynomial of Y, nonlinear methods were able to detect the causality, while the AR model did not manage to indicate it. The best results (in terms of the prediction accuracy) were obtained for the MLP for the lag of 150 (MSE equal to 0.011, compared to 0.041 and 0.036 for AR and GRBF, respectively). When there was no causality between the signals, none of the proposed and AR models did indicate false causality, while it was detected by GRBF models in one case. Only the proposed models gave the expected results in each of the tested scenarios.\nThe proposed method appeared to be superior to the compared methods. They were able to detect non-linear causality, make accurate forecasting and not indicate false causality. The created package enables easy usage of neural networks to study the causal relationship between signals. The neural-networks-based approach is a suitable method that allows the detection of a nonlinear causal relationship, which cannot be detected by the classical Granger method. Unlike other similar tools, the package allows for the study of changes in causality over time.",

    "abstract": "The Electromagnetic/Circuit cosimulation method represents a valuable and effective strategy to address those problems where a radiative structure has to interact with external supporting circuitries. This is of particular concern for Magnetic Resonance Imaging (MRI), radiofrequency (RF) coils design, where the supporting circuitry optimisation represents, generally, a crucial aspect. This article presents CoSimPy, an open-source Python circuit simulation library for Electromagnetic/Circuit cosimulations and specifically optimised for MRI, RF coils design.\nCoSimPy is designed following an Object-orientated programming. In addition to the essential methods aimed to performed the Electromagnetic/Circuit cosimulations, many others are implemented both to simplify the standard workflow and to evaluate the RF coils performance. In this article, the theory which underlies the fundamental methods of CoSimPy is shown together with the basic framework of the library.\nIn the paper, the reliability of CoSimPy is successfully tested against a full-wave electromagnetic simulations involving a reference setup. The library is made available under https://github.com/umbertozanovello/CoSimPy together with a detailed documentation providing guidelines and examples. CoSimPy is distributed under the Massachusetts Institute of Technology (MIT) license.\nCoSimPy demonstrated to be an agile tool employable for Electromagnetic/Circuit cosimulations. Its distribution is meant to fulfil the needs of several researchers also avoiding duplication of effort in writing custom implementations. CoSimPy is under constant development and aims to represent a coworking environment where scientists can implement additional methods whose sharing can represent an advantage for the community. Finally, even if CoSimPy is designed with special focus on MRI, it represents an efficient and practical tool potentially employable wherever electronic devices made of radiative and circuitry components are involved.",

    "abstract": "It is important to evaluate the role of captivity as a potential stressor. An understanding of stress responses to capture and transition to captivity may inform the limitations of laboratory studies on wild animals, aid in understanding the consequences of introducing animals into captive environments, and help predict which species may be successful invasives. We investigated physiological effects of captivity by comparing at-capture blood variables in wild Burmese pythons (Python bivittatus) in Florida to pythons recently brought into captivity (1-109\u00a0days). We conducted an acute restraint test by collecting samples at baseline (immediately at handling) and one hour post-restraint across wild field-sampled (n\u00a0=\u00a019) and recently-captive (n\u00a0=\u00a033) pythons to evaluate fluctuations in plasma corticosterone, bacterial killing ability, antibody response, leukogram, and serpentovirus infection. We observed higher baseline plasma corticosterone and monocytes in recently captive compared to wild snakes, which both subsided in snakes held for a longer time in captivity, and a mild decrease in lymphocytes in the middle of the captivity period. Functional immunity and viral infection were not affected by captivity, and pythons maintained restraint-induced responses in corticosterone, heterophil to lymphocyte ratio, and monocyte counts throughout captivity. Prevalence for serpentovirus was 50%, though infection status was related to sampling date rather than captivity, indicating that viral infection may be seasonal. The history of Burmese python as a common captive animal for research and pet trade, as well as its general resilience to effects of capture and short-term captivity, may contribute to its invasion success in Florida.",

    "abstract": "[This corrects the article DOI: 10.1016/j.dib.2021.106986.].",

    "abstract": null,

    "abstract": "BINding ANAlyzer (BINANA) is an algorithm for identifying and characterizing receptor/ligand interactions and other factors that contribute to binding. We recently updated BINANA to make the algorithm more accessible to a broader audience. We have also ported the Python3 codebase to JavaScript, thus enabling BINANA analysis in the web browser. As proof of principle, we created a web-browser application so students and chemical-biology researchers can quickly visualize receptor/ligand complexes and their unique binding interactions.",

    "abstract": "Cell-based models are becoming increasingly popular for applications in developmental biology. However, the impact of numerical choices on the accuracy and efficiency of the simulation of these models is rarely meticulously tested. Without concrete studies to differentiate between solid model conclusions and numerical artifacts, modelers are at risk of being misled by their experiments' results. Most cell-based modeling frameworks offer a feature-rich environment, providing a wide range of biological components, but are less suitable for numerical studies. There is thus a need for software specifically targeted at this use case.\nWe present CBMOS, a Python framework for the simulation of the center-based or cell-centered model. Contrary to other implementations, CBMOS' focus is on facilitating numerical study of center-based models by providing access to multiple ordinary differential equation solvers and force functions through a flexible, user-friendly interface and by enabling rapid testing through graphics processing unit (GPU) acceleration. We show-case its potential by illustrating two common workflows: (1) comparison of the numerical properties of two solvers within a Jupyter notebook and (2) measuring average wall times of both solvers on a high performance computing cluster. More specifically, we confirm that although for moderate accuracy levels the backward Euler method allows for larger time step sizes than the commonly used forward Euler method, its additional computational cost due to being an implicit method prohibits its use for practical test cases.\nCBMOS is a flexible, easy-to-use Python implementation of the center-based model, exposing both basic model assumptions and numerical components to the user. It is available on GitHub and PyPI under an MIT license. CBMOS allows for fast prototyping on a central processing unit for small systems through the use of NumPy. Using CuPy on a GPU, cell populations of up to 10,000 cells can be simulated within a few seconds. As such, it will substantially lower the time investment for any modeler to check the crucial assumption that model conclusions are independent of numerical issues.",

    "abstract": "Intracranial stereoelectroencephalography (SEEG) is broadly used in the presurgical evaluation of intractable epilepsy, due to its high temporal resolution in neural activity recording and high spatial resolution within suspected epileptogenic zones. Neurosurgeons or technicians face the challenge of conducting a workflow of post-processing operations with the multimodal data (e.g., MRI, CT, and EEG) after the implantation surgery, such as brain surface reconstruction, electrode contact localization, and SEEG data analysis. Several software or toolboxes have been developed to take one or more steps in the workflow but without an end-to-end solution. In this study, we introduced BrainQuake, an open-source Python software for the SEEG spatiotemporal analysis, integrating modules and pipelines in surface reconstruction, electrode localization, seizure onset zone (SOZ) prediction based on ictal and interictal SEEG analysis, and final visualizations, each of which is highly automated with a user-friendly graphical user interface (GUI). BrainQuake also supports remote communications with a public server, which is facilitated with automated and standardized preprocessing pipelines, high-performance computing power, and data curation management to provide a time-saving and compatible platform for neurosurgeons and researchers.",

    "abstract": "The surge in fabrication techniques for micro- and nanodevices gave room to rapid growth in these technologies and a never-ending range of possible applications emerged. These new products significantly improve human life, however, the evolution in the design, simulation and optimization process of said products did not observe a similarly rapid growth. It became thus clear that the performance of micro- and nanodevices would benefit from significant improvements in this area. This work presents a novel methodology for electro-mechanical co-optimization of micro-electromechanical systems (MEMS) inertial sensors. The developed software tool comprises geometry design, finite element method (FEM) analysis, damping calculation, electronic domain simulation, and a genetic algorithm (GA) optimization process. It allows for a facilitated system-level MEMS design flow, in which electrical and mechanical domains communicate with each other to achieve an optimized system performance. To demonstrate the efficacy of the methodology, an open-loop capacitive MEMS accelerometer and an open-loop Coriolis vibratory MEMS gyroscope were simulated and optimized-these devices saw a sensitivity improvement of 193.77% and 420.9%, respectively, in comparison to their original state.",

    "abstract": "Laboratory behavioural tasks are an essential research tool. As questions asked of behaviour and brain activity become more sophisticated, the ability to specify and run richly structured tasks becomes more important. An increasing focus on reproducibility also necessitates accurate communication of task logic to other researchers. To these ends, we developed pyControl, a system of open-source hardware and software for controlling behavioural experiments comprising a simple yet flexible Python-based syntax for specifying tasks as extended state machines, hardware modules for building behavioural setups, and a graphical user interface designed for efficiently running high-throughput experiments on many setups in parallel, all with extensive online documentation. These tools make it quicker, easier, and cheaper to implement rich behavioural tasks at scale. As important, pyControl facilitates communication and reproducibility of behavioural experiments through a highly readable task definition syntax and self-documenting features. Here, we outline the system's design and rationale, present validation experiments characterising system performance, and demonstrate example applications in freely moving and head-fixed mouse behaviour.",

    "abstract": "Novel materials are being enabled by advances in synthesis techniques that achieve ever better control over the atomic-scale structure of materials. The pace of materials development has been further increased by high-throughput computational experiments guided by informatics and machine learning. We have previously demonstrated complementary approaches using mathematical optimization models to search through highly combinatorial design spaces of atomic arrangements, guiding the design of nanostructured materials. In this paper, we highlight the common features of materials optimization problems that can be efficiently modeled via mixed-integer linear optimization models. To take advantage of these commonalities, we have created ",

    "abstract": "Lipids play important modulatory and structural roles for membrane proteins. Molecular dynamics simulations are frequently used to provide insights into the nature of these protein-lipid interactions. Systematic comparative analysis requires tools that provide algorithms for objective assessment of such interactions. We introduce PyLipID, a Python package for the identification and characterization of specific lipid interactions and binding sites on membrane proteins from molecular dynamics simulations. PyLipID uses a community analysis approach for binding site detection, calculating lipid residence times for both the individual protein residues and the detected binding sites. To assist structural analysis, PyLipID produces representative bound lipid poses from simulation data, using a density-based scoring function. To estimate residue contacts robustly, PyLipID uses a dual-cutoff scheme to differentiate between lipid conformational rearrangements while bound from full dissociation events. In addition to the characterization of protein-lipid interactions, PyLipID is applicable to analysis of the interactions of membrane proteins with other ligands. By combining automated analysis, efficient algorithms, and open-source distribution, PyLipID facilitates the systematic analysis of lipid interactions from large simulation data sets of multiple species of membrane proteins.",

    "abstract": "Cyanotoxins called microcystins (MCs) are highly toxic and can be present in drinking water sources. Determining the structure of MCs is paramount because of its effect on toxicity. Though over 300 MC congeners have been discovered, many remain unidentified. Herein, a method is described for the putative identification of MCs using liquid chromatography (LC) coupled with high-resolution (HR) Orbitrap mass spectrometry (MS) and a new bottom-up sequencing strategy. Maumee River water samples were collected during a harmful algal bloom and analyzed by LC-MS with simultaneous HRMS and MS/MS. Unidentified ions with characteristic MC fragments (135 and 213 ",

    "abstract": "Molecular dynamics simulations play an increasingly important role in the rational design of (nano)-materials and in the study of biomacromolecules. However, generating input files and realistic starting coordinates for these simulations is a major bottleneck, especially for high throughput protocols and for complex multi-component systems. To eliminate this bottleneck, we present the polyply software suite that provides 1) a multi-scale graph matching algorithm designed to generate parameters quickly and for arbitrarily complex polymeric topologies, and 2) a generic multi-scale random walk protocol capable of setting up complex systems efficiently and independent of the target force-field or model resolution. We benchmark quality and performance of the approach by creating realistic coordinates for polymer melt simulations, single-stranded as well as circular single-stranded DNA. We further demonstrate the power of our approach by setting up a microphase-separated block copolymer system, and by generating a liquid-liquid phase separated system inside a lipid vesicle.",

    "abstract": "State-of-the-art tomographic scanning techniques provide detailed pore-space geometries of natural porous media, which are central for the study of subsurface flow and transport. Due to experimental and computational limitations, the extraction of high-resolution images is limited to relatively small sample volumes. To reduce the amount of data and the physical complexity, pore-space geometries are routinely translated into pore network models. Subsequently, such networks are expanded in space with suitable computational methods to determine effective medium parameters at larger scales relevant in engineering applications. While existing methods can provide networks with effective flow parameters being consistent with experimental data for comparably homogeneous media such as bead packs and sandstones, these methods are inadequate for more complex heterogeneous rocks such as carbonates or become too expensive for large networks. The netflow Python library accompanying this paper extends existing methods by preserving pore clusters that are a key characteristic of heterogeneous rocks. To this end dendrograms are extracted from experimental data and perturbed when generating larger networks. Moreover, the methods included in the netflow library are implemented in computationally efficient ways and enable the generation of large periodic networks that virtually eliminate boundary effects, which interfere in existing methods. \u2022 The netflow Python library enables the generation of large irregular networks, as it preserves pore or node clusters which are present in certain natural rock types. \u2022 The netflow Python library allows for the generation and flow analysis of boundary-free periodic networks. It further includes methods to convert periodic networks into conventional cubical ones.",

    "abstract": "Reproducibility is at the heart of science. Nevertheless, with the advent of computer-based data processing and analysis, most spectroscopists have a hard time fully reproducing a figure from last year's publication starting from the raw data. Unfortunately, this renders their work eventually unscientific. To change this, we need to develop analysis tools that relieve their users from having to trace each processing and analysis step. Furthermore, these tools need to be modular, extendible, and easy to use in order to get used. To this end, we present here the open-source Python package cwepr based on the ASpecD framework for reproducible analysis of spectroscopic data. This package follows best practices of both, science and software development. Key features include an automatically generated gap-less record of each individual processing and analysis step from the raw data to the final published figure. Additionally, it provides a powerful user interface requiring no programming skills of the user. Due to its code quality, modularity, and extensive documentation, it can be easily extended and is actively developed by spectroscopists working in the field. We expect this approach to have a high impact in the field and to help fighting the looming reproducibility crisis in spectroscopy.",

    "abstract": null,

    "abstract": "Single-molecule force spectroscopy (SMFS) instruments (e.g., magnetic and optical tweezers) often use video tracking to measure the three-dimensional position of micron-scale beads under an applied force. The force in these experiments is calibrated by comparing the bead trajectory to a thermal motion-based model with the drag coefficient, \u03b3, and trap spring constant, \u03ba, as parameters. Estimating accurate parameters is complicated by systematic biases from spectral distortions, the camera exposure time, parasitic noise, and least-squares fitting methods. However, while robust calibration methods exist that correct for these biases, they are not always used because they can be complex to implement computationally. To address this barrier, we present Tweezepy: a Python package for calibrating forces in SMFS video-tracking experiments. Tweezepy uses maximum likelihood estimation (MLE) to estimate parameters and their uncertainties from a single bead trajectory via the power spectral density (PSD) and Allan variance (AV). It is well-documented, fast, easy to use, and accounts for most common sources of biases in SMFS video-tracking experiments. Here, we provide a comprehensive overview of Tweezepy's calibration scheme, including a review of the theory underlying thermal motion-based parameter estimates, a discussion of the PSD, AV, and MLE, and an explanation of their implementation.",

    "abstract": "The rapid development of computers and technology affects modern daily life. Individuals in the digital age need to develop computational thinking (CT) skills. Existing studies have shown that programming teaching is conducive to cultivating students' CT, and various learning models have different effects on the cultivation of CT. This study proposed a problem-oriented learning (POL) model that is closely related to programming and computational thinking. In all, 60 eighth-grade students from a middle school in China were divided into an experimental group (EG) which adopted the POL model, and a control group (CG) which adopted the lecture-and-practice (LAP) learning model. The results showed that the students who were instructed using the POL model performed better than those who were instructed using the LAP model on CT concepts, CT practices, and CT perspectives. Significant differences were found for CT concepts and CT perspectives, but not for CT practices. Findings have implications for teachers who wish to apply new learning models to facilitate students' CT skills, and the study provides a reference case for CT training and Python programming teaching.",

    "abstract": "Fiber photometry (FP) is an adaptable method for recording in vivo neural activity in freely behaving animals. It has become a popular tool in neuroscience due to its ease of use, low cost, the ability to combine FP with freely moving behavior, among other advantages. However, analysis of FP data can be challenging for new users, especially those with a limited programming background. Here, we present Guided Photometry Analysis in Python (GuPPy), a free and open-source FP analysis tool. GuPPy is designed to operate across computing platforms and can accept data from a variety of FP data acquisition systems. The program presents users with a set of graphic user interfaces (GUIs) to load data and provide input parameters. Graphs are produced that can be easily exported for integration into scientific figures. As an open-source tool, GuPPy can be modified by users with knowledge of Python to fit their specific needs.",

    "abstract": "Biomolecular interactions that modulate biological processes occur mainly in cavities throughout the surface of biomolecular structures. In the data science era, structural biology has benefited from the increasing availability of biostructural data due to advances in structural determination and computational methods. In this scenario, data-intensive cavity analysis demands efficient scripting routines built on easily manipulated data structures. To fulfill this need, we developed pyKVFinder, a Python package to detect and characterize cavities in biomolecular structures for data science and automated pipelines.\npyKVFinder efficiently detects cavities in biomolecular structures and computes their volume, area, depth and hydropathy, storing these cavity properties in NumPy arrays. Benefited from Python ecosystem interoperability and data structures, pyKVFinder can be integrated with third-party scientific packages and libraries for mathematical calculations, machine learning and 3D visualization in automated workflows. As proof of pyKVFinder's capabilities, we successfully identified and compared ADRP substrate-binding site of SARS-CoV-2 and a set of homologous proteins with pyKVFinder, showing its integrability with data science packages such as matplotlib, NGL Viewer, SciPy and Jupyter notebook.\nWe introduce an efficient, highly versatile and easily integrable software for detecting and characterizing biomolecular cavities in data science applications and automated protocols. pyKVFinder facilitates biostructural data analysis with scripting routines in the Python ecosystem and can be building blocks for data science and drug design applications.",

    "abstract": "Progressive disorders are highly heterogeneous. Symptom-based clinical classification of these disorders may not reflect the underlying pathobiology. Data-driven subtyping and staging of patients has the potential to disentangle the complex spatiotemporal patterns of disease progression. Tools that enable this are in high demand from clinical and treatment-development communities. Here we describe the pySuStaIn software package, a Python-based implementation of the Subtype and Stage Inference (SuStaIn) algorithm. SuStaIn unravels the complexity of heterogeneous diseases by inferring multiple disease progression patterns (",

    "abstract": "The chloride mass balance (CMB) method is widely used to estimate long-term rates of groundwater recharge. In regions where surface water runoff is negligible, recharge can be estimated using measurements of chloride concentrations of groundwater and precipitation, and an estimate of long-term average rainfall. This paper presents the Chloride Mass Balance Estimator of Australian Recharge (CMBEAR), a Jupyter (Python) Notebook that is set up to rapidly apply the CMB method using gridded maps of chloride deposition rates across the Australian continent. For an Australian context, the chloride deposition rate and rainfall maps have been provided. Thus, CMBEAR requires only a spreadsheet with the groundwater chloride concentration, the latitude and longitude of the sample location, and some simple user inputs. CMBEAR may be easily applied in other regions, providing that a gridded chloride deposition map is available. Recharge estimates from CMBEAR are compared against published applications of the CMB method. CMBEAR is also applied to a large dataset from the Northern Territory and is used to produce a gridded map of recharge for western Victoria. CMBEAR provides a reproducible and straightforward approach to apply the CMB method to estimate groundwater recharge.",

    "abstract": "An open-source Python library EMDA for cryo-EM map and model manipulation is presented with a specific focus on validation. The use of several functionalities in the library is presented through several examples. The utility of local correlation as a metric for identifying map-model differences and unmodeled regions in maps, and how it is used as a metric of map-model validation is demonstrated. The mapping of local correlation to individual atoms, and its use to draw insights on local signal variations are discussed. EMDA's likelihood-based map overlay is demonstrated by carrying out a superposition of two domains in two related structures. The overlay is carried out first to bring both maps into the same coordinate frame and then to estimate the relative movement of domains. Finally, the map magnification refinement in EMDA is presented with an example to highlight the importance of adjusting the map magnification in structural comparison studies.",

    "abstract": "Ectothermic vertebrates experience daily changes in body temperature, and anecdotal observations suggest these changes affect ventricular repolarization such that the T-wave in the ECG changes polarity. Mammals, in contrast, can maintain stable body temperatures, and their ventricular repolarization is strongly modulated by changes in heart rate and by sympathetic nervous system activity. The aim of this study was to assess the role of body temperature, heart rate, and circulating catecholamines on local repolarization gradients in the ectothermic ball python (Python regius). We recorded body-surface electrocardiograms and performed open-chest high-resolution epicardial mapping while increasing body temperature in five pythons, in all of which there was a change in T-wave polarity. However, the vector of repolarization differed between individuals, and only a subset of leads revealed T-wave polarity change. RNA sequencing revealed regional differences related to adrenergic signaling. In one denervated and Ringer's solution-perfused heart, heating and elevated heart rates did not induce change in T-wave polarity, whereas noradrenaline did. Accordingly, electrocardiograms in eight awake pythons receiving intra-arterial infusion of the \u03b2-adrenergic receptor agonists adrenaline and isoproterenol revealed T-wave inversion in most individuals. Conversely, blocking the \u03b2-adrenergic receptors using propranolol prevented T-wave change during heating. Our findings indicate that changes in ventricular repolarization in ball pythons are caused by increased tone of the sympathetic nervous system, not by changes in temperature. Therefore, ventricular repolarization in both pythons and mammals is modulated by evolutionary conserved mechanisms involving catecholaminergic stimulation.",

    "abstract": "RNA 3D architectures are stabilized by sophisticated networks of (non-canonical) base pair interactions, which can be conveniently encoded as multi-relational graphs and efficiently exploited by graph theoretical approaches and recent progresses in machine learning techniques. RNAglib is a library that eases the use of this representation, by providing clean data, methods to load it in machine learning pipelines and graph-based deep learning models suited for this representation. RNAglib also offers other utilities to model RNA with 2.5 D graphs, such as drawing tools, comparison functions or baseline performances on RNA applications.\nThe method is distributed as a pip package, RNAglib. Data are available in a repository and can be accessed on rnaglib's web page. The source code, data and documentation are available at https://rnaglib.cs.mcgill.ca.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Constraint-based reconstruction and analysis (COBRA) is a widely used modeling framework for analyzing and designing metabolic networks. Here, we present CNApy, an open-source cross-platform desktop application written in Python, which offers a state-of-the-art graphical front-end for the intuitive analysis of metabolic networks with COBRA methods. While the basic look-and-feel of CNApy is similar to the user interface of the MATLAB toolbox CellNetAnalyzer, it provides various enhanced features by using components of the powerful Qt library. CNApy supports a number of standard and advanced COBRA techniques and further functionalities can be easily embedded in its GUI facilitating modular extension in the future.\nCNApy can be installed via conda and its source code is freely available at https://github.com/cnapy-org/CNApy under the Apache 2 license.",

    "abstract": "Dispersal behavior is a critical component of invasive species dynamics, impacting both spatial spread and population density. In South Florida, Burmese pythons (Python bivittatus) are an invasive species that disrupt ecosystems and have the potential to expand their range northward. Control of python populations is limited by a lack of information on movement behavior and vital rates, especially within the younger age classes. We radio-tracked 28 Burmese pythons from hatching until natural mortality for approximately 3 years. Pythons were chosen from 4 clutches deposited by adult females in 4 different habitats: forested wetland, urban interface, upland pine, and agricultural interface.\nKnown-fate survival estimate was 35.7% (95% CI = 18% - 53%) in the first 6 months, and only 2 snakes survived 3 years post hatching. Snakes moving through 'natural' habitats had higher survival than snakes dispersing through 'modified' habitats in the first 6- months post-hatching. Predation was the most common source of mortality. Snakes from the agricultural interface utilized canals and displayed the largest net movements.\nOur results suggest that pythons may have lower survival if clutches are deposited in or near urbanized areas. Alternatively, juvenile pythons could quickly disperse to new locations by utilizing canals that facilitate linear movement. This study provides critical information about behavioral and life history characteristics of juvenile Burmese pythons that will inform management practices.",

    "abstract": "pystablemotifs is a Python 3 library for analyzing Boolean networks. Its non-heuristic and exhaustive attractor identification algorithm was previously presented in Rozum et al. (2021). Here, we illustrate its performance improvements over similar methods and discuss how it uses outputs of the attractor identification process to drive a system to one of its attractors from any initial state. We implement six attractor control algorithms, five of which are new in this work. By design, these algorithms can return different control strategies, allowing for synergistic use. We also give a brief overview of the other tools implemented in pystablemotifs.\nThe source code is on GitHub at https://github.com/jcrozum/pystablemotifs/.\nSupplementary data are available at Bioinformatics online.",

    "abstract": null,

    "abstract": "Functional enrichment analysis is an analytical method to extract biological insights from gene expression data, popularized by the ever-growing application of high-throughput techniques. Typically, expression profiles are generated for hundreds to thousands of genes/proteins from samples belonging to two experimental groups, and after ad-hoc statistical tests, researchers are left with lists of statistically significant entities, possibly lacking any unifying biological theme. Functional enrichment tackles the problem of putting overall gene expression changes into a broader biological context, based on pre-existing knowledge bases of reference: database collections of known expression regulation, relationships and molecular interactions. STRING is among the most popular tools, providing both protein-protein interaction networks and functional enrichment analysis for any given set of identifiers. For complex experimental designs, manually retrieving, interpreting, analyzing and abridging functional enrichment results is a daunting task, usually performed by hand by the average wet-biology researcher. We have developed reString, a cross-platform software that seamlessly retrieves from STRING functional enrichments from multiple user-supplied gene sets, with just a few clicks, without any need for specific bioinformatics skills. Further, it aggregates all findings into human-readable table summaries, with built-in features to easily produce user-customizable publication-grade clustermaps and bubble plots. Herein, we outline a complete reString protocol, showcasing its features on a real use-case.",

    "abstract": "In this chapter, we describe the process of obtaining medical imaging data and its storage protocol. The authors also explain in a step-by-step approach how to extract and prepare the medical imaging data for machine learning algorithms. And finally, the process of building and assessing a convolutional neural network for medical imaging data is illustrated.",

    "abstract": "As part of a larger project to understand the publishing choices of UVA Health authors and support open access publishing, a team from the Claude Moore Health Sciences Library analyzed an open data set from Europe PMC, which includes metadata from PubMed records. We used the Europe PMC REST API to search for articles published in 2017-2020 with \"University of Virginia\" in the author affiliation field. Subsequently, we parsed the JSON metadata in Python and used Streamlit to create a data visualization from our public GitHub repository. At present, this shows the relative proportions of open access versus subscription-only articles published by UVA Health authors. Although subscription services like Web of Science, Scopus, and Dimensions allow users to do similar analyses, we believe this is a novel approach to doing this type of bibliometric research with open data and open source tools.",

    "abstract": "This work introduces CGRdb2.0\u2500an open-source database management system for molecules, reactions, and chemical data. CGRdb2.0 is a Python package connecting to a PostgreSQL database that enables native searches for molecules and reactions without complicated SQL syntax. The library provides out-of-the-box implementations for similarity and substructure searches for molecules, as well as similarity and substructure searches for reactions in two ways\u2500based on reaction components and based on the Condensed Graph of Reaction approach, the latter significantly accelerating the performance. In benchmarking studies with the RDKit database cartridge, we demonstrate that CGRdb2.0 performs searches faster for smaller data sets, while allowing for interactive access to the retrieved data.",

    "abstract": "Visual illusions are fascinating phenomena that have been used and studied by artists and scientists for centuries, leading to important discoveries about the neurocognitive underpinnings of perception, consciousness, and neuropsychiatric disorders such as schizophrenia or autism. Surprisingly, despite their historical and theoretical importance as psychological stimuli, there is no dedicated software, nor consistent approach, to generate illusions in a systematic fashion. Instead, scientists have to craft them by hand in an idiosyncratic fashion, or use pre-made images not tailored for the specific needs of their studies. This, in turn, hinders the reproducibility of illusion-based research, narrowing possibilities for scientific breakthroughs and their applications. With the aim of addressing this gap, ",

    "abstract": "There is a growing interest in using unmanned aerial vehicles (UAVs) in the most diverse application areas from agriculture to remote sensing, that determine the need to project and define mission profiles of the UAVs. In addition, solar photovoltaic energy increases the flight autonomy of this type of aircraft, forming the term Solar UAV. This study proposes an extended methodology for sizing Solar UAVs that take off from a runway. This methodology considers mission parameters such as operating location, altitude, flight speed, flight endurance, and payload to sizing the aircraft parameters, such as wingspan, area of embedded solar cells panels, runway length required for takeoff and landing, battery weight, and the total weight of the aircraft. Using the Python language, we developed a framework to apply the proposed methodology and assist in designing a Solar UAV. With this framework, it was possible to perform a sensitivity analysis of design parameters and constraints. Finally, we performed a simulation of a mission, checking the output parameters.",

    "abstract": "The utilization of various feedstocks of unique characteristics in producing biogas could potentially enhance the application of clean fuel from biomass wastes. Two modelling tools were used to explore biogas production from plant and animal wastes. In this study, corn chaff was inoculated with cow dung digestate using different mixing ratios of substrate/inoculum (S/I) of 1:1, 1:1.55, and 1:3.5 for hydraulic retention time (HRT) of 25, 31, and 37 days as modelled using Central Composite Design (Face Centered Design) to optimize the process and predict the optimal response. The result shows that the mixture ratio of 1:1.55 for 37 days gave a cumulative highest biogas yield of 6.19 L under mesophilic conditions. The model p-value is <0.0001, an indication that the model term is significant. The python coding of the input factors gave the optimal value of 4.71 L, which is similar to the result obtained via CCD. Thus, both CCD (Face Centered Design) and python coding are reliable in the optimization of biogas production as they both predicted the same optimal values and approximately the same highest cumulative biogas yield. The GC-MS characterization of produced biogas revealed that it contains 68% methane and 22.76% CO",

    "abstract": "Tomography is a powerful tool for reconstructing the interior of an object from a series of projection images. Typically, the source and detector traverse a standard path (e.g., circular, helical). Recently, various techniques have emerged that use more complex acquisition geometries. Current software packages require significant handwork, or lack the flexibility to handle such geometries. Therefore, software is needed that can concisely represent, visualize, and compute reconstructions of complex acquisition geometries. We present tomosipo, a Python package that provides these capabilities in a concise and intuitive way. Case studies demonstrate the power and flexibility of tomosipo.",

    "abstract": "An important challenge for primary or secondary analysis of cytometry data is how to facilitate productive collaboration between domain and quantitative experts. Domain experts in cytometry laboratories and core facilities increasingly recognize the need for automated workflows in the face of increasing data complexity, but by and large, still conduct all analysis using traditional applications, predominantly FlowJo. To a large extent, this cuts domain experts off from the rapidly growing library of Single Cell Data Science algorithms available, curtailing the potential contributions of these experts to the validation and interpretation of results. To address this challenge, we developed FlowKit, a Gating-ML 2.0-compliant Python package that can read and write FCS files and FlowJo workspaces. We present examples of the use of FlowKit for constructing reporting and analysis workflows, including round-tripping results to and from FlowJo for joint analysis by both domain and quantitative experts.",

    "abstract": "Fungi are prolific producers of secondary metabolites (SMs), which are bioactive small molecules with important applications in medicine, agriculture and other industries. The backbones of a large proportion of fungal SMs are generated through the action of large, multi-domain megasynth(et)ases such as polyketide synthases (PKSs) and nonribosomal peptide synthetases (NRPSs). The structure of these backbones is determined by the domain architecture of the corresponding megasynth(et)ase, and thus accurate annotation and classification of these architectures is an important step in linking SMs to their biosynthetic origins in the genome.\nHere we report synthaser, a Python package leveraging the NCBI's conserved domain search tool for remote prediction and classification of fungal megasynth(et)ase domain architectures. Synthaser is capable of batch sequence analysis, and produces rich textual output and interactive visualisations which allow for quick assessment of the megasynth(et)ase diversity of a fungal genome. Synthaser uses a hierarchical rule-based classification system, which can be extensively customised by the user through a web application ( http://gamcil.github.io/synthaser ). We show that synthaser provides more accurate domain architecture predictions than comparable tools which rely on curated profile hidden Markov model (pHMM)-based approaches; the utilisation of the NCBI conserved domain database also allows for significantly greater flexibility compared to pHMM approaches. In addition, we demonstrate how synthaser can be applied to large scale genome mining pipelines through the construction of an Aspergillus PKS similarity network.\nSynthaser is an easy to use tool that represents a significant upgrade to previous domain architecture analysis tools. It is freely available under a MIT license from PyPI ( https://pypi.org/project/synthaser ) and GitHub ( https://github.com/gamcil/synthaser ).",

    "abstract": "Elementary flux mode (EFM) analysis is a well-established, yet computationally challenging approach to characterize metabolic networks. Standard algorithms require huge amounts of memory and lack scalability which limits their application to single servers and consequently limits a comprehensive analysis to medium-scale networks. Recently, Avis et al. developed mplrs-a parallel version of the lexicographic reverse search (lrs) algorithm, which, in principle, enables an EFM analysis on high-performance computing environments (Avis and Jordan. mplrs: a scalable parallel vertex/facet enumeration code. arXiv:1511.06487 , 2017). Here we test its applicability for EFM enumeration.\nWe developed EFMlrs, a Python package that gives users access to the enumeration capabilities of mplrs. EFMlrs uses COBRApy to process metabolic models from sbml files, performs loss-free compressions of the stoichiometric matrix, and generates suitable inputs for mplrs as well as efmtool, providing support not only for our proposed new method for EFM enumeration but also for already established tools. By leveraging COBRApy, EFMlrs also allows the application of additional reaction boundaries and seamlessly integrates into existing workflows.\nWe show that due to mplrs's properties, the algorithm is perfectly suited for high-performance computing (HPC) and thus offers new possibilities for the unbiased analysis of substantially larger metabolic models via EFM analyses. EFMlrs is an open-source program that comes together with a designated workflow and can be easily installed via pip.",

    "abstract": "Fast and accurate modelling of flood inundation has gained increasing attention in recent years. One approach gaining popularity recently is the development of emulation models using data driven methods, such as artificial neural networks. These emulation models are often developed to model flood depth for each grid cell in the modelling domain in order to maintain accurate spatial representation of the flood inundation surface. This leads to redundancy in modelling, as well as difficulties in achieving good model performance across floodplains where there are limited data available. In this paper, a spatial reduction and reconstruction (SRR) method is developed to (1) identify representative locations within the model domain where water levels can be used to represent flood inundation surface using deep learning models; and (2) reconstruct the flood inundation surface based on water levels simulated at these representative locations. The SRR method is part of the SRR-Deep-Learning framework for flood inundation modelling and therefore, it needs to be used together with data driven models. The SRR method is programmed using the Python programming language and is freely available from https://github.com/yuerongz/SRR-method.\u2022The SRR method identifies locations which are representative of flood inundation behavior in surrounding areas.\u2022The representative locations selected following the SRR method have sufficient flood data for developing emulation models.\u2022Flood inundation surfaces can be reconstructed using the SRR method with a detection rate of above 99%.",

    "abstract": "The blind spot is a region in the temporal monocular visual field in humans, which corresponds to a physiological scotoma within the nasal hemi-retina. This region has no photoreceptors, so is insensitive to visual stimulation. There is no corresponding perceptual scotoma because the visual stimulation is \"filled-in\" by the visual system. Investigations of visual perception in and around the blind spot allow us to investigate this filling-in process. However, because the location and size of the blind spot are individually variable, experimenters must first map the blind spot in every observer. We present an open-source tool, which runs in Psychopy software, to estimate the location and size of the blind spot psychophysically. The tool will ideally be used with an Eyelink eye-tracker (SR Research), but it can also run in standalone mode. Here, we explain the rationale for the tool and demonstrate its validity in normally-sighted observers. We develop a detailed map of the blind spot in one observer. Then, in a group of 12 observers, we propose a more efficient, pragmatic method to define a \"safe zone\" within the blind spot, for which the experimenter can be fully confident that visual stimuli will not be seen. Links are provided to this open-source tool and a user manual.",

    "abstract": "Genes undergo distinct selective sweeps, and also interact and coevolve, forming the bases of complex phenotypic traits. Therefore, the identification of genes that coevolve or are under artificial selective sweeps is of great importance. However, previous computational methods have been designed for either populations of closely related breeds or individuals of distinct species. Approaches intended specifically for closely related individuals without replicate (i.e. each breed/strain is represented by only one individual) are long overdue. We present a free, powerful, open source package, pyRSD-CoEv, that allows the identification of genes undergoing coevolution and/or selection-based sweeps. pyRSD-CoEv includes two main analysis workflows for genomic variant data: (i) the identification of selective sweeps using relative homozygous single nucleotide variant density (RSD); and (ii) the identification of coevolutionary gene clusters based on correlated evolutionary rates. The python package pyRSD-CoEv is written using python 3.7 and is freely available from the github website at https://github.com/QianZiTang/pyRSD-CoEv. It runs on Linux.",

    "abstract": "Quantifying changes in DNA and RNA levels is essential in numerous molecular biology protocols. Quantitative real time PCR (qPCR) techniques have evolved to become commonplace, however, data analysis includes many time-consuming and cumbersome steps, which can lead to mistakes and misinterpretation of data. To address these bottlenecks, we have developed an open-source Python software to automate processing of result spreadsheets from qPCR machines, employing calculations usually performed manually. Auto-qPCR is a tool that saves time when computing qPCR data, helping to ensure reproducibility of qPCR experiment analyses. Our web-based app ( https://auto-q-pcr.com/ ) is easy to use and does not require programming knowledge or software installation. Using Auto-qPCR, we provide examples of data treatment, display and statistical analyses for four different data processing modes within one program: (1) DNA quantification to identify genomic deletion or duplication events; (2) assessment of gene expression levels using an absolute model, and relative quantification (3) with or (4) without a reference sample. Our open access Auto-qPCR software saves the time of manual data analysis and provides a more systematic workflow, minimizing the risk of errors. Our program constitutes a new tool that can be incorporated into bioinformatic and molecular biology pipelines in clinical and research labs.",

    "abstract": "The commonly used principle for measuring the depth of anesthesia involves changes in the frequency components of the electroencephalogram (EEG) under general anesthesia. Therefore, it is essential to construct an effective spectrum and spectrogram to analyze the relationship between the depth of anesthesia and the EEG frequency during general anesthesia. This paper reviews the computer programming techniques for analyzing the spectrum and spectrogram derived from a single-channel EEG recorded during general anesthesia. A periodogram is obtained by repeating a Fourier transform on EEG segments separated by short time intervals, but spectral leakage (i.e., dissociation from the true spectrum) occurs as a consequence of unnatural segmentation and noise. While offsetting the securing of the dynamic range, practical analyses of the adaptation of the window function are explained. Finally, the multitaper method, which can suppress artifacts caused by the edges of the analysis segments, suppress noise, and probabilistically infer values that are close to the real power spectral density, is explained using practical examples of the analysis. All analyses were performed and all graphs plotted using Python under Jupyter Notebook. The analyses demonstrated the effectiveness of Python-based programming under the integrated development environment Jupyter Notebook for constructing an effective spectrum and spectrogram for analyzing the relationship between the depth of anesthesia and EEG frequency analysis in general anesthesia.",

    "abstract": "The main purpose of many medical studies is to estimate the effects of a treatment or exposure on an outcome. However, it is not always possible to randomize the study participants to a particular treatment, therefore observational study designs may be used. There are major challenges with observational studies; one of which is confounding. Controlling for confounding is commonly performed by direct adjustment of measured confounders; although, sometimes this approach is suboptimal due to modeling assumptions and misspecification. Recent advances in the field of causal inference have dealt with confounding by building on classical standardization methods. However, these recent advances have progressed quickly with a relative paucity of computational-oriented applied tutorials contributing to some confusion in the use of these methods among applied researchers. In this tutorial, we show the computational implementation of different causal inference estimators from a historical perspective where new estimators were developed to overcome the limitations of the previous estimators (ie, nonparametric and parametric g-formula, inverse probability weighting, double-robust, and data-adaptive estimators). We illustrate the implementation of different methods using an empirical example from the Connors study based on intensive care medicine, and most importantly, we provide reproducible and commented code in Stata, R, and Python for researchers to adapt in their own observational study. The code can be accessed at https://github.com/migariane/Tutorial_Computational_Causal_Inference_Estimators.",

    "abstract": "The CPqPy framework coupling COMSOL and PHREEQC based on Python was developed. This framework can achieve the simulation of diversified situations including multi-physics coupling and geochemical reactions of soil and groundwater. The multi-physics coupling models are calculated in COMSOL, whereas PHREEQC was applied to calculate the geochemical models through the Phreeqpy library in Python. Feasibility and accuracy of CPqPy were verified and applied to two cases, including a solute transport model considering equilibrium reaction and ion exchange as well as a reactive transport model of a variable saturation soil considering kinetic reaction. The results show a high degree of credibility of CPqPy. The framework has the advantages of strong portability, and it can be further used in conjunction with multiple Python calculation libraries, which greatly extends the application of the reactive transport model.",

    "abstract": "With an increasing number of scientific articles published each year, there is a need to synthesize and obtain insights across ever-growing volumes of literature. Here, we present ",

    "abstract": "New algorithms for determining the expected flow through simple cycles in a closed network are presented. Current network analysis software do not implement algorithms for expected cyclic flow decomposition, despite its potential value. Decomposing networks into expected cycle flows provides a quantitative characterization of network cycles that can be further analyzed for sensitivity and correlative behavior. An efficient, general algorithm has been coded into CycFlowDec, an open source Python module available at https://github.com/austenb28/CycFlowDec.",

    "abstract": "Dealing with uncertainty in applications of machine learning to real-life data critically depends on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the purpose of estimating ID, but no standard package to easily apply them one by one or all at once has been implemented in Python. This technical note introduces scikit-dimension, an open-source Python package for intrinsic dimension estimation. The scikit-dimension package provides a uniform implementation of most of the known ID estimators based on the scikit-learn application programming interface to evaluate the global and local intrinsic dimension, as well as generators of synthetic toy and benchmark datasets widespread in the literature. The package is developed with tools assessing the code quality, coverage, unit testing and continuous integration. We briefly describe the package and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID estimation for real-life and synthetic data.",

    "abstract": "A two-year-old ball python with a submandibular mass was evaluated. Fine needle aspiration resulted in debris containing purulent materials and bacterial cells on cytology. Radiography demonstrated multi-focal radiopaque lesions in the mass, which were suspected to be mineralization; there was an absence of mandibular invasion or lung involvement. Gross examination of the surgically excised mass revealed a multi-nodular, well-circumscribed lesion with purulent material. The postoperative recovery was uneventful. The histopathological examination followed by immunohistochemistry analysis gave a diagnosis of leiomyosarcoma. As tumors containing purulent materials can be confused with an abscess, diagnostic confirmation with various diagnostical tools should be considered.",

    "abstract": "Crystallography uses the diffraction of X-rays, electrons or neutrons by crystals to provide invaluable data on the atomic structure of matter, from single atoms to ribosomes. Much of crystallography's success is due to the software packages developed to enable automated processing of diffraction data. However, the analysis of unconventional diffraction experiments can still pose significant challenges - many existing programs are closed source, sparsely documented, or challenging to integrate with modern libraries for scientific computing and machine learning. Described here is ",

    "abstract": "Over the past 40 years, actigraphy has been used to study rest-activity patterns in circadian rhythm and sleep research. Furthermore, considering its simplicity of use, there is a growing interest in the analysis of large population-based samples, using actigraphy. Here, we introduce pyActigraphy, a comprehensive toolbox for data visualization and analysis including multiple sleep detection algorithms and rest-activity rhythm variables. This open-source python package implements methods to read multiple data formats, quantify various properties of rest-activity rhythms, visualize sleep agendas, automatically detect rest periods and perform more advanced signal processing analyses. The development of this package aims to pave the way towards the establishment of a comprehensive open-source software suite, supported by a community of both developers and researchers, that would provide all the necessary tools for in-depth and large scale actigraphy data analyses.",

    "abstract": null,

    "abstract": "The analysis of energy loss near edge structures in EELS is a powerful method for a precise characterization of elemental oxidation states and local atomic coordination with an outstanding lateral resolution, down to the atomic scale. Given the complexity and sizes of the EELS spectrum images datasets acquired by the state-of-the-art instrumentation, methods with low convergence times are usually preferred for spectral unmixing in quantitative analysis, such as multiple linear least squares fittings. Nevertheless, non-linear least squares fitting may be a superior choice for analysis in some cases, as it eliminates the need of calibrated reference spectra and provides information for each of the individual components included in the fitted model. To avoid some of the problems that the non-linear least squares algorithms may suffer dealing with mixed-composition samples and, thus, a model comprised by a large number of individual curves we proposed the combination of clustering analysis for segmentation and non-linear least squares fitting for spectral analysis. Clustering analysis is capable of a fast classification of pixels in smaller subsets divided by their spectral characteristics, and thus increases the control over the model parameters in separated regions of the samples, classified by their specific compositions. Furthermore, along with this manuscript we provide access to a self-contained and expandable modular software solution called WhatEELS. It was specifically designed to facilitate the combined use of clustering and NLLS, and includes a set of tools for white-lines analysis and elemental quantification. We successfully demonstrated its capabilities with a control sample of mesoporous cerium oxide doped with praseodymium and gadolinium, which posed challenging case-study given its spectral characteristics.",

    "abstract": "Genome-wide association study (GWAS) requires a researcher to perform a multitude of different actions during analysis. From editing and formatting genotype and phenotype information to running the analysis software to summarizing and visualizing the results. A typical GWAS workflow poses a significant challenge of utilizing the command-line, manual text-editing and requiring knowledge of one or more programming/scripting languages, especially for newcomers.\nvcf2gwas is a package that provides a convenient pipeline to perform all of the steps of a traditional GWAS workflow by reducing it to a single command-line input of a Variant Call Format file and a phenotype data file. In addition, all the required software is installed with the package. vcf2gwas also implements several useful features enhancing the reproducibility of GWAS analysis.\nThe source code of vcf2gwas is available under the GNU General Public License. The package can be easily installed using conda. Installation instructions and a manual including tutorials can be accessed on the package website at https://github.com/frankvogt/vcf2gwas.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Over the past decade, deep neural network (DNN) models have received a lot of attention due to their near-human object classification performance and their excellent prediction of signals recorded from biological visual systems. To better understand the function of these networks and relate them to hypotheses about brain activity and behavior, researchers need to extract the activations to images across different DNN layers. The abundance of different DNN variants, however, can often be unwieldy, and the task of extracting DNN activations from different layers may be non-trivial and error-prone for someone without a strong computational background. Thus, researchers in the fields of cognitive science and computational neuroscience would benefit from a library or package that supports a user in the extraction task. THINGSvision is a new Python module that aims at closing this gap by providing a simple and unified tool for extracting layer activations for a wide range of pretrained and randomly-initialized neural network architectures, even for users with little to no programming experience. We demonstrate the general utility of THINGsvision by relating extracted DNN activations to a number of functional MRI and behavioral datasets using representational similarity analysis, which can be performed as an integral part of the toolbox. Together, THINGSvision enables researchers across diverse fields to extract features in a streamlined manner for their custom image dataset, thereby improving the ease of relating DNNs, brain activity, and behavior, and improving the reproducibility of findings in these research fields.",

    "abstract": "Grain characteristics, including kernel length, kernel width, and thousand kernel weight, are critical component traits for grain yield. Manual measurements and counting are expensive, forming the bottleneck for dissecting these traits' genetic architectures toward ultimate yield improvement. High-throughput phenotyping methods have been developed by analyzing images of kernels. However, segmenting kernels from the image background and noise artifacts or from other kernels positioned in close proximity remain as challenges. In this study, we developed a software package, named GridFree, to overcome these challenges. GridFree uses an unsupervised machine learning approach, K-Means, to segment kernels from the background by using principal component analysis on both raw image channels and their color indices. GridFree incorporates users' experiences as a dynamic criterion to set thresholds for a divide-and-combine strategy that effectively segments adjacent kernels. When adjacent multiple kernels are incorrectly segmented as a single object, they form an outlier on the distribution plot of kernel area, length, and width. GridFree uses the dynamic threshold settings for splitting and merging. In addition to counting, GridFree measures kernel length, width, and area with the option of scaling with a reference object. Evaluations against existing software programs demonstrated that GridFree had the smallest error on counting seeds for multiple crop species. GridFree was implemented in Python with a friendly graphical user interface to allow users to easily visualize the outcomes and make decisions, which ultimately eliminates time-consuming and repetitive manual labor. GridFree is freely available at the GridFree website (https://zzlab.net/GridFree).",

    "abstract": "Visualizing two-dimensional embeddings (such as UMAP or tSNE) is a useful step in interrogating single-cell RNA sequencing (scRNA-Seq) data. Subsequently, users typically iterate between programmatic analyses (including clustering and differential expression) and visual exploration (e.g. coloring cells by interesting features) to uncover biological signals in the data. Interactive tools exist to facilitate visual exploration of embeddings such as performing differential expression on user-selected cells. However, the practical utility of these tools is limited because they don't support rapid movement of data and results to and from the programming environments where most of the data analysis takes place, interrupting the iterative process.\nHere, we present the Single-cell Interactive Viewer (Sciviewer), a tool that overcomes this limitation by allowing interactive visual interrogation of embeddings from within Python. Beyond differential expression analysis of user-selected cells, Sciviewer implements a novel method to identify genes varying locally along any user-specified direction on the embedding. Sciviewer enables rapid and flexible iteration between interactive and programmatic modes of scRNA-Seq exploration, illustrating a useful approach for analyzing high-dimensional data.\nCode and examples are provided at https://github.com/colabobio/sciviewer.",

    "abstract": "While quantitative analytical skills have always been a part of modern biomedical training, the big data revolution and digital research environment have increased the importance of computational approaches for biomedical graduate education. To address this growing need, Ph.D. programs have explored ways to integrate quantitative training into their existing curricula. However, these attempts have been hindered by limitations on total instructional time, faculty perceptions, and scalability. Here, we describe a flipped approach that combined a preexisting online course with group problem solving sessions to effectively and efficiently teach biomedical Ph.D. students key concepts in the use of the Python programming language for research. Following the COVID-19 related shutdowns in March 2020, we successfully adapted this approach to an all-online version where the formerly in-person problem-solving sessions occurred in small groups over Zoom. We found that students in both in-person and remote flipped formats showed increased confidence using Python and related this to their thesis research. Following the shift to the fully remote format, the lack of a physically present instructor seemed to increase students' reliance on their classmates, which in turn promoted peer learning and support. This flexible, scalable approach to computational training may address the needs of many biomedical Ph.D. programs.",

    "abstract": "Integrating experimental information across proteomic datasets with the wealth of publicly available sequence annotations is a crucial part in many proteomic studies that currently lacks an automated analysis platform. Here, we present AlphaMap, a Python package that facilitates the visual exploration of peptide-level proteomics data. Identified peptides and post-translational modifications in proteomic datasets are mapped to their corresponding protein sequence and visualized together with prior knowledge from UniProt and with expected proteolytic cleavage sites. The functionality of AlphaMap can be accessed via an intuitive graphical user interface or-more flexibly-as a Python package that allows its integration into common analysis workflows for data visualization. AlphaMap produces publication-quality illustrations and can easily be customized to address a given research question.\nAlphaMap is implemented in Python and released under an Apache license. The source code and one-click installers are freely available at https://github.com/MannLabs/alphamap.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Deep Neural Networks (DNNs) deployment for IoT Edge applications requires strong skills in hardware and software. In this paper, a novel design framework fully automated for Edge applications is proposed to perform such a deployment on System-on-Chips. Based on a high-level Python interface that mimics the leading Deep Learning software frameworks, it offers an easy way to implement a hardware-accelerated DNN on an FPGA. To do this, our design methodology covers the three main phases: (a) customization: where the user specifies the optimizations needed on each DNN layer, (b) generation: the framework generates on the Cloud the necessary binaries for both FPGA and software parts, and (c) deployment: the SoC on the Edge receives the resulting files serving to program the FPGA and related Python libraries for user applications. Among the study cases, an optimized DNN for the MNIST database can speed up more than 60\u00d7 a software version on the ZYNQ 7020 SoC and still consume less than 0.43W. A comparison with the state-of-the-art frameworks demonstrates that our methodology offers the best trade-off between throughput, power consumption, and system cost.",

    "abstract": "Transcription factors (TFs) are the upstream regulators that orchestrate gene expression, and therefore a centrepiece in bioinformatics studies. While a core strategy to understand the biological context of genes and proteins includes annotation enrichment analysis, such as Gene Ontology term enrichment, these methods are not well suited for analysing groups of TFs. This is particularly true since such methods do not aim to include downstream processes, and given a set of TFs, the expected top ontologies would revolve around transcription processes.\nWe present the TFTenricher, a Python toolbox that focuses specifically at identifying gene ontology terms, cellular pathways, and diseases that are over-represented among genes downstream of user-defined sets of human TFs. We evaluated the inference of downstream gene targets with respect to false positive annotations, and found an inference based on co-expression to best predict downstream processes. Based on these downstream genes, the TFTenricher uses some of the most common databases for gene functionalities, including GO, KEGG and Reactome, to calculate functional enrichments. By applying the TFTenricher to differential expression of TFs in 21 diseases, we found significant terms associated with disease mechanism, while the gene set enrichment analysis on the same dataset predominantly identified processes related to transcription.\nThe TFTenricher package enables users to search for biological context in any set of TFs and their downstream genes. The TFTenricher is available as a Python 3 toolbox at https://github.com/rasma774/Tftenricher , under a GNU GPL license and with minimal dependencies.",

    "abstract": "Many macromolecules form helical assemblies to carry out their functions. Helical reconstruction from electron microscopic images is a powerful approach for solving high-resolution structures of such assemblies. Determination of the symmetry parameters of the helical assemblies is a prerequisite step in helical reconstruction. The most widely used method for deducing the symmetry is through Fourier-Bessel indexing the diffraction pattern of the helical assemblies. This method, however, often leads to incorrect solutions, due to intrinsic ambiguities in indexing helical diffraction patterns. Here, we present Python-based Helix Indexer (PyHI), which provides a graphical user interface (GUI) to guide the users through the process of symmetry determination. Diffraction patterns can be read into the program directly or calculated on the fly from two-dimensional class averages of helical assemblies. PyHI allows deducing the Bessel orders of diffraction peaks by using both the amplitudes and phases of the diffraction data. Based on the Bessel orders of two unit vectors, the Fourier space lattice is constructed with minimal user inputs. The program then uses a refinement algorithm to optimize the Fourier space lattice, and subsequently generate the helical assembly in real space. The program provides both a publication-quality graphic representation of the helical assembly and the symmetry parameters required for subsequent helical reconstruction steps.",

    "abstract": "Nuclear magnetic resonance (NMR) provides site specific information on local environments through chemical shifts. NMR is widely used in the study of proteins, ranging from determination of three-dimensional (3D) structures to characterizing dynamics and binding of small molecules and other proteins or ligands. Assigned chemical shift data for the atoms within proteins is a treasure trove of information that can facilitate a broad range of biochemical and biophysical studies. The Biological Magnetic Resonance Data Bank (BMRB) is a publicly accessible database that contains a large number of assigned chemical shifts; however, translating this wealth of knowledge into a practical application is not straightforward. Herein we present nightshift: a Python command line utility and library for plotting simulated two-dimensional (2D) and 3D NMR spectra from assigned chemical shifts in the BMRB. This tool allows users to simulate routinely collected amide and methyl fingerprint spectra, backbone triple-resonance assignment spectra, and user-defined custom correlations, including ones that do not necessarily correspond to published experiments. This tool enables experienced NMR spectroscopists, those learning the craft, and interested scientists seeking to utilize NMR the ability to preview or examine a wide range of spectra for proteins whose assignments are deposited in the BMRB, irrespective of whether those experiments have been executed or reported. The tool applies equally to folded and intrinsically disordered proteins, limited only by the existence of a BMRB deposition. The features of nightshift are described along with applications that illustrate the ease with which complicated correlation spectra and binding events can be simulated.",

    "abstract": "Research on brain-computer interfaces (BCIs) has become more democratic in recent decades, and experiments using electroencephalography (EEG)-based BCIs has dramatically increased. The variety of protocol designs and the growing interest in physiological computing require parallel improvements in processing and classification of both EEG signals and bio signals, such as electrodermal activity (EDA), heart rate (HR) or breathing. If some EEG-based analysis tools are already available for online BCIs with a number of online BCI platforms (e.g., BCI2000 or OpenViBE), it remains crucial to perform offline analyses in order to design, select, tune, validate and test algorithms before using them online. Moreover, studying and comparing those algorithms usually requires expertise in programming, signal processing and machine learning, whereas numerous BCI researchers come from other backgrounds with limited or no training in such skills. Finally, existing BCI toolboxes are focused on EEG and other brain signals but usually do not include processing tools for other bio signals. Therefore, in this paper, we describe BioPyC, a free, open-source and easy-to-use Python platform for offline EEG and biosignal processing and classification. Based on an intuitive and well-guided graphical interface, four main modules allow the user to follow the standard steps of the BCI process without any programming skills: (1) reading different neurophysiological signal data formats, (2) filtering and representing EEG and bio signals, (3) classifying them, and (4) visualizing and performing statistical tests on the results. We illustrate BioPyC use on four studies, namely classifying mental tasks, the cognitive workload, emotions and attention states from EEG signals.",

    "abstract": "Hg-CATH and Pb-CATH4 are cathelicidins from ",

    "abstract": "Molecular dynamics simulations are now widely used to study emergent phenomena in lipid membranes with complex compositions. Here, we present LiPyphilic-a fast, fully tested, and easy-to-install Python package for analyzing such simulations. Analysis tools in LiPyphilic include the identification of cholesterol flip-flop events, the classification of local lipid environments, and the degree of interleaflet registration. LiPyphilic is both force field- and resolution-agnostic, and by using the powerful atom selection language of MDAnalysis, it can handle membranes with highly complex compositions. LiPyphilic also offers two on-the-fly trajectory transformations to (i) fix membranes split across periodic boundaries and (ii) perform nojump coordinate unwrapping. Our implementation of nojump unwrapping accounts for fluctuations in the box volume under the ",

    "abstract": "Custom-built microscopes often require control of multiple hardware devices and precise hardware coordination. It is also desirable to have a solution that is scalable to complex systems and that is translatable between components from different manufacturers. Here we report Python-Microscope, a free and open-source Python library for high-performance control of arbitrarily complex and scalable custom microscope systems. Python-Microscope offers simple to use Python-based tools, abstracting differences between physical devices by providing a defined interface for different device types. Concrete implementations are provided for a range of specific hardware, and a framework exists for further expansion. Python-Microscope supports the distribution of devices over multiple computers while maintaining synchronisation via highly precise hardware triggers. We discuss the architectural features of Python-Microscope that overcome the performance problems often raised against Python and demonstrate the different use cases that drove its design: integration with user-facing projects, namely the Microscope-Cockpit project; control of complex microscopes at high speed while using the Python programming language; and use as a microscope simulation tool for software development.",

    "abstract": "We present a model written in python to evaluate data from comprehensive ",

    "abstract": "Bitter tastes are innately aversive and are thought to help protect animals from consuming poisons. Children are extremely sensitive to drug tastes, and their compliance is especially poor with bitter medicine. Therefore, judging whether a drug is bitter and adopting flavor correction and taste-masking strategies are key to solving the problem of drug compliance in children. Although various machine learning models for bitterness and sweetness prediction have been reported in the literature, no learning model or bitterness database for children's medication has yet been reported. In this study, we trained four different machine learning models to predict bitterness. The goal of this study was to develop and validate a machine learning model called the \"Children's Bitter Drug Prediction System\" (CBDPS) based on Tkinter, which predicts the bitterness of a medicine based on its chemical structure. Users can enter the Simplified Molecular-Input Line-Entry System (SMILES) formula for a single compound or multiple compounds, and CBDPS will predict the bitterness of children's medicines made from those XGBoost-Molecular ACCess System (XgBoost-MACCS) model yielded an accuracy of 88% under cross-validation.",

    "abstract": "Burmese pythons Python bivittatus captured in the Florida Everglades as part of an invasive species monitoring program served as a model for the development of sperm cryopreservation protocols for endangered snakes. Spermatozoa were collected from the vas deferens and initial motility, plasma membrane integrity and acrosome integrity were recorded before cryopreservation. Spermatozoa were extended in TES and Tris (TEST) yolk buffer with glycerol (GLY) or dimethyl sulfoxide (DMSO) concentrations of 8%, 12% or 16%, or combinations of GLY and DMSO with final concentrations of 4%:4%, 6%:6% or 8%:8%, and frozen at a rate of 0.3\u00b0C min-1 . Sperm frozen in combinations of GLY and DMSO exhibited greater post-thaw motility and plasma membrane integrity than those frozen in GLY or DMSO alone. All DMSO and GLY:DMSO treatments preserved a greater proportion of intact acrosomes than GLY alone. To determine the best overall cryopreservation protocol for this species, a sperm quality index was calculated, giving equal weight to each of the three measured indicators of cryosurvival. This analysis revealed that Burmese python spermatozoa frozen in 6% GLY:6% DMSO or 4% GLY:4% DMSO exhibited the highest post-thaw viability. This study represents the first comparative, comprehensive attempt to develop a sperm cryopreservation protocol for any snake species.",

    "abstract": "We present a modular and extendable software suite, DJMol, for performing molecular simulations and it is demonstrated with DFTB+, Siesta, Atomic Simulation Environment, and OpenMD codes. It supports many of the standard features of an integrated development environment and consists of a structure builder and viewer, which could be connected with these electronic structure codes along with a set of data analyzers. This program comprises Java and Python modules and its libraries to carry out a different set of modeling tasks in materials science and chemistry. By adopting a Python interpreter into the software, a range of scriptable Python codes, such as Pymatgen can be incorporated into this programmable modeling platform. DJMol, through its common application programming interface (API), supports multiple modeling codes in the backend and several post-processing tools. It benefits an experienced user by increasing efficiency, while a nonexpert user by easy to use API.",

    "abstract": "The human leukocyte antigen (HLA) proteins play a fundamental role in the adaptive immune system as they present peptides to T cells. Mass-spectrometry-based immunopeptidomics is a promising and powerful tool for characterizing the immunopeptidomic landscape of HLA proteins, that is the peptides presented on HLA proteins. Despite the growing interest in the technology, and the recent rise of immunopeptidomics-specific identification pipelines, there is still a gap in data-analysis and software tools that are specialized in analyzing and visualizing immunopeptidomics data.\nWe present the IPTK library which is an open-source Python-based library for analyzing, visualizing, comparing, and integrating different omics layers with the identified peptides for an in-depth characterization of the immunopeptidome. Using different datasets, we illustrate the ability of the library to enrich the result of the identified peptidomes. Also, we demonstrate the utility of the library in developing other software and tools by developing an easy-to-use dashboard that can be used for the interactive analysis of the results.\nIPTK provides a modular and extendable framework for analyzing and integrating immunopeptidomes with different omics layers. The library is deployed into PyPI at https://pypi.org/project/IPTKL/ and into Bioconda at https://anaconda.org/bioconda/iptkl , while the source code of the library and the dashboard, along with the online tutorials are available at https://github.com/ikmb/iptoolkit .",

    "abstract": "As camera pixel arrays have grown larger and faster, and optical microscopy techniques ever more refined, there has been an explosion in the quantity of data acquired during routine light microscopy. At the single-molecule level, analysis involves multiple steps and can rapidly become computationally expensive, in some cases intractable on office workstations. Complex bespoke software can present high activation barriers to entry for new users. Here, we redevelop our quantitative single-molecule analysis routines into an optimized and extensible Python program, with GUI and command-line implementations to facilitate use on local machines and remote clusters, by beginners and advanced users alike. We demonstrate that its performance is on par with previous MATLAB implementations but runs an order of magnitude faster. We tested it against challenge data and demonstrate its performance is comparable to state-of-the-art analysis platforms. We show the code can extract fluorescence intensity values for single reporter dye molecules and, using these, estimate molecular stoichiometries and cellular copy numbers of fluorescently-labeled biomolecules. It can evaluate 2D diffusion coefficients for the characteristically short single-particle tracking data. To facilitate benchmarking we include data simulation routines to compare different analysis programs. Finally, we show that it works with 2-color data and enables colocalization analysis based on overlap integration, to infer interactions between differently labelled biomolecules. By making this freely available we aim to make complex light microscopy single-molecule analysis more democratized.",

    "abstract": "Ophidascaris species are parasitic roundworms that inhabit the python gut, resulting in severe granulomatous lesions or even death. However, the classification and nomenclature of these roundworms are still controversial. Our study aims to identify a snake roundworm from the Burmese python (Python molurus bivittatus) and analyze the mitochondrial genome. We identified this roundworm as Ophidascaris baylisi based on the morphology and cytochrome c oxidase subunit I (cox1) sequence. Ophidascaris baylisi complete mitochondrial genome was 14,784\u00a0bp in length, consisting of two non-coding regions and 36 mitochondrial genes (12 protein-coding genes, 22 tRNA genes, and two rRNA genes). The protein-coding genes used TTG, ATG, ATT, or TTA as start codons and TAG, TAA, or T as stop codons. All tRNA genes showed a TV-loop structure, except trnS1",

    "abstract": "",

    "abstract": "Diffusion-weighted magnetic resonance imaging (dMRI) measurements and models provide information about brain connectivity and are sensitive to the physical properties of tissue microstructure. Diffusional Kurtosis Imaging (DKI) quantifies the degree of non-Gaussian diffusion in biological tissue from dMRI. These estimates are of interest because they were shown to be more sensitive to microstructural alterations in health and diseases than measures based on the total anisotropy of diffusion which are highly confounded by tissue dispersion and fiber crossings. In this work, we implemented DKI in the Diffusion in Python (DIPY) project-a large collaborative open-source project which aims to provide well-tested, well-documented and comprehensive implementation of different dMRI techniques. We demonstrate the functionality of our methods in numerical simulations with known ground truth parameters and in openly available datasets. A particular strength of our DKI implementations is that it pursues several extensions of the model that connect it explicitly with microstructural models and the reconstruction of 3D white matter fiber bundles (tractography). For instance, our implementations include DKI-based microstructural models that allow the estimation of biophysical parameters, such as axonal water fraction. Moreover, we illustrate how DKI provides more general characterization of non-Gaussian diffusion compatible with complex white matter fiber architectures and gray matter, and we include a novel mean kurtosis index that is invariant to the confounding effects due to tissue dispersion. In summary, DKI in DIPY provides a well-tested, well-documented and comprehensive reference implementation for DKI. It provides a platform for wider use of DKI in research on brain disorders and in cognitive neuroscience.",

    "abstract": "Virtual reality (VR) is a new methodology for behavioral studies. In such studies, the millisecond accuracy and precision of stimulus presentation are critical for data replicability. Recently, Python, which is a widely used programming language for scientific research, has contributed to reliable accuracy and precision in experimental control. However, little is known about whether modern VR environments have millisecond accuracy and precision for stimulus presentation, since most standard methods in laboratory studies are not optimized for VR environments. The purpose of this study was to systematically evaluate the accuracy and precision of visual and auditory stimuli generated in modern VR head-mounted displays (HMDs) from HTC and Oculus using Python 2 and 3. We used the newest Python tools for VR and Black Box Toolkit to measure the actual time lag and jitter. The results showed that there was an 18-ms time lag for visual stimulus in both HMDs. For the auditory stimulus, the time lag varied between 40 and 60 ms, depending on the HMD. The jitters of those time lags were 1 ms for visual stimulus and 4 ms for auditory stimulus, which are sufficiently low for general experiments. These time lags were robustly equal, even when auditory and visual stimuli were presented simultaneously. Interestingly, all results were perfectly consistent in both Python 2 and 3 environments. Thus, the present study will help establish a more reliable stimulus control for psychological and neuroscientific research controlled by Python environments.",

    "abstract": "Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image subvolumes or patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes.\nWe present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. Most transforms can be inverted, making the library suitable for test-time augmentation and estimation of aleatoric uncertainty in the context of segmentation. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts.\nSource code, comprehensive tutorials and extensive documentation for TorchIO can be found at http://torchio.rtfd.io/. The package can be installed from the Python Package Index (PyPI) running pip install torchio. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical user interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms.\nTorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images.",

    "abstract": "In auditory behavioral and EEG experiments, the variability of stimulation solutions, for both software and hardware, adds unnecessary technical constraints. Currently, there is no easy to use, inexpensive, and shareable solution that could improve collaborations and data comparisons across different sites and contexts. This article outlines a system composed by a Raspberry Pi coupled with Python programming and associated with a HifiBerry sound card. We compare its sound performances with those of a wide variety of materials and configurations. This solution achieves the high timing accuracy and sound quality important in auditory cognition experiments, while being simple to use and open source. The present system shows high performances and results along with excellent feedback from users. It is inexpensive, easy to build, share, and improve on. Working with such low-cost, powerful, and collaborative hardware and software tools allows people to create their own specific, adapted, and shareable system that can be standardized across different collaborative sites, while being extremely simple and robust in use.",

    "abstract": "Smoldyn is a particle-based biochemical simulator that is frequently used for systems biology and biophysics research. Previously, users could only define models using text-based input or a C/C++ application programming interface (API), which were convenient, but limited extensibility.\nWe added a Python API to Smoldyn to improve integration with other software tools, such as Jupyter notebooks, other Python code libraries and other simulators. It includes low-level functions that closely mimic the existing C/C++ API and higher-level functions that are more convenient to use. These latter functions follow modern object-oriented Python conventions.\nSmoldyn is open source and free, available at http://www.smoldyn.org and can be installed with the Python package manager pip. It runs on Mac, Windows and Linux.\nDocumentation is available at http://www.smoldyn.org/SmoldynManual.pdf and https://smoldyn.readthedocs.io/en/latest/python/api.html.",

    "abstract": "PHOTONAI is a high-level Python API designed to simplify and accelerate machine learning model development. It functions as a unifying framework allowing the user to easily access and combine algorithms from different toolboxes into custom algorithm sequences. It is especially designed to support the iterative model development process and automates the repetitive training, hyperparameter optimization and evaluation tasks. Importantly, the workflow ensures unbiased performance estimates while still allowing the user to fully customize the machine learning analysis. PHOTONAI extends existing solutions with a novel pipeline implementation supporting more complex data streams, feature combinations, and algorithm selection. Metrics and results can be conveniently visualized using the PHOTONAI Explorer and predictive models are shareable in a standardized format for further external validation or application. A growing add-on ecosystem allows researchers to offer data modality specific algorithms to the community and enhance machine learning in the areas of the life sciences. Its practical utility is demonstrated on an exemplary medical machine learning problem, achieving a state-of-the-art solution in few lines of code. Source code is publicly available on Github, while examples and documentation can be found at www.photon-ai.com.",

    "abstract": null,

    "abstract": "AutoDock Vina is arguably one of the fastest and most widely used open-source programs for molecular docking. However, compared to other programs in the AutoDock Suite, it lacks support for modeling specific features such as macrocycles or explicit water molecules. Here, we describe the implementation of this functionality in AutoDock Vina 1.2.0. Additionally, AutoDock Vina 1.2.0 supports the AutoDock4.2 scoring function, simultaneous docking of multiple ligands, and a batch mode for docking a large number of ligands. Furthermore, we implemented Python bindings to facilitate scripting and the development of docking workflows. This work is an effort toward the unification of the features of the AutoDock4 and AutoDock Vina programs. The source code is available at https://github.com/ccsb-scripps/AutoDock-Vina.",

    "abstract": "We report on a Python toolbox for unbiased statistical analysis of fluorescence intermittency properties of single emitters. Intermittency, that is, step-wise temporal variations in the instantaneous emission intensity and fluorescence decay rate properties, is common to organic fluorophores, II-VI quantum dots, and perovskite quantum dots alike. Unbiased statistical analysis of intermittency switching time distributions, involved levels, and lifetimes are important to avoid interpretation artifacts. This work provides an implementation of Bayesian changepoint analysis and level clustering applicable to time-tagged single-photon detection data of single emitters that can be applied to real experimental data and as a tool to verify the ramifications of hypothesized mechanistic intermittency models. We provide a detailed Monte Carlo analysis to illustrate these statistics tools and to benchmark the extent to which conclusions can be drawn on the photophysics of highly complex systems, such as perovskite quantum dots that switch between a plethora of states instead of just two.",

    "abstract": "Adoption of the Digital Imaging and Communications in Medicine (DICOM) standard for whole slide images (WSIs) has been slow, despite significant time and effort by standards curators. One reason for the lack of adoption is that there are few tools which exist that can meet the requirements of WSIs, given an evolving ecosystem of best practices for implementation. Eventually, vendors will conform to the specification to ensure enterprise interoperability, but what about archived slides? Millions of slides have been scanned in various proprietary formats, many with examples of rare histologies. Our hypothesis is that if users and developers had access to easy to use tools for migrating proprietary formats to the open DICOM standard, then more tools would be developed as DICOM first implementations.\nThe technology we present here is dicom_wsi, a Python based toolkit for converting any slide capable of being read by the OpenSlide library into DICOM conformant and validated implementations. Moreover, additional postprocessing such as background removal, digital transformations (e.g., ink removal), and annotation storage are also described. dicom_wsi is a free and open source implementation that anyone can use or modify to meet their specific purposes.\nWe compare the output of dicom_wsi to two other existing implementations of WSI to DICOM converters and also validate the images using DICOM capable image viewers.\ndicom_wsi represents the first step in a long process of DICOM adoption for WSI. It is the first open source implementation released in the developer friendly Python programming language and can be freely downloaded at .",

    "abstract": "Introns are generally removed from primary transcripts to form mature RNA molecules in a post-transcriptional process called splicing. An efficient splicing of primary transcripts is an essential step in gene expression and its misregulation is related to numerous human diseases. Thus, to better understand the dynamics of this process and the perturbations that might be caused by aberrant transcript processing it is important to quantify splicing efficiency.\nHere, we introduce SPLICE-q, a fast and user-friendly Python tool for genome-wide SPLICing Efficiency quantification. It supports studies focusing on the implications of splicing efficiency in transcript processing dynamics. SPLICE-q uses aligned reads from strand-specific RNA-seq to quantify splicing efficiency for each intron individually and allows the user to select different levels of restrictiveness concerning the introns' overlap with other genomic elements such as exons of other genes. We applied SPLICE-q to globally assess the dynamics of intron excision in yeast and human nascent RNA-seq. We also show its application using total RNA-seq from a patient-matched prostate cancer sample.\nOur analyses illustrate that SPLICE-q is suitable to detect a progressive increase of splicing efficiency throughout a time course of nascent RNA-seq and it might be useful when it comes to understanding cancer progression beyond mere gene expression levels. SPLICE-q is available at: https://github.com/vrmelo/SPLICE-q.",

    "abstract": "Complex reaction networks can be generated with automated network generators from initial reactants and reaction rules. Reaction rule specification is central to network generation. These reaction rules are, at present, user-defined based on (intuitive) expert knowledge of chemistry and are often transferred from gas-phase to surface processes. The catalyst active site geometry is usually left out but is often responsible for selectivity. We propose a first-principles-based reaction mechanism generation framework using density functional theory (DFT) data of published reaction mechanisms. The framework \"learns the chemistry\" from published mechanisms. It can generate reaction networks not studied before, \"flag\" reactions not seen before for further DFT convergence tests, and easily reconcile differences between catalysts and reactants that may introduce new pathways never seen before. As such, it can be a diagnostic tool for data (mechanism) quality assessment and novel pathway discovery to new molecules. A software, the Python Reaction Stencil (pReSt), was developed for this purpose to wrap around automatic mechanism generation software. Multiple catalytic chemistries are considered to show the efficacy of the proposed framework.",

    "abstract": "The advent of large-scale fluorescence and electronic microscopy techniques along with maturing image analysis is giving life sciences a deluge of geometrical objects in 2D/3D(+t) to deal with. These objects take the form of large scale, localised, precise, single cell, quantitative data such as cells' positions, shapes, trajectories or lineages, axon traces in whole brains atlases or varied intracellular protein localisations, often in multiple experimental conditions. The data mining of those geometrical objects requires a variety of mathematical and computational tools of diverse accessibility and complexity. Here we present a new Python library for quantitative 3D geometry called GeNePy3D which helps handle and mine information and knowledge from geometric data, providing a unified application programming interface (API) to methods from several domains including computational geometry, scale space methods or spatial statistics. By framing this library as generically as possible, and by linking it to as many state-of-the-art reference algorithms and projects as needed, we help render those often specialist methods accessible to a larger community. We exemplify the usefulness of the \u00a0GeNePy3D toolbox by re-analysing a recently published whole-brain zebrafish neuronal atlas, with other applications and examples available online. Along with an open source, documented and exemplified code, we release reusable containers to allow for convenient and wide usability and increased reproducibility.",

    "abstract": "Despite recent progress in the analysis of neuroimaging data sets, our comprehension of the main mechanisms and principles which govern human brain cognition and function remains incomplete. Network neuroscience makes substantial efforts to manipulate these challenges and provide real answers. For the last decade, researchers have been modelling brain structure and function via a graph or network that comprises brain regions that are either anatomically connected via tracts or functionally via a more extensive repertoire of functional associations. Network neuroscience is a relatively new multidisciplinary scientific avenue of the study of complex systems by pursuing novel ways to analyze, map, store and model the essential elements and their interactions in complex neurobiological systems, particularly the human brain, the most complex system in nature. Due to a rapid expansion of neuroimaging data sets' size and complexity, it is essential to propose and adopt new empirical tools to track dynamic patterns between neurons and brain areas and create comprehensive maps. In recent years, there is a rapid growth of scientific interest in moving functional neuroimaging analysis beyond simplified group or time-averaged approaches and sophisticated algorithms that can capture the time-varying properties of functional connectivity. We describe algorithms and network metrics that can capture the dynamic evolution of functional connectivity under this perspective. We adopt the word 'chronnectome' (integration of the Greek word 'Chronos', which means time, and connectome) to describe this specific branch of network neuroscience that explores how mutually informed brain activity correlates across time and brain space in a functional way. We also describe how good temporal mining of temporally evolved dynamic functional networks could give rise to the detection of specific brain states over which our brain evolved. This characteristic supports our complex human mind. The temporal evolution of these brain states and well-known network metrics could give rise to new analytic trends. Functional brain networks could also increase the multi-faced nature of the dynamic networks revealing complementary information. Finally, we describe a python module (https://github.com/makism/dyconnmap) which accompanies this article and contains a collection of dynamic complex network analytics and measures and demonstrates its great promise for the study of a healthy subject's repeated fMRI scans.",

    "abstract": "Thanks to the recent multiplication of scientific Python packages in the open-source software landscape, Data Acquisition frameworks (DAQ-Fs) appear as versatile replacements of custom-made or costly commercial solutions. PyMoDAQ is a DAQ-F focusing on easy-to-use graphical user interfaces allowing a simple control and automation of a large variety of experimental setups. Its development included a highly modular structure allowing any experimental data acquisition as a function of multiple varying parameters. It offers numerous additional functionalities: instrument and setup configuration, plotting, saving, logging, etc. Live visual feedback is available at all times to monitor the ongoing experiment. Flexibility of its user interfaces is the key advantage of PyMoDAQ allowing also its integration as the core of more focused applications. Its hierarchical binary format data saving mechanism includes experimental metadata highly compatible with FAIR (Findable, Accessible,Interoperable, Reusable) data. Among the presented characteristics, seven criteria have been chosen to judge the pertinence of PyMoDAQ as a versatile DAQ-F. They are also the basis for a comparison with other existing frameworks highlighting the novelty of PyMoDAQ.",

    "abstract": "Since Bandt and Pompe's seminal work, permutation entropy has been used in several applications and is now an essential tool for time series analysis. Beyond becoming a popular and successful technique, permutation entropy inspired a framework for mapping time series into symbolic sequences that triggered the development of many other tools, including an approach for creating networks from time series known as ordinal networks. Despite increasing popularity, the computational development of these methods is fragmented, and there were still no efforts focusing on creating a unified software package. Here, we present ordpy (http://github.com/arthurpessa/ordpy), a simple and open-source Python module that implements permutation entropy and several of the principal methods related to Bandt and Pompe's framework to analyze time series and two-dimensional data. In particular, ordpy implements permutation entropy, Tsallis and R\u00e9nyi permutation entropies, complexity-entropy plane, complexity-entropy curves, missing ordinal patterns, ordinal networks, and missing ordinal transitions for one-dimensional (time series) and two-dimensional (images) data as well as their multiscale generalizations. We review some theoretical aspects of these tools and illustrate the use of ordpy by replicating several literature results.",

    "abstract": "Computational software workflows are emerging as all-in-one solutions to speed up the discovery of new materials. Many computational approaches require the generation of realistic structural models for property prediction and candidate screening. However, molecular and supramolecular materials represent classes of materials with many potential applications for which there is no go-to database of existing structures or general protocol for generating structures. Here, we report a new version of the supramolecular toolkit, stk, an open-source, extendable, and modular Python framework for general structure generation of (supra)molecular structures. Our construction approach works on arbitrary building blocks and topologies and minimizes the input required from the user, making stk user-friendly and applicable to many material classes. This version of stk includes metal-containing structures and rotaxanes as well as general implementation and interface improvements. Additionally, this version includes built-in tools for exploring chemical space with an evolutionary algorithm and tools for database generation and visualization. The latest version of stk is freely available at github.com/lukasturcani/stk.",

    "abstract": "We present the Core Imaging Library (CIL), an open-source Python framework for tomographic imaging with particular emphasis on reconstruction of challenging datasets. Conventional filtered back-projection reconstruction tends to be insufficient for highly noisy, incomplete, non-standard or multi-channel data arising for example in dynamic, spectral and ",

    "abstract": "Tumors are composed by a number of cancer cell subpopulations (subclones), characterized by a distinguishable set of mutations. This phenomenon, known as intra-tumor heterogeneity (ITH), may be studied using Copy Number Aberrations (CNAs). Nowadays ITH can be assessed at the highest possible resolution using single-cell DNA (scDNA) sequencing technology. Additionally, single-cell CNA (scCNA) profiles from multiple samples of the same tumor can in principle be exploited to study the spatial distribution of subclones within a tumor mass. However, since the technology required to generate large scDNA sequencing datasets is relatively recent, dedicated analytical approaches are still lacking.\nWe present PhyliCS, the first tool which exploits scCNA data from multiple samples from the same tumor to estimate whether the different clones of a tumor are well mixed or spatially separated. Starting from the CNA data produced with third party instruments, it computes a score, the Spatial Heterogeneity score, aimed at distinguishing spatially intermixed cell populations from spatially segregated ones. Additionally, it provides functionalities to facilitate scDNA analysis, such as feature selection and dimensionality reduction methods, visualization tools and a flexible clustering module.\nPhyliCS represents a valuable instrument to explore the extent of spatial heterogeneity in multi-regional tumour sampling, exploiting the potential of scCNA data.",

    "abstract": "X-ray propagation-based imaging techniques are well established at synchrotron radiation and laboratory sources. However, most reconstruction algorithms for such image modalities, also known as phase-retrieval algorithms, have been developed specifically for one instrument by and for experts, making the development and diffusion of such techniques difficult. Here, PyPhase, a free and open-source package for propagation-based near-field phase reconstructions, which is distributed under the CeCILL license, is presented. PyPhase implements some of the most popular phase-retrieval algorithms in a highly modular framework supporting its deployment on large-scale computing facilities. This makes the integration, the development of new phase-retrieval algorithms, and the deployment on different computing infrastructures straightforward. Its capabilities and simplicity are presented by application to data acquired at the synchrotron source MAX IV (Lund, Sweden).",

    "abstract": "The emergence of fourth-generation synchrotrons is prompting the development of new systems for experimental control and data acquisition. However, as\u00a0general control systems are designed to cover a wide set of instruments and\u00a0techniques, they tend to become large and complicated, at the cost of experimental flexibility. Here we present Contrast, a simple Python framework for interacting with beamline components, orchestrating experiments and managing data acquisition. The system is presented and demonstrated via its application at the NanoMAX beamline of the MAX IV Laboratory.",

    "abstract": "",

    "abstract": "Theoretical/computational description of excited state molecular dynamics is nowadays a crucial tool for understanding light-matter interactions in many materials. Here we present an open-source Python-based nonadiabatic molecular dynamics program package, namely PyUNIxMD, to deal with mixed quantum-classical dynamics for correlated electron-nuclear propagation. The PyUNIxMD provides many interfaces for quantum chemical calculation methods with commercial and noncommercial ab initio and semiempirical quantum chemistry programs. In addition, the PyUNIxMD offers many nonadiabatic molecular dynamics algorithms such as fewest-switch surface hopping and its derivatives as well as decoherence-induced surface hopping based on the exact factorization (DISH-XF) and coupled-trajectory mixed quantum-classical dynamics (CTMQC) for general purposes. Detailed structures and flows of PyUNIxMD are explained for the further implementations by developers. We perform a nonadiabatic molecular dynamics simulation for a molecular motor system as a simple demonstration.",

    "abstract": "While glycans are crucial for biological processes, existing analysis modalities make it difficult for researchers with limited computational background to include these diverse carbohydrates into workflows. Here, we present glycowork, an open-source Python package designed for glycan-related data science and machine learning by end users. Glycowork includes functions to, for instance, automatically annotate glycan motifs and analyze their distributions via heatmaps and statistical enrichment. We also provide visualization methods, routines to interact with stored databases, trained machine learning models and learned glycan representations. We envision that glycowork can extract further insights from glycan datasets and demonstrate this with workflows that analyze glycan motifs in various biological contexts. Glycowork can be freely accessed at https://github.com/BojarLab/glycowork/.",

    "abstract": "Phylogenies are a key part of research in many areas of biology. Tools that automate some parts of the process of phylogenetic reconstruction, mainly molecular character matrix assembly, have been developed for the advantage of both specialists in the field of phylogenetics and non-specialists. However, interpretation of results, comparison with previously available phylogenetic hypotheses, and selection of one phylogeny for downstream analyses and discussion still impose difficulties to one that is not a specialist either on phylogenetic methods or on a particular group of study.\nPhyscraper is a command-line Python program that automates the update of published phylogenies by adding public DNA sequences to underlying alignments of previously published phylogenies. It also provides a framework for straightforward comparison of published phylogenies with their updated versions, by leveraging upon tools from the Open Tree of Life project to link taxonomic information across databases. The program can be used by the nonspecialist, as a tool to generate phylogenetic hypotheses based on publicly available expert phylogenetic knowledge. Phylogeneticists and taxonomic group specialists will find it useful as a tool to facilitate molecular dataset gathering and comparison of alternative phylogenetic hypotheses (topologies).\nThe Physcraper workflow showcases the benefits of doing open science for phylogenetics, encouraging researchers to strive for better scientific\u00a0sharing practices. Physcraper can be used with any OS and is released under an open-source license. Detailed instructions for installation and usage are available at https://physcraper.readthedocs.io.",

    "abstract": "The characterisation of the HIV-1 reservoir, which consists of replication-competent integrated proviruses that persist on antiretroviral therapy (ART), is made difficult by the rarity of intact proviruses relative to those that are defective. While the only conclusive test for the replication-competence of HIV-1 proviruses is carried out in cell culture, genetic characterization of genomes by near full-length (NFL) PCR and sequencing can be used to determine whether particular proviruses have insertions, deletions, or substitutions that render them defective. Proviruses that are not excluded by having such defects can be classified as genetically intact and, possibly, replication competent. Identifying and quantifying proviruses that are potentially replication-competent is important for the development of strategies towards a functional cure. However, to date, there are no programs that can be incorporated into deep-sequencing pipelines for the automated characterization and annotation of HIV genomes. Existing programs that perform this work require manual intervention, cannot be widely installed, and do not have easily adjustable settings. Here, we present HIVIntact, a python-based software tool that characterises genomic defects in NFL HIV-1 sequences, allowing putative intact genomes to be identified in-silico. Unlike other applications that assess the genetic intactness of HIV genomes, this tool can be incorporated into existing sequence-analysis pipelines and applied to large next-generation sequencing datasets.",

    "abstract": "We present the open-source python package DockOnSurf which automates the generation and optimization of low-energy adsorption configurations of molecules on extended surfaces and nanoparticles. DockOnSurf is especially geared toward handling polyfunctional flexible adsorbates. The use of this high-throughput workflow allows us to carry out the screening of adsorbate-surface configurations in a systematic, customizable, and traceable way, while keeping the focus on the chemically relevant structures. The screening strategy consists in splitting the exploration of the adsorbate-surface configurational space into chemically meaningful domains, that is, by choosing among different conformers to adsorb, surface adsorption sites, adsorbate anchoring points, and orientations and allowing dissociation of (acidic) protons. We demonstrate the performance of the main features based on varying examples, ranging from CO adsorption on a gold nanoparticle to sorbitol adsorption on hematite. Through the use of the presented program, we aim to foster efficiency, traceability, and ease of use in research within tribology, catalysis, nanoscience, and surface science in general.",

    "abstract": "The study sought to develop and evaluate neural natural language processing (NLP) packages for the syntactic analysis and named entity recognition of biomedical and clinical English text.\nWe implement and train biomedical and clinical English NLP pipelines by extending the widely used Stanza library originally designed for general NLP tasks. Our models are trained with a mix of public datasets such as the CRAFT treebank as well as with a private corpus of radiology reports annotated with 5 radiology-domain entities. The resulting pipelines are fully based on neural networks, and are able to perform tokenization, part-of-speech tagging, lemmatization, dependency parsing, and named entity recognition for both biomedical and clinical text. We compare our systems against popular open-source NLP libraries such as CoreNLP and scispaCy, state-of-the-art models such as the BioBERT models, and winning systems from the BioNLP CRAFT shared task.\nFor syntactic analysis, our systems achieve much better performance compared with the released scispaCy models and CoreNLP models retrained on the same treebanks, and are on par with the winning system from the CRAFT shared task. For NER, our systems substantially outperform scispaCy, and are better or on par with the state-of-the-art performance from BioBERT, while being much more computationally efficient.\nWe introduce biomedical and clinical NLP packages built for the Stanza library. These packages offer performance that is similar to the state of the art, and are also optimized for ease of use. To facilitate research, we make all our models publicly available. We also provide an online demonstration (http://stanza.run/bio).",

    "abstract": "The T-cell receptor (TCR) determines the specificity of a T-cell towards an epitope. As of yet, the rules for antigen recognition remain largely undetermined. Current methods for grouping TCRs according to their epitope specificity remain limited in performance and scalability. Multiple methodologies have been developed, but all of them fail to efficiently cluster large datasets exceeding 1 million sequences. To account for this limitation, we developed ClusTCR, a rapid TCR clustering alternative that efficiently scales up to millions of CDR3 amino acid sequences, without knowledge about their antigen specificity.\nBenchmarking comparisons revealed similar accuracy of ClusTCR as compared to other TCR clustering methods, as measured by cluster retention, purity and consistency. ClusTCR offers a drastic improvement in clustering speed, which allows the clustering of millions of TCR sequences in just a few minutes through ultraefficient similarity searching and sequence hashing.\nClusTCR was written in Python 3. It is available as an anaconda package (https://anaconda.org/svalkiers/clustcr) and on github (https://github.com/svalkiers/clusTCR).\nSupplementary data are available at Bioinformatics online.",

    "abstract": "PAX2GRAPHML is an open-source Python library that allows to easily manipulate BioPAX source files as regulated reaction graphs described in.graphml format. The concept of regulated reactions, which allows connecting regulatory, signaling and metabolic levels, has been used. Biochemical reactions and regulatory interactions are homogeneously described by regulated reactions involving substrates, products, activators and inhibitors as elements. PAX2GRAPHML is highly flexible and allows generating graphs of regulated reactions from a single BioPAX source or by combining and filtering BioPAX sources. Supported by the graph exchange format .graphml, the large-scale graphs produced from one or more data sources can be further analyzed with PAX2GRAPHML or standard Python and R graph libraries.\nhttps://pax2graphml.genouest.org.",

    "abstract": null,

    "abstract": "The availability of terabytes of RNA-Seq data and continuous emergence of new analysis tools, enable unprecedented biological insight. There is a pressing requirement for a framework that allows for fast, efficient, manageable, and reproducible RNA-Seq analysis. We have developed a Python package, (pyrpipe), that enables straightforward development of flexible, reproducible and easy-to-debug computational pipelines purely in Python, in an object-oriented manner. pyrpipe provides access to popular RNA-Seq tools, within Python, via high-level APIs. Pipelines can be customized by integrating new Python code, third-party programs, or Python libraries. Users can create checkpoints in the pipeline or integrate pyrpipe into a workflow management system, thus allowing execution on multiple computing environments, and enabling efficient resource management. pyrpipe produces detailed analysis, and benchmark reports which can be shared or included in publications. pyrpipe is implemented in Python and is compatible with Python versions 3.6 and higher. To illustrate the rich functionality of pyrpipe, we provide case studies using RNA-Seq data from GTEx, SARS-CoV-2-infected human cells, and ",

    "abstract": "Tumor tile selection is a necessary prerequisite in patch-based cancer whole slide image analysis, which is labor-intensive and requires expertise. Whole slides are annotated as tumor or tumor free, but tiles within a tumor slide are not. As all tiles within a tumor free slide are tumor free, these can be used to capture tumor-free patterns using the one-class learning strategy.\nWe present a Python package, termed OCTID, which combines a pretrained convolutional neural network (CNN) model,\u00a0Uniform Manifold Approximation and Projection (UMAP) and one-class support vector machine to achieve accurate tumor tile classification using a training set of tumor free tiles. Benchmarking experiments on four H&E image datasets achieved remarkable performance in terms of F1-score (0.90\u2009\u00b1\u20090.06), Matthews correlation coefficient (0.93\u2009\u00b1\u20090.05) and accuracy (0.94\u2009\u00b1\u20090.03).\nDetailed information can be found in the Supplementary File.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Considering animal welfare, animals should be kept in animal-appropriate and stress-free housing conditions in all circumstances. To assure such conditions, not only basic needs must be met, but also possibilities must be provided that allow animals in captive care to express all species-typical behaviors. Rack housing systems for snakes have become increasingly popular and are widely used; however, from an animal welfare perspective, they are no alternative to furnished terrariums. In this study, we therefore evaluated two types of housing systems for ball pythons (Python regius) by considering the welfare aspect animal behavior. In Part 1 of the study, ball pythons (n = 35) were housed individually in a conventional rack system. The pythons were provided with a hiding place and a water bowl, temperature control was automatic, and the lighting in the room served as indirect illumination. In Part 2 of the study, the same ball pythons, after at least 8 weeks, were housed individually in furnished terrariums. The size of each terrarium was correlated with the body length of each python. The terrariums contained substrate, a hiding place, possibilities for climbing, a water basin for bathing, an elevated basking spot, and living plants. The temperature was controlled automatically, and illumination was provided by a fluorescent tube and a UV lamp. The shown behavior spectrum differed significantly between the two housing systems (p < 0.05). The four behaviors basking, climbing, burrowing, and bathing could only be expressed in the terrarium. Abnormal behaviors that could indicate stereotypies were almost exclusively seen in the rack system. The results show that the housing of ball pythons in a rack system leads to a considerable restriction in species-typical behaviors; thus, the rack system does not meet the requirements for animal-appropriate housing.",

    "abstract": "Fischer plots are widely used in paleoenvironmental research as graphic representations of sea- and lake-level changes through mapping linearly corrected variation of accumulative cycle thickness over cycle number or stratum depth. Some kinds of paleoenvironmental proxy data (especially subsurface data, such as natural gamma-ray logging data), which preserve continuous cyclic signals and have been largely collected, are potential materials for constructing Fischer Plots. However, it is laborious to count the cycles preserved in these proxy data manually and map Fischer plots with these cycles. In this paper, we introduce an original open-source Python code \"PyFISCHERPLOT\" for constructing Fischer Plots in batches utilizing paleoenvironmental proxy data series. The principle of constructing Fischer plots based on proxy data, the data processing and usage of the PyFISCHERPLOT code and the application cases of the code are presented. The code is compared with existing methods for constructing Fischer plots.",

    "abstract": "Next generation sequencing enabled the fast accumulation of genomic data at public repositories. This technology also made it possible to better understand the regulation of gene expression by transcription factors (TFs) and various chromatin-associated proteins through the integration of chromatin immunoprecipitation (ChIP-Seq). The Cistrome Project has become one of the indispensable research portals for biologists to access and analyze data generated with thousands of ChIP-Seq experiments. Integrative motif analysis on shared binding regions among a set of experiments is not yet achievable despite a set of search and analysis tools provided by Cistrome via its web interface and the Galaxy framework.\nWe implemented a python command-line tool for searching binding sequences of a TF common to multiple ChIP-Seq experiments. We use the peaks in the Cistrome database as identified by MACS 2.0 for each experiment and identify shared peak regions in a genomic locus of interest. We then scan these regions for binding sequences using a binding motif of a TF obtained from the JASPAR database. MotifGenie is developed in collaboration with molecular biologists and its findings are corroborated by laboratory experiments.\nMotifGenie is freely available at https://github.com/ceragoguztuzun/MotifGenie.",

    "abstract": "Biology is a data-driven discipline facilitated greatly by computer programming skills. This article describes an introductory experiential programming activity that can be integrated into distance learning environments. Students are asked to develop their own Python programs to identify the nature of alleles linked to disease. This activity effectively engages students in a problem solving exercise that provides an opportunity for application of basic programming skills as well as understanding eukaryotic gene structure. We provide sets of mapped alleles for two well-known genes, CFTR and HFE, as well as a suite of relevant Python programs to achieve these outcomes or allow subsequent exercise modifications.",

    "abstract": "Merging Sanger sequences is frequently needed during the gene cloning process. In this study, we provide a Python script that is able to assemble multiple overlapping Sanger sequences. The script utilizes the overlapping regions within the tandem Sanger sequences to merge the Sanger sequences. The results demonstrate that the script can produce the merged sequence from the input Sanger sequences in a single run. The script offers a simple and free method for merging Sanger sequences and is useful for gene cloning.",

    "abstract": "Determining the best partition for a dataset can be a challenging task because of the lack of ",

    "abstract": "The Open Tree of Life project constructs a comprehensive, dynamic, and digitally available tree of life by synthesizing published phylogenetic trees along with taxonomic data. Open Tree of Life provides web-service application programming interfaces (APIs) to make the tree estimate, unified taxonomy, and input phylogenetic data available to anyone. Here, we describe the Python package opentree, which provides a user friendly Python wrapper for these APIs and a set of scripts and tutorials for straightforward downstream data analyses. We demonstrate the utility of these tools by generating an estimate of the phylogenetic relationships of all bird families, and by capturing a phylogenetic estimate for all taxa observed at the University of California Merced Vernal Pools and Grassland Reserve.[Evolution; open science; phylogenetics; Python; taxonomy.].",

    "abstract": "More than half of the Top 10 supercomputing sites worldwide use GPU accelerators and they are becoming ubiquitous in workstations and edge computing devices. GeNN is a C++ library for generating efficient spiking neural network simulation code for GPUs. However, until now, the full flexibility of GeNN could only be harnessed by writing model descriptions and simulation code in C++. Here we present PyGeNN, a Python package which exposes all of GeNN's functionality to Python with minimal overhead. This provides an alternative, arguably more user-friendly, way of using GeNN and allows modelers to use GeNN within the growing Python-based machine learning and computational neuroscience ecosystems. In addition, we demonstrate that, in both Python and C++ GeNN simulations, the overheads of recording spiking data can strongly affect runtimes and show how a new spike recording system can reduce these overheads by up to 10\u00d7. Using the new recording system, we demonstrate that by using PyGeNN on a modern GPU, we can simulate a full-scale model of a cortical column faster even than real-time neuromorphic systems. Finally, we show that long simulations of a smaller model with complex stimuli and a custom three-factor learning rule defined in PyGeNN can be simulated almost two orders of magnitude faster than real-time.",

    "abstract": "We introduce a new Python interface for the Cassandra Monte Carlo software, molecular simulation design framework (MoSDeF) Cassandra. MoSDeF Cassandra provides a simplified user interface, offers broader interoperability with other molecular simulation codes, enables the construction of programmatic and reproducible molecular simulation workflows, and builds the infrastructure necessary for high-throughput Monte Carlo studies. Many of the capabilities of MoSDeF Cassandra are enabled via tight integration with MoSDeF. We discuss the motivation and design of MoSDeF Cassandra and proceed to demonstrate both simple use-cases and more complex workflows, including adsorption in porous media and a combined molecular dynamics - Monte Carlo workflow for computing lateral diffusivity in graphene slit pores. The examples presented herein demonstrate how even relatively complex simulation workflows can be reduced to, at most, a few files of Python code that can be version-controlled and shared with other researchers. We believe this paradigm will enable more rapid research advances and represents the future of molecular simulations.",

    "abstract": "In smoking cessation clinical trials, timeline followback (TLFB) interviews are widely used to track daily cigarette consumption. However, there are no standard tools for calculating abstinence based on TLFB data. Individual research groups have to develop their own calculation tools, which is not only time- and resource-consuming but might also lead to variability in the data processing and calculation procedures.\nTo address these issues, we developed a novel open-source Python package named abstcal to calculate abstinence using TLFB data. This package provides data verification, duplicate and outlier detection, missing-data imputation, integration of biochemical verification data, and calculation of a variety of definitions of abstinence, including continuous, point-prevalence, and prolonged abstinence.\nWe verified the accuracy of the calculator using data derived from a clinical smoking cessation study. To improve the package's accessibility, we have made it available as a free web app.\nThe abstcal package is a reliable abstinence calculator with open-source access, providing a shared validated online tool to the addiction research field. We expect that this open-source abstinence calculation tool will improve the rigor and reproducibility of smoking and addiction research by standardizing TLFB-based abstinence calculation.\nAbstinence calculation is an essential task in any smoking intervention study. However, there have not been standard open-source tools available to the researchers. This commentary describes a Python-based package called abstcal that can calculate abstinence from TLFB data, a common methodology to collect smoking consumption data in research settings. The package supports the calculation of point-prevalence, prolonged, and continuous abstinence. Importantly, the package has a web app interface that allows researchers to use the tool without any coding experience. This tool will facilitate smoking research by providing a standardized and easy-to-use abstinence calculation tool.",

    "abstract": "In this paper, we present PyKrev, a Python library for the analysis of complex mixture Fourier transform mass spectrometry (FT-MS) data. PyKrev is a comprehensive suite of tools for analysis and visualization of FT-MS data after formula assignment has been performed. These comprise formula manipulation and calculation of chemical properties, intersection analysis between multiple lists of formulas, calculation of chemical diversity, assignment of compound classes to formulas, multivariate analysis, and a variety of visualization tools producing van Krevelen diagrams, class histograms, PCA score, and loading plots, biplots, scree plots, and UpSet plots. The library is showcased through analysis of hot water green tea extracts and Scotch whisky FT-ion cyclotron resonance-MS data sets. PyKrev addresses the lack of a single, cohesive toolset for researchers to perform FT-MS analysis in the Python programming environment encompassing the most recent data analysis techniques used in the field.",

    "abstract": "Duplication and recombination of protein fragments have led to the highly diverse protein space that we observe today. By mimicking this natural process, the design of protein chimeras via fragment recombination has proven experimentally successful and has opened a new era for the design of customizable proteins. The in silico building of structural models for these chimeric proteins, however, remains a manual task that requires a considerable degree of expertise and is not amenable for high-throughput studies. Energetic and structural analysis of the designed proteins often require the use of several tools, each with their unique technical difficulties and available in different programming languages or web servers.\nWe implemented a Python package that enables automated, high-throughput design of chimeras and their structural analysis. First, it fetches evolutionarily conserved fragments from a built-in database (also available at fuzzle.uni-bayreuth.de). These relationships can then be represented via networks or further selected for chimera construction via recombination. Designed chimeras or natural proteins are then scored and minimized with the Charmm and Amber forcefields and their diverse structural features can be analyzed at ease. Here, we showcase Protlego's pipeline by exploring the relationships between the P-loop and Rossmann superfolds, building and characterizing their offspring chimeras. We believe that Protlego provides a powerful new tool for the protein design community.\nProtlego runs on the Linux platform and is freely available at (https://hoecker-lab.github.io/protlego/) with tutorials and documentation.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "We present a structured illumination microscopy system that projects a hexagonal pattern by the interference among three coherent beams, suitable for implementation in a light-sheet geometry. Seven images acquired as the illumination pattern is shifted laterally can be processed to produce a super-resolved image that surpasses the diffraction-limited resolution by a factor of over 2 in an exemplar light-sheet arrangement. Three methods of processing data are discussed depending on whether the raw images are available in groups of seven, individually in a stream or as a larger batch representing a three-dimensional stack. We show that imaging axially moving samples can introduce artefacts, visible as fine structures in the processed images. However, these artefacts are easily removed by a filtering operation carried out as part of the batch processing algorithm for three-dimensional stacks. The reconstruction algorithms implemented in Python include specific optimizations for calculation on a graphics processing unit and we demonstrate its operation on experimental data of static objects and on simulated data of moving objects. We show that the software can process over 239 input raw frames per second at 512\u2009\u00d7\u2009512 pixels, generating over 34 super-resolved frames per second at 1024\u2009\u00d7\u20091024 pixels. This article is part of the Theo Murphy meeting issue 'Super-resolution structured illumination microscopy (part 1)'.",

    "abstract": "This article presents a script developed to evaluate resilience in energy systems. The files corresponding to the system description, simulation and metrics calculation are included in the dataset, as well as partial raw and processed data from the associated paper [1]. The model was developed focusing on covering all cogeneration and power plants, being the user responsible for describing the system, simulating and processing the data in the files here available. In the present work, the steps for the simulation are presented in detail, which contributes to other researchers that are interested in either adopting resilience as one of the possible system analyses or understanding the processes of metrics calculation of the associated paper.",

    "abstract": "The Empirical Mode Decomposition (EMD) package contains Python (>=3.5) functions for analysis of non-linear and non-stationary oscillatory time series. EMD implements a family of sifting algorithms, instantaneous frequency transformations, power spectrum construction and single-cycle feature analysis. These implementations are supported by online documentation containing a range of practical tutorials.",

    "abstract": "Microsatellite instability (MSI) is a common genomic alteration in colorectal cancer, endometrial carcinoma, and other solid tumors. MSI is characterized by a high degree of polymorphism in microsatellite lengths owing to the deficiency in the mismatch repair system. Based on the degree, MSI can be classified as microsatellite instability-high (MSI-H) and microsatellite stable (MSS). MSI is a predictive biomarker for immunotherapy efficacy in advanced/metastatic solid tumors, especially in colorectal cancer patients. Several computational approaches based on target panel sequencing data have been used to detect MSI; however, they are considerably affected by the sequencing depth and panel size.\nWe developed MSIFinder, a python package for automatic MSI classification, using random forest classifier (RFC)-based genome sequencing, which is a machine learning technology. We included 19 MSI-H and 25 MSS samples as training sets. First, we selected 54 feature markers from the training sets, built an RFC model, and validated the classifier using a test set comprising 21 MSI-H and 379 MSS samples. With this test set, MSIFinder achieved a sensitivity (recall) of 1.0, a specificity of 0.997, an accuracy of 0.998, a positive predictive value of 0.954, an F1 score of 0.977, and an area under the curve of 0.999. To further verify the robustness and effectiveness of the model, we used a prospective cohort consisting of 18 MSI-H samples and 122 MSS samples. MSIFinder achieved a sensitivity (recall) of 1.0 and a specificity of 1.0. We discovered that MSIFinder is less affected by a low sequencing depth and can achieve a concordance of 0.993 while exhibiting a sequencing depth of 100\u00d7. Furthermore, we realized that MSIFinder is less affected by the panel size and can achieve a concordance of 0.99 when the panel size is 0.5\u00a0M (million bases).\nThese results indicate that MSIFinder is a robust and effective MSI classification tool that can provide reliable MSI detection for scientific and clinical purposes.",

    "abstract": "With the appearance of publicly available, high-resolution, physiological datasets in neurocritical care, like Collaborative European NeuroTrauma Effectiveness Research in Traumatic Brain Injury (CENTER-TBI), there is a growing need for tools that could be used by clinical researchers to interrogate this information-rich data. The ICM+ software is widely used for processing data acquired from bedside monitors. Considering the growing popularity of scripting simple-syntax programming languages like Python, particularly among clinical researchers, we have developed an interface in ICM+ that provides a streamlined way of adding Python scripting functionality to the ICM+ calculation engine. The new interface imposes certain requirements on the scripts and needs an accompanying descriptor file that tells ICM+ about the functions implemented, so that they become available to the end user in the same way as native ICM+ functions. ICM+ also now includes a tool that eases the creation of Python functions to be imported. The Python extension works very efficiently, and any user with some degree of experience in scripting can use it to enrich capabilities of ICM+. Depending on the data analysed and calculations performed, Python functions are 15-60% slower than built-in ICM+ functions, which is a more-than-acceptable trade-off for empowering ICM+ with the unlimited analytical freedom offered by extensive Python libraries.",

    "abstract": "ProDy, an integrated application programming interface developed for modelling and analysing protein dynamics, has significantly evolved in recent years in response to the growing data and needs of the computational biology community. We present major developments that led to ProDy 2.0: (i) improved interfacing with databases and parsing new file formats, (ii) SignDy for signature dynamics of protein families, (iii) CryoDy for collective dynamics of supramolecular systems using cryo-EM density maps and (iv) essential site scanning analysis for identifying sites essential to modulating global dynamics.\nProDy is open-source and freely available under MIT License from https://github.com/prody/ProDy.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "The Metabolomics Workbench (MW) is a public scientific data repository consisting of experimental data and metadata from metabolomics studies collected with mass spectroscopy (MS) and nuclear magnetic resonance (NMR) analyses. MW has been constantly evolving; updating its 'mwTab' text file format, adding a JavaScript Object Notation (JSON) file format, implementing a REpresentational State Transfer (REST) interface, and nearly quadrupling the number of datasets hosted on the repository within the last three years. In order to keep up with the quickly evolving state of the MW repository, the 'mwtab' Python library and package have been continuously updated to mirror the changes in the 'mwTab' and JSONized formats and contain many new enhancements including methods for interacting with the MW REST interface, enhanced format validation features, and advanced features for parsing and searching for specific metabolite data and metadata. We used the enhanced format validation features to evaluate all available datasets in MW to facilitate improved curation and FAIRness of the repository. The 'mwtab' Python package is now officially released as version 1.0.1 and is freely available on GitHub and the Python Package Index (PyPI) under a Clear Berkeley Software Distribution (BSD) license with documentation available on ReadTheDocs.",

    "abstract": "SMITER (Synthetic mzML writer) is a Python-based command-line tool designed to simulate liquid-chromatography-coupled tandem mass spectrometry LC-MS/MS runs. It enables the simulation of any biomolecule amenable to mass spectrometry (MS) since all calculations are based on chemical formulas. SMITER features a modular design, allowing for an easy implementation of different noise and fragmentation models. By default, SMITER uses an established noise model and offers several methods for peptide fragmentation, and two models for nucleoside fragmentation and one for lipid fragmentation. Due to the rich Python ecosystem, other modules, e.g., for retention time (RT) prediction, can easily be implemented for the tailored simulation of any molecule of choice. This facilitates the generation of defined gold-standard LC-MS/MS datasets for any type of experiment. Such gold standards, where the ground truth is known, are required in computational mass spectrometry to test new algorithms and to improve parameters of existing ones. Similarly, gold-standard datasets can be used to evaluate analytical challenges, e.g., by predicting co-elution and co-fragmentation of molecules. As these challenges hinder the detection or quantification of co-eluents, a comprehensive simulation can identify and thus, prevent such difficulties before performing actual MS experiments. SMITER allows the creation of such datasets easily, fast, and efficiently.",

    "abstract": "The understanding of the tribological behavior of natural structures has been used as inspiration to design and optimize surfaces for diverse applications in engineering. In the present work, morphological, microstructural, mechanical and tribological characterization of the shed skin of two snake species, namely Boa Red Tail and Python Regius was carried out. Atomic Force Microscopy (AFM) and Scanning Electron Microscopy (SEM) analyses showed the existence of deterministic patterns, i.e., ordered arrays of geometrical features at the surface, while Transmission Electron Microscopy (TEM) allowed studying the internal structure and chemical composition of the skin sheds. Nanoindentation measurements showed significant variations in hardness and elastic modulus from the surface to the inner layers of the skin, and pin-on-disc tests revealed anisotropic behavior of the friction coefficient (COF) as a function of the sliding direction against balsa wood in dry conditions. Correlations between the friction data, nano-indentation mechanical properties and subsurface skin structure were established for both species taking into account the ways in which the skins' deterministic patterns influence the tribological performance.",

    "abstract": "Liquid chromatography tandem mass spectrometry (LC/MS) and other mass spectrometric technologies have been widely applied for triacylglycerol profiling. One challenge for targeted identification of fatty acyl moieties that constitute triacylglycerol species in biological samples is the numerous combinations of 3 fatty acyl groups that can form a triacylglycerol molecule. Manual determination of triacylglycerol structures based on peak intensities and retention time can be highly inefficient and error-prone. To resolve this, we have developed TAILOR-MS, a Python (programming language) package that aims at assisting: (1) the generation of targeted LC/MS methods for triacylglycerol detection and (2) automating triacylglycerol structural determination and prediction. To assess the performance of TAILOR-MS, we conducted LC/MS triacylglycerol profiling of bovine milk and two infant formulas. Our results confirmed dissimilarities between bovine milk and infant formula triacylglycerol composition. Furthermore, we identified 247 triacylglycerol species and predicted the possible existence of another 317 in the bovine milk sample, representing one of the most comprehensive reports on the triacylglycerol composition of bovine milk thus far. Likewise, we presented here a complete infant formula triacylglycerol profile and reported >200 triacylglycerol species. TAILOR-MS dramatically shortened the time required for triacylglycerol structural identification from hours to seconds and performed decent structural predictions in the absence of some triacylglycerol constituent peaks. Taken together, TAILOR-MS is a valuable tool that can greatly save time and improve accuracy for targeted LC/MS triacylglycerol profiling.",

    "abstract": "Learning low-dimensional representations (embeddings) of nodes in large graphs is key to applying machine learning on massive biological networks. Node2vec is the most widely used method for node embedding. However, its original Python and C++ implementations scale poorly with network density, failing for dense biological networks with hundreds of millions of edges. We have developed PecanPy, a new Python implementation of node2vec that uses cache-optimized compact graph data structures and precomputing/parallelization to result in fast, high-quality node embeddings for biological networks of all sizes and densities.\nPecanPy software is freely available at https://github.com/krishnanlab/PecanPy.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Data was mined with the help of an artificial intelligence system based on Python, data was collected, and a database was established using a Python crawler, and the relationship between the outcome of neurosurgery ICU patients and treatment using traditional Chinese medicine was ascertained through data management and statistical processing.\nThe source data cases (\nThere were statistical differences in 5 evaluation items (",

    "abstract": "The Najmanovich Research Group Toolkit for Elastic Networks (NRGTEN) is a Python toolkit that implements four different NMA models in addition to popular and novel metrics to benchmark and measure properties from these models. Furthermore, the toolkit is available as a public Python package and is easily extensible for the development or implementation of additional normal mode analysis models. The inclusion of the Elastic Network Contact Model developed in our group within NRGTEN is noteworthy, owing to its account for the specific chemical nature of atomic interactions.\nhttps://github.com/gregorpatof/nrgten_package/.",

    "abstract": "Quantitative characterization of biotechnological production processes requires the determination of different key performance indicators (KPIs) such as titer, rate and yield. Classically, these KPIs can be derived by combining black-box bioprocess modeling with non-linear regression for model parameter estimation. The presented pyFOOMB package enables a guided and flexible implementation of bioprocess models in the form of ordinary differential equation systems (ODEs). By building on Python as powerful and multi-purpose programing language, ODEs can be formulated in an object-oriented manner, which facilitates their modular design, reusability, and extensibility. Once the model is implemented, seamless integration and analysis of the experimental data is supported by various Python packages that are already available. In particular, for the iterative workflow of experimental data generation and subsequent model parameter estimation we employed the concept of replicate model instances, which are linked by common sets of parameters with global or local properties. For the description of multi-stage processes, discontinuities in the right-hand sides of the differential equations are supported via event handling using the freely available assimulo package. Optimization problems can be solved by making use of a parallelized version of the generalized island approach provided by the pygmo package. Furthermore, pyFOOMB in combination with Jupyter notebooks also supports education in bioprocess engineering and the applied learning of Python as scientific programing language. Finally, the applicability and strengths of pyFOOMB will be demonstrated by a comprehensive collection of notebook\u00a0examples.",

    "abstract": "Substructure screening is widely applied to evaluate the molecular potency and ADMET properties of compounds in drug discovery pipelines, and it can also be used to interpret QSAR models for the design of new compounds with desirable physicochemical and biological properties. With the continuous accumulation of more experimental data, data-driven computational systems which can derive representative substructures from large chemical libraries attract more attention. Therefore, the development of an integrated and convenient tool to generate and implement representative substructures is urgently needed.\nIn this study, PySmash, a user-friendly and powerful tool to generate different types of representative substructures, was developed. The current version of PySmash provides both a Python package and an individual executable program, which achieves ease of operation and pipeline integration. Three types of substructure generation algorithms, including circular, path-based and functional group-based algorithms, are provided. Users can conveniently customize their own requirements for substructure size, accuracy and coverage, statistical significance and parallel computation during execution. Besides, PySmash provides the function for external data screening.\nPySmash, a user-friendly and integrated tool for the automatic generation and implementation of representative substructures, is presented. Three screening examples, including toxicophore derivation, privileged motif detection and the integration of substructures with machine learning (ML) models, are provided to illustrate the utility of PySmash in safety profile evaluation, therapeutic activity exploration and molecular optimization, respectively. Its executable program and Python package are available at https://github.com/kotori-y/pySmash.",

    "abstract": "For differential expression studies in all omics disciplines, data normalization is a crucial step that is often subject to a balance between speed and effectiveness. To keep up with the data produced by high-throughput instruments, researchers require fast and easy-to-use yet effective methods that fit into automated analysis pipelines. The CONSTANd normalization method meets these criteria, so we have made its source code available for R/BioConductor and Python. We briefly review the method and demonstrate how it can be used in different omics contexts for experiments of any scale. Widespread adoption across omics disciplines would ease data integration in multiomics experiments.",

    "abstract": "We achieve a significant improvement in thermodynamic-based flux analysis (TFA) by introducing multivariate treatment of thermodynamic variables and leveraging component contribution, the state-of-the-art implementation of the group contribution methodology. Overall, the method greatly reduces the uncertainty of thermodynamic variables.\nWe present multiTFA, a Python implementation of our framework. We evaluated our application using the core Escherichia coli model and achieved a median reduction of 6.8\u2009kJ/mol in reaction Gibbs free energy ranges, while three out of 12 reactions in glycolysis changed from reversible to irreversible.\nOur framework along with documentation is available on https://github.com/biosustain/multitfa.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Emotion recognition plays an important role in intelligent human-computer interaction, but the related research still faces the problems of low accuracy and subject dependence. In this paper, an open-source software toolbox called MindLink-Eumpy is developed to recognize emotions by integrating electroencephalogram (EEG) and facial expression information. MindLink-Eumpy first applies a series of tools to automatically obtain physiological data from subjects and then analyzes the obtained facial expression data and EEG data, respectively, and finally fuses the two different signals at a decision level. In the detection of facial expressions, the algorithm used by MindLink-Eumpy is a multitask convolutional neural network (CNN) based on transfer learning technique. In the detection of EEG, MindLink-Eumpy provides two algorithms, including a subject-dependent model based on support vector machine (SVM) and a subject-independent model based on long short-term memory network (LSTM). In the decision-level fusion, weight enumerator and AdaBoost technique are applied to combine the predictions of SVM and CNN. We conducted two offline experiments on the Database for Emotion Analysis Using Physiological Signals (DEAP) dataset and the Multimodal Database for Affect Recognition and Implicit Tagging (MAHNOB-HCI) dataset, respectively, and conducted an online experiment on 15 healthy subjects. The results show that multimodal methods outperform single-modal methods in both offline and online experiments. In the subject-dependent condition, the multimodal method achieved an accuracy of 71.00% in the valence dimension and an accuracy of 72.14% in the arousal dimension. In the subject-independent condition, the LSTM-based method achieved an accuracy of 78.56% in the valence dimension and an accuracy of 77.22% in the arousal dimension. The feasibility and efficiency of MindLink-Eumpy for emotion recognition is thus demonstrated.",

    "abstract": "Machine Learning-assisted Lipid Phase Analysis (MLLPA) is a new Python 3 module developed to analyze phase domains in a lipid membrane based on lipid molecular states. Reading standard simulation coordinate and trajectory files, the software first analyze the phase composition of the lipid membrane by using machine learning tools to label each individual molecules with respect to their state, and then decompose the simulation box using Voronoi tessellations to analyze the local environment of all the molecules of interest. MLLPA is versatile as it can read from multiple format (e.g., GROMACS, LAMMPS) and from either all-atom (e.g., CHARMM36) or coarse-grain models (e.g., Martini). It can also analyze multiple geometries of membranes (e.g., bilayers, vesicles). Finally, the software allows for training with more than two phases, allowing for multiple phase coexistence analysis.",

    "abstract": "High-dimensional data are pervasive in this bigdata era. To avoid the curse of the dimensionality problem, various dimensionality reduction (DR) algorithms have been proposed. To facilitate systematic DR quality comparison and assessment, this paper reviews related metrics and develops an open-source Python package pyDRMetrics. Supported metrics include reconstruction error, distance matrix, residual variance, ranking matrix, co-ranking matrix, trustworthiness, continuity, co-k-nearest neighbor size, LCMC (local continuity meta criterion), and rank-based local/global properties. pyDRMetrics provides a native Python class and a web-oriented API. A case study of mass spectra is conducted to demonstrate the package functions. A web GUI wrapper is also published to support user-friendly B/S applications.",

    "abstract": "Echocardiographic evaluation is a diagnostic tool for the in vivo diagnosis of heart diseases. Specific and unique anatomical characteristics of the ophidian heart such as the single ventricular cavity, a tubular sinus venosus opening into the right atrium, the presence of three arterial trunks and extreme mobility in the coelomic cavity during the cardiac cycle directly affect echocardiographic examination. Twenty-one awake, healthy ball pythons (Python regius) were analysed based on guidelines for performing echocardiographic examinations. Imaging in the sagittal plane demonstrated the caudal vena cava, sinus venosus valve (SVV) and right atrium and the various portions of the ventricle, horizontal septum, left aortic arch and pulmonary artery. Transverse imaging depicted the spatial relationship of the left and right aortic arches, the pulmonary artery and the horizontal septum. Basic knowledge of cardiac blood flow in reptiles is necessary to understand the echocardiographic anatomy. The flow of the arterial trunks and SVV was analysed using pulsed-wave Doppler based on the approach used for humans and companion mammals. The walls and diameters of the cavum arteriosum, cavum venosum and cavum pulmonale were also evaluated. This study should improve the veterinarian's knowledge of ophidian heart basal physiology and contribute to the development of cardiology in reptiles.",

    "abstract": "Deep learning is becoming increasingly popular and available to new users, particularly in the medical field. Deep learning image segmentation, outcome analysis, and generators rely on presentation of Digital Imaging and Communications in Medicine (DICOM) images and often radiation therapy (RT) structures as masks. Although the technology to convert DICOM images and RT structures into other data types exists, no purpose-built Python module for converting NumPy arrays into RT structures exists. The 2 most popular deep learning libraries, Tensorflow and PyTorch, are both implemented within Python, and we believe a set of tools built in Python for manipulating DICOM images and RT structures would be useful and could save medical researchers large amounts of time and effort during the preprocessing and prediction steps. Our module provides intuitive methods for rapid data curation of RT-structure files by identifying unique region of interest (ROI) names and ROI structure locations and allowing multiple ROI names to represent the same structure. It is also capable of converting DICOM images and RT structures into NumPy arrays and SimpleITK Images, the most commonly used formats for image analysis and inputs into deep learning architectures and radiomic feature calculations. Furthermore, the tool provides a simple method for creating a DICOM RT-structure from predicted NumPy arrays, which are commonly the output of semantic segmentation deep learning models. Accessing DicomRTTool via the public Github project invites open collaboration, and the deployment of our module in PyPi ensures painless distribution and installation. We believe our tool will be increasingly useful as deep learning in medicine progresses.",

    "abstract": "We present Python Statistical Analysis of Turbulence (P-SAT), a lightweight, Python framework that can automate the process of parsing, filtering, computation of various turbulent statistics, spectra computation for steady flows. P-SAT framework is capable to work with single as well as on batch inputs. The framework quickly filters the raw velocity data using various methods like velocity correlation, signal-to-noise ratio (SNR), and acceleration thresholding method in order to de-spike the velocity signal of steady flows. It is flexible enough to provide default threshold values in methods like correlation, SNR, acceleration thresholding and also provide the end user with an option to provide a user defined value. The framework generates a .csv file at the end of the execution, which contains various turbulent parameters mentioned earlier. The P-SAT framework can handle velocity time series of steady flows as well as unsteady flows. The P-SAT framework is capable to obtain mean velocities from instantaneous velocities of unsteady flows by using Fourier-component based averaging method. Since P-SAT framework is developed using Python, it can be deployed and executed across the widely used operating systems. The GitHub link for the P-SAT framework is: https://github.com/mayank265/flume.git .",

    "abstract": "In this work, we showcase SGTPy, a Python open-source code developed to calculate interfacial properties (interfacial concentration profiles and interfacial or surface tension) for pure fluids and fluid mixtures. SGTPy employs the Square Gradient Theory (SGT) coupled to the Statistical Associating Fluid Theory of Variable Range employing a Mie potential (SAFT-VR-Mie). SGTPy uses standard Python numerical packages (i.e., NumPy, SciPy) and can be used under Jupyter notebooks. Its features are the calculation of phase stability, phase equilibria, interfacial properties, and the optimization of the SGT and SAFT parameters for vapor-liquid, liquid-liquid and vapor-liquid-liquid equilibria for pure fluids and multicomponent mixtures. Phase equilibrium calculations include two-phase and multiphase flash, bubble and dew points, and the tangent plane distance. For the computation of interfacial properties, SGTPy incorporates several options to solve the interfacial concentration, such as the path technique, an auxiliary time function, and orthogonal collocation. Additionally, the SGTPy code allows the inclusion of subroutines from other languages (e.g., Fortran, and C++) through Cython and f2py Python tools, which opens the possibility for future extensions or recycling tested and optimized subroutines from other codes. Supporting Information includes a review of the theoretical expressions required to couple SAFT-VR-Mie equation of state with the SGT. The use and capabilities of SGTPy are illustrated through step by step examples written on Jupyter notebooks for the cases of pure fluids and binary and ternary mixtures in bi- and three- phasic equilibria. The SGTPy code can be downloaded from https://github.com/gustavochm/SGTPy.",

    "abstract": "To describe the surgical repair of traumatic complete spectaculectomy and keratomalacia in a snake.\nA 10.5-year-old, female, Boelen's python (Simalia boeleni) was presented with iatrogenic, near-complete spectaculectomy associated with bacterial keratitis, keratomalacia, and hypopyon.\nCorneal samples for cytological evaluation and bacterial culture were collected. Following medical stabilization of the bacterial keratitis, a double-layered dry amniotic membrane graft was placed. The first amniotic membrane layer was placed over the cornea with the edges tucked under the peripheral remnants of the spectacle and secured in place with fibrin glue. The second amniotic membrane layer was placed over the entirety of the spectacle remnant and secured in place with a combination of fibrin glue and sutures. Topical and systemic antimicrobials, topical ophthalmic lubricants, and systemic non-steroidal anti-inflammatory therapy were administered postoperatively.\nHeterophilic keratitis was identified by cytology and Enterobacter cloacae, Pseudomonas aeruginosa, and Staphylococcus sciuri were cultured from the corneal samples. The amniotic membrane grafts remained in place for several weeks. At 4\u00a0months postoperatively, the spectacle was completely regenerated, the subspectacular space restored, and the cornea was transparent. Spectacular vascularization and fibrosis then slowly cleared over the following 6\u00a0months.\nAmniotic membrane grafting with fibrin glue is a relatively simple and effective surgical method to reconstruct extensive defects in the reptilian spectacle and to assist in the management of bacterial keratitis associated with spectacle avulsion.",

    "abstract": "Python language has become the most popular computer language. Python is widely adopted in computer courses. However, Python language's effects on the college and university students' learning performance, motivations, computer programming self-efficacy, and maladaptive cognition have still not been widely examined. The main objective of this study is to explore the effects of learning Python on students' programming learning. The junior students of two classes in a college are the research participants. One class was taught Java language and the other class was taught Python language. The learning performance, motivations, and maladaptive cognition in the two classes were compared to evaluate the differences. The results showed that the motivations, computer programming self-efficacy, and maladaptive cognition on the learning performance were significant in the Python class. The results and findings of this study can be used in Python course arrangement and development.",

    "abstract": "The effects of \u0394Pb-CATH4, a cathelicidin derived from Python bivittatus, were evaluated against Staphylococcus aureus-infected wounds in mice. These effects were comparable to those of classical antibiotics. \u0394Pb-CATH4 was resistant to bacterial protease but not to porcine trypsin. A reduction in the level of inflammatory cytokines and an increase in the migration of immune cells was observed in vitro. Thus, \u0394Pb-CATH4 can promote wound healing by controlling infections including those caused by multidrug-resistant bacteria via its immunomodulatory effects.",

    "abstract": "Hi-C is the most widely used assay for investigating genome-wide 3D organization of chromatin. When working with Hi-C data, it is often useful to calculate the similarity between contact matrices in order to assess experimental reproducibility or to quantify relationships among Hi-C data from related samples. The HiCRep algorithm has been widely adopted for this task, but the existing R implementation suffers from run time limitations on high-resolution Hi-C data or on large single-cell Hi-C datasets.\nWe introduce a Python implementation of HiCRep and demonstrate that it is much faster and consumes much less memory than the existing R implementation. Furthermore, we give examples of HiCRep's ability to accurately distinguish replicates from non-replicates and to reveal cell type structure among collections of Hi-C data.\nHiCRep.py and its documentation are available with a GPL license at https://github.com/Noble-Lab/hicrep. The software may be installed automatically using the pip package installer.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Next generation sequencing (NGS) has promising applications in transfusion medicine. Exome sequencing (ES) is increasingly used in the clinical setting, and blood group interpretation is an additional value that could be extracted from existing data sets. We provide the first release of an open-source software tailored for this purpose and describe its validation with three blood group systems.\nThe DTM-Tools algorithm was designed and used to analyse 1018 ES NGS files from the ClinSeq\nOf 116 genomic variants queried, those corresponding to 18 known KEL, FY and JK alleles were identified in this cohort. 596 additional exonic variants were identified KEL, ACKR1 and SLC14A1, including 58 predicted frameshifts. Software predictions were validated by serology in 108 participants; one case in the FY blood group and three cases in the JK blood group were discrepant. Investigation revealed that these discrepancies resulted from (1) clerical error, (2) serologic failure to detect weak antigenic expression and (3) a frameshift variant absent in blood group databases.\nDTM-Tools can be employed for rapid Kell, Duffy and Kidd blood group antigen prediction from existing ES data sets; for discrepancies detected in the validation data set, software predictions proved accurate. DTM-Tools is open-source and in continuous development.",

    "abstract": "Commercial hyperspectral imagers (HSIs) are expensive and thus unobtainable for large audiences or research groups with low funding. In this study, we used an existing do-it-yourself push-broom HSI design for which we provide software to correct for spectral smile aberration without using an optical laboratory. The software also corrects an aberration which we call tilt. The tilt is specific for the particular imager design used, but correcting it may be beneficial for other similar devices. The tilt and spectral smile were reduced to zero in terms of used metrics. The software artifact is available as an open-source Github repository. We also present improved casing for the imager design, and, for those readers interested in building their own HSI, we provide print-ready and modifiable versions of the 3D-models required in manufacturing the imager. To our best knowledge, solving the spectral smile correction problem without an optical laboratory has not been previously reported. This study re-solved the problem with simpler and cheaper tools than those commonly utilized. We hope that this study will promote easier access to hyperspectral imaging for all audiences regardless of their financial status and availability of an optical laboratory.",

    "abstract": "NeuroKit2 is an open-source, community-driven, and user-centered Python package for neurophysiological signal processing. It provides a comprehensive suite of processing routines for a variety of bodily signals (e.g., ECG, PPG, EDA, EMG, RSP). These processing routines include high-level functions that enable data processing in a few lines of code using validated pipelines, which we illustrate in two examples covering the most typical scenarios, such as an event-related paradigm and an interval-related analysis. The package also includes tools for specific processing steps such as rate extraction and filtering methods, offering a trade-off between high-level convenience and fine-tuned control. Its goal is to improve transparency and reproducibility in neurophysiological research, as well as foster exploration and innovation. Its design philosophy is centred on user-experience and accessibility to both novice and advanced users.",

    "abstract": "The gut microbiota is the human body's largest population of microorganisms that interact with human intestinal cells. They use ingested nutrients for fundamental biological processes and have important impacts on human physiology, immunity and metabolome in the gastrointestinal tract.\nHere, we present M2R, a Python add-on to cobrapy that allows incorporating information about the gut microbiota metabolism models to human genome-scale metabolic models (GEMs) like RECON3D. The idea behind the software is to modify the lower bounds of the exchange reactions in the model using aggregated in- and out-fluxes from selected microbes. M2R enables users to quickly and easily modify the pool of the metabolites that enter and leave the GEM, which is particularly important for those looking into an analysis of the metabolic interaction between the gut microbiota and human cells and its dysregulation.\nM2R is freely available under an MIT License at https://github.com/e-weglarz-tomczak/m2r.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Mathematical models of metabolic networks utilize simulation to study system-level mechanisms and functions. Various approaches have been used to model the steady state behavior of metabolic networks using genome-scale reconstructions, but formulating dynamic models from such reconstructions continues to be a key challenge. Here, we present the Mass Action Stoichiometric Simulation Python (MASSpy) package, an open-source computational framework for dynamic modeling of metabolism. MASSpy utilizes mass action kinetics and detailed chemical mechanisms to build dynamic models of complex biological processes. MASSpy adds dynamic modeling tools to the COnstraint-Based Reconstruction and Analysis Python (COBRApy) package to provide an unified framework for constraint-based and kinetic modeling of metabolic networks. MASSpy supports high-performance dynamic simulation through its implementation of libRoadRunner: the Systems Biology Markup Language (SBML) simulation engine. Three examples are provided to demonstrate how to use MASSpy: (1) a validation of the MASSpy modeling tool through dynamic simulation of detailed mechanisms of enzyme regulation; (2) a feature demonstration using a workflow for generating ensemble of kinetic models using Monte Carlo sampling to approximate missing numerical values of parameters and to quantify biological uncertainty, and (3) a case study in which MASSpy is utilized to overcome issues that arise when integrating experimental data with the computation of functional states of detailed biological mechanisms. MASSpy represents a powerful tool to address challenges that arise in dynamic modeling of metabolic networks, both at small and large scales.",

    "abstract": "Ball pythons (",

    "abstract": "Protein immobilization, while widespread to unlock enzyme potential in biocatalysis, remains tied to a trial an error approach. Nonetheless, several databases and computational methods have been developed for protein characterization and their study. CapiPy is a user-friendly application for protein model creation and subsequent analysis with a special focus on the ease of use and interpretation of the results to help the users to make an informed decision on the immobilization approach which should be ideal for a protein of interest. The package has been tested with three separate random sets of 150 protein sequences from Uniprot with more than a 70% overall success rate (see Supplementary information and Supplementary Dataset).\nThe package is free to use under the GNU General Public License v3.0. All necessary files can be downloaded from https://github.com/drou0302/CapiPy or https://pypi.org/project/CapiPy/. All external requirements are also freely available, with some restrictions for non-academic users.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Metabolic Engineering aims to favour the overproduction of native, as well as non-native, metabolites by modifying or extending the cellular processes of a specific organism. In this context, Computational Strain Optimization (CSO) plays a relevant role by putting forward mathematical approaches able to identify potential metabolic modifications to achieve the defined production goals. We present MEWpy, a Python workbench for metabolic engineering, which covers a wide range of metabolic and regulatory modelling approaches, as well as phenotype simulation and CSO algorithms.\nMEWpy can be installed from PyPi (pip install mewpy), the source code being available at https://github.com/BioSystemsUM/mewpy under the GPL license.",

    "abstract": "One of the important steps in initial data processing of peptide mass spectra is the detection of peptide features in full-range mass spectra. Ion mobility offers advantages over previous methods performing this detection by providing an additional structure-specific separation dimension. However, there is a lack of open-source software that utilizes these advantages and detects peptide features in mass spectra acquired along with ion mobility data using new instruments such as timsTOF and/or FAIMS-Orbitrap.\nRecently, a utility called Dinosaur was presented, which provides an efficient way for feature detection in peptide ion mass spectra. In this work we extended its functionality by developing Biosaur software to fully employ the additional information provided by ion mobility data. Biosaur was developed using the Python 3.8 programming language.\nBiosaur supports the processing of data acquired using mass spectrometers with ion mobility capabilities, specifically timsTOF and FAIMS. In addition, it processes mass spectra obtained in negative ion mode and reports cosine correlation table for peptide features which is useful for differentiation between in-source fragments and semi-tryptic peptides.\nBiosaur is a utility for detecting peptide features in liquid chromatography-mass spectra with ion mobility and negative ion supports. The software is distributed with an open-source APACHE 2.0 license and is freely available on Github: https://github.com/abdrakhimov1/Biosaur.",

    "abstract": "Boid inclusion body disease (BIBD) causes losses in captive snake populations globally. BIBD is associated with the formation of cytoplasmic inclusion bodies (IBs), which mainly comprise reptarenavirus nucleoprotein (NP). In 2017, BIBD was reproduced by cardiac injection of boas and pythons with reptarenaviruses, thus demonstrating a causative link between reptarenavirus infection and the disease. Here, we report experimental infections of ",

    "abstract": "Root mean square displacement (RMSD) calculations play a fundamental role in the comparison of different conformers of the same ligand. This is particularly important in the evaluation of protein-ligand docking, where different ligand poses are generated by docking software and their quality is usually assessed by RMSD calculations. Unfortunately, many RMSD calculation tools do not take into account the symmetry of the molecule, remain difficult to integrate flawlessly in cheminformatics and machine learning pipelines-which are often written in Python-or are shipped within large code bases. Here we present a new open-source RMSD calculation tool written in Python, designed to be extremely lightweight and easy to integrate into existing software.",

    "abstract": "In studies of cognitive neuroscience, multivariate pattern analysis (MVPA) is widely used as it offers richer information than traditional univariate analysis. Representational similarity analysis (RSA), as one method of MVPA, has become an effective decoding method based on neural data by calculating the similarity between different representations in the brain under different conditions. Moreover, RSA is suitable for researchers to compare data from different modalities and even bridge data from different species. However, previous toolboxes have been made to fit specific datasets. Here, we develop NeuroRA, a novel and easy-to-use toolbox for representational analysis. Our toolbox aims at conducting cross-modal data analysis from multi-modal neural data (e.g., EEG, MEG, fNIRS, fMRI, and other sources of neruroelectrophysiological data), behavioral data, and computer-simulated data. Compared with previous software packages, our toolbox is more comprehensive and powerful. Using NeuroRA, users can not only calculate the representational dissimilarity matrix (RDM), which reflects the representational similarity among different task conditions and conduct a representational analysis among different RDMs to achieve a cross-modal comparison. Besides, users can calculate neural pattern similarity (NPS), spatiotemporal pattern similarity (STPS), and inter-subject correlation (ISC) with this toolbox. NeuroRA also provides users with functions performing statistical analysis, storage, and visualization of results. We introduce the structure, modules, features, and algorithms of NeuroRA in this paper, as well as examples applying the toolbox in published datasets.",

    "abstract": "Since the first human genome was sequenced in 2001, there has been a rapid growth in the number of bioinformatic methods to process and analyze next-generation sequencing (NGS) data for research and clinical studies that aim to identify genetic variants influencing diseases and traits. To achieve this goal, one first needs to call genetic variants from NGS data, which requires multiple computationally intensive analysis steps. Unfortunately, there is a lack of an open-source pipeline that can perform all these steps on NGS data in a manner, which is fully automated, efficient, rapid, scalable, modular, user-friendly and fault tolerant. To address this, we introduce xGAP, an extensible Genome Analysis Pipeline, which implements modified GATK best practice to analyze DNA-seq data with the aforementioned functionalities.\nxGAP implements massive parallelization of the modified GATK best practice pipeline by splitting a genome into many smaller regions with efficient load-balancing to achieve high scalability. It can process 30\u00d7 coverage whole-genome sequencing (WGS) data in \u223c90\u00a0min. In terms of accuracy of discovered variants, xGAP achieves average F1 scores of 99.37% for single nucleotide variants and 99.20% for insertion/deletions across seven benchmark WGS datasets. We achieve highly consistent results across multiple on-premises (SGE & SLURM) high-performance clusters. Compared to the Churchill pipeline, with similar parallelization, xGAP is 20% faster when analyzing 50\u00d7 coverage WGS on Amazon Web Service. Finally, xGAP is user-friendly and fault tolerant where it can automatically re-initiate failed processes to minimize required user intervention.\nxGAP is available at https://github.com/Adigorla/xgap.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Structural determination of molecular complexes by cryo-EM requires large, often complex processing of the image data that are initially obtained. Here, TEMPy2, an update of the TEMPy package to process, optimize and assess cryo-EM maps and the structures fitted to them, is described. New optimization routines, comprehensive automated checks and workflows to perform these tasks are described.",

    "abstract": "We present ",

    "abstract": "Electrical brain activity related to external stimulation and internal mental events can be measured at the scalp as tiny time-varying electric potential waveforms (electroencephalogram; EEG), typically a few tens of microvolts peak to peak (Berger, 1930). Even tinier brain responses, too small to be seen by naked eye in the EEG, can be detected by repeating the stimulation, aligning the EEG recordings to the triggering event and averaging them at each time point (Dawson, 1951, 1954). Under assumptions that the brain response (signal) is the same in each recording and the ongoing background EEG (noise) varies randomly, averaging improves the estimate of the \"true\" brain response at each time point as the random variation cancels. The average event-related brain potential (ERP) and its counterpart for event-related magnetic fields (ERFs) are cornerstones of experimental brain research in human sensation, perception, and cognition (Luck & Kappenman, 2013). Smith and Kutas pointed out that the average ERP at each time ",

    "abstract": "For high-dimensional supervised learning, it is often beneficial to use domain-specific knowledge to improve the performance of statistical learning models. When the problem contains covariates which form groups, researchers can include this grouping information to find parsimonious representations of the relationship between covariates and targets. These groups may arise artificially, as from the polynomial expansion of a smaller feature space, or naturally, as from the anatomical grouping of different brain regions or the geographical grouping of different cities. When the number of features is large compared to the number of observations, one seeks a subset of the features which is sparse at both the group and global level.",

    "abstract": "Gene clusters are sets of co-localized, often contiguous genes that together perform specific functions, many of which are relevant to biotechnology. There is a need for software tools that can extract candidate gene clusters from vast amounts of available genomic data. Therefore, we developed Opfi: a modular pipeline for identification of arbitrary gene clusters in assembled genomic or metagenomic sequences. Opfi contains functions for annotation, de-deduplication, and visualization of putative gene clusters. It utilizes a customizable rule-based filtering approach for selection of candidate systems that adhere to user-defined criteria. Opfi is implemented in Python, and is available on the Python Package Index and on Bioconda (Gr\u00fcning et al., 2018).",

    "abstract": "The proportion of silent (AMPAR-lacking) synapses is thought to be related to the plasticity potential of neural networks. We created a maximum-likelihood estimator of silent synapse fraction based on simulations of the underlying experimental methodology. Here, we provide a set of guidelines for running a Python package on compatible experimental synaptic data. Compared with traditional failure-rate approaches, this synthetic likelihood estimator improves the validity and accuracy of the estimates of the silent synapse fraction. For complete details on the use and execution of this protocol, please refer to Lynn et\u00a0al. (2020).",

    "abstract": "To determine the effects of dexmedetomidine, doxapram, and dexmedetomidine plus doxapram on ventilation ([Formula: see text]e), breath frequency, and tidal volume (Vt) in ball pythons (\n14 ball pythons.\nRespiratory effects of dexmedetomidine and doxapram were assessed with whole-body, closed-chamber plethysmography, which allowed for estimates of [Formula: see text]e and Vt. In the first experiment of this study with a complete crossover design, snakes were injected, SC, with saline (0.9% NaCl) solution, dexmedetomidine (0.1 mg/kg), doxapram (10 mg/kg), or dexmedetomidine and doxapram, and breath frequency, [Formula: see text]e, and Vt were measured before and every 30 minutes thereafter, through 240 minutes. In the second experiment, antinociceptive efficacy of saline solution, dexmedetomidine, and dexmedetomidine plus doxapram was assessed by measuring thermal withdrawal latencies before and 60 minutes after SC injection.\nDexmedetomidine significantly decreased breath frequency and increased Vt but did not affect [Formula: see text]e at all time points, compared with baseline. Doxapram significantly increased [Formula: see text]e, breath frequency, and Vt at 60 minutes after injection, compared with saline solution. The combination of dexmedetomidine and doxapram, compared with dexmedetomidine alone, significantly increased [Formula: see text]e at 30 and 60 minutes after injection and did not affect breath frequency and Vt at all time points. Thermal withdrawal latencies significantly increased when snakes received dexmedetomidine or dexmedetomidine plus doxapram, versus saline solution.\nConcurrent administration of doxapram may mitigate the dexmedetomidine-induced reduction of breathing frequency without disrupting thermal antinociceptive efficacy in ball pythons.",

    "abstract": "IOData is a free and open-source Python library for parsing, storing, and converting various file formats commonly used by quantum chemistry, molecular dynamics, and plane-wave density-functional-theory software programs. In addition, IOData supports a flexible framework for generating input files for various software packages. While designed and released for stand-alone use, its original purpose was to facilitate the interoperability of various modules in the HORTON and ChemTools software packages with external (third-party) molecular quantum chemistry and solid-state density-functional-theory packages. IOData is designed to be easy to use, maintain, and extend; this is why we wrote IOData in Python and adopted many principles of modern software development, including comprehensive documentation, extensive testing, continuous integration/delivery protocols, and package management. This article is the official release note of the IOData library.",

    "abstract": "High-throughput screening yields vast amounts of biological data which can be highly challenging to interpret. In response, knowledge-driven approaches emerged as possible solutions to analyze large datasets by leveraging prior knowledge of biomolecular interactions represented in the form of biological networks. Nonetheless, given their size and complexity, their manual investigation quickly becomes impractical. Thus, computational approaches, such as diffusion algorithms, are often employed to interpret and contextualize the results of high-throughput experiments. Here, we present MultiPaths, a framework consisting of two independent Python packages for network analysis. While the first package, DiffuPy, comprises numerous commonly used diffusion algorithms applicable to any generic network, the second, DiffuPath, enables the application of these algorithms on multi-layer biological networks. To facilitate its usability, the framework includes a command line interface, reproducible examples and documentation. To demonstrate the framework, we conducted several diffusion experiments on three independent multi-omics datasets over disparate networks generated from pathway databases, thus, highlighting the ability of multi-layer networks to integrate multiple modalities. Finally, the results of these experiments demonstrate how the generation of harmonized networks from disparate databases can improve predictive performance with respect to individual resources.\nDiffuPy and DiffuPath are publicly available under the Apache License 2.0 at https://github.com/multipaths.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "FASTA and FASTQ are the most widely used biological data formats that have become the de facto standard to exchange sequence data between bioinformatics tools. With the avalanche of next-generation sequencing data, the amount of sequence data being deposited and accessed in FASTA/Q formats is increasing dramatically. However, the existing tools have very low efficiency at random retrieval of subsequences due to the requirement of loading the entire index into memory. In addition, most existing tools have no capability to build index for large FASTA/Q files because of the limited memory. Furthermore, the tools do not provide support to randomly accessing sequences from FASTA/Q files compressed by gzip, which is extensively adopted by most public databases to compress data for saving storage. In this study, we developed pyfastx as a versatile Python package with commonly used command-line tools to overcome the above limitations. Compared to other tools, pyfastx yielded the highest performance in terms of building index and random access to sequences, particularly when dealing with large FASTA/Q files with hundreds of millions of sequences. A key advantage of pyfastx over other tools is that it offers an efficient way to randomly extract subsequences directly from gzip compressed FASTA/Q files without needing to uncompress beforehand. Pyfastx can easily be installed from PyPI (https://pypi.org/project/pyfastx) and the source code is freely available at https://github.com/lmdu/pyfastx.",

    "abstract": "Task-trained artificial recurrent neural networks (RNNs) provide a computational modeling framework of increasing interest and application in computational, systems, and cognitive neuroscience. RNNs can be trained, using deep-learning methods, to perform cognitive tasks used in animal and human experiments and can be studied to investigate potential neural representations and circuit mechanisms underlying cognitive computations and behavior. Widespread application of these approaches within neuroscience has been limited by technical barriers in use of deep-learning software packages to train network models. Here, we introduce PsychRNN, an accessible, flexible, and extensible Python package for training RNNs on cognitive tasks. Our package is designed for accessibility, for researchers to define tasks and train RNN models using only Python and NumPy, without requiring knowledge of deep-learning software. The training backend is based on TensorFlow and is readily extensible for researchers with TensorFlow knowledge to develop projects with additional customization. PsychRNN implements a number of specialized features to support applications in systems and cognitive neuroscience. Users can impose neurobiologically relevant constraints on synaptic connectivity patterns. Furthermore, specification of cognitive tasks has a modular structure, which facilitates parametric variation of task demands to examine their impact on model solutions. PsychRNN also enables task shaping during training, or curriculum learning, in which tasks are adjusted in closed-loop based on performance. Shaping is ubiquitous in training of animals in cognitive tasks, and PsychRNN allows investigation of how shaping trajectories impact learning and model solutions. Overall, the PsychRNN framework facilitates application of trained RNNs in neuroscience research.",

    "abstract": "Although several bioinformatics tools have been developed to examine signaling pathways, little attention has been given to ever long-distance crosstalk mechanisms. Here, we developed PETAL, a Python tool that automatically explores and detects the most relevant nodes within a KEGG pathway, scanning and performing an in-depth search. PETAL can contribute to discovering novel therapeutic targets or biomarkers that are potentially hidden and not considered in the network under study.\nPETAL is a freely available open-source software. It runs on all platforms that support Python3. The user manual and source code are accessible from https://github.com/Pex2892/PETAL.",

    "abstract": "Parallelization in Python integrates Message Passing Interface via the mpi4py module. Since mpi4py does not support parallelization of objects greater than 2",

    "abstract": "The advent of high-throughput technologies has provided researchers with measurements of thousands of molecular entities and enable the investigation of the internal regulatory apparatus of the cell. However, network inference from high-throughput data is far from being a solved problem. While a plethora of different inference methods have been proposed, they often lead to non-overlapping predictions, and many of them lack user-friendly implementations to enable their broad utilization. Here, we present Consensus Interaction Network Inference Service (COSIFER), a package and a companion web-based platform to infer molecular networks from expression data using state-of-the-art consensus approaches. COSIFER includes a selection of state-of-the-art methodologies for network inference and different consensus strategies to integrate the predictions of individual methods and generate robust networks.\nCOSIFER Python source code is available at https://github.com/PhosphorylatedRabbits/cosifer. The web service is accessible at https://ibm.biz/cosifer-aas.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Brain Predictability toolbox (BPt) represents a unified framework of machine learning (ML) tools designed to work with both tabulated data (e.g. brain derived, psychiatric, behavioral and physiological variables) and neuroimaging specific data (e.g. brain volumes and surfaces). This package is suitable for investigating a wide range of different neuroimaging-based ML questions, in particular, those queried from large human datasets.\nBPt has been developed as an open-source Python 3.6+ package hosted at https://github.com/sahahn/BPt under MIT License, with documentation provided at https://bpt.readthedocs.io/en/latest/, and continues to be actively developed. The project can be downloaded through the github link provided. A web GUI interface based on the same code is currently under development and can be set up through docker with instructions at https://github.com/sahahn/BPt_app.",

    "abstract": "This paper presents a Python-based algorithm, named INSCorNorm, to correct the inelastic neutron scattering (INS) spectra for both sample and container self-shielding and to normalize the experimental spectral intensity to an absolute physical scale (barn/energy unit) facilitating the comparison with computer simulations and interpretation. The algorithm is benchmarked against INS measurements of ZrH",

    "abstract": "Angiotensin II (ANG II) is part of the renin-angiotensin system (RAS) in vertebrates and exert vasoconstriction in all species studied. The present study examines the vasopressor effect of ANG II in the ball python (Python regius), and examines whether ANG II exert its effect through direct angiotensin receptors or through an activation of \u03b1-adrenergic receptors. The studies were conducted in snakes with chronic arterial catheters that had recovered from anesthesia. In addition to demonstrating a clear and pronounced dose-dependent rise in arterial blood pressure upon repeated injections of boluses with ANG II (0.001-1\u00a0\u03bcg/kg), we demonstrate that the pressor response persisted following \u03b1-adrenergic blockade using the \u03b1-adrenergic antagonist phentolamine (2.5\u00a0mg/kg). Unfortunately, it proved impossible to block the ANG receptors using losartan (1, 3 or even 10\u00a0mg/kg). The pressor response to ANG II was associated with a significant rise in heart rate at the higher dosages, pointing to a resetting of the barostatic mechanism for heart rate regulation. The responses were similar in fasting and digesting pythons despite the expected rise in baseline values for blood pressure and heart rate of the digesting snakes.",

    "abstract": "DNA methylation (5mC) and hydroxymethylation (5hmC) are chemical modifications of cytosine bases which play a crucial role in epigenetic gene regulation. However, cost, data complexity and unavailability of comprehensive analytical tools is one of the major challenges in exploring these epigenetic marks. Hydroxymethylation-and Methylation-Sensitive Tag sequencing (HMST-seq) is one of the most cost-effective techniques that enables simultaneous detection of 5mC and 5hmC at single base pair resolution. We present HMST-Seq-Analyzer as a comprehensive and robust method for performing simultaneous differential methylation analysis on 5mC and 5hmC data sets. HMST-Seq-Analyzer can detect Differentially Methylated Regions (DMRs), annotate them, give a visual overview of methylation status and also perform preliminary quality check on the data. In addition to HMST-Seq, our tool can be used on whole-genome bisulfite sequencing (WGBS) and reduced representation bisulfite sequencing (RRBS) data sets as well. The tool is written in Python with capacity to process data in parallel and is available at (https://hmst-seq.github.io/hmst/).",

    "abstract": "A growing body of evidence suggests that the lip products are polluted by heavy metals, which would inevitably cause safety problems with long-term exposure, but few studies have focused on their deeper health risk assessments. This study sets out to identify the lip cosmetics in good sale from Chinese e-commerce market utilizing Python crawler and then explore the probabilistic health risks caused by 6 trace elements in 34 most popular lip cosmetics with Monte Carlo simulation. The results found that there was no obvious non-carcinogenic risk to humans. As for high users, the carcinogenic risk levels of Cr exceeded the acceptable risk recommended by USEPA, approximately 10% and 25% for lipsticks and lip glosses, respectively. Cr was regarded as the priority metal for risk control in the present study. Finally, it was recommended that the minimum use period limit for using up one lip product ranged from 0.54 months to 5.74 months. Overall, this study appears to be the first to conduct a probabilistic health risk assessment of trace elements in lip products, which would be of significance for policy makers to take effective strategies to minimize exposure health risk and contamination.",

    "abstract": "Deep learning enables tremendous progress in medical image analysis. One driving force of this progress are open-source frameworks like TensorFlow and PyTorch. However, these frameworks rarely address issues specific to the domain of medical image analysis, such as 3-D data handling and distance metrics for evaluation. pymia, an open-source Python package, tries to address these issues by providing flexible data handling and evaluation independent of the deep learning framework.\nThe pymia package provides data handling and evaluation functionalities. The data handling allows flexible medical image handling in every commonly used format (e.g., 2-D, 2.5-D, and 3-D; full- or patch-wise). Even data beyond images like demographics or clinical reports can easily be integrated into deep learning pipelines. The evaluation allows stand-alone result calculation and reporting, as well as performance monitoring during training using a vast amount of domain-specific metrics for segmentation, reconstruction, and regression.\nThe pymia package is highly flexible, allows for fast prototyping, and reduces the burden of implementing data handling routines and evaluation methods. While data handling and evaluation are independent of the deep learning framework used, they can easily be integrated into TensorFlow and PyTorch pipelines. The developed package was successfully used in a variety of research projects for segmentation, reconstruction, and regression.\nThe pymia package fills the gap of current deep learning frameworks regarding data handling and evaluation in medical image analysis. It is available at https://github.com/rundherum/pymia and can directly be installed from the Python Package Index using pip install pymia.",

    "abstract": "In gene expression analysis, sample differences and experimental operation differences are common, but sometimes, these differences will cause serious errors to the results or even make the results meaningless. Finding suitable internal reference genes efficiently to eliminate errors is a challenge. Aside from the need for high efficiency, there is no package for screening endogenous genes available in Python. Here, we introduce ERgene, a Python library for screening endogenous reference genes. It has extremely high computational efficiency and simple operation steps. The principle is based on the inverse process of the internal reference method, and the robust matrix block operation makes the selection of internal reference genes faster than any other method.",

    "abstract": "Even though several computational methods for rhythmicity detection and analysis of biological data have been proposed in recent years, classical trigonometric regression based on cosinor still has several advantages over these methods and is still widely used. Different software packages for cosinor-based rhythmometry exist, but lack certain functionalities and require data in different, non-unified input formats.\nWe present CosinorPy, a Python implementation of cosinor-based methods for rhythmicity detection and analysis. CosinorPy merges and extends the functionalities of existing cosinor packages. It supports the analysis of rhythmic data using single- or multi-component cosinor models, automatic selection of the best model, population-mean cosinor regression, and differential rhythmicity assessment. Moreover, it implements functions that can be used in a design of experiments, a synthetic data generator, and import and export of data in different formats.\nCosinorPy is an easy-to-use Python package for straightforward detection and analysis of rhythmicity requiring minimal statistical knowledge, and produces publication-ready figures. Its code, examples, and documentation are available to download from https://github.com/mmoskon/CosinorPy . CosinorPy can be installed manually or by using pip, the package manager for Python packages. The implementation reported in this paper corresponds to the software release v1.1.",

    "abstract": "Despite being the focus of a thriving field of research, the biological mechanisms that underlie information integration in the brain are not yet fully understood. A theory that has gained a lot of traction in recent years suggests that multi-scale integration is regulated by a hierarchy of mutually interacting neural oscillations. In particular, there is accumulating evidence that phase-amplitude coupling (PAC), a specific form of cross-frequency interaction, plays a key role in numerous cognitive processes. Current research in the field is not only hampered by the absence of a gold standard for PAC analysis, but also by the computational costs of running exhaustive computations on large and high-dimensional electrophysiological brain signals. In addition, various signal properties and analyses parameters can lead to spurious PAC. Here, we present Tensorpac, an open-source Python toolbox dedicated to PAC analysis of neurophysiological data. The advantages of Tensorpac include (1) higher computational efficiency thanks to software design that combines tensor computations and parallel computing, (2) the implementation of all most widely used PAC methods in one package, (3) the statistical analysis of PAC measures, and (4) extended PAC visualization capabilities. Tensorpac is distributed under a BSD-3-Clause license and can be launched on any operating system (Linux, OSX and Windows). It can be installed directly via pip or downloaded from Github (https://github.com/EtienneCmb/tensorpac). By making Tensorpac available, we aim to enhance the reproducibility and quality of PAC research, and provide open tools that will accelerate future method development in neuroscience.",

    "abstract": "Comprehensive profiling of lipid species in a biological sample, or lipidomics, is a valuable approach to elucidating disease pathogenesis and identifying biomarkers. Currently, a typical lipidomics experiment may track hundreds to thousands of individual lipid species. However, drawing biological conclusions requires multiple steps of data processing to enrich significantly altered features and confident identification of these features. Existing solutions for these data analysis challenges (i.e., multivariate statistics and lipid identification) involve performing various steps using different software applications, which imposes a practical limitation and potentially a negative impact on reproducibility. Hydrophilic interaction liquid chromatography-ion mobility-mass spectrometry (HILIC-IM-MS) has shown advantages in separating lipids through orthogonal dimensions. However, there are still gaps in the coverage of lipid classes in the literature. To enable reproducible and efficient analysis of HILIC-IM-MS lipidomics data, we developed an open-source Python package, LiPydomics, which enables performing statistical and multivariate analyses (\"stats\" module), generating informative plots (\"plotting\" module), identifying lipid species at different confidence levels (\"identification\" module), and carrying out all functions using a user-friendly text-based interface (\"interactive\" module). To support lipid identification, we assembled a comprehensive experimental database of ",

    "abstract": "Various pre-trained deep learning models for the segmentation of bioimages have been made available as developer-to-end-user solutions. They are optimized for ease of use and usually require neither knowledge of machine learning nor coding skills. However, individually testing these tools is tedious and success is uncertain. Here, we present the Open Segmentation Framework (OpSeF), a Python framework for deep learning-based instance segmentation. OpSeF aims at facilitating the collaboration of biomedical users with experienced image analysts. It builds on the analysts' knowledge in Python, machine learning, and workflow design to solve complex analysis tasks at any scale in a reproducible, well-documented way. OpSeF defines standard inputs and outputs, thereby facilitating modular workflow design and interoperability with other software. Users play an important role in problem definition, quality control, and manual refinement of results. OpSeF semi-automates preprocessing, convolutional neural network (CNN)-based segmentation in 2D or 3D, and postprocessing. It facilitates benchmarking of multiple models in parallel. OpSeF streamlines the optimization of parameters for pre- and postprocessing such, that an available model may frequently be used without retraining. Even if sufficiently good results are not achievable with this approach, intermediate results can inform the analysts in the selection of the most promising CNN-architecture in which the biomedical user might invest the effort of manually labeling training data. We provide Jupyter notebooks that document sample workflows based on various image collections. Analysts may find these notebooks useful to illustrate common segmentation challenges, as they prepare the advanced user for gradually taking over some of their tasks and completing their projects independently. The notebooks may also be used to explore the analysis options available within OpSeF in an interactive way and to document and share final workflows. Currently, three mechanistically distinct CNN-based segmentation methods, the U-Net implementation used in Cellprofiler 3.0, StarDist, and Cellpose have been integrated within OpSeF. The addition of new networks requires little; the addition of new models requires no coding skills. Thus, OpSeF might soon become both an interactive model repository, in which pre-trained models might be shared, evaluated, and reused with ease.",

    "abstract": "Crystal orientation mapping experiments typically measure orientations that are similar within grains and misorientations that are similar along grain boundaries. Such (mis)orientation data cluster in (mis)orientation space, and clusters are more pronounced if preferred orientations or special orientation relationships are present. Here, cluster analysis of (mis)orientation data is described and demonstrated using distance metrics incorporating crystal symmetry and the density-based clustering algorithm DBSCAN. Frequently measured (mis)orientations are identified as corresponding to similarly (mis)oriented grains or grain boundaries, which are visualized both spatially and in three-dimensional (mis)orientation spaces. An example is presented identifying deformation twinning modes in titanium, highlighting a key application of the clustering approach in identifying crystallographic orientation relationships and similarly oriented grains resulting from specific transformation pathways. A new open-source Python library, ",

    "abstract": "One of the most widely used programs for detecting positive selection, at the molecular level, is the program ",

    "abstract": "The PySHS package is a new python open source software tool which simulates the second harmonic scattering (SHS) of different kinds of colloidal nano-objects in various experimental configurations. This package is able to compute polarizations resolved at a fixed scattered angle or angular distribution for different polarization configurations. This article presents the model implemented in the PySHS software and gives some computational examples. A comparison between computational results and experimental data concerning molecular dye intercalated inside liposomes membrane is presented to illustrate the possibilities with PySHS.",

    "abstract": "Preprocessing data in a reproducible and robust way is one of the current challenges in untargeted metabolomics workflows. Data curation in liquid chromatography-mass spectrometry (LC-MS) involves the removal of biologically non-relevant features (retention time, ",

    "abstract": "The aim of this work was to develop a digital dynamic cardiac phantom able to mimic gated myocardial perfusion single photon emission computed tomography (SPECT) images.\nA software code package was written to construct a cardiac digital phantom based on mathematical ellipsoidal model utilizing powerful numerical and mathematic libraries of python programing language. An ellipsoidal mathematical model was adopted to create the left ventricle geometrical volume including myocardial boundaries, left ventricular cavity, with incorporation of myocardial wall thickening and motion. Realistic myocardial count density from true patient studies was used to simulate statistical intensity variation during myocardial contraction. A combination of different levels of defect extent and severity were precisely modeled taking into consideration defect size variation during cardiac contraction. Wall thickening was also modeled taking into account the effect of partial volume.\nIt has been successful to build a python-based software code that is able to model gated myocardial perfusion SPECT images with variable left ventricular volumes and ejection fraction. The recent flexibility of python programming enabled us to manipulate the shape and control the functional parameters in addition to creating variable sized-defects, extents and severities in different locations. Furthermore, the phantom code also provides different levels of image filtration mimicking those filters used in image reconstruction and their influence on image quality. Defect extent and severity were found to impact functional parameter estimation in consistence to clinical examinations.\nA python-based gated myocardial perfusion SPECT phantom has been successfully developed. The phantom proved to be reliable to assess cardiac software analysis tools in terms of perfusion and functional parameters. The software code is under further development and refinement so that more functionalities and features can be added.",

    "abstract": "Ray tracing software systems are commonly used to analyze the optics of solar energy devices, since they allow to predict the energy gains of devices in real conditions, and also to compare them with other systems constantly emerging in the market. However, the available open-source packages apply excessive simplifications to the model of light-matter interaction, making that the optical behaviour of the systems can not be properly characterized, which in turn implies disagreements between physical experiments and computer simulations. We present here the open source python package OTSun, which applies the Fresnel equations in their most general form, without further simplifications, and is suitable for the simulation of both solar-thermal and photovoltaic systems. The geometrical objects used in this package are created using the parametric 3D modeler FreeCAD, which is also a free and open source program and allows for the construction of arbitrary geometries that can be analyzed with OTSun. These, and other software capabilities, make OTSun extremely flexible and accurate for the optical analysis of solar devices with arbitrary geometry. Additionally, OTSun has a companion webtool, OTSunWebApp, that allows for the usage of certain features of the package without the need to install anything locally. We also show here two numerical experiments that we performed in order to validate the model and implementation: The analysis of the optical efficiency of a Linear Fresnel Reflector (with moving objects), and of a second surface mirror (with variable wavelengths). In each case, the numerical computations had deviations of less than 0.25% from reference models (either computed with another program or with exact formulas).",

    "abstract": "The bulk of social neuroscience takes a 'stimulus-brain' approach, typically comparing brain responses to different types of social stimuli, but most of the time in the absence of direct social interaction. Over the last two decades, a growing number of researchers have adopted a 'brain-to-brain' approach, exploring similarities between brain patterns across participants as a novel way to gain insight into the social brain. This methodological shift has facilitated the introduction of naturalistic social stimuli into the study design (e.g. movies) and, crucially, has spurred the development of new tools to directly study social interaction, both in controlled experimental settings and in more ecologically valid environments. Specifically, 'hyperscanning' setups, which allow the simultaneous recording of brain activity from two or more individuals during social tasks, has gained popularity in recent years. However, currently, there is no agreed-upon approach to carry out such 'inter-brain connectivity analysis', resulting in a scattered landscape of analysis techniques. To accommodate a growing demand to standardize analysis approaches in this fast-growing research field, we have developed Hyperscanning Python Pipeline, a comprehensive and easy open-source software package that allows (social) neuroscientists to carry-out and to interpret inter-brain connectivity analyses.",

    "abstract": "The efficacy of afoxolaner was evaluated in two captive Burmese python snakes, which were naturally infested with Ophionyssus natricis mites. The administration of a single oral dose of afoxolaner eliminated live O.\u00a0natricis mites from both snakes by Day 3. Environmental samples collected from the snakes' terrariums were negative for dead mites by Day 30.\nL'efficacit\u00e9 de l'afoxolaner a \u00e9t\u00e9 \u00e9valu\u00e9e chez deux serpents python birmans captifs, naturellement infest\u00e9s par des acariens Ophionyssus natricis. L'administration d'une dose orale unique d'afoxolaner a \u00e9limin\u00e9 les acariens O. natricis vivants des deux serpents \u00e0 jour 3. Les \u00e9chantillons environnementaux pr\u00e9lev\u00e9s dans les terrariums des serpents \u00e9taient n\u00e9gatifs pour les acariens morts \u00e0 jour 30.\nSe evalu\u00f3 la eficacia de afoxolaner en dos serpientes pit\u00f3n birmanas cautivas, que estaban naturalmente infestadas con \u00e1caros Ophionyssus natricis. La administraci\u00f3n de una sola dosis oral de afoxolaner elimin\u00f3 los \u00e1caros O. natricis vivos de ambas serpientes en el d\u00eda 3. Las muestras ambientales recolectadas de los terrarios de las serpientes dieron negativo para \u00e1caros muertos en el d\u00eda 30.\nDie Wirksamkeit von Afoxolaner wurde bei zwei Burmesischen Pyton Schlangen in Gefangenschaft, die auf nat\u00fcrlichem Weg mit Ophionyssus natricis Milben infiziert worden waren, evaluiert. Die Verabreichung einer einzigen Dosis Afoxolaner per os eliminierte lebende O. natricis Milben von beiden Schlangen am 3. Tag. Umweltproben, die aus den Terrarien der Schlangen genommen wurden, zeigten am 30. Tag keine toten Milben mehr.\n\u30a2\u30d5\u30a9\u30ad\u30bd\u30e9\u30cd\u30eb\u306e\u6709\u52b9\u6027\u3092\u3001Ophionyssus natricis\u30c0\u30cb\u304c\u81ea\u7136\u5bc4\u751f\u3057\u305f2\u5339\u306e\u6355\u7372\u30d3\u30eb\u30de\u30cb\u30b7\u30ad\u30d8\u30d3\u3067\u8a55\u4fa1\u3057\u305f\u3002\u30a2\u30d5\u30a9\u30ad\u30bd\u30e9\u30cd\u30eb\u306e\u5358\u56de\u7d4c\u53e3\u6295\u4e0e\u306b\u3088\u308a\u30013\u65e5\u76ee\u307e\u3067\u306b\u4e21\u30d8\u30d3\u304b\u3089\u751f\u304d\u305fO. natricis\u30c0\u30cb\u304c\u6392\u9664\u3055\u308c\u305f\u3002\u30d8\u30d3\u306e\u98fc\u80b2\u5668\u304b\u3089\u53ce\u96c6\u3055\u308c\u305f\u74b0\u5883\u30b5\u30f3\u30d7\u30eb\u306b\u304a\u3044\u3066\u300130\u65e5\u76ee\u307e\u3067\u306b\u6b7b\u9ab8\u30c0\u30cb\u306f\u691c\u51fa\u3055\u308c\u306a\u304b\u3063\u305f\u3002.\n\u8bc4\u4f30\u88ab\u86c7\u87a8\uff08Ophionyssus natricis\uff09\u81ea\u7136\u4fb5\u88ad\u76842\u6761\u5708\u517b\u7f05\u7538\u87d2\u86c7\uff0c\u4f7f\u7528\u963f\u798f\u62c9\u7eb3\u7684\u6cbb\u7597\u6548\u679c\u3002\u5355\u6b21\u7ecf\u53e3\u7ed9\u4e88\u963f\u798f\u62c9\u7eb3\uff0c\u53ef\u5728\u7b2c3\u5929\u6d88\u9664\u4e24\u6761\u86c7\u7684\u6d3bO. natricis\u86c7\u87a8\u3002\u7b2c30\u5929\u4ece\u86c7\u7684\u5bb9\u5668\u73af\u5883\u4e2d\u91c7\u96c6\u6837\u672c\uff0c\u663e\u793a\u4e86\u53ea\u6709\u6b7b\u87a8\u866b\u7684\u9634\u6027\u7ed3\u679c\u3002.\nA efic\u00e1cia do afoxolaner foi avaliada em duas cobras python birmanesas de cativeiro, que foram naturalmente infestadas com \u00e1caros Ophionyssus natricis. A administra\u00e7\u00e3o de uma \u00fanica dose oral de afoxolaner eliminou \u00e1caros O. natricis vivos de ambas as cobras no dia 3. As amostras ambientais coletadas dos terr\u00e1rios das cobras foram negativas para \u00e1caros mortos no dia 30.",

    "abstract": "Slide-free digital pathology techniques, including nondestructive 3D microscopy, are gaining interest as alternatives to traditional slide-based histology. In order to facilitate clinical adoption of these fluorescence-based techniques, software methods have been developed to convert grayscale fluorescence images into color images that mimic the appearance of standard absorptive chromogens such as hematoxylin and eosin (H&E). However, these false-coloring algorithms often require manual and iterative adjustment of parameters, with results that can be inconsistent in the presence of intensity nonuniformities within an image and/or between specimens (intra- and inter-specimen variability). Here, we present an open-source (Python-based) rapid intensity-leveling and digital-staining package that is specifically designed to render two-channel fluorescence images (i.e. a fluorescent analog of H&E) to the traditional H&E color space for 2D and 3D microscopy datasets. However, this method can be easily tailored for other false-coloring needs. Our package offers (1) automated and uniform false coloring in spite of uneven staining within a large thick specimen, (2) consistent color-space representations that are robust to variations in staining and imaging conditions between different specimens, and (3) GPU-accelerated data processing to allow these methods to scale to large datasets. We demonstrate this platform by generating H&E-like images from cleared tissues that are fluorescently imaged in 3D with open-top light-sheet (OTLS) microscopy, and quantitatively characterizing the results in comparison to traditional slide-based H&E histology.",

    "abstract": "Combinations of multiple pharmacological agents can achieve a substantial benefit over treatment with single agents alone. Combinations that achieve 'more than the sum of their parts' are called synergistic. There have been many proposed frameworks to understand and quantify drug combination synergy with different assumptions and domains of applicability. We introduce here synergy, a Python library that (i) implements a broad array of popular synergy models, (ii) provides tools for evaluating confidence intervals and conducting power analysis and (iii) provides standardized tools to analyze and visualize drug combinations and their synergies and antagonisms.\nsynergy is available on all operating systems for Python >=3.5. It is freely available from https://pypi.org/project/synergy, and its source code is available at https://github.com/djwooten/synergy. This software is released under the GNU General Public License, version 3.0 or later.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "The pathogenic Entamoeba species in snakes is widely regarded to be Entamoeba invadens, which can cause severe amoebiasis with up to 100% mortality. In this case report, we describe a ball python (Python regius) that died after short-term weight loss. Necropsy revealed severe necrotizing colitis with large numbers of intralesional Entamoeba trophozoites. Molecular genetic analysis identified these trophozoites as Entamoeba ranarum, a parasite more usually found in amphibians. Furthermore, the extended history revealed that toads (Rhinella marina) had been housed together with the python. This report illustrates the danger of protozoal cross-infections in exotic animals as well as the importance of molecular genetic tools in Entamoeba diagnosis.",

    "abstract": "The increasing role of topology in (bio)physical properties of matter creates a need for an efficient method of detecting the topology of a (bio)polymer. However, the existing tools allow one to classify only the simplest knots and cannot be used in automated sample analysis. To answer this need, we created the Topoly Python package. This package enables the distinguishing of knots, slipknots, links and spatial graphs through the calculation of different topological polynomial invariants. It also enables one to create the minimal spanning surface on a given loop, e.g. to detect a lasso motif or to generate random closed polymers. It is capable of reading various file formats, including PDB. The extensive documentation along with test cases and the simplicity of the Python programming language make it a very simple to use yet powerful tool, suitable even for inexperienced users. Topoly can be obtained from https://topoly.cent.uw.edu.pl.",

    "abstract": "Wound age estimation is a complex, multifactorial issue. It is considered to have great practical significance that combining multi-biomarkers and multi-methods for injury time estimation. We optimized our earlier \"up, no change, or down\" model by adding data on the expression levels of mRNAs encoding ABHD2, MAD2L2, and ARID5A, and we converted the relative quantitative expression levels of seven genes into a vector rather than a color model. We used Python to derive the cosine similarity (CS) between a test set and the vector matrix; the highest similarity most accurately reflected the injury time. For the optimized model, the internal and external verifications were approximately 0.71 and 0.66, respectively. The good double-blinded results indicated that the model was stable and reliable. In summary, we used a vector matrix and cosine similarities derived by Python to mine the levels of genes expressed in contused skeletal muscle. We are the first to combine several biomarkers and methods for wound age estimation.",

    "abstract": "Experimental design is fundamental to research, but formal methods to identify good designs are lacking. Advances in Bayesian statistics and machine learning offer algorithm-based ways to identify good experimental designs. Adaptive design optimization (ADO; Cavagnaro, Myung, Pitt, & Kujala, 2010; Myung, Cavagnaro, & Pitt, 2013) is one such method. It works by maximizing the informativeness and efficiency of data collection, thereby improving inference. ADO is a general-purpose method for conducting adaptive experiments on the fly and can lead to rapid accumulation of information about the phenomenon of interest with the fewest number of trials. The nontrivial technical skills required to use ADO have been a barrier to its wider adoption. To increase its accessibility to experimentalists at large, we introduce an open-source Python package, ADOpy, that implements ADO for optimizing experimental design. The package, available on GitHub, is written using high-level modular-based commands such that users do not have to understand the computational details of the ADO algorithm. In this paper, we first provide a tutorial introduction to ADOpy and ADO itself, and then illustrate its use in three walk-through examples: psychometric function estimation, delay discounting, and risky choice. Simulation data are also provided to demonstrate how ADO designs compare with other designs (random, staircase).",

    "abstract": "Tools and software that automate repetitive tasks, such as metadata extraction and deposition to data repositories, are essential for researchers to share Open Data, routinely. For research that generates microscopy image data, OMERO is an ideal platform for storage, annotation and publication according to open research principles. We present ",

    "abstract": "High-throughput screening (HTS) and virtual screening (VS) have been widely used to identify potential hits from large chemical libraries. However, the frequent occurrence of 'noisy compounds' in the screened libraries, such as compounds with poor drug-likeness, poor selectivity or potential toxicity, has greatly weakened the enrichment capability of HTS and VS campaigns. Therefore, the development of comprehensive and credible tools to detect noisy compounds from chemical libraries is urgently needed in early stages of drug discovery.\nIn this study, we developed a freely available integrated python library for negative design, called Scopy, which supports the functions of data preparation, calculation of descriptors, scaffolds and screening filters, and data visualization. The current version of Scopy can calculate 39 basic molecular properties, 3 comprehensive molecular evaluation scores, 2 types of molecular scaffolds, 6 types of substructure descriptors and 2 types of fingerprints. A number of important screening rules are also provided by Scopy, including 15 drug-likeness rules (13 drug-likeness rules and 2 building block rules), 8 frequent hitter rules (four assay interference substructure filters and four promiscuous compound substructure filters), and 11 toxicophore filters (five human-related toxicity substructure filters, three environment-related toxicity substructure filters and three comprehensive toxicity substructure filters). Moreover, this library supports four different visualization functions to help users to gain a better understanding of the screened data, including basic feature radar chart, feature-feature-related scatter diagram, functional group marker gram and cloud gram.\nScopy provides a comprehensive Python package to filter out compounds with undesirable properties or substructures, which will benefit the design of high-quality chemical libraries for drug design and discovery. It is freely available at https://github.com/kotori-y/Scopy.",

    "abstract": "We present pyflosic, an open-source, general-purpose python implementation of the Fermi-L\u00f6wdin orbital self-interaction correction (FLO-SIC), which is based on the python simulation of chemistry framework (pyscf) electronic structure and quantum chemistry code. Thanks to pyscf, pyflosic can be used with any kind of Gaussian-type basis set, various kinds of radial and angular quadrature grids, and all exchange-correlation functionals within the local density approximation, generalized-gradient approximation (GGA), and meta-GGA provided in the libxc and xcfun libraries. A central aspect of FLO-SIC is the Fermi-orbital descriptors, which are used to estimate the self-interaction correction. Importantly, they can be initialized automatically within pyflosic; they can also be optimized within pyflosic with an interface to the atomic simulation environment, a python library that provides a variety of powerful gradient-based algorithms for geometry optimization. Although pyflosic has already facilitated applications of FLO-SIC to chemical studies, it offers an excellent starting point for further developments in FLO-SIC approaches, thanks to its use of a high-level programming language and pronounced modularity.",

    "abstract": "Phasepy is a Python based package for fluid phase equilibria and interfacial properties calculation from equation of state (EoS). Phasepy uses several tools (i.e., NumPy, SciPy, Pandas, Matplotlib) allowing use Phasepy under Jupyter Notebooks. Phasepy models phase equilibria with the traditional \u03d5-\u03b3 and \u03d5-\u03d5 approaches, where \u03d5 (fugacity coefficient) can be modeled as a perfect gas, virial gas or EoS fluid, whereas \u03b3 (activity coefficient) can be described by conventional models (NRTL, Wilson, Redlich-Kister expansion, and the group contribution modified-UNIFAC). Interfacial properties are based on the square gradient theory couple to \u03d5-\u03d5 approach. The available EoSs are the cubic EoS family extended to mixtures through the quadratic, modified-Huron-Vidal, and Wong-Sandler mixing rules. Phasepy allows to analyze phase stability, compute phase equilibria, interfacial properties, and optimize their parameters for vapor-liquid, liquid-liquid, and vapor-liquid-liquid equilibria for multicomponent mixtures. Phasepy implementation, and robustness are illustrated for binary and ternary mixtures.",

    "abstract": "Camera images and video recordings are simple and non-invasive tools to investigate animals in their natural habitat. Quantitative evaluations, however, often require an exact reconstruction of object positions, sizes, and distances in the image. Here, we provide an open source software package to perform such calculations. Our approach allows the user to correct for perspective distortion, transform images to \"bird's-eye\" view projections, or transform image-coordinates to real-world coordinates and vice versa. The extrinsic camera parameters that are necessary to perform such image corrections and transformations (elevation, tilt/roll angle, and heading of the camera) are obtained from the image using contextual information such as a visible horizon, GPS coordinates of landmarks, known object sizes, or images of the same object obtained from different viewing angles. All mathematical operations are implemented in the Python package ",

    "abstract": "The protonation of titratable residues has a significant impact on the structure and function of biomolecules, influencing many physicochemical and ADME properties. Thus, the importance of the estimation of protonation free energies (p",

    "abstract": "Networks of water molecules can play a critical role at the protein-ligand interface and can directly influence drug-target interactions. Grand canonical methods aid in the sampling of these water molecules, where conventional molecular dynamics equilibration times are often long, by allowing waters to be inserted and deleted from the system, according to the chemical potential. Here, we present our open source Python module, ",

    "abstract": "The proposed work utilizes support vector regression model to predict the number of total number of deaths, recovered cases, cumulative number of confirmed cases and number of daily cases. The data is collected for the time period of 1",

    "abstract": "The present study was designed to characterize phenotypically and genotypically two Trueperella pyogenes strains isolated from an okapi (Okapia johnstoni) and a royal python (Python regius).\nThe species identity could be confirmed by phenotypic properties, by MALDI-TOF MS analysis and by detection of T. pyogenes chaperonin-encoding gene cpn60 with a previously developed loop-mediated isothermal amplification (LAMP) assay. Furthermore, sequencing of the 16S ribosomal RNA (rRNA) gene, the 16S-23S rDNA intergenic spacer region (ISR), the target genes rpoB encoding the \u03b2-subunit of bacterial RNA polymerase, tuf encoding elongation factor tu and plo encoding the putative virulence factor pyolysin allowed the identification of both T. pyogenes isolates at species level.\nBoth strains could be clearly identified as T. pyogenes. The T. pyogenes strain isolated in high number from the vaginal discharge of an okapi seems to be of importance for the infectious process; the T. pyogenes strain from the royal python could be isolated from an apparently non-infectious process. However, both strains represent the first isolation of T. pyogenes from these animal species.",

    "abstract": "Lipidomic analyses address the problem of characterizing the lipid components of given cells, tissues and organisms by means of chromatographic separations coupled to high-resolution, tandem mass spectrometry analyses. A number of software tools have been developed to help in the daunting task of mass spectrometry signal processing and cleaning, peak analysis and compound identification, and a typical finished lipidomic dataset contains hundreds to thousands of individual molecular lipid species. To provide researchers without a specific technical expertise in mass spectrometry the possibility of broadening the exploration of lipidomic datasets, we have developed liputils, a Python module that specializes in the extraction of fatty acid moieties from individual molecular lipids. There is no prerequisite data format, as liputils extracts residues from RefMet-compliant textual identifiers and from annotations of other commercially available services. We provide three examples of real-world data processing with liputils, as well as a detailed protocol on how to readily process an existing dataset that can be followed with basic informatics skills.",

    "abstract": "Stormwater management seeks to reduce runoff from rain or melted snow and improve water quality. Where it can absorb into soil, runoff is filtered and returns to streams, rivers, and aquifers, but in developed areas, precipitation often cannot soak into the ground because impervious surfaces (e.g., pavement, buildings), and already saturated soils can create excess runoff. This water, which can contain pollutants, then runs across urban surfaces and into storm drains, drainage ditches, and sewer systems. Stormwater runoff can cause flooding, erosion, infrastructure and habitat damage, and contamination (including combined and sanitary sewer overflows). In urban and developed areas, effective stormwater management that routes and detains stormwater helps to mitigate these impacts and improve water quality.",

    "abstract": "Currently, gene information available for Oryza sativa species is located in various online heterogeneous data sources. Moreover, methods of access are also diverse, mostly web-based and sometimes query APIs, which might not always be straightforward for domain experts. The challenge is to collect information quickly from these applications and combine it logically, to facilitate scientific research. We developed a Python package named PyRice, a unified programing API to access all supported databases at the same time with consistent output. PyRice design is modular and implements a smart query system, which fits the computing resources to optimize the query speed. As a result, PyRice is easy to use and produces intuitive results.\nhttps://github.com/SouthGreenPlatform/PyRice.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Two-photon calcium imaging is now widely used to infer neuronal dynamics from changes in fluorescence of an indicator. However, state-of-the-art computational tools are not optimized for the reliable detection of fluorescence transients from highly synchronous neurons located in densely packed regions such as the CA1 pyramidal layer of the hippocampus during early postnatal stages of development. Indeed, the latest analytical tools often lack proper benchmark measurements. To meet this challenge, we first developed a graphical user interface (GUI) allowing for a precise manual detection of all calcium transients from imaged neurons based on the visualization of the calcium imaging movie. Then, we analyzed movies from mouse pups using a convolutional neural network (CNN) with an attention process and a bidirectional long-short term memory (LSTM) network. This method is able to reach human performance and offers a better F1 score (harmonic mean of sensitivity and precision) than CaImAn to infer neural activity in the developing CA1 without any user intervention. It also enables automatically identifying activity originating from GABAergic neurons. Overall, DeepCINAC offers a simple, fast and flexible open-source toolbox for processing a wide variety of calcium imaging datasets while providing the tools to evaluate its performance.",

    "abstract": "Allele-specific expression (ASE) is involved in many important biological mechanisms. We present a python package BYASE and its graphical user interface (GUI) tool BYASE-GUI for the identification of ASE from single-end and paired-end RNA-seq data based on Bayesian inference, which can simultaneously report differences in gene-level and isoform-level expression. BYASE uses both phased SNPs and non-phased SNPs, and supports polyploid organisms.\nThe source codes of BYASE and BYASE-GUI are freely available at https://github.com/ncjllld/byase and https://github.com/ncjllld/byase_gui.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Although the Python programming language counts many Bioinformatics and Computational Biology libraries; none offers customizable sequence annotation visualizations with layout optimization.\nDNA Features Viewer is a sequence annotation plotting library which optimizes plot readability while letting users tailor other visual aspects (colors, labels, highlights etc.) to their particular use case.\nOpen-source code and documentation are available on Github under the MIT license (https://github.com/Edinburgh-Genome-Foundry/DnaFeaturesViewer).\nSupplementary data are available at Bioinformatics online.",

    "abstract": "We present Ogre, an open-source code for generating surface slab models from bulk molecular crystal structures. Ogre is written in Python and interfaces with the FHI-aims code to calculate surface energies at the level of density functional theory (DFT). The input of Ogre is the geometry of the bulk molecular crystal. The surface is cleaved from the bulk structure with the molecules on the surface kept intact. A slab model is constructed according to the user specifications for the number of molecular layers and the length of the vacuum region. Ogre automatically identifies all symmetrically unique surfaces for the user-specified Miller indices and detects all possible surface terminations. Ogre includes utilities to analyze the surface energy convergence and Wulff shape of the molecular crystal. We present the application of Ogre to three representative molecular crystals: the pharmaceutical aspirin, the organic semiconductor tetracene, and the energetic material HMX. The equilibrium crystal shapes predicted by Ogre are in agreement with experimentally grown crystals, demonstrating that DFT produces satisfactory predictions of the crystal habit for diverse classes of molecular crystals.",

    "abstract": "Metallocages offer a diverse and underexplored region of chemical space in which to search for novel catalysts and substrate hosts. However, the ability to tailor such structures toward applications in binding and catalysis is a challenging task. Here, we present an open-source computational toolkit, ",

    "abstract": "Python web crawler technology, which automatically and massively getting information from the Internet by mimicking net users' browsing behavior, is a basic supporting technique to extract and integrate multi-source heterogeneous data in the field of Infodemiology. There are two types of Python web crawler: simple and massive-scale, both collect information simultaneously from the database establishment. Advantages of this technique are characterized as: being simple syntax, in high flexibility and low cost in learning and maintenance. Contents of the current application scenarios include surveillance, implementation and evaluation of health intervention programs on public health issues, as well as on smart doctor seeking. For the last two years, the Chinese government started to encourage the integration and utilization of multi-source heterogeneous data including internet information. Hence, the number of application scenarios for Python web crawler technology are bound to increase in the foreseeable future. Corresponding matched talent cultivations and technical innovations are suggested to add to the current education and research systems on public health issues.\nPython\u7f51\u7edc\u722c\u866b\u6280\u672f\u662f\u4e00\u79cd\u901a\u8fc7\u6a21\u62df\u7528\u6237\u7684\u7f51\u7edc\u6d4f\u89c8\u884c\u4e3a\u4ee5\u5b9e\u73b0\u4ece\u7f51\u7edc\u4e2d\u81ea\u52a8\u3001\u5927\u91cf\u63d0\u53d6\u4fe1\u606f\u7684\u6280\u672f\uff0c\u662f\u4fe1\u606f\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u6536\u96c6\u5e76\u6574\u5408\u591a\u6e90\u5f02\u6784\u4fe1\u606f\u6570\u636e\u7684\u5173\u952e\u57fa\u7840\u3002Python\u7f51\u7edc\u722c\u866b\u53ef\u5206\u4e3a\u7b80\u5355\u722c\u866b\u4e0e\u5927\u578b\u722c\u866b\uff0c\u96c6\u6570\u636e\u91c7\u96c6\u4e0e\u6570\u636e\u5e93\u6784\u5efa\u4e8e\u4e00\u4f53\uff0c\u8bed\u6cd5\u7b80\u6d01\u3001\u7075\u6d3b\u6027\u9ad8\u3001\u5b66\u4e60\u6210\u672c\u4f4e\u3001\u7ef4\u62a4\u6210\u672c\u4f4e\u3002\u5b83\u9002\u7528\u4e8e\u4fe1\u606f\u6d41\u884c\u75c5\u5b66\u7684\u5404\u79cd\u5e94\u7528\u573a\u666f\uff0c\u901a\u8fc7\u5bf9\u4e92\u8054\u7f51\u4e2d\u5065\u5eb7\u76f8\u5173\u4fe1\u606f\u7684\u5206\u6790\uff0c\u5b9e\u73b0\u591a\u79cd\u516c\u5171\u536b\u751f\u76d1\u6d4b\u3001\u5065\u5eb7\u5e72\u9884\u5b9e\u65bd\u53ca\u6548\u679c\u8bc4\u4ef7\u3001\u667a\u6167\u5bfb\u533b\u65b9\u7565\u4f18\u5316\u7b49\u76ee\u6807\u3002\u8fd1\u5e74\uff0c\u6211\u56fd\u653f\u5e9c\u5f00\u59cb\u9f13\u52b1\u5bf9\u542b\u4e92\u8054\u7f51\u4fe1\u606f\u5728\u5185\u7684\u591a\u6e90\u5927\u6570\u636e\u7684\u6574\u5408\u5229\u7528\uff0c\u5728\u6b64\u80cc\u666f\u4e0b\uff0cPython\u722c\u866b\u6280\u672f\u7684\u5e94\u7528\u573a\u666f\u52bf\u5fc5\u4f1a\u8d8a\u6765\u8d8a\u591a\uff0c\u76f8\u5e94\u7684\u4eba\u624d\u57f9\u517b\u3001\u6280\u672f\u9769\u65b0\u5efa\u8bae\u7eb3\u5165\u5230\u516c\u5171\u536b\u751f\u6559\u80b2\u548c\u79d1\u7814\u4f53\u7cfb\u4e4b\u4e2d\u3002.",

    "abstract": "One of the important issues during the response to a mass disaster is the identification of victims. In this study, we verified the use of the occlusal morphology of molars for individual identification. The aim of this study was to establish a simple new method for identifying individuals from molar data. Using Python, we developed programming that included the perceptual Hash (pHash) function and the Hamming distance (HD) between antemortem data (AMD) and postmortem data (PMD). The AMD comprised 2,215 dental models. The PMD were selected from the AMD set and comprised 17 models from the same individual with changes over time. As a result, 16 PMD models (over 90%) were ranked in the top 5%. Although identification using only a single molar is difficult, there is the possibility of narrowing down victims' identity with high accuracy through verification using multiple teeth. This system is expected to be useful as a very simple method of identification.",

    "abstract": "Many disease causing genes have been identified through different methods, but there have been no uniform annotations of biomedical named entity (bio-NE) of the disease phenotypes of these genes yet. Furthermore, semantic similarity comparison between two bio-NE annotations has become important for data integration or system genetics analysis.\nThe package pyMeSHSim recognizes bio-NEs by using MetaMap which produces Unified Medical Language System (UMLS) concepts in natural language process. To map the UMLS concepts to Medical Subject Headings (MeSH), pyMeSHSim is embedded with a house-made dataset containing the main headings (MHs), supplementary concept records (SCRs), and their relations in MeSH. Based on the dataset, pyMeSHSim implemented four information content (IC)-based algorithms and one graph-based algorithm to measure the semantic similarity between two MeSH terms. To evaluate its performance, we used pyMeSHSim to parse OMIM and GWAS phenotypes. The pyMeSHSim introduced SCRs and the curation strategy of non-MeSH-synonymous UMLS concepts, which improved the performance of pyMeSHSim in the recognition of OMIM phenotypes. In the curation of 461 GWAS phenotypes, pyMeSHSim showed recall >\u20090.94, precision >\u20090.56, and F1\u00a0>\u00a00.70, demonstrating better performance than the state-of-the-art tools DNorm and TaggerOne in recognizing MeSH terms from short biomedical phrases. The semantic similarity in MeSH terms recognized by pyMeSHSim and the previous manual work was calculated by pyMeSHSim and another semantic analysis tool meshes, respectively. The result indicated that the correlation of semantic similarity analysed by two tools reached as high as 0.89-0.99.\nThe integrative MeSH tool pyMeSHSim embedded with the MeSH MHs and SCRs realized the bio-NE recognition, normalization, and comparison in biomedical text-mining.",

    "abstract": "Small-mammal neuroimaging offers incredible opportunities to investigate structural and functional aspects of the brain. Many tools have been developed in the last decade to analyse small animal data, but current softwares are less mature than the available tools that process human brain data. The Python package Sammba-MRI (SmAll-MaMmal BrAin MRI in Python; http://sammba-mri.github.io) allows flexible and efficient use of existing methods and enables fluent scriptable analysis workflows, from raw data conversion to multimodal processing.",

    "abstract": "Microbial fitness screens are a key technique in functional genomics. We present an all-in-one solution, ",

    "abstract": "Membrane computing is a computational paradigm inspired by the structure and behavior of a living cell. P Systems are the computing devices that are used to realize membrane computing models. Numerous theoretical studies on many variants of P Systems have shown them to be computationally universal. There is a wide range of applications of P Systems from modeling of biological processes to image processing. Among many variants of P Systems, one of the most important is Enzymatic Numerical P System (ENPS). ENPS is a class of P System in which membranes operate on numerical values. To realize the power of ENPS there are a few simulators developed. Each and every simulator has some advantages as well as some disadvantages. Here, a GPU based simulator using Python as a user interaction language is developed. This tool is a completely parallel variant, compatible with a Python based sequential simulator (PeP) which was the first Python based work for ENPS. The developed simulator uses CUDA to interact with GPU and gives the desired speed up, while processing the membranes. There are two important case studies which show the performance of the developed tool to be far better than the other serial simulators.",

    "abstract": "Nowadays, the manipulation and analysis of genomic data stored in publicly accessible repositories have become a daily task in genomics and bioinformatics laboratories. Due to the enormous advancement in the field of genome sequencing and the emergence of many projects, bioinformaticians have pushed for the creation of a variety of programs and pipelines that will automatically analyze such big data, in particular the pipelines of gene annotation. Dealing with annotation files using easy and simple programs is very important, particularly for non-developers, enhancing the genomic data analysis acceleration. One of the first tasks required to work with genomic annotation files is to extract different features. In this regard, we have developed GAD ( https://github.com/bio-projects/GAD ) using Python to be a fast, easy, and controlled script that has a high ability to handle annotation files such as GFF3 and GTF. GAD is a cross-platform graphical interface tool used to extract genome features such as intergenic regions, upstream, and downstream genes. Besides, GAD finds all names of ambiguous sequence ontology, and either extracts them or considers them as genes or transcripts. The results are produced in a variety of file formats, such as BED, GTF, GFF3, and FASTA, supported by other bioinformatics programs. The GAD can handle large sizes of different genomes and an infinite number of files with minimal user effort. Therefore, our script could be integrated into various pipelines in all genomic laboratories to accelerate data analysis.",

    "abstract": "Recent years have witnessed a massive push towards reproducible research in neuroscience. Unfortunately, this endeavor is often challenged by the large diversity of tools used, project-specific custom code and the difficulty to track all user-defined parameters. NeuroPycon is an open-source multi-modal brain data analysis toolkit which provides Python-based template pipelines for advanced multi-processing of MEG, EEG, functional and anatomical MRI data, with a focus on connectivity and graph theoretical analyses. Importantly, it provides shareable parameter files to facilitate replication of all analysis steps. NeuroPycon is based on the NiPype framework which facilitates data analyses by wrapping many commonly-used neuroimaging software tools into a common Python environment. In other words, rather than being a brain imaging software with is own implementation of standard algorithms for brain signal processing, NeuroPycon seamlessly integrates existing packages (coded in python, Matlab or other languages) into a unified python framework. Importantly, thanks to the multi-threaded processing and computational efficiency afforded by NiPype, NeuroPycon provides an easy option for fast parallel processing, which critical when handling large sets of multi-dimensional brain data. Moreover, its flexible design allows users to easily configure analysis pipelines by connecting distinct nodes to each other. Each node can be a Python-wrapped module, a user-defined function or a well-established tool (e.g. MNE-Python for MEG analysis, Radatools for graph theoretical metrics, etc.). Last but not least, the ability to use NeuroPycon parameter files to fully describe any pipeline is an important feature for reproducibility, as they can be shared and used for easy replication by others. The current implementation of NeuroPycon contains two complementary packages: The first, called ephypype, includes pipelines for electrophysiology analysis and a command-line interface for on the fly pipeline creation. Current implementations allow for MEG/EEG data import, pre-processing and cleaning by automatic removal of ocular and cardiac artefacts, in addition to sensor or source-level connectivity analyses. The second package, called graphpype, is designed to investigate functional connectivity via a wide range of graph-theoretical metrics, including modular partitions. The present article describes the philosophy, architecture, and functionalities of the toolkit and provides illustrative examples through interactive notebooks. NeuroPycon is available for download via github (https://github.com/neuropycon) and the two principal packages are documented online (https://neuropycon.github.io/ephypype/index.html, and https://neuropycon.github.io/graphpype/index.html). Future developments include fusion of multi-modal data (eg. MEG and fMRI or intracranial EEG and fMRI). We hope that the release of NeuroPycon will attract many users and new contributors, and facilitate the efforts of our community towards open source tool sharing and development, as well as scientific reproducibility.",

    "abstract": "Several species of bacteria are able to modify their swimming behavior in response to chemical attractants or repellents. Methods for the quantitative analysis of bacterial chemotaxis such as quantitative capillary assays are tedious and time-consuming. Computer-based video analysis of swimming bacteria represents a valuable method to directly assess their chemotactic response. Even though multiple studies have used this approach to elucidate various aspects of bacterial chemotaxis, to date, no computer software for such analyses is freely available. Here, we introduce TaxisPy, a Python-based software for the quantitative analysis of bacterial chemotaxis. The software comes with an intuitive graphical user interface and can be accessed easily through Docker on any operating system. Using a video of freely swimming cells as input, TaxisPy estimates the culture's average tumbling frequency over time. We demonstrate the utility of the software by assessing the effect of different concentrations of the attractant shikimate on the swimming behavior of Pseudomonas putida F1 and by capturing the adaptation process that Escherichia coli undergoes after being exposed to l-aspartate.",

    "abstract": "Lizard and snake remains from the early Miocene (Burdigalian) of the Moghra Formation, Egypt, are described herein. This material comprises the first fossil remains of squamates recovered from the otherwise rich and well known vertebrate assemblage of Moghra. The material pertains to two different genera, the varanid lizard ",

    "abstract": "The Dalton Project provides a uniform platform access to the underlying full-fledged quantum chemistry codes Dalton and LSDalton as well as the PyFraME package for automatized fragmentation and parameterization of complex molecular environments. The platform is written in Python and defines a means for library communication and interaction. Intermediate data such as integrals are exposed to the platform and made accessible to the user in the form of NumPy arrays, and the resulting data are extracted, analyzed, and visualized. Complex computational protocols that may, for instance, arise due to a need for environment fragmentation and configuration-space sampling of biochemical systems are readily assisted by the platform. The platform is designed to host additional software libraries and will serve as a hub for future modular software development efforts in the distributed Dalton community.",

    "abstract": "We present PyCDFT, a Python package to compute diabatic states using constrained density functional theory (CDFT). PyCDFT provides an object-oriented, customizable implementation of CDFT, and allows for both single-point self-consistent-field calculations and geometry optimizations. PyCDFT is designed to interface with existing density functional theory (DFT) codes to perform CDFT calculations where constraint potentials are added to the Kohn-Sham Hamiltonian. Here, we demonstrate the use of PyCDFT by performing calculations with a massively parallel first-principles molecular dynamics code, Qbox, and we benchmark its accuracy by computing the electronic coupling between diabatic states for a set of organic molecules. We show that PyCDFT yields results in agreement with existing implementations and is a robust and flexible package for performing CDFT calculations. The program is available at https://dx.doi.org/10.5281/zenodo.3821097.",

    "abstract": "A script was developed to perform homogenous radiolysis calculations. It was used specifically to calculate radiolysis products under various neutron and gamma flux environments [1]. The routine may be used to calculate a single radiolysis condition, multiple independent conditions, or multiple conditions computed in series (the final concentration set of run i is the initial concentration of run i+1). While designed for radiolysis of water, the routine is easily adapted to a variety of aqueous reaction systems and may even be altered with minimal effort for more general homogenous chemical analysis. In the present article, the Python routine is explained along with various outputs and inputs. It and the relevant input and output text files are included as supplementary materials. They are the raw data used for calculation of figures in the associated journal article.",

    "abstract": "High-performance numerical codes are an indispensable tool for hydrogeologists when modeling subsurface flow and transport systems. But as they are written in compiled languages, like C/C++ or Fortran, established software packages are rarely user-friendly, limiting a wider adoption of such tools. OpenGeoSys (OGS), an open-source, finite-element solver for thermo-hydro-mechanical-chemical processes in porous and fractured media, is no exception. Graphical user interfaces may increase usability, but do so at a dramatic reduction of flexibility and are difficult or impossible to integrate into a larger workflow. Python offers an optimal trade-off between these goals by providing a highly flexible, yet comparatively user-friendly environment for software applications. Hence, we introduce ogs5py, a Python-API for the OpenGeoSys 5 scientific modeling package. It provides a fully Python-based representation of an OGS project, a large array of convenience functions for users to interact with OGS and connects OGS to the scientific and computational environment of Python.",

    "abstract": "ipcoal is a free and open source Python package for simulating and analyzing genealogies and sequences. It automates the task of describing complex demographic models (e.g. with divergence times, effective population sizes, migration events) to the msprime coalescent simulator by parsing a user-supplied species tree or network. Genealogies, sequences and metadata are returned in tabular format allowing for easy downstream analyses. ipcoal includes phylogenetic inference tools to automate gene tree inference from simulated sequence data, and visualization tools for analyzing results and verifying model accuracy. The ipcoal package is a powerful tool for posterior predictive data analysis, for methods validation and for teaching coalescent methods in an interactive and visual environment.\nSource code is available from the GitHub repository (https://github.com/pmckenz1/ipcoal/) and is distributed for packaged installation with conda. Complete documentation and interactive notebooks prepared for teaching purposes, including an empirical example, are available at https://ipcoal.readthedocs.io/.\np.mckenzie@columbia.edu.",

    "abstract": "Reptile-associated nidoviruses (serpentoviruses) have been reported to cause proliferative interstitial pneumonia in pythons and other reptile species. A captive, younger than 2 years old, intact female ball python (Python regius) showed increased oral mucus, wheezing, and audible breathing with weight loss. Gross and microscopic examination revealed large amounts of mucus in the esophagus and proliferative interstitial pneumonia. Serpentovirus genes were detected from the lung tissues by polymerase chain reaction. The current serpentoviruses was phylogenetically grouped with the serpentovirus previously identified in the US. No case of serpentovirus infection has been reported in Asia. The present report provides information of complete genome sequence and global distribution of serpentovirus.",

    "abstract": "Cancer contributes to significant morbidity and mortality despite advances in treatment and supportive care. There is a need for the identification of effective anticancer agents. Reptiles such as tortoise, python, and water monitor lizards are exposed to heavy metals, tolerate high levels of radiation, feed on rotten/germ-infested feed, thrive in unsanitary habitat and yet have prolonged lifespans. Such species are rarely reported to develop cancer, suggesting the presence of anticancer molecules/mechanisms.\nHere, we tested effects from sera of Asian water monitor lizard (Varanus salvator), python (Malayopython reticulatus) and tortoise (Cuora kamaroma amboinensis) against cancer cells. Sera were collected and cytotoxicity assays were performed using prostate cancer cells (PC3), Henrietta Lacks cervical adenocarcinoma cells (HeLa) and human breast adenocarcinoma cells (MCF7), as well as human keratinized skin cells (Hacat), by measuring lactate dehydrogenase release as an indicator for cell death. Growth inhibition assays were performed to determine the effects on cancer cell proliferation. Liquid chromatography mass spectrometry was performed for molecular identification.\nThe findings revealed that reptilian sera, but not bovine serum, abolished viability of Hela, PC3 and MCF7 cells. Samples were subjected to liquid chromatography mass spectrometry, which detected 57 molecules from V. salvator, 81 molecules from Malayopython reticulatus and 33 molecules from C. kamaroma amboinensis and putatively identified 9 molecules from V. salvator, 20 molecules from Malayopython reticulatus and 9 molecules from C. kamaroma amboinensis when matched against METLIN database. Based on peptide amino acid composition, binary profile, dipeptide composition and pseudo-amino acid composition, 123 potential Anticancer Peptides (ACPs) were identified from 883 peptides from V. salvator, 306 potential ACPs from 1074 peptides from Malayopython reticulatus and 235 potential ACPs from 885 peptides from C. kamaroma amboinensis.\nTo our knowledge, for the first time, we reported comprehensive analyses of selected reptiles' sera using liquid chromatography mass spectrometry, leading to the identification of potentially novel anticancer agents. We hope that the discovery of molecules from these animals will pave the way for the rational development of new anticancer agents.",

    "abstract": "In single-cell RNA-seq (scRNA-seq) experiments, the number of individual cells has increased exponentially, and the sequencing depth of each cell has decreased significantly. As a result, analyzing scRNA-seq data requires extensive considerations of program efficiency and method selection. In order to reduce the complexity of scRNA-seq data analysis, we present scedar, a scalable Python package for scRNA-seq exploratory data analysis. The package provides a convenient and reliable interface for performing visualization, imputation of gene dropouts, detection of rare transcriptomic profiles, and clustering on large-scale scRNA-seq datasets. The analytical methods are efficient, and they also do not assume that the data follow certain statistical distributions. The package is extensible and modular, which would facilitate the further development of functionalities for future requirements with the open-source development community. The scedar package is distributed under the terms of the MIT license at https://pypi.org/project/scedar.",

    "abstract": "Multiple sequence alignment (MSA) consists of finding the optimal alignment of three or more biological sequences to identify highly conserved regions that may be the result of similarities and relationships between the sequences. MSA is an optimization problem with NP-hard complexity (non-deterministic polynomial-time hardness), because the time needed to find optimal alignments raises exponentially along with the number of sequences and their length. Furthermore, the problem becomes multiobjective when more than one score is considered to assess the quality of an alignment, such as maximizing the percentage of totally conserved columns and minimizing the number of gaps. Our motivation is to provide a Python tool for solving MSA problems using evolutionary algorithms, a nonexact stochastic optimization approach that has proven to be effective to solve multiobjective problems.\nThe software tool we have developed, called Sequoya, is written in the Python programming language, which offers a broad set of libraries for data analysis, visualization and parallelism. Thus, Sequoya offers a graphical tool to visualize the progress of the optimization in real time, the ability to guide the search toward a preferred region in run-time, parallel support to distribute the computation among nodes in a distributed computing system, and a graphical component to assist in the analysis of the solutions found at the end of the optimization.\nSequoya can be freely obtained from the Python Package Index (pip) or, alternatively, it can be downloaded from Github at https://github.com/benhid/Sequoya.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "The rapid strike of snakes has long been of interest in terms of mechanical performance. Recently, several nonvenomous taxa have been found to strike with the same incredible strike velocity and acceleration as the high-performing vipers. However, little is known regarding how these patterns change through ontogeny. Here I present ontogenetic strike data on ten ball pythons (Python regius) over a three year time period, from birth to sexual maturity. I found that performance declined rapidly over the first 18 months in nearly all kinematic measures. This puts the adult data out of the currently developing trend of high performance being maintained across the diversity of snakes. The underlying cause of the decline in performance is unclear, but there are several avenues of behavior, morphology, biomechanics, and ecology to be investigated.",

    "abstract": "Signaling pathways capable of switching between two states are ubiquitous within living organisms. They provide the cells with the means to produce reversible or irreversible decisions. Switch-like behavior of biological systems is realized through biochemical reaction networks capable of having two or more distinct steady states, which are dependent on initial conditions. Investigation of whether a certain signaling pathway can confer bistability involves a substantial amount of hypothesis testing. The cost of direct experimental testing can be prohibitive. Therefore, constraining the hypothesis space is highly beneficial. One such methodology is based on chemical reaction network theory (CRNT), which uses computational techniques to rule out pathways that are not capable of bistability regardless of kinetic constant values and molecule concentrations. Although useful, these methods are complicated from both pure and computational mathematics perspectives. Thus, their adoption is very limited amongst biologists.\nWe brought CRNT approaches closer to experimental biologists by automating all the necessary steps in CRNT4SMBL. The input is based on systems biology markup language (SBML) format, which is the community standard for biological pathway communication. The tool parses SBML and derives C-graph representations of the biological pathway with mass action kinetics. Next steps involve an efficient search for potential saddle-node bifurcation points using an optimization technique. This type of bifurcation is important as it has the potential of acting as a switching point between two steady states. Finally, if any bifurcation points are present, continuation analysis with respect to a user-defined parameter extends the steady state branches and generates a bifurcation diagram. Presence of an S-shaped bifurcation diagram indicates that the pathway acts as a bistable switch for the given optimization parameters.\nCRNT4SBML is available via the Python Package Index. The documentation can be found at https://crnt4sbml.readthedocs.io. CRNT4SBML is licensed under the Apache Software License 2.0.",

    "abstract": "Cryo-EM Single Particle Analysis workflows require tens of thousands of high-quality particle projections to unveil the three-dimensional structure of macromolecules. Conventional methods for automatic particle picking tend to suffer from high false-positive rates, hampering the reconstruction process. One common cause of this problem is the presence of carbon and different types of high-contrast contaminations. In order to overcome this limitation, we have developed MicrographCleaner, a deep learning package designed to discriminate, in an automated fashion, between regions of micrographs which are suitable for particle picking, and those which are not. MicrographCleaner implements a U-net-like deep learning model trained on a manually curated dataset compiled from over five hundred micrographs. The benchmarking, carried out on approximately one hundred independent micrographs, shows that MicrographCleaner is a very efficient approach for micrograph preprocessing. MicrographCleaner (micrograph_cleaner_em) package is available at PyPI and Anaconda Cloud and also as a Scipion/Xmipp protocol. Source code is available at https://github.com/rsanchezgarc/micrograph_cleaner_em.",

    "abstract": "Laser ablation inductively coupled plasma mass spectrometry (LA-ICP-MS) imaging has been extensively used to determine the distributions of metals in biological tissues for a wide variety of applications. To be useful for identifying metal biodistributions, the acquired raw data needs to be reconstructed into a two-dimensional image. Several approaches have been developed for LA-ICP-MS image reconstruction, but less focus has been placed on software for more in-depth statistical processing of the imaging data. Yet, improved image processing can allow the biological ramifications of metal distributions in tissues to be better understood. In this work, we describe software written in Python that automatically reconstructs, analyzes, and segments images from LA-ICP-MS imaging data. Image segmentation is achieved using LA-ICP-MS signals from the biological metals Fe and Zn together with k-means clustering to automatically identify sub-organ regions in different tissues. Spatial awareness also can be incorporated into the images through a neighboring pixel evaluation that allows regions of interest to be identified that are at the limit of the LA-ICP-MS imaging resolution. The value of the described algorithms is demonstrated for LA-ICP-MS images of nanomaterial biodistributions. The developed image reconstruction and processing approach reveals that nanomaterials distribute in different sub-organ regions based on their chemical and physical properties, opening new possibilities for understanding the impact of such nanomaterials in vivo.",

    "abstract": "The aim of this study was to establish a measurement method for filler and matrix in cured resin composite (RC) using Python programming and to investigate the correlation between matrix ratio and curing temperature rise. Eight kinds of RCs were used. Backscattered electron images were taken for each cured specimen. Matrix and filler contents were calculated using Python programming with the K-means or area segmentation method. Volume measurement methods were assessed for comparison. Heat released during the polymerization reaction was measured. The matrix ratio was calculated without human intervention. Three specimens contained only inorganic filler, and other specimens contained multiple types of fillers. Almost the same values of the matrix ratio were obtained by programming and the volume measurement methods for specimens containing a single type of inorganic filler. Moreover, a strong correlation was found between the matrix ratio obtained by the programming method and curing temperature rise (R=0.9826).",

    "abstract": "Near infrared spectroscopy (NIRS) is an analytical technique for determining the chemical composition or structure of a given sample. For several decades, NIRS has been a frequently used analysis tool in agriculture, pharmacology, medicine, and petrochemistry. The popularity of NIRS is constantly growing as new application areas are discovered. Contrary to mid infrared spectral region, the absorption bands in near infrared spectral region are often non-specific, broad, and overlapping. Analysis of NIR spectra requires multivariate methods which are highly subjective to noise arising from instrumentation, scattering effects, and measurement setup. NIRS measurements are also frequently performed outside of a laboratory which further contributes to the presence of noise. Therefore, preprocessing is a critical step in NIRS as it can vastly improve the performance of multivariate models. While extensive research regarding various preprocessing methods exists, selection of the best preprocessing method is often determined through trial-and-error. A more powerful approach for optimizing preprocessing in NIRS models would be to automatically compare a large number of preprocessing techniques (e.g., through grid-search or hyperparameter tuning). To enable this, we present, nippy, an open-source Python module for semi-automatic comparison of NIRS preprocessing methods (available at https://github.com/uef-bbc/nippy). We provide here a brief overview of the capabilities of nippy and demonstrate the typical usage through two examples with public datasets.",

    "abstract": "f90wrap is a tool to automatically generate Python extension modules which interface to Fortran libraries that makes use of derived types. It builds on the capabilities of the popular f2py utility by generating a simpler Fortran 90 interface to the original Fortran code which is then suitable for wrapping with f2py, together with a higher-level Pythonic wrapper that makes the existance of an additional layer transparent to the final user. f90wrap has been used to wrap a number of large software packages of relevance to the condensed matter physics community, including the QUIP molecular dynamics code and the CASTEP density functional theory code.",

    "abstract": "Natural microbial communities contain hundreds to thousands of interacting species. For this reason, computational simulations are playing an increasingly important role in microbial ecology. In this manuscript, we present a new open-source, freely available Python package called Community Simulator for simulating microbial population dynamics in a reproducible, transparent and scalable way. The Community Simulator includes five major elements: tools for preparing the initial states and environmental conditions for a set of samples, automatic generation of dynamical equations based on a dictionary of modeling assumptions, random parameter sampling with tunable levels of metabolic and taxonomic structure, parallel integration of the dynamical equations, and support for metacommunity dynamics with migration between samples. To significantly speed up simulations using Community Simulator, our Python package implements a new Expectation-Maximization (EM) algorithm for finding equilibrium states of community dynamics that exploits a recently discovered duality between ecological dynamics and convex optimization. We present data showing that this EM algorithm improves performance by between one and two orders compared to direct numerical integration of the corresponding ordinary differential equations. We conclude by listing several recent applications of the Community Simulator to problems in microbial ecology, and discussing possible extensions of the package for directly analyzing microbiome compositional data.",

    "abstract": "Methods that survey protein surfaces for binding hotspots can help to evaluate target tractability and guide exploration of potential ligand binding regions. Fragment Hotspot Maps builds upon interaction data mined from the CSD (Cambridge Structural Database) and exploits the idea of identifying hotspots using small chemical fragments, which is now widely used to design new drug leads. Prior to this publication, Fragment Hotspot Maps was only publicly available through a web application. To increase the accessibility of this algorithm we present the Hotspots API (application programming interface), a toolkit that offers programmatic access to the core Fragment Hotspot Maps algorithm, thereby facilitating the interpretation and application of the analysis. To demonstrate the package's utility, we present a workflow which automatically derives protein hydrogen-bond constraints for molecular docking with GOLD. The Hotspots API is available from https://github.com/prcurran/hotspots under the MIT license and is dependent upon the commercial CSD Python API.",

    "abstract": "Precision oncology depends on the matching of tumor variants to relevant knowledge describing the clinical significance of those variants. We recently developed the Clinical Interpretations for Variants in Cancer (CIViC; civicdb.org) crowd-sourced, expert-moderated, and open-access knowledgebase. CIViC provides a structured framework for evaluating genomic variants of various types (eg, fusions, single-nucleotide variants) for their therapeutic, prognostic, predisposing, diagnostic, or functional utility. CIViC has a documented application programming interface for accessing CIViC records: assertions, evidence, variants, and genes. Third-party tools that analyze or access the contents of this knowledgebase programmatically must leverage this application programming interface, often reimplementing redundant functionality in the pursuit of common analysis tasks that are beyond the scope of the CIViC Web application.\nTo address this limitation, we developed CIViCpy (civicpy.org), a software development kit for extracting and analyzing the contents of the CIViC knowledgebase. CIViCpy enables users to query CIViC content as dynamic objects in Python. We assess the viability of CIViCpy as a tool for advancing individualized patient care by using it to systematically match CIViC evidence to observed variants in patient cancer samples.\nWe used CIViCpy to evaluate variants from 59,437 sequenced tumors of the American Association for Cancer Research Project GENIE data set. We demonstrate that CIViCpy enables annotation of > 1,200 variants per second, resulting in precise variant matches to CIViC level A (professional guideline) or B (clinical trial) evidence for 38.6% of tumors.\nThe clinical interpretation of genomic variants in cancers requires high-throughput tools for interoperability and analysis of variant interpretation knowledge. These needs are met by CIViCpy, a software development kit for downstream applications and rapid analysis. CIViCpy is fully documented, open-source, and available free online.",

    "abstract": "RNA-binding proteins interact with their target RNAs at specific sites. These binding sites can be determined genome-wide through individual nucleotide resolution crosslinking immunoprecipitation (iCLIP). Subsequently, the binding sites have to be visualized. So far, no visualization tool exists that is easily accessible but also supports restricted access so that data can be shared among collaborators.\nHere we present SEQing, a customizable interactive dashboard to visualize crosslink sites on target genes of RNA-binding proteins that have been obtained by iCLIP. Moreover, SEQing supports RNA-seq data that can be displayed in a different window tab. This allows, e.g. crossreferencing the iCLIP data with genes differentially expressed in mutants of the RBP and thus obtain some insights into a potential functional relevance of the binding sites. Additionally, detailed information on the target genes can be incorporated in another tab.\nSEQing is written in Python3 and runs on Linux. The web-based access makes iCLIP data easily accessible, even with mobile devices. SEQing is customizable in many ways and has also the option to be secured by a password. The source code is available at https://github.com/malewins/SEQing.",

    "abstract": "The analysis of environmental microplastic particles using FTIR microscopy is a challenging task, due to the very high number of individual particles within a single sample. Therefore, automatable, fast and robust approaches are highly requested. Micro particles were commonly enriched on filters, and sub- or the whole filter area was investigated, which took more than 20h and produced millions of data, which had to be evaluated. This paper presents a new approach of such filter area analysis using an intelligent algorithm to measure only those spots on a filter that would produce evaluable FTIR data. Empty spaces or IR absorbers like carbon black particles were not measured which successfully reduced the total analysis time from 50h to 7h. The presented method is based on system independent Python workflow and can easily be implemented on other FTIR systems. \u2022",

    "abstract": "The ccbmlib Python package is a collection of modules for modeling similarity value distributions based on Tanimoto coefficients for fingerprints available in RDKit. It can be used to assess the statistical significance of Tanimoto coefficients and evaluate how molecular similarity is reflected when different fingerprint representations are used. Significance measures derived from ",

    "abstract": "",

    "abstract": "The atmospheric correction of satellite images based on radiative transfer calculations is a prerequisite for many remote sensing applications. The software package ATCOR, developed at the German Aerospace Center (DLR), is a versatile atmospheric correction software, capable of processing data acquired by many different optical satellite sensors. Based on this well established algorithm, a new Python-based atmospheric correction software has been developed to generate L2A products of Sentinel-2, Landsat-8, and of new space-based hyperspectral sensors such as DESIS (DLR Earth Sensing Imaging Spectrometer) and EnMAP (Environmental Mapping and Analysis Program). This paper outlines the underlying algorithms of PACO, and presents the validation results by comparing L2A products generated from Sentinel-2 L1C images with in situ (AERONET and RadCalNet) data within VNIR-SWIR spectral wavelengths range.",

    "abstract": "Regional connectivity-based parcellation (rCBP) is a widely used procedure for investigating the structural and functional differentiation within a region of interest (ROI) based on its long-range connectivity. No standardized software or guidelines currently exist for applying rCBP, making the method only accessible to those who develop their own tools. As such, there exists a discrepancy between the laboratories applying the procedure each with their own software solutions, making it difficult to compare and interpret the results. Here, we outline an rCBP procedure accompanied by an open source software package called CBPtools. CBPtools is a Python (version 3.5+) package that allows users to run an extensively evaluated rCBP analysis workflow on a given ROI. It currently supports two modalities: resting-state functional connectivity and structural connectivity based on diffusion-weighted imaging, along with support for custom connectivity matrices. Analysis parameters are customizable and the workflow can be scaled to a large number of subjects using a parallel processing environment. Parcellation results with corresponding validity metrics are provided as textual and graphical output. Thus, CBPtools provides a simple plug-and-play, yet customizable way to conduct rCBP analyses. By providing an open-source software we hope to promote reproducible and comparable rCBP analyses and, importantly, make the rCBP procedure readily available. Here, we demonstrate the utility of CBPtools using a voluminous data set on an average compute-cluster infrastructure by performing rCBP on three ROIs prominently featured in parcellation literature.",

    "abstract": "Likelihood-free inference for simulator-based models is an emerging methodological branch of statistics which has attracted considerable attention in applications across diverse fields such as population genetics, astronomy and economics. Recently, the power of statistical classifiers has been harnessed in likelihood-free inference to obtain either point estimates or even posterior distributions of model parameters. Here we introduce PYLFIRE, an open-source Python implementation of the inference method LFIRE (likelihood-free inference by ratio estimation) that uses penalised logistic regression.\u00a0PYLFIRE is made available as part of the general ELFI inference software http://elfi.ai to benefit both the user and developer communities for likelihood-free inference.",

    "abstract": "Reproducibility of research findings has been recently questioned in many fields of science, including psychology and neurosciences. One factor influencing reproducibility is the simultaneous testing of multiple hypotheses, which entails false positive findings unless the analyzed p-values are carefully corrected. While this multiple testing problem is well known and studied, it continues to be both a theoretical and practical problem.\nHere we assess reproducibility in simulated experiments in the context of multiple testing. We consider methods that control either the family-wise error rate (FWER) or false discovery rate (FDR), including techniques based on random field theory (RFT), cluster-mass based permutation testing, and adaptive FDR. Several classical methods are also considered. The performance of these methods is investigated under two different models.\nWe found that permutation testing is the most powerful method among the considered approaches to multiple testing, and that grouping hypotheses based on prior knowledge can improve power. We also found that emphasizing primary and follow-up studies equally produced most reproducible outcomes.\nWe have extended the use of two-group and separate-classes models for analyzing reproducibility and provide a new open-source software \"MultiPy\" for multiple hypothesis testing.\nOur simulations suggest that performing strict corrections for multiple testing is not sufficient to improve reproducibility of neuroimaging experiments. The methods are freely available as a Python toolkit \"MultiPy\" and we aim this study to help in improving statistical data analysis practices and to assist in conducting power and reproducibility analyses for new experiments.",

    "abstract": "Single borehole dilution tests (SBDTs) are an inexpensive but effective technique for hydrogeological characterization of hard-rock aquifers. We present a freely available, easy-to-use, open-source Python package, DISOLV, for plotting, analyzing, and modeling SBDT data. DISOLV can significantly reduce the time spent interpreting field data by helping to identify flowing fractures intersecting the borehole and estimate the corresponding flow rates. DISOLV is successfully benchmarked against two analytical solutions. We also present an example application to real data collected in a borehole in a crystalline basement aquifer in southern India.",

    "abstract": "We present a real-time time-dependent four-component Dirac-Kohn-Sham (RT-TDDKS) implementation based on the BERTHA code. This new implementation takes advantage of modern software engineering, including the prototyping techniques. The software design follows a three step approach: (i) the prototype implementation of a time-propagation algorithm in nonrelativistic real-time TDDFT within the Psi4NumPy framework, which provides a suitable environment for the creation of a clear, readable, and easy to test reference code in Python, (ii) the design of an original Python application programming interface for the relativistic four-component code BERTHA (PyBERTHA), which has an efficient computational kernel for relativistic integrals written in FORTRAN, and (iii) the porting of the time-propagation scheme enveloped within the Psi4NumPy framework to PyBERTHA. The propagation scheme consequently resides in a single readable Python computer code that is easy to maintain and in which the key quantities, such as the Dirac-Kohn-Sham and dipole matrices, can be accessed directly from the PyBERTHA module. For linear algebra operations (matrix-matrix multiplications and diagonalization) we use the highly optimized procedures implemented in the popular NumPy library. The overhead introduced by the Python interface to BERTHA is almost negligible (less than 1% evaluated on the SCF procedure), and the interoperability between different programming languages (FORTRAN, C, and Python) does not affect the numerical stability of the time-propagation scheme. Our new RT-TDDKS implementation has been employed to investigate the stability of the time-propagation procedure in combination with a density-fitting algorithm (both for the Coulomb and for the exchange-correlation matrix construction), which are employed in BERTHA to speed up the Dirac-Kohn-Sham matrix evaluation. On the basis of systematic calculations, employing several density-fitting basis sets of increasing accuracy, we showed that quantitative agreement can be achieved in combination with extended-fitting basis sets, with an error in the Coulomb energy below 1 \u03bc-hartree. Convergence of the transition energies increasing of quality of the fitting basis sets has been also observed. Our data suggest that the error in the Coulomb energy may also represent a good estimate of the fitting basis set quality for real-time electron dynamics simulations. Further, we study the applicability of the RT-TDDKS method in combination with both weak- and extreme strong-field regime. Numerical results of excited-state transitions for the Group 12 atoms are reported and compared with a previous real-time Dirac-Kohn-Sham implementation (Repisky et al. ",

    "abstract": "An amendment to this paper has been published and can be accessed via a link at the top of the paper.",

    "abstract": "To evaluate SC administration of alfaxalone-midazolam and dexmedetomidine-midazolam for sedation of ball pythons \n12 healthy juvenile ball pythons.\nIn a randomized crossover study, each snake was administered a combination of alfaxalone (5 mg/kg [2.3 mg/lb]) and midazolam (0.5 mg/kg [0.23 mg/lb]) and a combination of dexmedetomidine (0.05 mg/kg [0.023 mg/lb]) and midazolam (0.5 mg/kg), SC, with a washout period of at least 7 days between protocols. Respiratory and heart rates and various reflexes and behaviors were assessed and compared between protocols. Forty-five minutes after protocol administration, sedation was reversed by SC administration of flumazenil (0.05 mg/kg) alone or in combination with atipamezole (0.5 mg/kg; dexmedetomidine-midazolam protocol only). Because of difficulties with visual assessment of respiratory effort after sedative administration, the experiment was repeated for a subset of 3 ball pythons, with plethysmography used to assess respiration.\nBoth protocols induced a similar level of moderate sedation with no adverse effects aside from transient apnea. Cardiopulmonary depression was more profound, but time to recovery after reversal was significantly shorter, for the dexmedetomidine-midazolam protocol than for the alfaxalone-midazolam protocol. Plethysmographic findings were consistent with visual observations and suggested that snakes compensated for a decrease in respiratory rate by increasing tidal volume amplitude.\nResults indicated that both protocols induced clinically relevant sedation in ball pythons and should be useful for minor procedures such as venipuncture and diagnostic imaging. However, caution should be used when sedating snakes with compromised cardiopulmonary function. (",

    "abstract": "The recent developments at microdiffraction X-ray beamlines are making microcrystals of macromolecules appealing subjects for routine structural analysis. Microcrystal diffraction data collected at synchrotron microdiffraction beamlines may be radiation damaged with incomplete data per microcrystal and with unit-cell variations. A multi-stage data assembly method has previously been designed for microcrystal synchrotron crystallography. Here the strategy has been implemented as a Python program for microcrystal data assembly (",

    "abstract": "Structural biology relies on specific file formats to convey information about macromolecular structures. Traditionally this has been the PDB format, but increasingly newer formats, such as PDBML, mmCIF and MMTF are being used. Here we present atomium, a modern, lightweight, Python library for parsing, manipulating and saving PDB, mmCIF and MMTF file formats. In addition, we provide a web service, pdb2json, which uses atomium to give a consistent JSON representation to the entire Protein Data Bank.\natomium is implemented in Python and its performance is equivalent to the existing library BioPython. However, it has significant advantages in features and API design. atomium is available from atomium.bioinf.org.uk and pdb2json can be accessed at pdb2json.bioinf.org.uk.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Modern genomic research is driven by next-generation sequencing experiments such as ChIP-seq and ChIA-PET that generate coverage files for transcription factor binding, as well as DHS and ATAC-seq that yield coverage files for chromatin accessibility. Such files are in a bedGraph text format or a bigWig binary format. Obtaining summary statistics in a given region is a fundamental task in analyzing protein binding intensity or chromatin accessibility. However, the existing Python package for operating on coverage files is not optimized for speed.\nWe developed pyBedGraph, a Python package to quickly obtain summary statistics for a given interval in a bedGraph or a bigWig file. When tested on 12 ChIP-seq, ATAC-seq, RNA-seq and ChIA-PET datasets, pyBedGraph is on average 260 times faster than the existing program pyBigWig. On average, pyBedGraph can look up the exact mean signal of 1 million regions in \u223c0.26\u2009s and can compute their approximate means in <0.12\u2009s on a conventional laptop.\npyBedGraph is publicly available at https://github.com/TheJacksonLaboratory/pyBedGraph under the MIT license.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "Genomic prediction (GP) is a method whereby DNA polymorphism information is used to predict breeding values for complex traits. Although GP can significantly enhance predictive accuracy, it can be expensive and difficult to implement. To help design optimum breeding programs and experiments, including genome-wide association studies and genomic selection experiments, we have developed SeqBreed, a generic and flexible forward simulator programmed in python3.\nSeqBreed accommodates sex and mitochondrion chromosomes as well as autopolyploidy. It can simulate any number of complex phenotypes that are determined by any number of causal loci. SeqBreed implements several GP methods, including genomic best linear unbiased prediction (GBLUP), single-step GBLUP, pedigree-based BLUP, and mass selection. We illustrate its functionality with Drosophila genome reference panel (DGRP) sequence data and with tetraploid potato genotype data.\nSeqBreed is a flexible and easy to use tool that can be used to optimize GP or genome-wide association studies. It incorporates some of the most popular GP methods and includes several visualization tools. Code is open and can be freely modified. Software, documentation, and examples are available at https://github.com/miguelperezenciso/SeqBreed.",

    "abstract": "SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.",

    "abstract": "We demonstrate how the recently developed Python-based Molecular Simulation and Design Framework (MoSDeF) can be used to perform molecular dynamics screening of functionalized monolayer films, focusing on tribological effectiveness. MoSDeF is an open-source package that allows for the programmatic construction and parametrization of soft matter systems and enables TRUE (transferable, reproducible, usable by others, and extensible) simulations. The MoSDeF-enabled screening identifies several film chemistries that simultaneously show low coefficients of friction and adhesion. We additionally develop a Python library that utilizes the RDKit cheminformatics library and the scikit-learn machine learning library that allows for the development of predictive models for the tribology of functionalized monolayer films and use this model to extract information on terminal group characteristics that most influence tribology, based on the screening data.",

    "abstract": "Granular cell tumours (GCTs) are uncommon neoplasms mostly reported in man, dogs and horses. The origin of GCT is thought to be Schwann cells, with the associated characteristics of neural crest morphology. Neoplastic cells often demonstrate positive immunoreactivity for S100, LC3, vimentin and p62. They are also periodic acid-Schiff (PAS) positive and diastase resistant. A female green tree python (Morelia viridis) was presented for severe constipation and hyporexia of 4 month's duration and, despite treatment, died the next day. A 4.8\u00a0\u00d7\u00a03.4\u00a0mm intracalvarial GCT was identified, compressing the overlying cerebrum without invasion. Neoplastic cells were immunoreactive to S100 and had brightly eosinophilic cytoplasmic granules that were PAS positive and diastase resistant. Electron microscopy revealed numerous cytoplasmic lysosomes in neoplastic cells. GCTs are reported rarely in non-mammalian species with three reports in birds. This represents the first report of a GCT in a reptile.",

    "abstract": "The daily work in data science involves a set of essential tools: the programming languages Python and R, the version control tool Git and the virtualization tool Docker. Proficiency in at least one programming language is required for data science. R is tied to a computing environment that focuses on statistics, in which many new algorithms in genomics and biomedicine are first published. Python has a root in system administration, and is a superb language for general programming. Version control is critical to managing complex projects, even if software development is not involved. Docker container is becoming a key tool for deployment, portability, and reproducibility. This chapter provides a self-contained practical guide of these topics so that readers can use it as a reference and to plan their training.",

    "abstract": "Atomic neural networks (ANNs) constitute a class of machine learning methods for predicting potential energy surfaces and physicochemical properties of molecules and materials. Despite many successes, developing interpretable ANN architectures and implementing existing ones efficiently are still challenging. This calls for reliable, general-purpose, and open-source codes. Here, we present a python library named PiNN as a solution toward this goal. In PiNN, we designed a new interpretable and high-performing graph convolutional neural network variant, PiNet, as well as implemented the established Behler-Parrinello neural network. These implementations were tested using datasets of isolated small molecules, crystalline materials, liquid water, and an aqueous alkaline electrolyte. PiNN comes with a visualizer called PiNNBoard to extract chemical insight \"learned\" by ANNs. It provides analytical stress tensor calculations and interfaces to both the atomic simulation environment and a development version of the Amsterdam Modeling Suite. Moreover, PiNN is highly modularized, which makes it useful not only as a standalone package but also as a chain of tools to develop and to implement novel ANNs. The code is distributed under a permissive BSD license and is freely accessible at https://github.com/Teoroo-CMC/PiNN/ with full documentation and tutorials.",

    "abstract": "Reptiles can suffer from infectious and noninfectious cardiac pathologies, requiring the need for standardized diagnostic approaches and reference intervals. Despite the popularity of ball pythons (",

    "abstract": null,

    "abstract": "The increasing availability of complex data in biology and medicine has promoted the use of machine learning in classification tasks to address important problems in translational and fundamental science. Two important obstacles, however, may limit the unraveling of the full potential of machine learning in these fields: the lack of generalization of the resulting models and the limited number of labeled data sets in some applications. To address these important problems, we developed an unsupervised ensemble algorithm called strategy for unsupervised multiple method aggregation (SUMMA). By virtue of being an ensemble method, SUMMA is more robust to generalization than the predictions it combines. By virtue of being unsupervised, SUMMA does not require labeled data. SUMMA receives as input predictions from a diversity of models and estimates their classification performance even when labeled data are unavailable. It then uses these performance estimates to combine these different predictions into an ensemble model. SUMMA can be applied to a variety of binary classification problems in bioinformatics including but not limited to gene network inference, cancer diagnostics, drug response prediction, somatic mutation, and differential expression calling. In this application note, we introduce the R/PY-SUMMA packages, available in R or Python, that implement the SUMMA algorithm.",

    "abstract": "Advances in sequencing technology have made it possible to generate large numbers of long, high-accuracy sequencing reads. For instance, the new PacBio Sequel platform can generate hundreds of thousands of high-quality circular consensus sequences in a single run (Hebert et al., 2018; Rhoads & Au, 2015). Good programs exist for aligning these reads for genome assembly (Chaisson & Tesler, 2012; Li, 2018). However, these long reads can also be used for other purposes, such as sequencing PCR amplicons that contain various features of interest. For instance, PacBio circular consensus sequences have been used to identify the mutations in influenza viruses in single cells (Russell et al, 2019), or to link barcodes to gene mutants in deep mutational scanning (Matreyek et al., 2018). For such applications, the alignment of the sequences to the targets may be fairly trivial, but it is not trivial to then parse specific features of interest (such as mutations, unique molecular identifiers, cell barcodes, and flanking sequences) from these alignments. Here we describe alignparse, a Python package for parsing complex sets of features from long sequences that map to known targets. Specifically, it allows the user to provide complex target sequences in Genbank Flat File format that contain an arbitrary number of user-defined sub-sequence features (Sayers et al., 2019). It then aligns the sequencing reads to these targets and filters alignments based on whether the user-specified features are present with the desired identities (which can be set to different thresholds for different features). Finally, it parses out the sequences, mutations, and/or accuracy (sequence quality) of these features as specified by the user. The flexibility of this package therefore fulfills the need for a tool to extract and analyze complex sets of features in large numbers of long sequencing reads.",

    "abstract": "We present a Python object-oriented computer program for simulating various aspects of laser cooling physics. Our software is designed to be both easy to use and adaptable, allowing the user to specify the level structure, magnetic field profile, or the laser beams' geometry, detuning, and intensity. The program contains three levels of approximation for the motion of the atom, applicable in different regimes offering cross checks for calculations and computational efficiency depending on the physical situation. We test the software by reproducing well-known phenomena, such as damped Rabi flopping, electromagnetically induced transparency, stimulated Raman adiabatic passage, and optical molasses. We also use our software package to quantitatively simulate recoil-limited magneto-optical traps, like those formed on the narrow ",

    "abstract": "Phylogenetic trees are essential to evolutionary biology, and numerous methods exist that attempt to extract phylogenetic information applicable to a wide range of disciplines, such as epidemiology and metagenomics. Currently, the three main Python packages for trees are Bio.Phylo, DendroPy, and the ETE Toolkit, but as dataset sizes grow, parsing and manipulating ultra-large trees becomes impractical for these tools. To address this issue, we present TreeSwift, a user-friendly and massively scalable Python package for traversing and manipulating trees that is ideal for algorithms performed on ultra-large trees.",

    "abstract": null,

    "abstract": null,

    "abstract": null,

    "abstract": "It has long been known that even closely related species can vary in their antipredator behavior, and in the last two decades there has been mounting interest in how these differences might relate to the hormonal stress response. We tested the relationship between fear-based aggression, a form of antipredator behavior, and plasma corticosterone levels in three species of python [Children's Python (Antaresia childreni), Ball Python (Python regius), Bismarck Ring Python (Bothrochilus boa)]. We recorded the amount of striking in response to perturbation before and after a controlled, stressful confinement. We also measured plasma corticosterone levels prior to confinement, after confinement, and after confinement plus an adrenocorticotropin hormone (ACTH) injection, the later to induce a maximal corticosterone response. We performed among species analyses using two mixed models, and we determined between individual variance within each species to estimate repeatability. Bismarck Ring Pythons struck more than either Ball Pythons or Children's Pythons, and Ball Pythons had a suppressed corticosterone response compared to Children's and Bismarck Ring Pythons. Thus, mean species fear-based aggression correlated with species level differences in corticosterone profile. We also found evidence suggesting behaviors are repeatable within individuals. Our results point to a need for further exploration of aggression, anti-predator behavior, and corticosterone profile.",

    "abstract": "Technologies such as microscopy, sequential hybridization, and mass spectrometry enable quantitative single-cell phenotypic and molecular measurements in situ. Deciphering spatial phenotypic and molecular effects on the single-cell level is one of the grand challenges and a key to understanding the effects of cell-cell interactions and microenvironment. However, spatial information is usually overlooked by downstream data analyses, which usually consider single-cell read-out values as independent measurements for further averaging or clustering, thus disregarding spatial locations. With this work, we attempt to fill this gap. We developed a toolbox that allows one to test for the presence of a spatial effect in microscopy images of adherent cells and estimate the spatial scale of this effect. The proposed Python module can be used for any light microscopy images of cells as well as other types of single-cell data such as in situ transcriptomics or metabolomics. The input format of our package matches standard output formats from image analysis tools such as CellProfiler, Fiji, or Icy and thus makes our toolbox easy and straightforward to use, yet offering a powerful statistical approach for a wide range of applications. \u00a9 2019 International Society for Advancement of Cytometry.",

    "abstract": "Primer design is essential to conduct whole plasmid site-directed mutagenesis for protein study. Traditionally, primers of mutagenesis are designed manually that is time-consuming and fallible. Here, we present a Python script for searching primers by presetting parameters of nucleotide composition and percentage of guanine-cytosine (GC content). The running results showed that the script is able to search primers with mutations of target residue automatically. This script may facilitate primer design for whole plasmid site-directed mutagenesis and aid protein mutant construction.",

    "abstract": "Partial differential equations (PDEs) is a well-established and powerful tool to simulate multi-cellular biological systems. However, available free tools for validation against data are on development.\nThe PDEparams module provides a flexible interface and readily accommodates different parameter analysis tools in PDE models such as computation of likelihood profiles, and parametric bootstrapping, along with direct visualization of the results. To our knowledge, it is the first open, freely available tool for parameter fitting of PDE models.\nPDEparams is distributed under the MIT license. The source code, usage instructions and examples are freely available on GitHub at github.com/systemsmedicine/PDE_params.\nSupplementary data are available at Bioinformatics online.",

    "abstract": "SimpactCyan is an open-source simulator for individual-based models in HIV epidemiology. Its core algorithm is written in C++ for computational efficiency, while the R and Python interfaces aim to make the tool accessible to the fast-growing community of R and Python users. Transmission, treatment and prevention of HIV infections in dynamic sexual networks are simulated by discrete events. A generic \"intervention\" event allows model parameters to be changed over time, and can be used to model medical and behavioural HIV prevention programmes. First, we describe a more efficient variant of the modified Next Reaction Method that drives our continuous-time simulator. Next, we outline key built-in features and assumptions of individual-based models formulated in SimpactCyan, and provide code snippets for how to formulate, execute and analyse models in SimpactCyan through its R and Python interfaces. Lastly, we give two examples of applications in HIV epidemiology: the first demonstrates how the software can be used to estimate the impact of progressive changes to the eligibility criteria for HIV treatment on HIV incidence. The second example illustrates the use of SimpactCyan as a data-generating tool for assessing the performance of a phylodynamic inference framework.",

    "abstract": "Recent empirical findings have indicated that gaze allocation plays a crucial role in simple decision behaviour. Many of these findings point towards an influence of gaze allocation onto the speed of evidence accumulation in an accumulation-to-bound decision process (resulting in generally higher choice probabilities for items that have been looked at longer). Further, researchers have shown that the strength of the association between gaze and choice behaviour is highly variable between individuals, encouraging future work to study this association on the individual level. However, few decision models exist that enable a straightforward characterization of the gaze-choice association at the individual level, due to the high cost of developing and implementing them. The model space is particularly scarce for choice sets with more than two choice alternatives. Here, we present GLAMbox, a Python-based toolbox that is built upon PyMC3 and allows the easy application of the gaze-weighted linear accumulator model (GLAM) to experimental choice data. The GLAM assumes gaze-dependent evidence accumulation in a linear stochastic race that extends to decision scenarios with many choice alternatives. GLAMbox enables Bayesian parameter estimation of the GLAM for individual, pooled or hierarchical models, provides an easy-to-use interface to predict choice behaviour and visualize choice data, and benefits from all of PyMC3's Bayesian statistical modeling functionality. Further documentation, resources and the toolbox itself are available at https://glambox.readthedocs.io.",

    "abstract": "In neuroscience, computational modeling has become an important source of insight into brain states and dynamics. A basic requirement for computational modeling studies is the availability of efficient software for setting up models and performing numerical simulations. While many such tools exist for different families of neural models, there is a lack of tools allowing for both a generic model definition and efficiently parallelized simulations. In this work, we present PyRates, a Python framework that provides the means to build a large variety of rate-based neural models. PyRates provides intuitive access to and modification of all mathematical operators in a graph, thus allowing for a highly generic model definition. For computational efficiency and parallelization, the model is translated into a compute graph. Using the example of two different neural models belonging to the family of rate-based population models, we explain the mathematical formalism, software structure and user interfaces of PyRates. We show via numerical simulations that the behavior of the PyRates model implementations is consistent with the literature. Finally, we demonstrate the computational capacities and scalability of PyRates via a number of benchmark simulations of neural networks differing in size and connectivity.",

    "abstract": "Fiber-optic distributed temperature sensing (FO-DTS) has proven to be a transformative technology for the hydrologic sciences, with application to diverse problems including hyporheic exchange, groundwater/surface-water interaction, fractured-rock characterization, and cold regions hydrology. FO-DTS produces large, complex, and information-rich datasets. Despite the potential of FO-DTS, adoption of the technology has been impeded by lack of tools for data processing, analysis, and visualization. New tools are needed to efficiently and fully capitalize on the information content of FO-DTS datasets. To this end, we present DTSGUI, a public-domain Python-based software package for editing, parsing, processing, statistical analysis, georeferencing, and visualization of FO-DTS data.",

    "abstract": "Ligand enrichment assessment based on benchmarking data sets has become a necessity for the rational selection of the best-suited approach for prospective data mining of drug-like molecules. Up to now, a variety of benchmarking data sets had been generated and frequently used. Among them, MUBD-HDACs from our prior research efforts was regarded as one of five state-of-the-art benchmarks in 2017 by Frontiers in Pharmacology. This benchmarking set was generated by one of our unique de-biasing algorithms. It also rendered quite a few other cases of successful applications in recent years, thus is expected to have more impact in modern drug discovery. To make our algorithm amenable to more users, we developed a Python GUI application called MUBD-DecoyMaker 2.0. Moreover, it has two new additional functional modules, i.\u2009e. \"Detect 2D Bias\" and \"Quality Control\". This new GUI version had been proved to be easy to use while generate benchmarking data sets of the same quality. MUBD-DecoyMaker 2.0 is freely available at https://github.com/jwxia2014/MUBD-DecoyMaker2.0, along with its manual and testcase.",

    "abstract": "Sequence logos are visually compelling ways of illustrating the biological properties of DNA, RNA and protein sequences, yet it is currently difficult to generate and customize such logos within the Python programming environment. Here we introduce Logomaker, a Python API for creating publication-quality sequence logos. Logomaker can produce both standard and highly customized logos from either a matrix-like array of numbers or a multiple-sequence alignment. Logos are rendered as native matplotlib objects that are easy to stylize and incorporate into multi-panel figures.\nLogomaker can be installed using the pip package manager and is compatible with both Python 2.7 and Python 3.6. Documentation is provided at http://logomaker.readthedocs.io; source code is available at http://github.com/jbkinney/logomaker.",

    "abstract": "A 9-year-old female diamond python (Morelia spilota) was presented with a submandibular swelling. The cytological, macroscopic and histological features of this lesion indicated a diagnosis of branchial (pharyngeal) cyst. Branchial cysts are benign lesions caused by anomalous development of the branchial apparatus and are described rarely in veterinary medicine. We suggest that possible persistence of branchial remnants should be included in the consideration of differential diagnoses for neck masses in adult snakes.",

